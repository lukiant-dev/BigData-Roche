<?xml version="1.0" encoding="UTF-8" standalone="no"?><searchresult><query> </query><document id="0"><title>Sneak Preview: "Case Studies" Track at HBaseCon 2014</title><url>http://blog.cloudera.com/blog/2014/04/sneak-preview-case-studies-track-at-hbasecon-2014/</url><snippet>The HBaseCon 2014 “Case Studies” track surfaces some of the most interesting (and diverse) use cases in the HBase ecosystem — and in the world of NoSQL overall — today. The�HBaseCon 2014�(May 5, 2014 in San Francisco) is not just about internals and best practices — it’s also a place to explore use cases that you not have even considered before. HBaseCon is just a couple short weeks away, so don’t wait to register. “A Graph Service for Global Web Entities Traversal and Reputation Evaluation Based on HBase” Chris Huang and Scott Miao (Trend Micro)� Trend Micro collects lots of threat knowledge data for clients containing many different threat (web) entities. Most threat entities will be observed along with relations, such as malicious behaviors or interaction chains among them. So, we built a graph model on HBase to store all the known threat entities and their relationships, allowing clients to query threat relationships via any given threat entity. This presentation covers what problems we try to solve, what and how the design decisions we made, how we design such a graph model, and the graph computation tasks involved.� “A Survey of HBase Application Archetypes”� Lars George and Jon Hsieh (Cloudera)� Today, there are hundreds of production HBase clusters running a multitude of applications and use cases. Many well-known implementations exercise opposite ends of the HBase’s capabilities emphasizing either entity-centric schemas or event-based schemas. This talk presents these archetypes and others based on a use-case survey of clusters conducted by Cloudera’s development, product, and services teams. By analyzing the data from the nearly 20,000 HBase cluster nodes Cloudera has under management, we’ll categorize HBase users and their use cases into a few simple archetypes, describe workload patterns, and quantify the usage of advanced features. “Blackbird: Storing Billions of Rows a Couple of Milliseconds Away” Ishan Chhabra, Shrijeet Paliwal &amp; Abhijit Pol (Rocket Fuel)� Blackbird, Rocket Fuel’s system built on top of HBase, makes billions of rich user profiles available for AI based optimization under the tight latency requirements of real time auction. It relies on our novel collections API, a constrained yet useful append only model that is sympathetic to HBase internals and allows us to scale our writes easily while keeping strict read performance guarantees. In this talk, we describe the key abstractions Blackbird exposes, utilities we built over time to support our use cases and our hardware and software configuration (including HBase configs) that helps us achieve our strict latency guarantees. “Content Identification using HBase” (20-minute session) Daniel Nelson (Nielsen)� The motivation behind content identification is to determine the media people are consuming (via TV shows, movies, or streaming). Nielsen collects that data via its Fingerprints system, which generates significant amounts of structured data that is stored in HBase. This presentation will review the options a developer has for HBase querying and retrieval of hash data. Also covered is the use of wire protocols (Protocol Buffers), and how they can improve network efficiency and throughput, especially when combined with an HBase coprocessor.� “Data Evolution in HBase” Eric Czech and Alec Zopf (Next Big Sound)� Managing the evolution of data within HBase over time is not easy: Data resulting from Hadoop processing pipelines or otherwise placed in HBase is subject to the same kinds of oversights, bugs, and faulty assumptions inherent to the software that creates it. While the development of this software is often effectively managed through revision control systems, data itself is rarely modeled in a way that affords the same flexibility. In this session, we’ll talk about how to build a versioned, time-series data store using HBase that can provide significantly greater adaptability and performance than similar systems.� “Digital Library Collection Management using HBase” (20-minute session) Ron Buckley (OCLC)� OCLC has been working over the last year to move its massive repository to HBase. This talk will focus on the impetus behind the move, implementation details and technology choices we’ve made (key design, shredding PDFs and other digital objects into HBase, scaling), and the value-add that HBase brings to digital collection management.� “HBase at Bloomberg: High Availability Needs for the Financial Industry” (20-minute session) Sudarshan Kadambi and Matthew Hunt (Bloomberg LP)� Bloomberg is a financial data and analytics provider, so data management is core to what we do. There’s tremendous diversity in the type of data we manage, and HBase is a natural fit for many of these datasets – from the perspective of the data model as well as in terms of a scalable, distributed database. This talk covers data and analytics use cases at Bloomberg and operational challenges around HA. We’ll explore the work currently being done under HBASE-10070, further extensions to it, and how this solution is qualitatively different to how failover is handled by Apache Cassandra.� “HBase Design Patterns @ Yahoo!” (20-minute session)� Francis Liu (Yahoo!)� HBase’s introduction into the Yahoo! Grid has provided our users with new ways to process and store data. A year after its availability, there has been varied usages: Event processing for personalization, incremental processing for ingestion, time-based aggregations for analytics, etc. All these were possible thanks to features HBase brings beyond working with HDFS files. This talk will review some recurring HBase design patterns at Yahoo! as well as share our learnings and experiences.� “Large-scale Web Apps @ Pinterest” Varun Sharma (Pinterest)� Over the past year, HBase has become an integral component of Pinterest’s storage stack. HBase has enabled us to quickly launch and iterate on new products and create amazing pinner experiences. This talk briefly describes some of these applications, the underlying schema, and how our HBase setup stays highly available and performant despite billions of requests every week. It will also include some performance tips for running on SSDs. Finally, we will talk about a homegrown serving technology we built from a mashup of HBase components that has gained wide adoption across Pinterest.�   Thank you to our sponsors�– Continuuity, Hortonworks, Intel, LSI, MapR, Salesforce.com, Splice Machine,�WibiData (Gold); BrightRoll, Facebook, Pepperdata (Silver); ASF (Community); O’Reilly Media, The Hive, NoSQL Weekly (Media) — without which HBaseCon would be impossible!</snippet></document><document id="1"><title>Cloudera Live: The Instant Apache Hadoop Experience</title><url>http://blog.cloudera.com/blog/2014/04/cloudera-live/</url><snippet>Get started with Apache Hadoop and use-case examples online in just seconds. Today, we announced Cloudera Live, a new online service for developers and analysts (currently in public beta) that makes it easy to learn, explore, and try out CDH, Cloudera’s open source software distribution containing Apache Hadoop and related projects. No downloads, no installations, no waiting — just point-and-play! Try Cloudera Live (Beta) Cloudera Live is just that: a complete, live, CDH 5�cluster with a Hue interface (based on Hue 3.5.0, the latest and greatest). It includes pre-packaged examples/patterns for using Impala, Search, Apache HBase, and many other Hadoop ecosystem components. (Note: Cloudera Live is currently read-only, so loading data via the Apache Sqoop app isn’t possible. To explore CDH with ingested data, download our QuickStart VM.) After spending some time with Cloudera Live (within a three-hour session), you may be wondering: How did we do it? As you’ll find from the answer below, the combination of Amazon Web Services (AWS) and Cloudera Manager made it easy. Inside Cloudera Live Cloudera Live is hosted on four AWS m3.large�instances containing Ubuntu 12.04 and 100GB storage. (If you ever try to build your own cluster on AWS for your own use and thus need less performance, one xlarge instance will be enough — or, you could install fewer services on an even smaller instance.) We configured the security group as shown below. We allow everything between the instances (the first row — don�t forget that on multi-machine clusters!) and opened up Cloudera Manager and Hue ports to the outside. We used Cloudera Manager to auto-install everything for us based on this�guide. Moreover, post-install monitoring and configuration was greatly simplified. The first step was to connect to one of the machines: ssh -i ~/demo.pem ubuntu@ec2-11-222-333-444.compute-1.amazonaws.com   Next, we retrieved and started Cloudera Manager: wget http://archive.cloudera.com/cm5/installer/latest/cloudera-manager-installer.bin
chmod +x
sudo ./cloudera-manager.bin   After logging in with the default credentials (admin/admin), we entered all the Public DNS IP addresses (such as ec2-11-222-333-444.compute-1.amazonaws.com) on our machines in the Install Wizard and clicked Go. Et voila, Cloudera Manager set up the entire cluster automatically! Hence, Cloudera Live was born. We hope you enjoy Cloudera Live, and we need your feedback whether you do or not! You can do that via the upstream Hue list, the Hue forum at cloudera.com/community, or by clicking on the “Feedback” tab in the demo itself.</snippet></document><document id="2"><title>Making Apache Spark Easier to Use in Java with Java 8</title><url>http://blog.cloudera.com/blog/2014/04/making-apache-spark-easier-to-use-in-java-with-java-8/</url><snippet>Our thanks to Prashant Sharma and Matei Zaharia of Databricks for their permission to re-publish the post below about future Java 8 support in Apache Spark. Spark is now generally available inside CDH 5. One of Apache Spark‘s main goals is to make big data applications easier to write. Spark has always had concise APIs in Scala and Python, but its Java API was verbose due to the lack of function expressions. With the addition of lambda expressions in Java 8, we’ve updated Spark’s API to transparently support these expressions, while staying compatible with old versions of Java. This new support will be available in Spark 1.0. A Few Examples The following examples show how Java 8 makes code more concise. In our first example, we search a log file for lines that contain “error”, using Spark’s filter and count operations. The code is simple to write, but passing a Function object to filter is clunky: Java 7 search example: 
JavaRDD&lt;String&gt; lines = sc.textFile("hdfs://log.txt").filter(
  new Function&lt;String, Boolean&gt;() {
    public Boolean call(String s) {
      return s.contains("error");
    }
});
long numErrors = lines.count();
   (If you’re new to Spark, JavaRDD is a distributed collection of objects, in this case lines of text in a file. We can apply operations to these objects that will automatically be parallelized across a cluster.) With Java 8, we can replace the Function object with an inline function expression, making the code a lot cleaner: Java 8 search example: 
JavaRDD&lt;String&gt; lines = sc.textFile("hdfs://log.txt")
                          .filter(s -&gt; s.contains("error"));
long numErrors = lines.count();
   The gains become even bigger for longer programs. For instance, the program below implements Word Count, by taking a file (read as a collection of lines), splitting each line into multiple words, then counting the words with a reduce function. Java 7 word count: 
JavaRDD&lt;String&gt; lines = sc.textFile("hdfs://log.txt");

// Map each line to multiple words
JavaRDD&lt;String&gt; words = lines.flatMap(
  new FlatMapFunction&lt;String, String&gt;() {
    public Iterable&lt;String&gt; call(String line) {
      return Arrays.asList(line.split(" "));
    }
});

// Turn the words into (word, 1) pairs
JavaPairRDD&lt;String, Integer&gt; ones = words.mapToPair(
  new PairFunction&lt;String, String, Integer&gt;() {
    public Tuple2&lt;String, Integer&gt; call(String w) {
      return new Tuple2&lt;String, Integer&gt;(w, 1);
    }
});

// Group up and add the pairs by key to produce counts
JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey(
  new Function2&lt;Integer, Integer, Integer&gt;() {
    public Integer call(Integer i1, Integer i2) {
      return i1 + i2;
    }
});

counts.saveAsTextFile("hdfs://counts.txt");
   With Java 8, we can write this program in just a few lines: Java 8 word count: 
JavaRDD&lt;String&gt; lines = sc.textFile("hdfs://log.txt");
JavaRDD&lt;String&gt; words =
    lines.flatMap(line -&gt; Arrays.asList(line.split(" ")));
JavaPairRDD&lt;String, Integer&gt; counts =
    words.mapToPair(w -&gt; new Tuple2&lt;String, Integer&gt;(w, 1))
         .reduceByKey((x, y) -&gt; x + y);
counts.saveAsTextFile("hdfs://counts.txt");
   We are very excited to offer this functionality, as it opens up the simple, concise programming style that Scala and Python Spark users are familiar with to a much broader set of developers. Availability Java 8 lambda support will be available in Spark 1.0, which will be released in early May. Although using this syntax requires Java 8, Spark 1.0 will still support older versions of Java through the old form of the API. Lambda expressions are simply a shorthand for anonymous inner classes, so the same API can be used in any Java version. Prashant Sharma is a committer on the Spark project. Matei Zaharia is the creator of Spark and the CTO of Databricks. Spark Summit 2014 is coming (June 30 � July 2)! Register here to get 20% off the regular conference price.</snippet></document><document id="3"><title>Meet the Data Scientist: Stuart Horsman</title><url>http://blog.cloudera.com/blog/2014/04/meet-the-data-scientist-stuart-horsman/</url><snippet>Meet Stuart Horsman, among the first to earn the CCP: Data Scientist distinction. Big Data success requires professionals who can prove their mastery with the tools and techniques of the Hadoop stack. However, experts predict a major shortage of advanced analytics skills over the next few years. At Cloudera, we’re drawing on our industry leadership and early corpus of real-world experience to address the Big Data talent gap with the Cloudera Certified Professional (CCP) program. As part of this blog series, we’ll introduce the proud few who have earned the CCP: Data Scientist distinction. Featured today is CCP-01, Stuart Horsman.� You can start on your own journey to data science and CCP:DS with Cloudera’s new Data Science Challenge on “Detecting Anomalies in Medicare Claims.” What’s your current role? I am a Systems Engineer with Cloudera, based in Sydney, Australia. Prior to taking CCP:DS, what was your experience with Big Data, Hadoop, and data science? I was the Business Development Manager for Big Data with Oracle, focused on building Oracle’s Big Data business in Australia and New Zealand.�Central to this offering is the Oracle Big Data Appliance, which runs CDH, as well as Oracle’s statistical and data mining capabilities, which integrate R, the Oracle RDBMS, and Hadoop.�I wore a number of hats and managed a variety of responsibilities, from creating and presenting marketing content and providing architectural consultation to customers to running POCs and writing data processing pipelines on Hadoop. What’s most interesting about data science, and what made you want to become a data scientist? I studied Economics and Statistics at Lancaster University in the U.K. and was mostly interested in the application of macro-economic statistical analysis.�After graduation, I started my career as a software programmer before moving into systems and database administration. I discovered Hadoop in the context of my work as a DBA. As a natural extension of Hadoop’s capabilities, I became interested in machine learning�Big Data is all about machine learning, in my opinion�and was reintroduced to statistics.�I had forgotten how much I enjoyed delving deeper into large data sets and performing advanced analyses. My background as a programmer and administrator helped me incorporate machine learning techniques and ignited my interest in data science. I earned the CCP:DS credential with the first class of participants in the Web Analytics Challenge, which focused on classification, clustering, and collaborative filtering. I’m proud to carry this distinction, but it just makes me hungrier to learn more data science.�Shortly after becoming CCP-01, I joined the Cloudera team in Sydney. Working with Cloudera’s customers has been the best next step in my journey, since I get to think about, gain experience with, and help organizations address their real Big Data challenges in the wild.�I can’t imagine a better way to continue my data science learning path. How did you prepare for the Data Science Essentials exam and CCP:DS? What advice would you give to aspiring data scientists? People contact me all the time to ask how I passed the Data Science Essentials exam and earned the top score on the Data Science Challenge. In all honesty, a deep interest in and enthusiasm for the subject matter make all the difference. I read a number of books, the most useful of which were Machine Learning for Hackers by Drew Conway and John Myles White and An Introduction to Statistical Learning by Trevor Hastie, et al.� I also completed a Coursera course on machine learning. �Cloudera offers both Data Analyst Training to get experience with ecosystem tools like Impala, Hive, and Pig and an Introduction to Data Science course, which would serve as a great onramp to machine learning and recommender systems. When all was said and done, I had been preparing for CCP:DS for two years without even knowing it. Ultimately, there are no shortcuts, and dedicated study and practice always yield the best results. By the time I took the written exam, I had all the experience I needed to answer the questions�it was clearly a challenge, but I was prepared.� When I read the requirements for the practicum, I knew I had the techniques down cold and was eager to get started. I’d recommend using the study guide and Data Science Challenge Solution Kit and study, study, study (then study some more). Since becoming a CCP:DS in November 2013, what has changed in your career and/or in your life? Cloudera hired me! It’s great to be working at one of the most exciting companies in the world and helping to change the way businesses store, process, and analyze all their data. I’m a member of an amazing team. Why should aspiring data scientists consider taking CCP:DS? Why should you take the CCP:DS Challenge?�You might land your dream job like I did! Seriously, I have had (and continue to have) lots of employers and recruiters ask me about my current employment status. There’s huge demand in the market right now for people who understand scalable programming with machine learning and who have experience applying these techniques within a business context. CCP:DS provides proof of experience and a necessary level of expertise to close the talent gap. Further reading: Learn more about Cloudera Certified Professional: Data Scientist Get hands-on experience with the Cloudera Data Scientist Challenge Solution Kit Meet the CCP:DS Class of 2013 &gt;&gt;</snippet></document><document id="4"><title>How-to: Run a Simple Apache Spark App in CDH 5</title><url>http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/</url><snippet>Getting started with Spark (now shipping inside CDH 5) is easy using this simple example. Apache Spark is a general-purpose, cluster computing framework that, like MapReduce in Apache Hadoop, offers powerful abstractions for processing large datasets. For various reasons pertaining to performance, functionality, and APIs, Spark is already becoming more popular than MapReduce for certain types of workloads. (For more background about Spark, read this post.) In this how-to, you’ll learn how to write, compile, and run a simple Spark program written in Scala on CDH 5 (in which Spark ships and is supported by Cloudera). The full code for the example is hosted at https://github.com/sryza/simplesparkapp. Writing Our example app will be a souped-up version of WordCount, the classic MapReduce example. In WordCount, the goal is to learn the distribution of letters in the most popular words in our corpus. That is, we want to: Read an input set of text documents Count the number of times each word appears Filter out all words that show up less than a million times For the remaining set, count the number of times each letter occurs In MapReduce, this would require two MapReduce jobs, as well as persisting the intermediate data to HDFS in between them. In constrast, in Spark, you can write a single job in about 90 percent fewer lines of code. Our input is a huge text file where each line contains all the words in a document, stripped of punctuation. The full Scala program looks like this: 
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SparkWordCount {
  def main(args: Array[String]) {
    val sc = new SparkContext(new SparkConf().setAppName("Spark Count"))
    val threshold = args(1).toInt

    // split each document into words
    val tokenized = sc.textFile(args(0)).flatMap(_.split(" "))

    // count the occurrence of each word
    val wordCounts = tokenized.map((_, 1)).reduceByKey(_ + _)

    // filter out words with less than threshold occurrences
    val filtered = wordCounts.filter(_._2 &gt;= threshold)

    // count characters
    val charCounts = filtered.flatMap(_._1.toCharArray).map((_, 1)).reduceByKey(_ + _)

    System.out.println(charCounts.collect().mkString(", "))
  }
}
   Spark uses “lazy evaluation”, meaning that transformations don’t execute on the cluster until an “action” operation is invoked. Examples of action operations are collect, which pulls data to the client, and saveAsTextFile, which writes data to a filesystem like HDFS. It’s worth noting that in Spark, the definition of “reduce” is slightly different than that in MapReduce. In MapReduce, a reduce function call accepts all the records corresponding to a given key. In Spark, the function passed to reduce, or reduceByKey function call, accepts just two arguments � so if it’s not associative, bad things will happen. A positive consequence is that Spark knows it can always apply a combiner. Based on that definition, the Spark equivalent of MapReduce’s reduce is similar to a groupBy followed by a map. For those more comfortable with Java, here’s the same program using Spark’s Java API: 
import java.util.ArrayList;
import java.util.Arrays;
import org.apache.spark.api.java.*;
import org.apache.spark.api.java.function.*;
import org.apache.spark.SparkConf;
import scala.Tuple2;

public class JavaWordCount {
  public static void main(String[] args) {
    JavaSparkContext sc = new JavaSparkContext(new SparkConf().setAppName("Spark Count"));
    final int threshold = Integer.parseInt(args[1]);

    // split each document into words
    JavaRDD&lt;String&gt; tokenized = sc.textFile(args[0]).flatMap(
      new FlatMapFunction&lt;String, String&gt;() {
        public Iterable&lt;String&gt; call(String s) {
          return Arrays.asList(s.split(" "));
        }
      }
    );

    // count the occurrence of each word
    JavaPairRDD&lt;String, Integer&gt; counts = tokenized.map(
      new PairFunction&lt;String, String, Integer&gt;() {
        public Tuple2&lt;String, Integer&gt; call(String s) {
          return new Tuple2(s, 1);
        }
      }
    ).reduceByKey(
      new Function2&lt;Integer, Integer, Integer&gt;() {
        public Integer call(Integer i1, Integer i2) {
          return i1 + i2;
        }
      }
    );

    // filter out words with less than threshold occurrences
    JavaPairRDD&lt;String, Integer&gt; filtered = counts.filter(
      new Function&lt;Tuple2&lt;String, Integer&gt;, Boolean&gt;() {
        public Boolean call(Tuple2&lt;String, Integer&gt; tup) {
          return tup._2 &gt;= threshold;
        }
      }
    );

    // count characters
    JavaPairRDD&lt;Character, Integer&gt; charCounts = filtered.flatMap(
      new FlatMapFunction&lt;Tuple2&lt;String, Integer&gt;, Character&gt;() {
        public Iterable&lt;Character&gt; call(Tuple2&lt;String, Integer&gt; s) {
          ArrayList&lt;Character&gt; chars = new ArrayList&lt;Character&gt;(s._1.length());
          for (char c : s._1.toCharArray()) {
            chars.add(c);
          }
          return chars;
        }
      }
    ).map(
      new PairFunction&lt;Character, Character, Integer&gt;() {
        public Tuple2&lt;Character, Integer&gt; call(Character c) {
          return new Tuple2(c, 1);
        }
      }
    ).reduceByKey(
      new Function2&lt;Integer, Integer, Integer&gt;() {
        public Integer call(Integer i1, Integer i2) {
          return i1 + i2;
        }
      }
    );

    System.out.println(charCounts.collect());
  }
}
   Because Java doesn’t support anonymous functions, the program is considerably more verbose, but it still requires a fraction of the code needed in an equivalent MapReduce program. Compiling We’ll use Maven to compile our program. Maven expects a specific directory layout that informs it where to look for source files. Our Scala code goes under src/main/scala, and our Java code goes under src/main/java. That is, we place SparkWordCount.scala in the src/main/scala/com/cloudera/sparkwordcount directory and JavaWordCount.java in the src/main/java/com/cloudera/sparkwordcount directory. Maven also requires you to place a pom.xml file in the root of the project directory that tells it how to build the project. A few noteworthy excerpts are included below. To compile Scala code, include: 
&lt;plugin&gt;
  &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;
      &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;
      &lt;executions&gt;
        &lt;execution&gt;
          &lt;goals&gt;
            &lt;goal&gt;compile&lt;/goal&gt;
            &lt;goal&gt;testCompile&lt;/goal&gt;
          &lt;/goals&gt;
        &lt;/execution&gt;
      &lt;/executions&gt;
&lt;/plugin&gt;
   which requires adding the scala-tools plugin repository: 
&lt;pluginRepositories&gt;
&lt;pluginRepository&gt;
    &lt;id&gt;scala-tools.org&lt;/id&gt;
    &lt;name&gt;Scala-tools Maven2 Repository&lt;/name&gt;
    &lt;url&gt;http://scala-tools.org/repo-releases&lt;/url&gt;
  &lt;/pluginRepository&gt;
&lt;/pluginRepositories&gt;
   Then, include Spark and Scala as dependencies: 
&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
    &lt;artifactId&gt;scala-library&lt;/artifactId&gt;
    &lt;version&gt;2.10.2&lt;/version&gt;
  &lt;/dependency&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
    &lt;version&gt;0.9.0-cdh5.0.0&lt;/version&gt;
  &lt;/dependency&gt;
&lt;/dependencies&gt;
   Finally, to generate our app jar, simply run: 
mvn package
   It will show up in the target directory as sparkwordcount-0.0.1-SNAPSHOT.jar. Running Running your Spark application is like running any Java program: You include the application jar and its dependencies in the classpath and pass apps the main class. In a CDH installation, the Spark and Hadoop jars are included on every node. Some will recommend packaging these dependencies inside your Spark application jar itself, but Cloudera recommends referencing the locally installed jars. Doing so ensures that the client uses the same versions of these jars as the server, and means you don’t need to recompile apps when you upgrade the cluster. The following includes local Hadoop and Spark jars in the classpath and then runs the application. Before running, place the input file into a directory on HDFS. The repository supplies an example input file in its “data” directory. Spark’s trunk contains a script called spark-submit that abstracts away the pain of building the classpath. Its inclusion in an upcoming release will make launching an application much easier. source /etc/spark/conf/spark-env.sh

export JAVA_HOME=/usr/java/jdk1.7.0_45-cloudera

# system jars:
CLASSPATH=/etc/hadoop/conf
CLASSPATH=$CLASSPATH:$HADOOP_HOME/*:$HADOOP_HOME/lib/*
CLASSPATH=$CLASSPATH:$HADOOP_HOME/../hadoop-mapreduce/*:$HADOOP_HOME/../hadoop-mapreduce/lib/*
CLASSPATH=$CLASSPATH:$HADOOP_HOME/../hadoop-yarn/*:$HADOOP_HOME/../hadoop-yarn/lib/*
CLASSPATH=$CLASSPATH:$HADOOP_HOME/../hadoop-hdfs/*:$HADOOP_HOME/../hadoop-hdfs/lib/*
CLASSPATH=$CLASSPATH:$SPARK_HOME/assembly/lib/*

# app jar:
CLASSPATH=$CLASSPATH:target/sparkwordcount-0.0.1-SNAPSHOT.jar

$JAVA_HOME/bin/java -cp $CLASSPATH -Dspark.master=local com.cloudera.sparkwordcount.SparkWordCount hdfs:///somedirectory/inputfile.txt 2
    -Dspark.master specifies the cluster against which to run the application; local will run all tasks in the same local process. To run against a Spark standalone cluster instead, include a URL containing the master’s address (such as spark://masterhost:7077). To run against a YARN cluster, include yarn-client — Spark will determine the YARN ResourceManager’s address from the YARN configuration file. The output of the program should look something like this: 
(e,6), (f,1), (a,4), (t,2), (u,1), (r,2), (v,1), (b,1), (c,1), (h,1), (o,2), (l,1), (n,4), (p,2), (i,1)
  Congratulations, you have just run a simple Spark application in CDH 5. Happy Sparking! Sandy Ryza is an engineer on the data science team at Cloudera. He is an Apache Hadoop committer and recently led Cloudera’s Spark development. Spark Summit 2014 is coming (June 30 – July 2)! Register here to get 20% off the regular conference price.</snippet></document><document id="5"><title>How-to: Use cron-like Scheduling in Apache Oozie</title><url>http://blog.cloudera.com/blog/2014/04/how-to-use-cron-like-scheduling-in-apache-oozie/</url><snippet>Improved scheduling capabilities via Oozie in CDH 5 makes for far fewer headaches. One of the best new Apache Oozie features in CDH 5, Cloudera’s software distribution, is the ability to use cron-like syntax for coordinator frequencies. Previously, the frequencies had to be at fixed intervals (every hour or every two days, for example) � making scheduling anything more complicated (such as every hour from 9am to 5pm on weekdays or the second-to-last day of every month) complex and difficult.� In this post, you’ll learn how to use this new syntax in a practical way. Scheduling in CDH 4 Before we get into the cron-like syntax, let’s take a quick look at the fixed-frequency scheduling supported in CDH 4 and earlier. Oozie treats frequency in terms of minutes; however, due to the variability in the number of minutes in each day and month, we recommend using the appropriate EL (Expression Language) functions instead of trying to do the calculations by hand. With that in mind, coordinator frequencies are typically specified with one of these four EL functions: ${coord:minutes(int n)} ${coord:hours(int n)} ${coord:days(int n)} ${coord:months(int n)} Let’s try to use these functions to schedule a coordinator for the first example: every hour from 9am to 5pm on weekdays. The most obvious approach is to use ${coord:hours(1)} to schedule the coordinator to run every hour. But as you’ll quickly realize, that won’t work — because the job will continue after 5pm, before 9am, and also into the weekend. In the end, the easiest approach is to create 40 similar coordinator jobs! Yes, you read that correctly: you’d create a new job to run every seven days, and start them at offsets of an hour between 9am and 5pm on Monday, Tuesday, Wednesday, Thursday, and Friday — that’s eight coordinators per day and five days = 40 coordinators.� True, you can make this process less painful by reusing a single coordinator.xml and simply submitting it with multiple start times by making the start attribute a variable and setting it at submission time: 
&lt;coordinator-app name="coord-1" frequency="${coord:days(7)}" start="{$start}" end="2015-01-27T00:00Z" timezone="UTC" xmlns="uri:oozie:coordinator:0.4"&gt;
	...
&lt;/coordinator-app&gt;
   Here’s how the first and last few of the 40 submissions would look, with the first on Monday, January 27, 2014, at 9am and the last on Friday, January 31, 2014, at 5pm: oozie job -config job.properties -run -Dstart=2014-01-27T09:00Z
oozie job -config job.properties -run -Dstart=2014-01-27T10:00Z
oozie job -config job.properties -run -Dstart=2014-01-27T11:00Z
...
oozie job -config job.properties -run -Dstart=2014-01-31T15:00Z
oozie job -config job.properties -run -Dstart=2014-01-31T16:00Z
oozie job -config job.properties -run -Dstart=2014-01-31T17:00Z
   You’d have to do the same thing for the other example, the second-to-last day of every month. In fact, you’d probably have to manually calculate the specific dates and submit a coordinator for each month! Scheduling in CDH 5 cron is a utility included with most Unix/Linux operating systems for scheduling time-based jobs.�For example, you might want it to run a script that cleans out your Internet history once a week.� As I hinted at earlier, the syntax for specifying the schedule for cron is more flexible and powerful than previously. We’ll take a quick look at the syntax below, but for more details, review the documentation here. (Note that there are variations even among standard cron tools, so it’s a good idea to quickly read the Oozie-specific documentation even if you are already familiar with cron.) Oozie uses the Quartz Scheduler to parse the cron syntax.� The cron syntax used by Oozie is a string with five space-separated fields: Minute, Hour, Day-of-Month, Month, and Day-of-Week. Below is a chart, adapted from the documentation, summarizing the different values for each field: The Allowed Values for each field are fairly self-explanatory (but note that while in many cron implementations, Day-of-Week accepts 0-6, here we accept 1-7, instead).� Allowed Special Characters are allowed in all fields: “*” (asterisk), which matches all values; “,” (comma), which lets you specify multiple values; “-” (dash), which lets you specify ranges; and “/”, which lets you specify increments.� It’s probably easiest to explain these characters with some examples. Remember, Oozie’s processing time zone is UTC, so if you”re in a different time zone, you’ll have to add/subtract the appropriate offset from the examples ahead.� 30 * * * * This expression indicates that the job should run at the 30th minute of every hour (1:30am, 2:30am, and so on, assuming the job is set to start on the hour). We’ve set the Minute field to 30, and the remaining fields to “*” so they match every value.� 30 14 * * * This expression indicates that the job should run at 2:30pm everyday. The Minute field is set to 30, the Hour field is set to 14, and the remaining fields are set to “*”.� 30 14 * 2 * This expression indicates that the job should run at 2:30pm everyday during February. This is similar to the previous expression, except that we’ve now restricted the Month field to February instead of including every month.� 0/20 5-9,12-14 0/5 * * This expression is a bit trickier and highlights the flexibility of the cron-like scheduling. It indicates that the job should run every 20 minutes (0, 20, and 40 past the hour) between 5am and 10am (with the last job starting at 9:40am) and between noon and 2pm (with the last job starting at 1:40pm) on every fifth day of every month. The Minute field is set to 0/20, the Hour field is set to 5-9,12-14, the Day-of-Month field is set to 0/5, and the remaining fields are set to “*”.� You may have also noticed that there are some additional Allowed Special Characters — “?”, “L”, “W”, and “#” � which you can use in some of the fields to provide more specialized results: Use “?” in the Day-of-Month and Day-of-Week fields to indicate no specific value (if you want to specify one but not the other). Use “L” in the Day-of-Month and Day-of-Week fields to indicate the last day of the month or the last day of the week (Saturday) respectively. The “L” can do other things in the Day-of-Week field, too; for example, if “6L” is in the Day-of-Week field, it indicates the last Friday of the month.� Use “W” in the Day-of-Month field to indicate the nearest weekday to the given day. And the “#” can be used in the Day-of-Week field to indicate the nth day of the month.� These values have more subtle and complex use cases than the other values. Before using them, read the documentation carefully — especially for “L” and “W” as they have some additional behavior. Here are some examples using these special values: 0 5 ? * MON This expression indicates that the job should run every Monday at 5am. The Minute field is set to 0, the Hour field is set to 5, the Day-of-Month field is set to “?”, the Month field is set to “*”, and the Day-of-Week field is set to MON. Notice that if the “?” were a “*”, then this expression would indicate that the job should run every day at 5am, not just Mondays. The difference between the “?” and the “*” is sometimes tricky, but this example is pretty helpful.� 0 5 L * ? This expression indicates that the job should run on the last day of every month at 5am.� The Minute field is set to 0, the Hour field is set to 5, the Day-of-Month field is set to L, the Month field is set to “*”, and the Day-of-Week field is set to “?”.� 0 5 15W * ? This expression indicates that the job should run at 5am on the weekday closest to the 15th day of every month. The Minute field is set to 0, the Hour field is set to 5, the Day-of-Month field is set to 15W, the Month field is set to “*”, and the Day-of-Week field is set to “?”.� 0/33 9-14 ? * 2#1 This expression indicates that the job should run every 33 minutes between 9am and 3pm on the first Monday of every month. The Minute field is set to 0/33, the Hour field is set to 9-14, the Day-of-Week field is set to 2#1 (the first Monday), and the remaining fields are set to “*”.� With all this in mind, it is now fairly straightforward to create one coordinator for each of our examples. Using cron syntax, every hour from 9am to 5pm on Weekdays can be expressed as: 0 9-17 ? * 2-6 and the second-to-last day of every month can be expressed as: 0 0 L-1 * ? (Note that “L-1″ means the second-to-last day of the month.) You would need only one coordinator for each of the two examples instead of the (ridiculous) number of them you saw earlier. For more examples, take a look at the cron tutorial from the Quartz Scheduler website. Keep in mind that they have a Second and optional Year fields, whereas Oozie has neither, so their examples have six or seven fields instead of only five.� It’s Even Easier with Hue The Hue team has added support for Oozie’s cron-like scheduling syntax. too. (Hopefully you are familiar with Hue’s great Oozie dashboard and editor.) In fact, it’s easier to configure the frequency with Hue because you don’t even have to know the cron syntax!�You can also just use Hue to create the cron expression for you, and then put it into your own coordinator.� Find out more and watch a demo on the Hue blog.� Conclusion If you found the cron-like scheduling syntax to be a little overwhelming, don’t worry: the fixed-frequency scheduling syntax isn’t going anywhere and will still work. Or, you can try using Hue.� Otherwise, cron-like scheduling is a much more powerful and flexible way to schedule jobs and will make Oozie an even more valuable tool in CDH 5. Also, it requires no extra setup or configuration, so go and try it out! Robert Kanter is a Software Engeer at Cloudera, and an Oozie Committer/PMC Member.</snippet></document><document id="6"><title>Hello, Apache Hadoop 2.4.0</title><url>http://blog.cloudera.com/blog/2014/04/hello-apache-hadoop-2-4-0/</url><snippet>The community has voted to release Apache Hadoop 2.4.0. Hadoop 2.4.0 includes myriad improvements to HDFS and MapReduce, including (but not limited to): ACL Support in HDFS — which allows, among other things, easier access to Apache Sentry-managed data by components that use it (already shipping in CDH 5.0.0) Native support for rolling upgrades in HDFS (equivalent functionality already shipping inside CDH 4.5.0 and later) Usage of protocol-buffers for HDFS FSImage for smooth operational upgrades Complete HTTPS support in HDFS Automatic Failover for ResourceManager HA in YARN Preview version of the YARN Timeline Server for storing and serving generic application history Congratulations to everyone who contributed! See full release notes here. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="7"><title>Sneak Preview: "Ecosystem" Track at HBaseCon 2014</title><url>http://blog.cloudera.com/blog/2014/04/sneak-preview-ecosystem-track-at-hbasecon-2014/</url><snippet>The HBaseCon 2014 “Ecosystem” track offers a cross-section view of the most interesting projects emerging on top of, or alongside, HBase. The�HBaseCon 2014�(May 5, 2014 in San Francisco) is not just a reflection of HBase itself — it’s also a celebration of the entire ecosystem. Thanks again,�Program Committee! HBaseCon is just a few short weeks away, so don’t wait to register. “Cross-Site BigTable using HBase” Jingcheng Du and Ramkrishna Vasudevan (Intel)� As HBase continues to expand in application and enterprise or government deployments, there is a growing demand for storing data across geographically distributed datacenters for improved availability and disaster recovery. The Cross-Site BigTable extends HBase to make it well-suited for such deployments, providing the capabilities of creating and accessing HBase tables that are partitioned and asynchronously backed-up over a number of distributed datacenters. This talk reveals how the Cross-Site BigTable manages data access over multiple datacenters and removes the data center itself as a single point of failure in geographically distributed HBase deployments.� “Design Patterns for Building 360-degree Views with HBase and Kiji” Jonathan Natkins (WibiData)� Many companies aspire to have 360-degree views of their data. Whether they’re concerned about customers, users, accounts, or more abstract things like sensors, organizations are focused on developing capabilities for analyzing all the data they have about these entities. This talk will introduce the concept of entity-centric storage, discuss what it means, what it enables for businesses, and how to develop an entity-centric system using the open-source Kiji framework and HBase. It will also compare and contrast traditional methods of building a 360-degree view on a relational database versus building against a distributed key-value store, and why HBase is a good choice for implementing an entity-centric system.� “HBase Data Modeling and Access Patterns with Kite SDK” Adam Warrington (Cloudera)� The Kite SDK is a set of libraries and tools focused on making it easier to build systems on top of the Hadoop ecosystem. HBase support has recently been added to the Kite SDK Data Module, which allows a developer to model and access data in HBase consistent with how they would model data in HDFS using Kite. This talk will focus on Kite’s HBase support by covering Kite basics and moving through the specifics of working with HBase as a data source. This feature overview will be supplemented by specifics of how that feature is being used in production applications at Cloudera.� “OpenTSDB 2.0″ Chris Larsen (Limelight Networks) and Benoit Sigoure (Arista Networks)� The OpenTSDB community continues to grow and with users looking to store massive amounts of time-series data in a scalable manner. In this talk, we will discuss a number of use cases and best practices around naming schemas and HBase configuration. We will also review OpenTSDB 2.0′s new features, including the HTTP API, plugins, annotations, millisecond support, and metadata, as well as what’s next in the roadmap.� “Presto + HBase: A Distributed SQL Query Execution Engine on Top of HBase” Manukranth Kolloju (Facebook)� Presto is a distributed SQL query engine optimized for ad hoc analysis at interactive speed in use at Facebook. At Facebook scale, having ad hoc SQL query capabilities for high-volume NoSQL data stores has been a very valuable asset, and Presto enabled this by supporting connectors on top of HDFS and other data providers. To effectively process the Presto SQL-based workload, HBase needs to be able to efficiently support a critical set of data access patterns over large data sets with high performance. This talk covers the improvements we’ve made to enhance scan performance and optimize the read path, as well as a number of other new features that help push down the work from the query execution to the database.� “Tasmo: Building HBase Applications From Event Streams” Pete Matern and Jonathan Colt (Jive Software)� Tasmo is a system that enables application development on top of event streams and HBase. Its functionality is similar to a materialized view in a relational database, where data is maintained at write time in the forms it is needed at read time for display and indexing. Tasmo is designed for significantly read-heavy applications that display the same underlying data in multiple forms, where repeatedly performing the required selects and joins at read time can be prohibitively expensive. In this talk, we’ll explore the features and roadmap for Tasmo.� “Taming HBase with Apache Phoenix and SQL” Eli Levine, James Taylor (Salesforce.com) &amp; Maryann Xue (Intel)� HBase is the Turing machine of the Big Data world. It’s been scientifically proven that you can do *anything* with it. This is, of course, a blessing and a curse, as there are so many different ways to implement a solution. Apache Phoenix (incubating), the SQL engine over HBase to the rescue. Come learn about the fundamentals of Phoenix and how it hides the complexities of HBase while giving you optimal performance, and hear about new features from our recent release, including updatable views that share the same physical HBase table and n-way equi-joins through a broadcast hash join mechanism. We’ll conclude with a discussion about our roadmap and plans to implement a cost-based query optimization to dynamically adapt query execution based on your data sizes.� Interested yet? If not, next week, we’ll offer a preview of the “Case Studies” track. Thank you to our sponsors�– Continuuity, Hortonworks, Intel, LSI, MapR, Salesforce.com, Splice Machine,�WibiData (Gold); BrightRoll, Facebook, Pepperdata (Silver); ASF (Community); O’Reilly Media, The Hive, NoSQL Weekly (Media) — without which HBaseCon would be impossible!</snippet></document><document id="8"><title>Hue Flies High at Goibibo</title><url>http://blog.cloudera.com/blog/2014/04/hue-flies-high-at-goibibo/</url><snippet>Our thanks to Amar Parkash, a Software Developer at Goibibo, a leading travel portal in India, for the enthusiastic support of Hue you’ll read below. At Goibibo, we use Hue in our production environment. I came across Hue while looking for a near real-time log search tool and got to know about Cloudera Search and the interface provided by Hue. I tried it on my machine and was really impressed by the UI it provides for Apache Hive, Apache Pig, HDFS, job browser, and basically everything in the Big Data domain. We immediately deployed Hue in production, and that has been one of the best decisions we have ever made for our data platform at Goibibo. Following are just the some of the problems Hue has solved for us: Cloudera Search: As I mentioned above, search was the first use case we started using Hue for. We have integrated Cloudera Search with Flume NG. At present, developers are actively using Hue (for search) and it has helped them save a lot of time. Previously people used grep or other shell commands which was painful and time consuming. Hive: This has been the use case where Hue has been a boon for us in data platform. Previously, businesspeople would bring us their queries, and then we would give them the output by running the Hive queries in terminal. Thanks to Hue, we have created saved queries that businesspeople can directly run by specifying the parameters. Also, the option to download the output as csv and xls is really amazing. Previously, it was a big hassle to transfer the files. Cloudera Impala: We were not using Impala much until December 2013. Since Impala is integrated with Hue, I thought of trying �it. I quickly set up some impala daemons using Cloudera Manager and I was amazed to see the performance improvement in the execution of queries as compared to Hive. Since then, we have been actively using Impala, and as mentioned previously, it has been a big timesaver for our businesspeople as well. Now we are running some queries that we never thought of running in Hive due to poor performance. File Browser: I can’t remember when the last time I used a hadoop fs -ls command. The file browser is just too good. Job Browser: The job browser comes in very handy in cases where Hive queries take too much time or get stuck. User Admin: The user admin panel is also very helpful. There are some cases where we don’t want to give some particular access to every user and the user admin panel does that precisely. I would give Hue a 5-out-of-5 rating and recommend it to anyone working on a Hadoop cluster!</snippet></document><document id="9"><title>How-to: Process Data using Morphlines (in Kite SDK)</title><url>http://blog.cloudera.com/blog/2014/04/how-to-process-data-using-morphlines-in-kite-sdk/</url><snippet>Our thanks to Janos Matyas, CTO and Founder of SequenceIQ, for the guest post below about his company’s use case for Morphlines (part of the Kite SDK). SequenceIQ has an Apache Hadoop-based platform and API that consume and ingest various types of data from different sources to offer predictive analytics and actionable insights. Our datasets are structured, unstructured, log files, and communication records, and they require constant refining, cleaning, and transformation. These datasets come from different sources (industry-standard and proprietary adapters, Apache Flume, MQTT, iBeacon, and so on), so we need a flexible, embeddable framework to support our ETL process chain. Hello, Morphlines! (As you may know, originally the Morphlines library was developed as part of�Cloudera Search;�eventually, it graduated into the Kite SDK as a general-purpose framework.) To define a�Morphline�transformation chain, you need to describe the steps in a configuration file, and the framework will then turn into an in-memory container for transformation commands. Commands perform tasks such as transforming, loading, parsing, and processing records, and they can be linked in a processing chain. In this blog post, I’ll demonstrate such an ETL process chain containing custom Morphlines commands (defined via config file and Java), and use the framework within MapReduce jobs and Flume. For the sample ETL with Morphlines use case, we have picked a publicly available “million song” dataset from Last.fm. The raw data consist of one JSON file/entry for each track; the dictionary contains the following keywords: artist
timestamp
similars
tags
track_id
title
   The keys are not always available for each object. {
    "artist": "Topazz",
    "timestamp": "2011-08-01 15:42:24.789597",
    "similars": [
        [
            "TRLTTOC128F4238C01",
            0.522931
        ],
        [
            "TRBFOXE128F4238C18",
            0.499313
        ]
    ],
    "tags": [
        [
            "drjazzmrfunkmusic",
            "100"
        ],
        [
            "mid",
            "75"
        ]
    ],
    "track_id": "TRAAEYW128F4238BF3",
    "title": "Behind the Wheel"
}
   During the ETL steps, we will use two process chains with built-in, as well as custom, commands (and you will learn two different ways to write the latter): Import Data into the Hadoop Cluster The initial data import flow from an external source into Hadoop/HDFS is the following: During the import phase, set up a Flume Interceptor with a Morphlines configuration file to load data into HDFS. The load process uses a custom Morphlines Java command to do a preliminary ETL process on the data�(selecting songs before and after a given date). For this process, you can use a simple custom Java command, called LatestSongCommand, below: @Override
protected boolean doProcess(Record record) {
    Map attachmentBody = (Map) record.get(Fields.ATTACHMENT_BODY).get(0);
    String fieldValue = attachmentBody.get(fieldName).toString();

    try {
        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

        Date fieldDate = sdf.parse(fieldValue);
        Date patternDate = sdf.parse(pattern + " 00:00:00");

        if (operator.equals(HIGHER)) {
            if (!fieldDate.after(patternDate)) {
                return true;
            }
        } else if (operator.equals(LOWER)) {
            if (!fieldDate.before(patternDate)) {
                return true;
            }
        } else if (operator.equals(EQUALS)) {
            if (fieldDate.getYear() != patternDate.getYear()
                    || fieldDate.getMonth() != patternDate.getMonth()
                    || fieldDate.getDay() != patternDate.getDay()) {
                return true;
            }
        } else {
            LOG.info("bad operator syntax");
        }
    } catch (Exception e) {
        LOG.info("parse exception: " + e.getMessage());
        return false;
    }
    record.removeAll(Fields.ATTACHMENT_BODY);
    try {
        record.put(Fields.MESSAGE, OBJECTMAPPER.writeValueAsString(attachmentBody));
    } catch (JsonProcessingException e) {
        LOG.info("parse exception: " + e.getMessage());
        return false;
    }
    return super.doProcess(record);
}    To configure your new Morphline command: morphlines : [
    {
        id : morphline1
        importCommands : ["org.kitesdk.**", "com.sequenceiq.lastfm.etl.**"]
        commands : [
            {
                readJson {
                    outputClass : java.util.Map
                }
            }
            {
                latestSongs {
                    field : timestamp
                    operator: &gt;
                    pattern: "2011-08-03"
                }
            }
        ]
    }]
   The data is coming through the Flume agent, so the Morphlines commands are applied to the records and the Flume sink will receive the cleaned data. Note that when using Morphlines with the Flume HdfsSink, configuring a custom data serializer for the HdfsSink is often handy, as Flume by default persists the body but not the headers. For your convenience, we have written a Flume serializer for Morphlines (CustomLastfmHeaderAndBodyTextEventSerializer) that will retain the same input data format, with the ETL commands applied. Post-process the Entries Once the data is imported into HDFS, you can post-process it and apply additional ETL steps. The flow used in this example is the following: The post-processing ETL uses a chain of Morphline commands set up in a configuration file. Each track in the Last.fm dataset has similar tracks associated with it, which are the results of precomputed song-level similarities. The custom Morphlines command lets you select only the entries where the similarity is less than, equal to, or over the filter value (in our case, over 0.1). morphlines : [
  {
    id : morphline1
    importCommands : ["org.kitesdk.**"]
    commands : [
      {
        readJson {
          outputClass : com.fasterxml.jackson.databind.JsonNode
        }
      }
      {
        extractJsonPaths {
          flatten : false
          paths : {
            similars : "/similars/[]"
          }
        }
      }
      {
        java {
          imports : """
            import java.util.*;
            import com.fasterxml.jackson.databind.*;
            import com.fasterxml.jackson.databind.node.*;
          """
          code: """
            List similars = record.get("similars");
            for (ArrayNode similar : similars) {
                Iterator iterator = similar.elements();
                while (iterator.hasNext()) {
                    JsonNode pair = iterator.next();
                    JsonNode jsonNode = pair.get(1);
                    //we remove entries with similarity &lt; 0.1
                    if (jsonNode == null || jsonNode.asDouble() &lt; 0.1) {
                        iterator.remove();
                    }
                }
            }
            return child.process(record);
          """
        }
      }
    ]
  }
]
   In this Morphline config file there are two default�commands (readJson, extractJsonPaths)�and one custom command (written in Java). As you can see, custom Morphline commands can be defined in a command file as well, so you don’t have to compile them or write a Java class beforehand. This is an extremely useful feature; using the�JavaBuilder�class the framework compiles the commands at runtime. Using Morphlines from a MapReduce job is straightforward. During the setup phase of the MapReduce job you build a context and a�Morphline — that’s it.  @Override
  protected void setup(Context context) throws IOException, InterruptedException {
      File morphLineFile = new File(context.getConfiguration().get(MORPHLINE_FILE));
      String morphLineId = context.getConfiguration().get(MORPHLINE_ID);
      RecordEmitter recordEmitter = new RecordEmitter(context);
      MorphlineContext morphlineContext = new MorphlineContext.Builder().build();
      morphline = new org.kitesdk.morphline.base.Compiler()
              .compile(morphLineFile, morphLineId, morphlineContext, recordEmitter);
  }
    Also, we have created a�RecordEmitter inner class: java private static final class RecordEmitter implements Command
   in the mapper. This method: org.kitesdk.morphline.base.Compiler class public Command compile(File morphlineFile, String morphlineId, MorphlineContext morphlineContext, Command finalChild, Config... overrides)
   takes an important parameter called�finalChild,�which in this case is the RecordEmitter. The returned command will feed records into finalChild, which means that if this parameter is not provided, a DropRecord command will be assigned automatically.�(In Flume there is a Collector command to avoid losing any transformed record). The only thing left is to out-box the processed record and write the results to HDFS. The RecordEmitter will serve this purpose writing out to HDFS:�context.write(line, null). Once the�morphline�is created, you can now process the records: public void map(Object key, Text value, Context context) throws IOException, InterruptedException {     record.put(Fields.ATTACHMENT_BODY, new ByteArrayInputStream(value.toString().getBytes()));     if (!morphline.process(record)) {         LOGGER.info("Morphline failed to process record: {}", record);     }     record.removeAll(Fields.ATTACHMENT_BODY);   }
   Testing Morphlines has a nice test framework built in to the SDK; your unit tests can extend the abstract AbstractMorphlineTest�class, thus you can test your custom-built commands the same way as Morphlines does for the built-in ones. (Use our�LatestSongCommandTest�test case as a reference.) Building and Running You can get the code from our�GitHub�page and build the project with Maven�mvn clean install. Download the Last.fm sample dataset from�S3�and save it to your computer.�(Alternatively, you can use our other Morphlines�example�to process/ETL files directly from an S3 bucket.) Start Flume using the following configuration�and make sure you change the input and output folders accordingly. Once the data is processed and available on HDFS, you can run the second ETL process, this time using Morphlines from a MapReduce job: yarn jar lastfm-morphlines-etl-1.0.jar com.sequenceiq.lastfm.etl.MapperCleaner input output morphline-file morphlineId   Conclusion As you can see, embedding Morphlines in your application and using it is very easy. The increasing number of built-in commands will satisfy most needs, but the framework offers flexible ways to write custom commands, as well. Happy Morphlining!</snippet></document><document id="10"><title>This Month in the Ecosystem (March 2014)</title><url>http://blog.cloudera.com/blog/2014/04/this-month-in-the-ecosystem-march-2014/</url><snippet>Welcome to our seventh edition of “This Month in the Ecosystem,” a digest of highlights from March 2014 (never intended to be comprehensive; for completeness, see the excellent�Hadoop Weekly). More good news for the ecosystem! Cloudera Enterprise (comprising CDH 5.0.0 and Cloudera Manager 5.0.0) became�GA! This is the most expansive (now including Apache Spark, Parquet, Cloudera Impala, and Cloudera Search) and integrated release yet, and arrives with 100 partner certifications out of the gate. The HBaseCon 2014 agenda�(May 5, 2014, in San Francisco)�was locked down. Among other things, it includes keynotes from Facebook, Google, and Salesforce.com engineers — including the first public talk since 2006 about Google’s Bigtable internal uses cases and roadmap. Strata + Hadoop World 2014 Call for Papers opened. The conference has moved to a much bigger venue to accommodate heavy growth in the past couple of years. Cloudera published a definitive study of MapReduce performance on SSDs. The results are likely to guide deployment decisions in this area for a good length of time. The Apache Mahout community announced a new effort to support Spark, with�experimental bindings�contributed to the Mahout trunk. The results of the “Future of Open Source 2014″ survey were published (with largest number of responses ever). Conclusion? “Open source continues to eat the software world.” That’s all for this month, folks! Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="11"><title>Sneak Preview: "Features &amp; Internals" Track at HBaseCon 2014</title><url>http://blog.cloudera.com/blog/2014/04/sneak-preview-features-track-at-hbasecon-2014/</url><snippet>The HBaseCon 2014 “Features &amp; Internals” track covers the newest developments in Apache HBase functionality. The�HBaseCon 2014�(May 5, 2014 in San Francisco) agenda has something for everyone – particularly, developers building apps on HBase. Thanks again,�Program Committee! HBaseCon is just a few short weeks away, so don’t wait to register. “Bulk Loading in the Wild: Ingesting the World’s Energy Data” Eric Chang (Opower) and Jean-Daniel Cryans (Cloudera) HBase is designed to store your big data and provide low latency random access to that data. One of its most compelling features is Bulk Loading, which enables the generation of HFiles that can then be passed to the RegionServers. Opower’s energy insights platform uses it to ingest the hundreds of millions of meter reads it receives daily from its partner utility companies. This presentation will walk you through the HBase Bulk Loading process and Opower’s adoption of it as an important piece of its HBase ecosystem. “HBase at Xiaomi” Liang Xie and Honghua Feng (Xiamoi) This talk covers the HBase environment at Xiaomi, including thoughts and practices around latency, hardware/OS/VM configuration, GC tuning, the use of a new write thread model and reverse scan, and block index optimization. It will also include some discussion of planned JIRAs based on these approaches. “HBase: Extreme Makeover” Vladimir Rodionov (bigbase.org) This talks introduces a totally new implementation of a multilayer caching in HBase called BigBase. BigBase has a big advantage over HBase 0.94/0.96 because of an ability to utilize all available server RAM in the most efficient way, and because of a novel implementation of a L3 level cache on fast SSDs. The talk will show that different type of caches in BigBase work best for different type of workloads, and that a combination of these caches (L1/L2/L3) increases the overall performance of HBase by a very wide margin. “HBase Read High Availability Using Timeline-Consistent Region Replicas” Enis Soztutar and Devaraj Das (Hortonworks) HBase has ACID semantics within a row that make it a perfect candidate for a lot of real-time serving workloads. However, single homing a region to a server implies some periods of unavailability for the regions after a server crash. Although the mean time to recovery has improved a lot recently, for some use cases, it is still preferable to do possibly stale reads while the region is recovering. In this talk, you will get an overview of our design and implementation of region replicas in HBase, which provide timeline-consistent reads even when the primary region is unavailable or busy. “HBase: Where Online Meets Low Latency” Nick Dimiduk (Hortonworks) and Nicolas Liochon (Scaled Risk) HBase is an online database so response latency is critical. This talk will examine sources of latency in HBase, detailing steps along the read and write paths. We’ll examine the entire request lifecycle, from client to server and back again. We’ll also look at the different factors that impact latency, including GC, cache misses, and system failures. Finally, the talk will highlight some of the work done in 0.96+ to improve the reliability of HBase. “New Security Features in Apache HBase 0.98: An Operator’s Guide” Andrew Purtell and Ramkrishna Vasudevan (Intel) HBase 0.98 introduces several new security features: visibility labels, cell ACLs, transparent encryption, and coprocessor framework changes. This talk will cover the new capabilities available in HBase 0.98+, the threat models and use cases they cover, how these features stack up against other data stores in the Apache big data ecosystem, and how operators and security architects can take advantage of them. “State of HBase: Meet the Release Managers” HBase release managers Lars Hofhansl, Andrew Purtell, Enis Soztutar, Michael Stack, and Liyin Tang jointly present highlights from their releases, and take your questions throughout. Interested yet? If not, next week, we’ll offer a preview of the “Ecosystem” track. Thank you to our sponsors�– Continuuity, Hortonworks, Intel, LSI, MapR, Salesforce.com, Splice Machine,�WibiData (Gold); BrightRoll, Facebook, Pepperdata (Silver); ASF (Community); O’Reilly Media, The Hive, NoSQL Weekly (Media) — without which HBaseCon would be impossible!</snippet></document><document id="12"><title>Cloudera Enterprise 5 is Now Generally Available!</title><url>http://blog.cloudera.com/blog/2014/04/cloudera-enterprise-5-is-now-generally-available/</url><snippet>The GA release of Cloudera Enterprise 5 signifies the evolution of the platform from a mere Apache Hadoop distribution into an enterprise data hub. We are thrilled to announce the GA release of Cloudera Enterprise 5 (comprising CDH 5.0 and Cloudera Manager 5.0).� When it became generally available 18 months ago, Cloudera Enterprise 4 was widely recognized to be more flexible, more scalable, and less expensive than traditional data management platforms; with Cloudera Enterprise 5, we can also say it is faster (HDFS caching FTW!), more functional (includes new components such as Apache Spark and Parquet), and better integrated (for example, Cloudera Impala and Cloudera Search are now both inside CDH). Furthermore, in an impressive demonstration of ecosystem growth, 100 Cloudera partners invested technical resources to certify on Cloudera Enterprise 5�before its release. Cloudera Enterprise contains many new components and features, with the major ones listed below. (See CDH 5 Release Notes and Cloudera Manager 5 Release Notes for full details.) Thanks to everyone who participated in the beta process to help make this release as good as it could be! New components Apache Spark 0.9.0 Apache Crunch 0.9.0 Parquet 1.2.5 Kite SDK 0.10.0 Apache Avro 1.7.5 Apache Hadoop Hadoop 2.3.0 HDFS HDFS Caching NFS gateway YARN &amp; MapReduce MR2 is production ready MR2 Management by Cloudera Manager MR1 and MR2 are both supported Resource Management/YARN HA support for Resource Manager Cloudera Manager Platform support for CDH 5 Resource management and scheduler configuration Extensibility support for third party applications Advanced Impala monitoring Support for Spark Enhancements to monitoring and charts Single sign-on support Apache HBase HBase 0.96.1.1 Significant reductions to Mean time to Recovery (MTTR) Impala Impala 1.3.0 Language extensibility with native UDFs and UDAF, plus support for existing Hive UDFs Automatic and coordinated metadata refresh Concurrency controls via admission controls Beta of YARN-integrated resource management Apache Flume New Kite Dataset Sink Apache Hive Apache Hive 0.12.0 Improved JDBC spec coverage SSL encryption support on non-kerberos authentication Native Parquet support Search Support for CDH 5 Apache Sqoop Rebase to Sqoop 1.99.3 Apache Oozie Oozie 4.0.0 Support for CRON-like scheduling Oozie HA Hue Hue 3.5.0 Single sign-on support Graphical facets in Search application SQL editors re-vamp: richer GUI for Impala and Hive, result graphing, and more Apache ZooKeeper application Apache Pig Pig 0.12.0 Apache DataFu (incubating) 1.1.0 Back-up and Disaster Recovery (BDR) HDFS Snapshots HBase Snapshots Support for MR2 Global replication page Apache Mahout Mahout 0.8.0 In the case of existing clusters, Cloudera fully supports upgrades from Cloudera Manager 4.x and CDH 4.x to Cloudera Enterprise 5. If you were a participant in the beta, we recommend that you start with a new cluster with Cloudera Enterprise 5. If you have a particular need to upgrade a cluster running Cloudera Enterprise 5 Beta or CDH 5 Beta to Cloudera Enterprise 5, customers can contact Cloudera Support for further instructions. If you are not a customer, please ask for assistance on the Cloudera Manager forum.� Please note that Cloudera Manager 5.0 does not support CDH 5.0 Beta 1 or Beta 2. As always, we are happy to hear your feedback; please send your comments and suggestions through our community forums. You can also file bugs through our external JIRA system at issues.cloudera.org.</snippet></document><document id="13"><title>How-to: Use the HBase Thrift Interface, Part 3 – Using Scans</title><url>http://blog.cloudera.com/blog/2014/04/how-to-use-the-hbase-thrift-interface-part-3-using-scans/</url><snippet>The conclusion to this series covers how to use scans, and considerations for choosing the Thrift or REST APIs. In this series of how-tos, you have learned how to use Apache HBase’s Thrift interface. Part 1 covered the basics of the API, working with Thrift, and some boilerplate code for connecting to Thrift. Part 2 showed how to insert and to get multiple rows at a time.�In this third and final post, you will learn how to use scans and some considerations when choosing between REST and Thrift. Scanning with Thrift A "scan" allows you to retrieve all or a range of rows in a table.�Here is the code for doing a scan: 
scan = Hbase.TScan(startRow="shakespeare-comedies-000001", stopRow="shakespeare-comedies-999999")
scannerId = client.scannerOpenWithScan(tablename, scan)

row = client.scannerGet(scannerId)

rowList = client.scannerGetList(scannerId,numRows)

while rowList:
          for row in rowList:
                    message = row.columns.get(messagecolumncf).value
                  linenumber = decode(row.columns.get(linenumbercolumncf).value)
                   rowKey = row.row
        rowList = client.scannerGetList(scannerId,numRows)

client.scannerClose(scannerId)
   You start off by creating a TScan object, which allows you to specify the start and stop rows for the scan.�The start row is inclusive and the stop row is exclusive. Using the TScan object, you call scannerOpenWithScan with the table name.�This returns a scanner id. The Thrift client cannot directly hold onto a Scan object; instead, the Thrift server holds on to this object for us.�The scanner id uniquely identifies this Scan object on the Thrift server so your code can use it. With the scanner id, you start to get rows back with the scannerGetList call.�You also need to specify the number of rows you want to come back at a time.�This number will vary depending on your code and data — I recommend making it a variable and spending some time to optimize it. Now that you have a row list, you can start iterating through it.�The iteration code is a little more complex than the GET code.�Here, you have to deal with the rowList being None as well as iterating through the values. Since Python lacks an assignment syntax in a WHILE loop, you have to assign it elsewhere and then iterate. The scanner returns the same TRowResult as a GET.� You can pull the row’s columns with row.columns.get("COLUMNFAMILY:COLUMNDESCRIPTOR"). As previously explained, the Thrift server holds the Scan object.�So, you need to tell the Thrift server to close the Scan object on its side.�You do this by calling scannerClose with the scanner id that the scannerOpen returned.�Forgetting to do this could leak Scan objects onto the Thrift server. Choosing Between Thrift and REST You may be deciding between using HBase’s Thrift and REST interfaces.�I have covered the REST interface in great detail in another series of blog posts.�This series of blog posts covers the Thrift side of things. I should start the discussion by mentioning that the Thrift and REST interfaces are not mutually exclusive.�Both daemons can run on the same server because they use different ports.�Most shops choose just one to standardize their codebase. Program completion time (in seconds) There are a couple of things to consider when standardizing on an interface.�First and foremost is speed.�The chart above shows the program completion times for the PUT and GET scripts of Thrift, REST with JSON, and REST with XML.�These numbers were from a three-node cluster with the Thrift and REST server running on localhost.�It gives you a general idea of speed for each interface and data format.�Thrift comes out ahead in both programs. Another consideration is ease of use.�REST is much easier to set up and get going.�Any programming language with an HTTP library can access the HBase REST interface.�With Thrift, you will have to create the bindings and learn the data types. Thrift really excels at creating a seamless experience with the language bindings. You will not have to deal with XML or JSON with Thrift.�Thrift has generated all of the data classes for you. Comparing the three examples’ code, you can see the Thrift code is much cleaner. If I were to start a brand new, non-Java project, I would use Thrift.�It just comes out ahead.�The REST interface is great for projects where you are already exposing your own REST interfaces. REST is also good for scripting and shell commands.�You could write a quick bash script that uses curl to perform some actions. I should stress that choosing between Thrift and REST always precludes Java.�The only first-class citizen for HBase is Java.�If were to start a brand-new project without any language constraints, I would use the Java API. Conclusion The HBase Thrift interface is a good way to use HBase if you don’t want to use Java.�It gives you code generation for a lot of different languages.�This native, non-Java language support will improve your code’s performance and readability. These code samples and explanations will save you a lot of googling when embarking on your HBase Thrift project. Jesse Anderson is a Cloudera University instructor. HBaseCon 2014 is coming May 5! Register now while there’s room at hbasecon.com/registration.</snippet></document><document id="14"><title>How Impala Brings Real-Time, Big Data Analytics to Digital Reasoning’s Users</title><url>http://blog.cloudera.com/blog/2014/03/how-impala-brings-real-time-big-data-analytics-to-digital-reasonings-users/</url><snippet>The following post, by Sarah Cannon of Digital Reasoning, was originally published in that company’s blog. Digital Reasoning has graciously permitted us to re-publish here for your convenience. At the beginning of each release cycle, engineers at Digital Reasoning are given time to explore the latest in Big Data technologies, examining how the frequently changing landscape might be best adapted to serve our mission. As we sat down in the early stages of planning for Synthesys 3.8 one of the biggest issues we faced involved reconciling the tradeoff between flexibility and performance. How can users quickly and easily retrieve knowledge from Synthesys without being tied to one strict data model? Previously we had developed solutions to tackle each concern individually. Apache Pig scripts, Apache Hive queries, and Apache Hadoop streaming all provided unbounded access to Synthesys�but with the caveat of extended run times, while our own query language implementation offered real-time results but was limited in scope. Was it possible to have it all? Cloudera’s Impala project offered a worthwhile investigation. What is Impala? From our friends at Cloudera: “Cloudera Impala is an open source Massively Parallel Processing (MPP) query engine that runs natively on Apache Hadoop”. Built for performance, Impala uses in-memory data transfers with its native query engine allowing users to issue SQL queries against HDFS and Apache HBase and receive results in seconds. Impala fits right in to the Hadoop ecosystem, making it simple to get started. If you’ve used Hive, the learning curve is even simpler — Impala shares Hive’s metastore and supports many of the same functions. Luckily, our recent work with Hive left us perfectly poised to take advantage of Impala’s great features. Laying the Groundwork In our previous release, Synthesys 3.7, we introduced Synthesys-Hive integration. Hive is a data warehousing tool that allows users to query distributed systems with a SQL-like language that projects structure onto the data. Hive queries generate map-reduce jobs, allowing data analysts the ability to perform �ETL operations on data in a Hadoop environment in ways that previously required in-depth knowledge of the Java MapReduce API. Our integration efforts made it possible to query Synthesys Knowledge bases with Hive whether the backend was HBase, Apache Accumulo, or Apache Cassandra, �and we provided several default tables, views, and UDFs (user defined functions) for getting started. This was a great first step at making the output of Synthesys more accessible and addressing the flexibility concerns mentioned above. Users could now use SQL to explore the power of concept resolution in the entities table, view author, date and summaries of each document through the document_metadata table, or discover significant relationships in the assertions table. Once we got a Synthesys-Hive environment up and running, adding Impala was simple. All we had to do was issue a single Hive query to pull data in a view from the backend onto a materialized table in HDFS, and we were up and running with Impala! Real Time Results Impala quickly proved to offer a robust solution to the performance/flexibility quandary. Now we can do things like fuzzy searches on concepts, Browse assertions with ease, Ask corpus summary statistics such as “How many categorized elements do I have, by category?” and even answer questions like “With whom�has President Obama met most often?” All the above was available before with Hive, but with extended wait times between each query. Now anyone with a little bit of SQL know-how can get answers to their questions in an instant. Impala’s Role in Synthesys 3.8 The latest release of Synthesys�introduced the concept of Knowledge Objects — an aggregation of the important information connected to an entity. The Knowledge Objects workflow automatically writes out several Impala tables ready for querying immediately after ingestion. Thanks to Impala’s ODBC and JDBC connectors, Synthesys Knowledge bases can connect to a number of third-party visualization tools such as Tableau, Centrifuge, and Zoomdata. In just a few short months, Impala went from a concept to us here at Digital Reasoning to an invaluable tool solving pertinent problems. We even started using it within Synthesys’ latest user experience, Glance (see screenshot below). Synthesys Glance offers a comprehensive and intuitive user experience for exploring the output of Synthesys in real-time. Users can search for concepts and discover relevant details, recent news, and key relationships. Search results are displayed in easy to understand graphics and icons representing business entities, geopolitical entities, locations, and persons. Impala drives the Glance query layer, using the the tables generated by the Knowledge Objects analytic. Thanks to Impala, we found that Digital Reasoning really can “have it all”�in terms of a flexible and fast query engine.�</snippet></document><document id="15"><title>Index-Level Security Comes to Cloudera Search</title><url>http://blog.cloudera.com/blog/2014/03/index-level-security-comes-to-cloudera-search/</url><snippet>The integration of Apache Sentry with Apache Solr helps Cloudera Search meet important security requirements. As you have learned in previous blog posts, Cloudera Search brings the power of Apache Hadoop to a wide variety of business users via the ease and flexibility of full-text querying provided by Apache Solr. We have also done significant work to make Cloudera Search easy to add to an existing Hadoop cluster: It uses the same pool of data and system resources as other workloads, so you avoid the time and expense of transferring data to an external search service. It provides a familiar and trusted security framework for organizations with strict security requirements. It is well integrated with our existing management platform (Cloudera Manager) in order to ease adoption and simplify operations. In this post, we’ll focus on the security features of Cloudera Search. In particular, you’ll learn how Cloudera Search solves authentication, or verifying a user’s identity; and authorization, or controlling access to resources. We’ll also discuss secure impersonation and how it is used with the Hue Search App. Authentication Overview Cloudera Search, via Solr and Apache Lucene, provides an HTTP interface for querying, updating, and managing full-text search indices. Like the other HTTP-level services in an enterprise data hub (such as HttpFS and Apache Oozie), Cloudera Search uses the following frameworks for authentication over HTTP: Kerberos: a mutual authentication protocol that works on the basis of “tickets” SPNego: a negotiation mechanism for selecting an underlying authentication protocol Cloudera Search uses SPNego HTTP authentication to select Kerberos as the underlying authentication protocol. Using Kerberos and SPNego in this manner is advantageous for users because many tools for accessing HTTP resources have built-in support for the protocol. For example, you can use curl with the --negotiate option, and many popular browsers, including Firefox and Chrome, can be configured to access Kerberos/SPNego protected resources. Furthermore, although Kerberos is an authentication, not authorization, protocol, you can use it to provide cluster-level access control by granting Kerberos credentials to only those users who should have access to the cluster. If finer-grained control is required than the cluster level, see the section on authorization below. For information on configuring Cloudera Search to use authentication, see the documentation. Authorization Overview Solr itself does not provide access control support, but rather provides “hooks” to allow other systems to build access control on top of it. We have used these hooks to develop index-level access control using Apache Sentry (incubating). Sentry supports role-based granting of privileges in Solr; each role can be granted query, update, and/or admin privileges on any Solr index (called a “collection” in Solr terminology). Let’s look at a specification of these privileges, called a policy file (typically stored in HDFS): 
[groups]
# Assigns each Hadoop group to its set of roles
dev_ops = engineer_role, ops_role
[roles]
engineer_role = collection = source_code-&gt;action=Query,
  collection = source_code- &gt; action=Update
ops_role = collection = hbase_logs-&gt;action=Query
   The policy file comprises two main sections: [groups]: maps a Hadoop group to its set of Sentry roles [roles]: maps a Sentry role to its set of privileges. One privilege in Solr is the ability to query, update, or perform administrative actions on a given collection. So, for example, the privilege specification collection = hbase_logs-&gt;action=Query grants the role the ability to query the hbase_logs collection in Solr. Now that we’ve seen how to specify policies in Sentry, let’s look at how you would integrate Sentry and Solr. To understand this, let’s first look at how Solr processes an incoming request: Processing of incoming Solr HTTP request First, the HTTP request comes into Solr and is sent to the SolrDispatchFilter. The SolrDispatchFilter is responsible for sending the request to correct RequestHandler for the collection. If the request is to query data from the collection, it will be sent to the Select RequestHandler; if the request is to update the collection, it will be sent to the Update RequestHandler.� The request handlers themselves are specified in the collection-specific configuration file called solrconfig.xml.� For example, specifying the Select RequestHandler may look like this: 
&lt;requestHandler name="/select" class="solr.SearchHandler"&gt;
   ...
&lt;/requestHandler&gt;
   Let’s assume this is the configuration for a Solr collection called “collection1”.� This request handler specification tells Solr that a request to the path http://localhost:8983/solr/collection1/select should be dispatched to an instance of solr.SearchHandler. In addition to the standard solrconfig.xml, Cloudera Search ships with a modified version (solrconfig.xml.secure) that has request handlers integrated with Sentry. For example, with the select handler above, Sentry uses a Solr SearchComponent to check permissions before the query request is processed: Solr RequestHandler with Sentry Component The secure versions of the other standard collection request handlers are implemented in a similar fashion. Administrative Requests The section above covered requests on specific Solr collections, but what about cluster-level administrative actions? In Solr, administrative requests are sent to the /admin path. For example, a request to create a collection looks like: http://localhost:8983/solr/admin/collections?action=CREATE&amp;name=mycollection If you compare this URL to the collection-specific URL above, you’ll see that “admin” just looks like any other collection but with a different set of request handlers. Sentry mirrors this structure for privilege-granting purposes: instead of granting “admin” access to a role, query or update access is granted to the “admin” collection. Query access grants privileges for read-only administrative commands (for example, dump the state of all the threads running in a Solr server), while update grants privileges for write-only administrative commands (such as changing the level of logging output for a Solr server). For example, to grant a Sentry role read-only administrative command privileges and the ability to update a collection called “collection1”, add this to the sentry policy file: 
sample_role = collection = admin -&gt; action = QUERY
  collection = collection1 -&gt; action=Update
    Solr ships with a wide variety of collection-specific and administrative-level request handlers. For a complete list of the Sentry privileges required for the built-in Solr request handlers, see the documentation. Secure Impersonation and Hue Like Hadoop and Oozie, Cloudera Search has support for secure impersonation: the ability of a “super-user” to submit requests on behalf of another user, conceptually similar to sudo functionality on Unix. For security reasons, this functionality is limited to only the groups and hosts that are explicitly configured. (See the documentation for more information.) The excellent Hue Search App makes use of this functionality in order to integrate with its own security mechanisms. Without this impersonation support, Hue would need access to Kerberos credentials for every user of the Hue App who wants to access Solr — an unacceptable requirement for many organizations. Instead, Hue can integrate with LDAP (and other authentication systems) in order to make requests on behalf of the LDAP authenticated user by using Secure Impersonation, seamlessly integrating with Solr and Sentry. Conclusion We believe the integration of Solr and Sentry in Cloudera Search is an exciting development that opens up new workloads in CDH for organizations with strict security requirements, all in an easily consumed application provided by Hue. Cloudera Search is available for download with extensive documentation. If you have any questions, please contact us at the Cloudera Search Forum. Gregory Chanan is a Software Engineer at Cloudera and an Apache HBase Committer.</snippet></document><document id="16"><title>Sneak Preview: HBaseCon 2014 "Operations" Track</title><url>http://blog.cloudera.com/blog/2014/03/sneak-preview-hbasecon-2014-operations-track/</url><snippet>HBaseCon 2014 “Operations” track reveals best practices used by some of the world’s largest production-cluster operators. The�HBaseCon 2014 (May 5, 2014 in San Francisco) agenda is particularly strong in the area of operations. Thanks again,�Program Committee! “From MongoDB to HBase in Six Easy Months” Shreeganesh Ramanan and Mike Davis (Optimizely)� Pushing well past MongoDB’s limits (2TB data every week) is an interesting exercise in operational frustration. It also severely hampers flexibility of design for new use cases. This talk covers the architectural journey from MongoDB/Redis to HBase at Optimizely — including the performance, design flexibility, speed of implementation, and other gains made.� “Harmonizing Multi-tenant HBase Clusters for Managing Workload Diversity” Dheeraj Kapur, Rajiv Chittajallu &amp; Anish Mathew (Yahoo!) In early 2013, Yahoo! introduced multi-tenancy to HBase. A certain degree of customization per tenant (a user or a project) was achieved through RegionServer groups, namespaces, and customized configs for each tenant. This talk covers how to accommodate diverse needs to individual tenants on the cluster, as well as operational tips and techniques that allow Yahoo! to automate the management of multi-tenant clusters at petabyte scale without errors. “HBase Backups” Jesse Yates (Salesforce.com), Demai Ni, Richard Ding &amp; Jing Chen He (IBM) This talk provides an overview of enterprise-scale backup strategies for HBase: Jesse Yates will describe how Salesforce.com runs backup and recovery on its multi-tenant, enterprise scale HBase deploys; Demai Ni, Richard Ding, and Jing Chen of the IBM InfoSphere BigInsights development team will then follow with a description of IBM’s recently open-sourced disaster/recovery solution based on HBase snapshots and replication. “Real-time HBase: Lessons from the Cloud” Bryan Beaudreault (HubSpot) Running HBase in real time in the cloud provides an interesting and ever-changing set of challenges — instance types are not ideal, neighbors can degrade your performance, and instances can randomly die in unanticipated ways. This talk will cover what HubSpot has learned about running in production on Amazon EC2, how it handles DR and redundancy, and the tooling the team has found to be the most helpful. “The State of HBase Replication” Jean-Daniel Cryans (Cloudera) HBase Replication has come a long way since its inception in HBase 0.89. Today, master-master and cyclic replication setups are supported; many bug fixes and new features like log compression, per-family peers configuration, and throttling have been added; and a major refactoring has been done. This presentation will recap the work done during the past four years, present a few use cases that are currently in production, and take a look at the roadmap. “Tales from the Cloudera Field” Kevin O’Dell, Aleksandr Shulman &amp; Kathleen Ting (Cloudera) From supporting the 0.90.x, 0.92, 0.94, and 0.96 HBase installations on clusters ranging from tens to hundreds of nodes, Cloudera has seen it all. Having automated the upgrade paths from the different Apache releases, we have developed a smooth path that can help the community with upcoming upgrades. In addition to automation best practices, in this talk you’ll also learn proactive configuration tweaks and operational best practices to keep your HBase cluster always up and running.� Smooth Operators Panel Moderated by Eric Sammer (Cloudera) Includes Jeremy Carroll (Pinterest), Adam Frank (Flurry), and Paul Tuckfield (Facebook). Interested yet? If not, next week, we’ll offer a preview of the Features &amp; Internals track. Thank you to our sponsors�– Continuuity, Hortonworks, Intel, LSI, MapR, Salesforce.com, Splice Machine,�WibiData (Gold); BrightRoll, Facebook, Pepperdata (Silver); ASF (Community); O’Reilly Media, The Hive, NoSQL Weekly (Media) — without which HBaseCon would be impossible!</snippet></document><document id="17"><title>Meet the Data Scientist: David F. McCoy</title><url>http://blog.cloudera.com/blog/2014/03/meet-the-data-scientist-david-f-mccoy/</url><snippet>Meet David F. McCoy, one of the first to have earned the title “CCP: Data Scientist” from Cloudera University. Big Data success requires professionals who can prove their mastery with the tools and techniques of the Hadoop stack. However, experts predict a major shortage of advanced analytics skills over the next few years. At Cloudera, we�re drawing on our industry leadership and early corpus of real-world experience to address the Big Data talent gap with the Cloudera Certified Professional (CCP) program. As part of this blog series, we�ll introduce the proud few who have earned the CCP: Data Scientist distinction. Featured today is CCP-03, David F. McCoy. You can start on your own journey to data science and CCP:DS with Cloudera�s free Data Science Challenge Solution Kit, featuring a live data set, a step-by-step tutorial, and a detailed explanation of the processes required to arrive at the correct outcomes so that you can get hands-on experience with a real-world scenario at your own pace. What�s your current role? Since I became certified CCP:DS, I�ve had the credentials to seek full-time employment as a data scientist. I�m first trying to find data science work within my current employer, but my experience thus far has indicated that not every company has the resources required to conduct data science projects at scale. My recommendation to someone entering this field is to identify and seek roles at organizations that manage their own large, diverse data. There are non-trivial logistical, privacy, bandwidth, and financial barriers to working with truly Big Data if the organization does not actually own it. Prior to taking CCP:DS, what was your experience with Big Data, Hadoop, and data science? Prior to CCP:DS, I had a bit of exposure to Hadoop from taking part in a local hack-a-thon in Plano, Texas. I have 20 years of experience in remote sensing, so I�ve picked up a lot of data-science-style algorithms in the context of my work in image and video processing. I competed in a Kaggle contest, which was a good preliminary step towards a full data science project and great practice for Cloudera�s Data Science Challenge on web analytics for classification, clustering, and collaborative filtering. What�s most interesting about data science, and what made you want to become a data scientist? In the Sherlock Holmes stories, his sidekick, Dr. Watson, describes their adventures as a �half-sporting, half-intellectual pleasure.� I like to say data science is a �half-scientific, half-engineering pleasure.� There is the unknown a-ha or eureka factor of scientific investigation paired with the constructive design satisfaction of engineering code to perform the analysis and deal with large size and large dimensionality data sets. I originally became interested in Hadoop to get more computing resources for image analysis. At one point, I realized that the sorts of algorithms I had been using to manage and analyze pixels all these years could also be used with other data. It has been a fairly smooth transition so far. How did you prepare for the Data Science Essentials exam and CCP:DS? What advice would you give to aspiring data scientists? I went through the study guide and read some of the recommended materials. I dedicated a little time to the ecosystem tools (e.g., Hive, Pig), but spending a few days with each�perhaps as part of Cloudera�s Data Analyst Training�instead of a few hours would have helped more. I�d also recommend becoming more familiar with machine learning and recommender systems. Cloudera offers an Introduction to Data Science course, and Coursera offers a few basic on-demand classes, as well. My advice is to work your way through the study guide in detail, including the parts that don�t seem important at first. Spend a few days with each tool in the Hadoop ecosystem, especially Mahout. Expect to put some serious thought and effort into the project. At least for me, it was not a simple application of APIs. Since becoming a CCP:DS in November 2013, what has changed in your career and/or in your life? It helped me get data science work! I�ve acquired a Big Data perspective on problem solving: Big Data allows a data scientist to sample the rare corner cases and the rare data glitches. It provides more accurate statistics that enable training more complex models. The neural networks that never made it out of the lab in the 1990s are now practical solutions. Also, my Kaggle score improved. Why should aspiring data scientists consider taking CCP:DS? The business world�s understanding of who and what a data scientist is remains fuzzy. CCP:DS goes a long way towards removing that ambiguity. Being associated with Cloudera earns instant respect, as well. Ultimately, if you are applying for a job in a hot but ill-defined area like data science, having a certification that is relevant, recognized, and verifiable makes it a no-brainer for a busy human resources person or hiring manager to move you on to the next round of interviews. Because the exam is based on real-world challenges and is fully vetted by some of the world�s top experts, the certification does the hard work of pre-evaluating candidates against the multiple highly technical areas that would otherwise be difficult to qualify. Further reading: Read more about the first class of CCP: Data Scientists Get hands-on experience with the Cloudera Data Scientist Challenge Solution Kit</snippet></document><document id="18"><title>Where to Find Cloudera Tech Talks (Through June 2014)</title><url>http://blog.cloudera.com/blog/2014/03/where-to-find-cloudera-tech-talks-through-june-2014/</url><snippet>Find Cloudera tech talks in Amsterdam, Boston, Berlin, Sao Paulo, Singapore, Zurich, and other cities across Europe and the US during the next calendar quarter. Below please find our regularly scheduled quarterly update about where to find tech talks by Cloudera employees – this time, for the second calendar quarter of 2014 (April through June). Note that this list will be continually curated during the period; complete logistical information may not be available yet. And remember, many of these talks are in “free” venues (no cost of entry). As always, we’re standing by to assist your meetup by providing speakers, sponsorships, and schwag! Date City Venue Speaker(s) March 31-April 2 Boston Big Data TechCon Mark Grover on app architectures for Hadoop, Prasad Mujumdar on Hadoop security, Jonathan Seidman on Data Infrastructure for Hadoop April 1 Santa Clara, Calif. Percona Live Gwen Shapira on MySQL-Hadoop integration April 1 Amsterdam Amsterdam HUG Doug Cutting on balancing data collection with privacy; Andrew Wang on HDFS caching April 2 Phoenix, Ariz. Phoenix HUG Kamal Maheshwari on SQL-on-Hadoop April 2-3 Amsterdam Hadoop Summit Andrew Wang &amp; Colin McCabe on HDFS caching, Kate Ting on Hadoop config best practices, Lars George on HBase 0.98 features April 2-3 Atlanta, Ga. Great Wide Open Eli Collins on Building a Data-Driven Application on an Open Source Platform April 4 Leuven, Belgium BigData.be Kate Ting on Apache Sqoop April 7 Zurich Swiss Big Data User Group Marcel Kornacker on Hadoop Data Warehousing with Impala April 8 Munich HUG Munich Marcel Kornacker on Hadoop Data Warehousing with Impala April 7-9 Denver, Colo. ApacheCon NA Jarcec Cecho and Abe Elmahrek on Apache Sqoop, Colin McCabe on HDFS caching, Sean Mackrory on Linux packaging, Mark Miller on Solr+Hadoop, Hari Shreedharan on Apache Flume, Xuefu Zhang on Apache Sentry April 7-11 Las Vegas IOUG Collaborate Gwen Shapira on Impala, Jayant Shekhar on Building Recommender Systems April 9 San Francisco (at Cloudera) San Francisco HUG Xuefu Zhang and Srayva Tirukkovalur on Apache Sentry/Hadoop security April 9-11 Sao Paulo, Brazil QCon Sao Paulo Todd Lipcon on the Hadoop ecosystem, Aaron Myers on Hadoop security April 10 London HUG UK Kate Ting on Sqoop, Jon Hsieh on HBase, Marcel Kornacker on Hadoop Data Warehousing with Hadoop April 10 New York, NY NYC Data Science Josh Wills on credit-risk modeling April 10 Singapore Hadoop SG Enrico Berti on Hue April 11 New York, NY MLconf Josh Wills on data science tools Apri 12 Sao Paulo, Brazil SouJava/Brazil HUG Todd Lipcon and Aaron Myers on Hadoop ecosystem April 14 Washington DC Hadoop-DC Michael Ridley on Apache Sentry/Hadoop security April 16 Sunnyvale, Calif. Bay Area Hadoop Meetups Sravya Tirukkovalur on Apache Sentry/Hadoop security April 17 Hamburg, Germany Big Data &amp; NoSQL Meetup Hamburg Marcel Kornacker on Hadoop Data Warehousing with Impala April 22 Philadelphia PhillyDB Sean Pabba on Impala April 22 Boston Boston HUG Doug Cutting on “The Future of Data” April 23 Santa Clara, Calif. Big Data Bootcamp Justin Erickson on Impala April 23 Palo Alto, Calif. (at Cloudera) Bay Area Search Patrick Hunt on Solr+Hadoop, Greg Chanan on index-level security in Solr April 23 Boulder, Colo. Big Data Boulder Gwen Shapira on Apache Spark use cases May 1 San Francisco SF Machine Learning Sandy Ryza on machine learning with Apache Spark May 1 Menlo Park, Calif. Hadoop Talks Mark Donsky on Hadoop data governance with Cloudera Navigator May 5 San Francisco HBaseCon 2014 JD Cryans on HBase replication, Adam Warrington on using Kite SDK for HBase dev, Jon Hsieh &amp; Lars George on HBase app design patterns, Kate Ting, Kevin O’Dell &amp; Aleks Shulman on running HBase in production May 8 Brighton, UK Rittman Mead BI Masterclass Lars George leads a full-day class on Hadoop for Oracle users May 12-16 Mainz, Germany JAX Amr Awadallah on enterprise data hubs, Lars George on Solr+Hadoop May 13 San Francisco (at Cloudera) Bay Area Impala User Group Alex Behm on query optimization, Henry Robinson &amp; Matt Jacobs on RM, Uri Laserson on Python UDFs May 18 Atlanta, Ga. Rittman Mead BI Masterclass Lars George leads a full-day class on Hadoop for Oracle users May 20-21 Chicago GOTO Chicago Eva Andreasson on Hadoop use cases May 21-22 Bloomfield, Colo. Glue Conference 2014 Mark Grover on Impala May 25 Berlin Berlin Buzzwords 2014 Mark Miller on Solr+Hadoop, Wolfgang Hoschek on Morphlines June 3-5 San Jose, Calif. Hadoop Summit San Jose Joey Echeverria on Hadoop security, Daniel Templeton &amp; Aaron Myers on Hadoop puzzlers, Alex Moundalexis on Hadoop cluster performance tuning, and Nong Li (co-speaker) on Parquet, Yanpei Chen &amp; Govind Kamat on measuring HBase performance June 11-13 New York, NY QCon New York Jeff Hammerbacher on data science at Mount Sinai, Marcel Kornacker on Hadoop Data Warehousing with Impala June 12 East Hyattsville, Md. Accumulo Summit TBD; agenda not announced Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="19"><title>Letting It Flow with Spark Streaming</title><url>http://blog.cloudera.com/blog/2014/03/letting-it-flow-with-spark-streaming/</url><snippet>Our thanks to Russell Cardullo and Michael Ruggiero, Data Infrastructure Engineers at Sharethrough, for the guest post below about its use case for Spark Streaming. At Sharethrough, which offers an advertising exchange for delivering in-feed ads, we�ve been running on CDH for the past three years (after migrating from Amazon EMR), primarily for ETL. With the launch of our exchange platform in early 2013 and our desire to optimize content distribution in real time, our needs changed, yet CDH remains an important part of our infrastructure. In mid-2013, we began to examine stream-based approaches to accessing click-stream data from our pipeline. We asked ourselves: Rather than �warm up our cold path� by running those larger batches more frequently, can we give our developers a programmatic model and framework optimized for incremental, small batch processing, yet continue to rely on the Cloudera platform? Ideally, our engineering team focuses on the data itself, rather than worrying about details like consistency of state across the pipeline or fault recovery. Spark (and Spark Streaming) Apache Spark�is a fast and general framework for large-scale data processing, with a programming model that supports building applications that would be more complex or less feasible using conventional MapReduce. (Spark ships inside Cloudera Enterprise 5, and is already supported for use with CDH 4.4 and later.) With an in-memory persistent storage abstraction, Spark supports complete MapReduce functionality without the long execution times required by things like data replication, disk I/O, and serialization. Because Spark Streaming shares the same API as Spark�s batch and interactive modes, we now use Spark Streaming to aggregate business-critical data in real time. A consistent API means that we can develop and test locally in the less complex batch mode and have that job work seamlessly in production streaming. For example, we can now optimize bidding in real time, using the entire dataset for that campaign without waiting for our less frequently run ETL flows to complete. We are also able to perform real-time experiments and measure results as they come in. Before and After Our batch-processing system looks like this: Apache Flume writes out files based on optimal HDFS block size (64MB) to hourly buckets. MapReduce (Scalding) jobs are scheduled N times per day. Apache Sqoop moves results into the data warehouse. Latency is ~1 hour behind, plus Hadoop processing time. �Sharethrough’s former batch-processing dataflow For our particular use case, this batch-processing workflow wouldn�t provide access to performance data while the results of those calculations would still be valuable. For example, knowing that a client�s optimized content performance is 4.2 percent an hour after their daily budget is spent, means our advertisers aren�t getting their money�s worth, and our publishers aren�t seeing the fill they need. Even when the batch jobs take minutes, a spike in traffic could slow down a given batch job, causing it to “bump into” newly launched jobs. For these use cases, a streaming dataflow is the viable solution: Flume writes out clickstream data to HDFS. Spark reads from HDFS at batch sizes of five seconds. Output to a key-value store, updating our predictive modeling. Sharethrough’s new Spark Streaming-based dataflow In this new model, our latency is only Spark processing time and the time it takes Flume to transmit files to HDFS? in practice, this works out to be about five seconds. On the Journey When we began using Spark Streaming, we shipped quickly with minimal fuss. To get the most out of our new streaming jobs, we quickly adjusted to the Spark programming model. Here are some things we discovered along the way: The profile of a 24 x 7 streaming app is different than an hourly batch job — you may need finer-grained alerting and more patience with repeated errors. And with a streaming application, good exception handling is your friend. (Be prepared to answer questions like: �What if the Spark receiver is unavailable? Should the application retry? Should it forget data that was lost? Should it alert you?�) Take time to validate output against the input. A stateful job that, for example, keeps a count of clicks, may return results you didn�t expect in testing. Confirm that supporting objects are being serialized. The Scala DSL makes it easy to close over a non-serializable variable or reference. In our case, a GeoCoder object was not getting serialized and our app became very slow; it had to return to the driver program for the original, non-distributed object. The output of your Spark Streaming job is only as reliable as the queue that feeds Spark. If the producing queue drops, say, 1 percent of messages, you may need a periodic reconciliation strategy (such as merging your lossy �hot path� with �cold path� persistent data). For these kinds of merges, the monoid abstraction can be helpful when you need certainty that associative calculations (counts, for example) are accurate and reliable. For more on this, see merge-able stores like Twitter�s Storehaus or Oscar Boykin�s �Algebra for Analytics“. Conclusion Sharethrough Engineering intends to do a lot more with Spark Streaming. Our engineers can interactively craft an application, test it in batch, move it into streaming and it just works. We�d encourage others interested in unlocking real-time processing to look at Spark Streaming. Because of the concise Spark API, engineers comfortable with MapReduce can build streaming applications today without having to learn a completely new programming model. Spark Streaming equips your organization with the kind of insights only available from up-to-the-minute data, either in the form of machine-learning algorithms or real-time dashboards: It�s up to you! Spark Summit 2014 is coming (June 30 – July 2)! Register here to get 20% off the regular conference price.</snippet></document><document id="20"><title>How-to: Use Parquet with Impala, Hive, Pig, and MapReduce</title><url>http://blog.cloudera.com/blog/2014/03/how-to-use-parquet-with-impala-hive-pig-mapreduce/</url><snippet>The CDH software stack lets you use your tool of choice with the Parquet file format – - offering the benefits of columnar storage at each phase of data processing.� An open source project co-founded by Twitter and Cloudera, Parquet was designed from the ground up as a state-of-the-art, general-purpose, columnar file format for the Apache Hadoop ecosystem. In particular, Parquet has several features that make it highly suited to use with Cloudera Impala for data warehouse-style operations: Columnar storage layout: A query can examine and perform calculations on all values for a column while reading only a small fraction of the data from a data file or table. Flexible compression options: The data can be compressed with any of several codecs. Different data files can be compressed differently. The compression is transparent to applications that read the data files. Innovative encoding schemes: Sequences of identical, similar, or related data values can be represented in ways that save disk space and memory, yet require little effort to decode. The encoding schemes provide an extra level of space savings beyond the overall compression for each data file. Large file size: The layout of Parquet data files is optimized for queries that process large volumes of data, with individual files in the multi-megabyte or even gigabyte range. Impala can create Parquet tables, insert data into them, convert data from other file formats to Parquet, and then perform SQL queries on the resulting data files. Parquet tables created by Impala can be accessed by Apache Hive, and vice versa. That said, the CDH software stack lets you use the tool of your choice with the Parquet file format, for each phase of data processing. For example, you can read and write Parquet files using Apache Pig and MapReduce jobs. You can convert, transform, and query Parquet tables through Impala and Hive. And you can interchange data files between all of those components — including ones external to CDH, such as Cascading and Apache Tajo. In this blog post, you will learn the most important principles involved. Using Parquet Tables with Impala Impala can create tables that use Parquet data files; insert data into those tables, converting the data into Parquet format; and query Parquet data files produced by Impala or by other components. The only syntax required is the�STORED AS PARQUET�clause on the�CREATE TABLE�statement. After that, all�SELECT,�INSERT, and other statements recognize the Parquet format automatically. For example, a session in the�impala-shell�interpreter might look as follows: [localhost:21000] &amp;gt; create table parquet_table (x int, y string) stored as parquet;
[localhost:21000] &amp;gt; insert into parquet_table select x, y from some_other_table;
Inserted 50000000 rows in 33.52s
[localhost:21000] &amp;gt; select y from parquet_table where x between 70 and 100;      Once you create a Parquet table this way in Impala, you can query it or insert into it through either Impala or Apache Hive. Remember that Parquet format is optimized for working with large data files, typically 1GB each. Avoid using the�INSERT ... VALUES�syntax, or partitioning the table at too granular a level, if that would produce a large number of small files that cannot take advantage of the Parquet optimizations for large data chunks. Inserting data into a partitioned Impala table can be a memory-intensive operation, because each data file requires a 1GB memory buffer to hold the data before being written. Such inserts can also exceed HDFS limits on simultaneous open files, because each node could potentially write to a separate data file for each partition, all at the same time. Consider splitting up such insert operations into one�INSERT�statement per partition. For complete instructions and examples, see�the Parquet section in the Impala documentation. Using Parquet Tables in Hive To create a table named�PARQUET_TABLE�that uses the Parquet format, you would use a command like the following, substituting your own table name, column names, and data types: hive&gt; create table parquet_table_name (x INT, y STRING)
  ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'
  STORED AS
    INPUTFORMAT "parquet.hive.DeprecatedParquetInputFormat"
    OUTPUTFORMAT "parquet.hive.DeprecatedParquetOutputFormat";
      Note: Once you create a Parquet table this way in Hive, you can query it or insert into it through either Impala or Hive. Before the first time you access a newly created Hive table through Impala, issue a one-time INVALIDATE METADATA�statement in the�impala-shell�interpreter to make Impala aware of the new table. If the table will be populated with data files generated outside of Impala and Hive, it is often useful to create the table as an external table pointing to the location where the files will be created: hive&gt; create external table parquet_table_name (x INT, y STRING)
  ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'
  STORED AS
    INPUTFORMAT "parquet.hive.DeprecatedParquetInputFormat"
    OUTPUTFORMAT "parquet.hive.DeprecatedParquetOutputFormat"
    LOCATION '/test-warehouse/tinytable';
      To populate the table with an�INSERT�statement, and to read the table with a�SELECT�statement, see the�Impala documentation�for Parquet. Select the compression to use when writing data with the�parquet.compression�property, for example:  set parquet.compression=GZIP;
 INSERT OVERWRITE TABLE tinytable SELECT * FROM texttable;   The valid options for compression are: UNCOMPRESSED GZIP SNAPPY Using Parquet Files in Pig Reading Parquet Files in Pig Assuming the external table was created and populated with Impala or Hive as described above, the Pig instruction to read the data is: grunt&gt; A = LOAD '/test-warehouse/tinytable' USING parquet.pig.ParquetLoader AS (x: int, y int);   Writing Parquet Files in Pig Create and populate a Parquet file with the�ParquetStorer�class: grunt&gt; store A into '/test-warehouse/tinytable' USING parquet.pig.ParquetStorer;   There are three compression options:�uncompressed,�snappy, and�gzip. The default is�snappy. You can specify one of them once before the first store instruction in a Pig script: SET parquet.compression gzip;   Note that with CDH 4.5, you must add Thrift to Pig’s additional JAR files: export PIG_OPTS="-Dpig.additional.jars=$THRIFTJAR"   You can find Thrift as follows: if [ -e /opt/cloudera/parcels/CDH ] ; then
  CDH_BASE=/opt/cloudera/parcels/CDH
else
  CDH_BASE=/usr
fi
THRIFTJAR=`ls -l $CDH_BASE/lib/hive/lib/libthrift*jar | awk '{print $9}' | head -1`
   To use a Pig action involving Parquet files with Apache Oozie, add Thrift to the Oozie sharelib: sudo -u oozie hdfs dfs -put $THRIFTJAR share/lib/pig   Using Parquet Files in MapReduce MapReduce needs thrift in its�CLASSPATH�and in�libjars�to access Parquet files. It also needs�parquet-format�in�libjars. Perform the following setup before running MapReduce jobs that access Parquet data files: if [ -e /opt/cloudera/parcels/CDH ] ; then
    CDH_BASE=/opt/cloudera/parcels/CDH
else
    CDH_BASE=/usr
fi
THRIFTJAR=`ls -l $CDH_BASE/lib/hive/lib/libthrift*jar | awk '{print $9}' | head -1`
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$THRIFTJAR
export LIBJARS=`echo "$CLASSPATH" | awk 'BEGIN { RS = ":" } { print }' | grep parquet-format | tail -1`
export LIBJARS=$LIBJARS,$THRIFTJAR

hadoop jar my-parquet-mr.jar -libjars $LIBJARS
   Reading Parquet Files in MapReduce Taking advantage of the�Example�helper classes in the Parquet JAR files, a simple map-only MapReduce job that reads Parquet files can use the�ExampleInputFormat�class and the�Group�value class. There is nothing special about the reduce phase when using Parquet files; the following example demonstrates how to read a Parquet file in a MapReduce job. (Lines pertaining to Parquet are highlighted.) import static java.lang.Thread.sleep;
import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import parquet.Log;
import parquet.example.data.Group;
import parquet.hadoop.example.ExampleInputFormat;

public class TestReadParquet extends Configured
  implements Tool {
  private static final Log LOG =
  Log.getLog(TestReadParquet.class);

    /*
     * Read a Parquet record
     */
    public static class MyMap extends
      Mapper {

      @Override
      public void map(LongWritable key, Group value, Context context) throws IOException, InterruptedException {
          NullWritable outKey = NullWritable.get();
          String outputRecord = "";
          // Get the schema and field values of the record
          String inputRecord = value.toString();
          // Process the value, create an output record
          // ...
          context.write(outKey, new Text(outputRecord));
      }
  }

  public int run(String[] args) throws Exception {

    Job job = new Job(getConf());

    job.setJarByClass(getClass());
    job.setJobName(getClass().getName());
    job.setMapOutputKeyClass(LongWritable.class);
    job.setMapOutputValueClass(Text.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);
    job.setMapperClass(MyMap.class);
    job.setNumReduceTasks(0);

    job.setInputFormatClass(ExampleInputFormat.class);
    job.setOutputFormatClass(TextOutputFormat.class);

    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));

    job.waitForCompletion(true);
    return 0;
  }

  public static void main(String[] args) throws Exception {
    try {
      int res = ToolRunner.run(new Configuration(), new TestReadParquet(), args);
      System.exit(res);
    } catch (Exception e) {
      e.printStackTrace();
      System.exit(255);
    }
  }
}
   Writing Parquet Files in MapReduce When writing Parquet files you will need to provide a schema. The schema can be specified in the run method of the job before submitting it; for example: ...
import parquet.Log;
import parquet.example.data.Group;
import parquet.hadoop.example.GroupWriteSupport;
import parquet.hadoop.example.ExampleInputFormat;
import parquet.hadoop.example.ExampleOutputFormat;
import parquet.hadoop.metadata.CompressionCodecName;
import parquet.hadoop.ParquetFileReader;
import parquet.hadoop.metadata.ParquetMetadata;
import parquet.schema.MessageType;
import parquet.schema.MessageTypeParser;
import parquet.schema.Type;
...
public int run(String[] args) throws Exception {
...

  String writeSchema = "message example {n" +
  "required int32 x;n" +
  "required int32 y;n" +
  "}";
  ExampleOutputFormat.setSchema(
    job,
    MessageTypeParser.parseMessageType(writeSchema));

  job.submit();
    or it can be extracted from the input file(s) if they are in Parquet format: import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.LocatedFileStatus;
import org.apache.hadoop.fs.RemoteIterator;
...

public int run(String[]
  args) throws Exception {
...

String inputFile = args[0];
  Path parquetFilePath = null;
  // Find a file in case a directory was passed

  RemoteIterator it = FileSystem.get(getConf()).listFiles(new Path(inputFile), true);
  while(it.hasNext()) {
      FileStatus fs = it.next();

    if(fs.isFile()) {
      parquetFilePath = fs.getPath();
      break;
    }
  }
  if(parquetFilePath == null) {
    LOG.error("No file found for " + inputFile);
    return 1;
  }
  ParquetMetadata readFooter =
    ParquetFileReader.readFooter(getConf(), parquetFilePath);
  MessageType schema =
    readFooter.getFileMetaData().getSchema();
  GroupWriteSupport.setSchema(schema, getConf());

  job.submit();
    Records can then be written in the mapper by composing a Group as value using the Example classes and no key: protected void map(LongWritable key, Text value,
  Mapper.Context context)
  throws java.io.IOException, InterruptedException {
    int x;
    int y;
    // Extract the desired output values from the input text
    //
    Group group = factory.newGroup()
      .append("x", x)
      .append("y", y);
    context.write(null, group);
  }
}
   You can set ompression before submitting the job with: ExampleOutputFormat.setCompression(job, codec);   …using one of the following codec: CompressionCodecName.UNCOMPRESSED CompressionCodecName.SNAPPY CompressionCodecName.GZIP Parquet File Interoperability Impala has included Parquet support from the beginning, using its own high-performance code written in C++ to read and write the Parquet files. The Parquet JARs for use with Hive, Pig, and MapReduce are available with CDH 4.5 and higher. Using the Java-based Parquet implementation on a CDH release prior to CDH 4.5 is not supported. A Parquet table created by Hive can typically be accessed by Impala 1.1.1 and higher with no changes, and vice versa. Prior to Impala 1.1.1, when Hive support for Parquet was not available, Impala wrote a dummy SerDes class name into each data file. These older Impala data files require a one-time�ALTER TABLE�statement to update the metadata for the SerDes class name before they can be used with Hive. (See�the Impala Release Notes�for details.) A Parquet file written by Hive, Impala, Pig, or MapReduce can be read by any of the others. Different defaults for file and block sizes, compression and encoding settings, and so on might cause performance differences depending on which component writes or reads the data files. For example, Impala typically sets the HDFS block size to 1GB and divides the data files into 1GB chunks, so that each I/O request reads an entire data file. There may be limitations in a particular release. The following are known limitations in CDH 4: The�TIMESTAMP�data type in Parquet files is not supported in Hive, Pig, or MapReduce in CDH 4. Attempting to read a Parquet table created with Impala that includes a�TIMESTAMP�column will fail. At the time of writing, Parquet had not been tested with HCatalog. Without HCatalog, Pig cannot correctly read dynamically partitioned tables; that is true for all file formats. Currently, Impala does not support table columns using nested data types or composite data types such as maps, structs, or arrays. Any Parquet data files that include such types cannot be queried through Impala. Conclusion You can find full examples of Java code at the Cloudera�Parquet examples�Github repository: The�TestReadParquet.java�example demonstrates the�”identity”�transform. It reads any Parquet data file and writes a new file with exactly the same content. The�TestReadWriteParquet.java�example reads a Parquet data file, and produces a new text file in CSV format with the same content. John Russell is a technical writer at Cloudera and the author of the O’Reilly e-book, Cloudera Impala.</snippet></document><document id="21"><title>Sneak Preview: HBaseCon 2014 General Session</title><url>http://blog.cloudera.com/blog/2014/03/sneak-preview-hbasecon-2014-general-session/</url><snippet>The HBaseCon 2014 General Session – with keynotes by Facebook, Google, and Salesforce.com engineers – is arguably the best ever. HBaseCon 2014 (May 5, 2014 in San Francisco) is coming very, very soon. Over the next few weeks, as I did for last year’s conference, I’ll be bringing you sneak previews of session content (across Operations, Features &amp; Internals, Ecosystem, and Case Studies tracks) accepted by the Program Committee. Before we get to the session tracks, however, first let’s take a tour of the morning content before the tracks begin that day. I don’t think it’s an exaggeration when I say that the HBaseCon 2014 General Session is the most interesting one ever, but you be the judge. See below, in chronological order: “Welcome” Messages Michael Stack and Michael Olson (Cloudera) The host and MC of HBaseCon, HBase PMC chair Michael “Good-on-you” Stack, welcomes the Apache HBase community to the conference and previews the day ahead. Next, Michael Olson, Cloudera’s chief strategy officer and a Bit of a Big Deal in the history of the key/value data store in general, briefly explains why Cloudera heavily invests in the HBase community.� “Bigtable at Google: Yesterday, Today, and Tomorrow” Avtandil Garakanidze and Carter Page (Google) Bigtable is the world’s largest multi-purpose database, supporting 90 percent of Google’s applications around the world. This talk provides a brief overview of Bigtable evolution since it was originally described in an OSDI ’06 paper, its current use cases at Google, and future directions. � “HydraBase: Facebook’s Highly Available and Strongly Consistent Storage Service Based on Replicated HBase Instances” Liyin Tang (Facebook) HBase powers multiple mission-critical online applications at Facebook. This talk will cover the design of HydraBase (including the replication protocol), an analysis of a failure scenario, and a plan for contributions to the HBase trunk. “HBase @ Salesforce.com” Lars Hofhansl (Salesforce.com) Lars will explain how Salesforce.com’s scalability requirements led it to HBase and the multiple use cases for HBase there today. You’ll also learn how Salesforce.com works with the HBase community, and get a detailed look into its operational environment.� Furthermore, as optional pre-conference prep for attendees who are new to HBase, Jesse Anderson (Cloudera University) talk will offer a brief Cliff’s Notes-level talk before the General Session that covers HBase architecture, API, and schema design.� Interested yet? If not, next week, we’ll offer a preview of the Operations track. Thank you to our sponsors�– Continuuity, Hortonworks, Intel, LSI, MapR, Salesforce.com, Splice Machine,�WibiData (Gold); Facebook, Pepperdata (Silver); ASF (Community); O’Reilly Media, The Hive, NoSQL Weekly (Media) — without which HBaseCon would be impossible!</snippet></document><document id="22"><title>How-to: Implement Role-based Security in Impala using Apache Sentry</title><url>http://blog.cloudera.com/blog/2014/03/how-to-implement-role-based-security-in-impala-using-apache-sentry/</url><snippet>This quick demo illustrates how easy it is to implement role-based access and control in Impala using Sentry. Apache Sentry (incubating) is the Apache Hadoop ecosystem tool for role-based access control (RBAC). In this how-to, I will demonstrate how to implement Sentry for RBAC in Impala. I feel this introduction is best motivated by a use case. Data warehouse optimization is one of the most common Hadoop use cases. After migrating data transformation workloads to Hadoop, customers typically want to provide self-service business intelligence access on Hadoop. Self-service BI results in many distinct users logging in and executing queries each under their own user id. When end users start using the cluster, fine-grained authorization is a requirement to satisfy internal controls and governmental regulations. Sentry was initially created originally for this use case. I won’t go into detail here about why fined-grained authorization is useful; my colleague Shreepadma Venugopalan covered this topic in her post “With Sentry, Cloudera Fills Hadoop’s Enterprise Security Gap.” Furthermore, Sravya Tirukkovalur wrote a post about using Sentry with Apache Hive (“How-to: Get Started with Sentry in Hive”). Sentry and Impala work together in a similar fashion as Sentry and Hive. In fact, since the policy file syntax is identical, users who use both Hive and Impala are encouraged to share the same policy file. The two systems have different architectures resulting in some divergence in how they interact with Sentry. For example, Hive is typically configured with a single or small number of HiveServer2 instances. Impala works differently as each Impala daemon accepts queries, one of the many design features which helps Impala scale to a large number of concurrent queries. In the Hive case, a small number of HiveServer2 instances will read the policy file from HDFS, whereas in the Impala case, each daemon will. (Since many Impala daemons will be reading the file from HDFS and the file is small, setting the replication count equal to the number of slave nodes is reasonable.) One additional difference is that while Hive reads and parses the policy file for each query, Impala checks to see if the policy file has been updated every five minutes. If you’d like to learn more about configuring Sentry, watch the video below or go straight to our documentation on Configuring Sentry and Impala Security. In the video below, we will use a policy file, shown below, which in addition to an admin role has hierarchical roles manager_role, analyst_role, and junior_analyst_role. As you can see below, the manager_role has ALL on the default database, whereas the analyst_role has ALL on the analyst1_table and SELECT on the manager1_table. The junior_analyst_role has ALL on jranalyst1_table. 
[groups]
management = manager_role
analyst = analyst_role, junior_analyst_role
jranalyst = junior_analyst_role
admin = admin_role 

[roles]
manager_role = server=server1-&gt;db=default
analyst_role = server=server1-&gt;db=default-&gt;table=analyst1_table-&gt;action=select
junior_analyst_role = server=server1-&gt;db=default-&gt;table=jranalyst1_table-&gt;action=select

# Implies everything on server1.
admin_role = server=server1
   In the demo below, I will first enable Sentry with Impala and then create and share a view of manager1_table for junior analysts that restricts their access to roles as well as columns. Conclusion You should now understand the relatively straightforward procedure of implementing RBAC in Impala using Sentry! Brock Noland is a Software Engineer at Cloudera and an Apache committer on the Crunch and Hive projects.</snippet></document><document id="23"><title>Apache ZooKeeper Resilience at Pinterest</title><url>http://blog.cloudera.com/blog/2014/03/zookeeper-resilience-at-pinterest/</url><snippet>The guest post below was originally authored by Pinterest engineer Raghavendra Prabhu and published by the Pinterest Engineering blog. Being big ZooKeeper fans, we re-publish it here for your convenience. Thanks, Pinterest! Apache ZooKeeper�is an open source distributed coordination service that’s popular for use cases like service discovery, dynamic configuration management and distributed locking. While it’s versatile and useful, it has failure modes that can be hard to prepare for and recover from, and if used for site critical functionality, can have a significant impact on site availability. It’s important to structure the usage of ZooKeeper in a way that prevents outages and data loss, so it doesn’t become a single point of failure (SPoF). Here, you’ll learn how Pinterest uses ZooKeeper, the problems we’ve dealt with, and a creative solution to benefit from ZooKeeper in a fault-tolerant and highly resilient manner. Service Discovery and Dynamic Configuration Like many large scale web sites, Pinterest’s infrastructure consists of servers that communicate with backend services composed of a number of individual servers for managing load and fault tolerance. Ideally, we’d like the configuration to reflect only the active hosts, so clients don’t need to deal with bad hosts as often. ZooKeeper provides a well known pattern to solve this problem. Each backend service host registers an�ephemeral node�in ZooKeeper in a path specific to that service. Clients can watch that path to determine the list of hosts. Since each node is ephemeral, it will automatically be removed by ZooKeeper if the host registering fails or shuts down. New hosts brought up automatically register themselves at the correct path, so clients will notice within a few seconds. Thus the list stays up to date and reflects the hosts that are active, which addresses the issues mentioned above. Another use case is any type of application configuration that needs to be updated dynamically and applied within a few seconds to multiple machines. Imagine a distributed database fronted by a service layer, referred to as ‘Data Service’. Let’s assume the database is partitioned by a user. When a user request comes in, it needs to know which database holds the information for that user. This information could be deployed statically with the application, but that has the same problems as described above with service discovery. Additionally, it’s important that database configuration changes converge quickly across all the Data Service machines. Otherwise, we wouldn’t be able to apply the update quickly. Here again, ZooKeeper is of help. The configuration can be stored in a node in ZooKeeper that all the Data Service servers watch. A command line tool or GUI can update the node, and within a few seconds, all the Data Service machines will reflect the update. Voila! ZooKeeper Failure Modes While ZooKeeper can play a useful role in a backend infrastructure stack as shown above, like all software systems, it can fail. Here are some possible reasons: Too many connections: Let’s say someone brought up a large Hadoop job that needs to communicate with some of the core Pinterest services. For service discovery, the workers need to connect to ZooKeeper. If not properly managed, this could temporarily overload the ZooKeeper hosts with a huge volume of incoming connections, causing it to get slow and partially unavailable. Too many transactions: When there’s a surge in ZooKeeper transactions, such as a large number of servers restarting in a short period and attempting to re-register themselves with ZooKeeper (a variant of the�thundering herd problem). In this case, even if the number of connections isn’t too high, the spike in transactions could take down ZooKeeper. Protocol bugs: Occasionally under high load, we’ve run into protocol bugs in ZooKeeper that result in data corruption. In this case, recovery usually involves taking down the cluster, bringing it back up from a clean slate and then restoring data from backup. Human errors: In all software systems, there’s a possibility of human error. For example, we’ve had a manual replacement of a bad ZooKeeper host unintentionally take the whole ZooKeeper quorum offline for a short time due to erroneous configuration being put in place. Network partitions: While relatively rare, network connectivity issues resulting in a network partition of the quorum hosts can result in downtime till the quorum can be restored. While site outages due to service failures are never completely unavoidable when running a web service as large and fast moving as Pinterest, there were a few reasons why ZooKeeper issues were particularly problematic: There was no easy immediate mitigation. If bad code is deployed, we can rollback immediately to resolve the issue and investigate what happened later. If a particular part of the site fails, we can disable that service temporarily to mitigate impact. There wasn’t such an option available for ZooKeeper failures. Recovery would take multiple hours in some cases, due to a combination of the thundering herd problem and having to restart from a clean slate and recover from backup. The centrality of ZooKeeper in our stack meant a pretty wide impact on the site. That is, the outages weren’t isolated to a particular part of the site or feature, but rather impacted the site as a whole. Initial Mitigation Attempts A few strategies to mitigate the problem were suggested, but would only provided limited relief. Add capacity: The simplest strategy is to tackle load issues is to add capacity. Unfortunately, this doesn’t quite work since the ZooKeeper design and protocol is such that the quorum (voting members) cannot scale out very well. In practice, we have observed that having more than 7-10 hosts tends to make write performance significantly worse. Add observers: ZooKeeper�observers�offer a solution to the scaling problem. These are non-voting members of the ZooKeeper ensemble that otherwise function like any other ZooKeeper host, i.e. can accept watches and other types of requests and proxy them back to the quorum. Adding a fleet of observers and shunting off traffic to them indeed substantially alleviated the load on the ZooKeeper cluster. However, this only helped with watches and other reads, not with writes. It partially addressed failure cause (1) mentioned in the previous section, but not the others. Use multiple ZooKeeper clusters for isolation: A mitigation we attempted was to use different ZooKeeper clusters for each use case, e.g. our deploy system uses a separate cluster from the one used for service discovery, and our�HBase�clusters each use independent ZooKeeper clusters. Again, this helped alleviate some of the problems, but wasn’t a complete solution. Fallback to static files: Another possibility is to use ZooKeeper for service discovery and configuration in normal operation, but if it fails, fallback to static host lists and configuration files. This is reasonable, but becomes problematic when keeping the static data up-to-date and accurate. This is particularly difficult for services that are auto scaled and have high churn in the host set. Another option is to fallback to a different storage system (e.g. MySQL) in case of failure, but that too is a manageability nightmare, with multiple sources of truth and another system to be provisioned to handle the full query and spike in the event of ZooKeeper failing. We also considered a couple more radical steps: Look for an alternative technology that is more robust: There are a few alternative distributed coordinator implementations available through open source, but we found them to generally be less mature than ZooKeeper and less tested in production systems at scale. We also realized these problems are not necessarily implementation specific, rather, any distributed coordinator can be prone to most of the above issues. Don’t use central coordination at all: Another radical option was to not use ZooKeeper at all, and go back to using static host lists for service discovery and an alternate way to deploy and update configuration. But doing this would mean compromising on latency of update propagation, ease of use and/or accuracy. Narrowing in on the Solution We ultimately realized that the fundamental problem here was not ZooKeeper itself. Like any service or component in our stack, ZooKeeper can fail. The problem was our�complete reliance on it for overall functioning of our site. In essence, ZooKeeper was a Single Point of Failure (SPoF) in our stack. SPoF terminology usually refers to single machines or servers, but in this case, it was a single distributed service. How could we continue to take advantage of the conveniences that ZooKeeper provides while tolerate its unavailability? The solution was actually quite simple:�decouple our applications from ZooKeeper. The drawing on the left represents how we were using ZooKeeper originally, and the one on the right shows what we decided to do instead. Applications that were consuming service information and dynamic configuration from ZooKeeper connected to it directly. They cached data in memory but otherwise relied on ZooKeeper as the source of truth for the data. If there were multiple processes running on a single machine, each would maintain a separate connection to ZooKeeper. ZooKeeper outages directly impacted the applications. Instead of this approach, we moved to a model where the application is in fact completely isolated from ZooKeeper. Instead, a daemon process running on each machine connects to ZooKeeper, establishes watches on the data it is configured to monitor, and whenever the data changes, downloads it into a file on local disk at a well known location. The application itself only consumes the data from the local file and reloads when it changes. It doesn’t need to care that ZooKeeper is involved in propagating updates. Looking at Scenarios Let’s see how this simple idea helps our applications tolerate the ZooKeeper failure modes. If ZooKeeper is down, no applications are impacted since they read data from the file on local disk. The daemons indefinitely attempt to reestablish connection to ZooKeeper. Until ZooKeeper is back up, configuration updates cannot take place. In the event of an emergency, the files can always be pushed directly to all machines manually. The key point is that there is�no availability impact whatsoever. Now let’s consider the case when ZooKeeper experiences a data corruption and there’s partial or complete data loss till data can be restored from backup. It’s possible a single piece of data in ZooKeeper (referred to as�znode) containing important configuration is wiped out temporarily. We protect against this case by building a fail-safe rule into the daemon: it rejects data that is suspiciously different from the last known good value. A znode containing configuration information entirely disappearing is obviously suspicious and would be rejected, but so would more subtle changes like the set of hosts for a service reducing suddenly by 50%. With carefully tuned thresholds, we’ve been able to effectively prevent disastrous configuration changes from propagating to the application. The daemon logs such errors and we can alert an on-call operator so the suspicious change can be investigated. One might ask, why build an external daemon? Why not implement some sort of disk backed cache and fail-safe logic directly into the application? The main reason is that our stack isn’t really homogenous: we run applications in multiple languages like Python, Java, and Go. Building a robust ZooKeeper client and fault tolerant library in each language would be a non-trivial undertaking, and worse still, painful to manage in the long run since bug fixes and updates would need to be applied to all the implementations. Instead, by building this logic into a single daemon process, we can have the application itself only perform simple file read coupled with periodic check/reload, which is trivial to implement. Another advantage of the daemon is it reduces overall connections into ZooKeeper since we only need one connection per machine rather than one per process. This is particularly a big win for our large Python service fleet, where we typically run 8 to 16 processes per machine. How about services�registering�with ZooKeeper for discovery purposes? That too can in theory be outsourced to an external daemon, with some work to ensure the daemon correctly reflects the server state in all cases. However, we ended up keeping the registration path in the application itself and instead spent some time taking care to harden the code path to make it resilient to ZooKeeper outages and equally importantly, be guaranteed to re-register after an outage. This was considered sufficient since individual machines failing to register does not typically have a large impact on the site. On the other hand, if a large group of machines suddenly lose connection to ZooKeeper thereby getting de-registered, the fail-safe rules in the daemon on the consumption side will trigger and result in a rejection of the update, thereby protecting the service and its clients. Rolling out the Final Product We implemented the daemon and modified our base machine configuration to ensure it was installed on all machine groups that needed to use ZooKeeper. The daemon operation is controlled by a configuration file. Here’s a sample configuration snippet for the daemon:  [dataservice]
 type = service_discovery
 zk_cluster = default
 zk_path = /discovery/dataservice/prod
 command = /usr/local/bin/zk_download_data.py -f /var/service
 /discovery.dataservice.prod -p /discovery/dataservice/prod
   Here we’re telling the daemon to monitor the /discovery/dataservice/prod path in the default ZooKeeper cluster for service discovery use, and when ZooKeeper notifies it of a change within that path, to run the zk_download_data script. This script reads all the children znodes of that path, retrieves the host and port information for each server and writes it to the specified file path on local disk, one host:port per line. A configuration file can contain several snippets like the above, comprising service discovery, dynamic configuration and other use cases. The format is designed to be generic enough to be able to extend to other use cases in future. Next, our client libraries that deal with configuration or service discovery were modified to use files on local disk instead of ZooKeeper as the source of data. We attempted to make this change as opaque to the application code as possible. For example, some of our Java services use Twitter’s ServerSet�libraries for service discovery. In this case, we built a ServerSet implementation that is backed by host:port combinations retrieved from a file on local disk (e.g. /var/service/discovery.dataservice.prod mentioned above). This implementation takes care of reloading data when the file content changes. Consumers of the ServerSet interface, including RPC client libraries, don’t need to be aware of the change at all. Since our services use a common set of RPC libraries, it was fairly easy to roll this change out across all our services. Similar relatively simple changes were made to our Python and other language libraries. Validating the New Design To be sure our infrastructure was resilient to ZooKeeper failure, we ran a number of test scenarios until the real thing happened. A couple of weeks after roll out, there was another ZooKeeper outage, triggered by load introduced by an unrelated bug in one of our client libraries. We were happy to see that this outage caused�no site impact, and in fact, went unnoticed till ZooKeeper monitoring alerts themselves fired. Since the outage happened late in the evening, a decision was made to do the cluster restoration the next morning. Thus�the entire site functioned normally all night despite ZooKeeper being down, thanks to our resilience changes being in place. Acknowledgements: This project was a joint effort between the infrastructure and technical operations teams at Pinterest. The core project members were Michael Fu, Raghavendra Prabhu and Yongsheng Wu, but a number of other folks across these teams provided very useful feedback and help along the way to make the project successful.</snippet></document><document id="24"><title>Inside Apache Oozie HA</title><url>http://blog.cloudera.com/blog/2014/03/inside-apache-oozie-ha/</url><snippet>Oozie’s new HA qualities help cluster operators sleep well at night. Here’s how it works. One of the big new features in CDH 5 for Apache Oozie is High Availability (HA). In designing this feature, the Oozie team at Cloudera had two main goals: 1) Don’t change the API or usage patterns, and 2) the user shouldn’t even have to know that HA is enabled. In other words, we wanted Oozie HA to be as easy and transparent as possible.� In this blog post, I’ll explain how Oozie HA works and how it achieves those goals.� What is Active-Active HA? HA can be defined as a system without non-planned downtime, even when partial failures occur.� This goal is usually achieved via redundancies and by removing single points of failure. For example, we’ve had HDFS HA for quite a while now, and, in a nutshell, it works by having two NameNodes; if the first goes down, the system “fails-over” to the second one automatically, which takes over. This setup is often called “active standby” or “hot-warm” because you have one active server and one server ready to take over if something bad happens. For Oozie, we implemented “active-active” or “hot-hot” HA, which means that both Oozie servers are active at the same time — there is no failover. In fact, you can actually have as many active Oozie servers as you want (within reason, of course!). A nice bonus of this architecture is that you get horizontal scalability for free: the Oozie service can now have more computing power.� Architecture: Database Oozie stores almost all its state (submitted jobs, workflow definitions, and so on) in a database. In fact, Oozie was designed to be stateless: if the Oozie server goes down, you don’t actually lose anything. Already-running jobs will actually continue running while the Oozie server is down; once the server comes back up, it will start any pending jobs and transition any workflows with finished actions.� This is convenient because you don’t have to worry about losing anything when an Oozie server goes down.� As a summary, Oozie maintains in-memory locks for each job to prevent multiple threads (intra-process) from trying to process the same job at the same time. With multiple Oozie servers, we have to extend these locks to distributed locks that all the Oozie servers can share (inter-process). To that end, we used Apache ZooKeeper for the distributed locks, and to interact with ZooKeeper, we used Apache Curator, which is a set of libraries that make interacting with ZooKeeper much easier (and something you’ve read about here before).� Now that we have the distributed locks, we simply have to point all the Oozie servers at the same database. The database must support multiple concurrent connections (Postgres, MySQL, or Oracle — not Apache Derby); it should also ideally be an HA database to prevent it from now becoming the single point of failure. Below is a diagram of what this would look like: Architecture: Access Usually, when you use the Oozie client, REST API, or Web UI, there’s a single address to use (http://myhost:11000/oozie, for example). But now that you have multiple Oozie servers, you have multiple addresses to which users can connect — so what happens if the one they pick goes down?� There are also many clients or tools that only support a single entry point for Oozie, such as the JobTracker. To fix this issue, you need to provide a single address that will round-robin between the Oozie servers. You can use a load balancer, a virtual IP address, or DNS round-robin for this purpose. As with the database, this setup technically needs to be HA as well.�Below is a diagram of what it would look like with a load balancer: Architecture: Log Streaming Oozie has a feature where you can stream logs for a particular job from the Oozie server to the Oozie client, REST API, or Web UI.� But each Oozie server has its own log file(s) on the local filesystem; unlike the information in the database discussed earlier, these log messages are not available to every Oozie server directly.� This could be a problem. Suppose you have two servers, A and B. B processed a workflow in which you’re interested, but when you went through the load balancer, it directed your request to A. If A were to look into its own logs, it wouldn’t find any messages for the workflow you asked about.� The solution is that A will ask B for any logs it has about the workflow. This is especially important because Oozie jobs are not assigned to a specific Oozie server. And not only can a workflow be processed by any Oozie server, but even different actions within the same workflow can be processed by any server!� So, whenever you ask an Oozie server for logs, it has to go and ask each of the other servers for their relevant logs, and collate the messages before streaming them back to the user. The diagram below will make this more clear: ZooKeeper is coordinating this effort so that the Oozie servers know about each other dynamically. The only downside with this solution for log streaming is that if an Oozie server goes down, its logs will obviously be unavailable until it comes back up. (We intend to address this issue in the future but it will likely require a major refactoring of how logs are stored and streamed.) Security Security is very important to Cloudera’s customers, so we made sure that Oozie HA works with Kerberos. The only time that the Oozie servers actually talk to each other is for the log streaming; any other “extra” communication for HA is with ZooKeeper. The log streaming works with Kerberos and/or HTTPS if configured. Furthemore, you can enable Kerberos-backed ACLs in ZooKeeper so that only the Oozie principal can read/write/edit/etc the znodes for HA.�This is especially important in preventing a malicious user or program from acquiring one of the distributed locks, which would block Oozie from ever processing the job associated with that lock, forever! Quality-of-Life Improvements Because Oozie servers can come and go dynamically, it can be difficult to tell which Oozie servers are actually currently working together.� So, we’ve added a new admin command to list the currently connected Oozie servers and their addresses: $ oozie admin -oozie http://loadbalancer-hostname:11000/oozie -servers
  hostA : http://hostA:11000/oozie
  hostB : http://hostB:11000/oozie
  hostC : http://hostC:11000/oozie   The loadbalancer-hostname in the above example would be the address of either the load balancer, virtual IP address, or DNS round-robin previously mentioned. It also turned out that Oozie HA was perhaps a little too transparent; it’s hard to tell which Oozie server processed which job at any given time. To help make it easier to tell, log messages now indicate from which server they came; for example: 2013-09-29 16:46:20,182 WARN org.apache.oozie.command.wf.ActionStartXCommand: SERVER[hostA] USER[root] GROUP[-] TOKEN[] APP[demo-wf] JOB[0000000-130925230553293-oozie-oozi-W] ACTION[0000000-130925230553293-oozie-oozi-W@streaming-node] [***0000000-130925230553293-oozie-oozi-W@streaming-node***]Action status=RUNNING
   In the above log message, you can see that there is now a “SERVER” field.� Conclusion I haven’t gone into any of the setup or configuration in this blog post because our documentation already does that a lot of detail, which you can find here.�If you plan to use Kerberos, you’ll also want to take a look at this page.� While setting up Oozie HA manually isn’t too difficult (there aren’t that many configuration properties to think about), using Cloudera Manager makes this much easier. In a few clicks, it will configure and deploy the additional Oozie servers for you, plus the ZooKeeper ensemble.� I believe we’ve accomplished our goals of making Oozie HA as easy and as transparent as possible.�From the user’s perspective, all that’s changed is the host:port to use in the Oozie client (it’s now the host:port for the load balancer); everything else is exactly the same. For cluster admins, there aresome extra setup prerequisites but nothing extraordinary, and there’s very little to configure on the Oozie end of things, especially when using Cloudera Manager.� We added the core of Oozie HA in CDH 5 Beta 1 and added Cloudera Manager support in CDH 5 Beta 2.�We’re continuing to make further improvements, bug fixes, and add new features for Oozie HA that will go into CDH 5 and later versions. For those interested in keeping up with development, or even participating in the development yourself, the list of Oozie HA JIRAs can be found on this page.� Robert Kanter is a Software Engineer at Cloudera and an Oozie Committer/PMC Member.</snippet></document><document id="25"><title>Apache Spark: A Delight for Developers</title><url>http://blog.cloudera.com/blog/2014/03/apache-spark-a-delight-for-developers/</url><snippet>Sure, Spark is fast, but it also gives developers a positive experience they won’t soon forget. Apache Spark is well known today for its performance benefits over MapReduce, as well as its versatility. However, another important benefit � the elegance of the development experience � gets less mainstream attention. In this post, you’ll learn just a few of the features in Spark that make development purely a pleasure. Language Flexibility Spark natively provides support for a variety of popular development languages. Out of the box, it supports Scala, Java, and Python, with some promising work ongoing to support R. One common element among these languages (with the temporary exception of Java, which is due for a major update imminently in the form of Java 8) is that they all provide concise ways to express operations using “closures” and lambda functions. Closures allow users to define functions in-line with the core logic of the application, thereby preserving application flow and making for tight and easy-to-read code: Closures in Python with Spark: 
lines = sc.textFile(...)
lines.filter(lambda s: "ERROR" in s).count()   Closures in Scala with Spark: 
val lines = sc.textFile(...)
lines.filter(s =&gt; s.contains("ERROR")).count()   Closures in Java with Spark: 
JavaRDD&lt;String&gt; lines = sc.textFile(...);
lines.filter(new Function&lt;String, Boolean&gt;()Â  {
   Boolean call(String s) {
       return s.contains("error");
    }
  }).count();   On the performance front, a lot of work has been done to optimize all three of these languages to run efficiently on the Spark engine. Scala is written in Scala, which runs on the JVM, so Java can run efficiently in the same JVM container. Via the smart use of Py4J, the overhead of Python accessing memory that is managed in Scala is also minimal. APIs That Match User Goals When developing in MapReduce, you are often forced to stitch together basic operations as custom Mapper/Reducer jobs because there are no built-in features to simplify this process. For that reason, many developers turn to the higher-level APIs offered by frameworks like Apache Crunch or Cascading to write their MapReduce jobs. � In contrast, Spark natively provides a rich and ever-growing library of operators. Spark APIs include functions for: cartesian cogroup collect count countByValue distinct filter flatMap fold groupByKey join map mapPartitions reduce reduceByKey sample sortByKey subtract take union and many more. In fact, there are more than 80 operators available out of the box in Spark! While many of these operations often boil down to Map/Reduce equivalent operations, the high-level API matches user intentions closely, allowing you to write much more concise code. An important note here is that while scripting frameworks like Apache Pig provide many high-level operators as well, Spark allows you to access these operators in the context of a full programming language — thus, you can use control statements, functions, and classes as you would in a typical programming environment. Automatic Parallelization of Complex Flows When constructing a complex pipeline of MapReduce jobs, the task of correctly parallelizing the sequence of jobs is left to you. Thus, a scheduler tool such as Apache Oozie is often required to carefully construct this sequence. With Spark, a whole series of individual tasks is expressed as a single program flow that is lazily evaluated so that the system has a complete picture of the execution graph. This approach allows the core scheduler to correctly map the dependencies across different stages in the application, and automatically parallelize the flow of operators without user intervention. This capability also has the property of enabling certain optimizations to the engine while reducing the burden on the application developer. Win, and win again! For example, consider the following job: 
rdd1.map(splitlines).filter("ERROR")
rdd2.map(splitlines).groupBy(key)
rdd2.join(rdd1, key).take(10)
   This simple application expresses a complex flow of six stages. But the actual flow is completely hidden from the user — the system automatically determines the correct parallelization across stages and constructs the graph correctly. In contrast, alternate engines would require you to manually construct the entire graph as well as indicate the proper parallelization. Interactive Shell Spark also lets you access your datasets through a simple yet specialized Spark shell for Scala and Python. With the Spark shell, developers and users can get started accessing their data and manipulating datasets without the full effort of writing an end-to-end application. Exploring terabytes of data without compiling a single line of code means you can understand your application flow by literally test-driving your program before you write it up. Just open up a shell, type a few commands, and you’re off to the races! Performance While this post has focused on how Spark not only improves performance but also programmability, we should’t ignore one of the best ways to make developers more efficient: performance! Developers often have to run applications many times over the development cycle, working with subsets of data as well as full data sets to repeatedly follow the develop/test/debug cycle. In a Big Data context, each of these cycles can be very onerous, with each test cycle, for example, being hours long. While there are various ways systems to alleviate this problem, one of the best is to simply run your program fast. Thanks to the performance benefits of Spark, the development lifecycle can be materially shortened merely due to the fact that the test/debug cycles are much shorter. And your end-users will love you too! Example: WordCount To give you a sense of the practical impact of these benefits in a concrete example, the following two snippets of code reflect a WordCount implementation in MapReduce versus one in Spark. The difference is self-explanatory: WordCount the MapReduce way: 
public static class WordCountMapClass extends MapReduceBase
public static class WordCountMapClass extends MapReduceBase
  implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
  private final static IntWritable one = new IntWritable(1);
  private Text word = new Text();
  public void map(LongWritable key, Text value,
                  OutputCollector&lt;Text, IntWritable&gt; output,
                  Reporter reporter) throws IOException {
    String line = value.toString();
    StringTokenizer itr = new StringTokenizer(line);
    while (itr.hasMoreTokens()) {
      word.set(itr.nextToken());
      output.collect(word, one);
    }
  }
}
public static class WorkdCountReduce extends MapReduceBase
  implements Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
  public void reduce(Text key, Iterator&lt;IntWritable&gt; values,
                     OutputCollector&lt;Text, IntWritable&gt; output,
                     Reporter reporter) throws IOException {
    int sum = 0;
    while (values.hasNext()) {
      sum += values.next().get();
    }
    output.collect(key, new IntWritable(sum));
  }
}
   WordCount the Spark way: 
val spark = new SparkContext(master, appName, [sparkHome], [jars])
val file = spark.textFile("hdfs://...")
val counts = file.flatMap(line =&gt; line.split(" "))
                 .map(word =&gt; (word, 1))
                 .reduceByKey(_ + _)
  counts.saveAsTextFile("hdfs://...")
   One cantankerous data scientist at Cloudera, Uri Laserson, wrote his first PySpark job recently after several years of tussling with raw MapReduce. Two days into Spark, he declared his intent to never write another MapReduce job again. Uri, we got your back, buddy: Spark will ship inside CDH 5. Further Reading: Spark Quick Start Spark API for Java/Scala Spark API for Python Jai Ranganathan is Director of Product at Cloudera. Matei Zaharia is CTO of Databricks. Spark Summit 2014 is coming (June 30 – July 2)! Register here to get 20% off the regular conference price.</snippet></document><document id="26"><title>The Truth About MapReduce Performance on SSDs</title><url>http://blog.cloudera.com/blog/2014/03/the-truth-about-mapreduce-performance-on-ssds/</url><snippet>Cost-per-performance, not cost-per-capacity, turns out to be the better metric for evaluating the true value of SSDs. In the Big Data ecosystem, solid-state drives (SSDs) are increasingly considered a viable, higher-performance alternative to rotational hard-disk drives (HDDs). However, few results from actual testing are available to the public. Recently, Cloudera engineers did such a study based on a combination of SSDs and HDDs, with the goal of determining to what extent SSDs accelerate different MapReduce workloads, as well as the optimal configurations for getting the best performance on each workload. We considered two scenarios: When setting up a new cluster — whether tradeoffs exist between SSDs or HDDs of the same aggregate bandwidth When upgrading an HDDs-only cluster — whether adding SSDs or HDDs offers greater benefit And here is a preview of our findings, to be explained in detail below: For a new cluster, SSDs deliver up to 70 percent higher MapReduce performance compared to HDDs of equal aggregate IO bandwidth. For an existing HDD cluster, adding SSDs lead to more gains if configured properly. On average, SSDs show 2.5x higher cost-per-performance, a gap far narrower than the 50x difference in cost-per-capacity. These results are based on running MapReduce v2 (MR2) on YARN in Cloudera Enterprise 5 Beta 1, on physical clusters, comparing HDDs with PCI Express (PCIe) SSDs. This work builds on recognized past studies in the Apache Hadoop ecosystem on SSD performance that used memory to approximate SSD behavior (�Hadoop and Solid State Drives�, by Dhruba Borthakur) and simulated SSD performance as a tiered cache for Apache HBase (�Analysis of HDFS under HBase: A Facebook Messages Case Study�, presented by Tyler Harter et al at FAST ’14). Background on MapReduce Dataflow A MapReduce job proceeds as indicated in the diagram below. The data movements in different stages generate roughly two kinds of IO patterns: Source: Hadoop – The Definitive Guide, by Tom White HDFS – large, sequential reads and writes: The job reads input splits from HDFS initially, and writes output partitions to HDFS at the end. Each task (dotted box) performs relatively long sequential IO of 100s of MBs. When multiple tasks are scheduled on the same machine, they can access the disks on the machine in parallel, with each task accessing its own input split or output partition. Thus, an HDD-only configuration of 11 disks of 120MBps each can potentially achieve HDFS read/write bandwidth comparable to a SSD drive of 1.3GBps. Shuffle intermediate data – smaller, more random reads and writes: MapReduce partitions each map output across all the reduce tasks. This leads to significantly lower IO size. For example, suppose a job has map tasks that each produces 1GB of output. When divided among, say, 1,000 reduce tasks, each reduce task fetches only 1MB. Analysis of our customer traces indicate that many deployments indeed have a per-reduce shuffle granularity of just a few MBs (and sometimes less). We used the Linux collectl tool to verify that these two behaviors indeed hold for the MapReduce jobs used in our tests (see appendix). Based on the MapReduce dataflow and storage medium characteristics, we expect that: SSDs improve performance of shuffle-heavy jobs. SSDs and HDDs perform similarly for HDFS-read-heavy and HDFS-write-heavy jobs. For hybrid clusters (both SSDs and HDDs), using SSDs for intermediate shuffle data leads to significant performance gains. The remainder of the post describes our experimental setup and results. Setup Hardware We used PCIe SSDs with 1.3TB capacity with a list price of US$14,000 each, and SATA HDDs with 2TB capacity with a list price of US$400 each. Each storage device is mounted with the Linux ext4 file system, with default options and 4KB block size. Otherwise, the machines are Intel Xeon 2-socket, 8-core, 16-thread systems, with 10Gbps Ethernet and 48GB RAM. They are connected as a single rack cluster. To get a sense of the user-visible storage bandwidth without HDFS and MapReduce, we measured the duration of copying a 100GB file to each storage device. This test indicates the SSDs can do roughly 1.3GBps sequential read and write, while the HDDs have roughly 120MBps sequential read and write. We evaluate the following storage configurations: The SSD and HDD-11 setups allow us to compare SSDs versus HDDs on an equal-bandwidth basis. The HDD-6 setup serves as a baseline of IO-constrained cluster. The HDD-6, HDD-11, and Hybrid setups allow us to investigate the effects of adding either HDDs or SSDs to an existing cluster. Workload We run the following MapReduce jobs. Each is either a common benchmark, or a job constructed specifically to isolate a stage of the MapReduce IO pipeline. More details: Each job is set to shuffle, read, write, or sort 33GB of data per node. Where possible, each job runs with either a single wave of map tasks (TeraGen, Shuffle, HDFS Data Write), or a single wave of reduce tasks (TeraSort, WordCount, Shuffle). We record average and standard deviation of job duration from five runs. We clear the OS buffer cache on all machines between each measurement. We use collectl to track IO size, counts, bytes, merges to each storage device, as well as network and CPU utilization. We use default MapReduce configurations in CDH 5 Beta 1, aside from map output compression, discussed below. Note that the jobs here are IO-heavy jobs selected and sized specifically to compare two different storage media. In general, real-world customer workloads have a variety of sizes and create load for multiple resources including IO, CPU, memory, and network. Compression Our experiments involve runs with as well as without map output compression enabled. Compression is a common technique to shift load from IO to CPU. Map output compression is turned on by default in CDH, as most common kinds of data are readily compressible. Tuning compression allows us to examine tradeoffs in storage media under two different IO and CPU mixes. Job output compression is disabled by default in CDH and all our tests. Results We present the results of our benchmarking in the context of these two questions: For a new cluster, should one prefer SSDs or HDDs of the same aggregate bandwidth? For an existing cluster of HDDs, should one add SSDs or HDDs? Question 1: SSDs versus HDDs for a New Cluster Our goal here is to compare SSDs versus HDDs of the same aggregate bandwidth. Let�s look at a straightforward comparison between the SSD (1 SSD) and HDD-11 (11 HDDs) configurations. The graphs below show job durations for the two storage options, with the SSD values normalized against the HDD-11 values for each job. The first graph shows results with intermediate data compressed, and the second one without. Based on this comparison, our observations are: General trend: SSD is better than HDD-11 for all jobs, with and without intermediate data compression. However, the benefits of using SSD vary across jobs. SSD benefits shuffle, with improvements correlated to large shuffle size: SSD does benefit shuffle, as seen in TeraSort and Shuffle workloads for uncompressed intermediate data. Data from collectl�indicates that shuffle read-and-write IO sizes are one-half to three-quarters those of HDFS, in agreement with our discussion of MapReduce IO patterns previously. Interestingly, the benefits are barely visible when the intermediate data is compressed. We believe this is due to shuffle data being served from buffer cache RAM instead of disk. The data in TeraSort and Shuffle are both highly compressible, allowing compressed intermediate data to fit in the buffer cache. When we increase the data size per job 10x, the SSD benefits are visible even with compressed intermediate data. SSD also benefits HDFS read and write: A surprising result was that SSD also benefits HDFS read and write, as indicated by TeraGen, TeraValidate, TeraRead, and HDFS Data Write. When we analyzed collectl data, it turns out that our SSD is capable of roughly 2x the sequential IO size of the hard disks. Note that these jobs do not involve large amounts of shuffle data, so compressing intermediate data has no visible effect. CPU-heavy jobs not affected by choice of storage media: Our benchmarks include WordCount, a job that involves much text parsing and arithmetic aggregation in the map-side combiner – the CPU utilization was at 90 percent regardless of storage and compression configurations. The CPU utilization was lower for other jobs. As the IO path is not the bottleneck for such jobs, the choice of storage media has little impact on performance. Question 2: SSDs versus HDDs for an Existing Cluster Our goal here is to compare adding an SSD or many HDDs to an existing cluster, and to compare the various configurations possible in a hybrid SSD-HDD cluster. We use a baseline cluster with six HDDs per node (HDD-6). To this baseline we add an SSD or five HDDs, resulting in the Hybrid and HDD-11 setups. Note that on an equal bandwidth basis, �adding one SSD� should ideally be compared to �adding 11 HDDs�. (Our machines do not have 6 + 11 = 17 disks.) However, the setups are sufficient to lead us to the following observations: For default configurations, a Hybrid cluster offers lower than expected performance: The graph below compares job durations for the HDD-6, HDD-11, and Hybrid setups. For brevity, we show the results with uncompressed intermediate data, since that setting more clearly highlights the tradeoffs. Both HDD-11 and Hybrid provide visible improvement over HDD-6. However, even with its additional hardware bandwidth (add one SSD versus add five HDDs), the Hybrid setup offers no improvement over HDD-11. This observation triggered further investigations below. On a Hybrid cluster, when HDFS and shuffle use separate storage media, the benefits depend on workload: The default Hybrid configuration assigns HDDs and SSD to both the HDFS and shuffle local directories. We tested whether separating the storage media offers any improvement. Doing so requires two more cluster configurations: HDDs for HDFS with SSD for intermediate data, and vice versa. From the results, we see that the shuffle-heavy jobs (TeraSort and Shuffle) benefit from assigning SSD completely to intermediate data, while the HDFS-heavy jobs (TeraGen, TeraValidate, TeraRead, HDFS Data Write) are penalized. We see the opposite when the SSD is assigned to only HDFS. This is expected, as the SSD has a higher bandwidth than six HDDs combined. However, one would expect the simple hybrid to perform half way between assigning SSD to intermediate data and HDFS. This led to the next set of tests. On a Hybrid cluster, SSD should be split into multiple local directories: A closer look at HDFS and MapReduce implementations reveals a critical insight: both the DataNode and the NodeManager pick local directories in a round-robin fashion. A typical setup would mount each piece of storage hardware as a separate directory (for example: /mnt/disk-1, /mnt/disk-2, /mnt/ssd-1). HDFS and MapReduce both have the concept of �local directories�. For HDFS, local directories store the actual blocks. For MapReduce, local directories contain the intermediate shuffle data. One can configure HDFS and MapReduce to use multiple local directories (/mnt/disk-1 through /mnt/disk-11 plus /mnt/ssd-1) for our Hybrid setup. There, when the NodeManager decides to write out intermediate shuffle data, it will pick the 11 HDD local directories and the single SSD directory a round-robin fashion. Hence, when the job is optimized for a single wave of map tasks, each local directory receives the same amount of data, and faster progress on the SSD is held up by slower progress on the HDDs. So, to fully utilize the SSD, we need to split the SSD into multiple directories to maintain equal bandwidth per local directory. In our case, SSDs should be split into 10 directories. In our single-wave map output example, the SSDs would then receive 10x the data directed at each HDD, written at 10x the speed, and complete in the same amount of time. (Note: While splitting the SSD into multiple local directories improves performance, the SSD will fill up faster than the HDDs.) The graph below shows the performance of the split-SSD setup, compared against the HDD-6, HDD-11, and Hybrid-default setups. Splitting SSD into 10 local directories invariably leads to a major improvement over the default Hybrid setup. Conclusions Our findings suggest SSD has higher performance compared to HDD-11. However, from an economic point of view, the choice of storage media depends on the cost-per-performance for each. This differs from the cost-per-capacity metric ($-per-TB) that appears more frequently in HDD versus SSD comparisons. Cost-per-capacity makes sense for capacity-constrained use cases. As the primary benefit of SSD is high performance rather than high capacity, we believe storage vendors and customers should also track $-per-performance for different storage media. From our tests, SSDs have up to 70 percent higher performance, for 2.5x higher $ per performance (average performance divided by cost). This is far lower than the 50x difference in $ per TB computed in the table below. Customers can consider paying a premium cost to obtain up to 70 percent higher performance. (Note: Our tests focus on equal aggregate bandwidth for SSDs and HDDs. In the future, we would like to revisit this for setups with equal costs. That translates to 1 SSD against 35 HDDs. We do not have the necessary hardware to test this setup; however, we suspect the performance bottleneck likely shifts from IO to CPU [2 slots per core for MR2, not enough slots to keep all disks occupied].) Our tests also show that SSD benefits vary depending on the MapReduce job involved. Hence, the choice of storage media needs to consider the aggregate performance impact across the entire production workload. The precise improvement depends on how compressible the data is across all datasets, and the ratio of IO versus CPU load across all jobs. Future Work Enterprise data hubs�(EDHs) enable data to be ingested, processed, and analyzed in many different ways. To fully understand the implications of SSDs for EDHs, we need to study the tradeoffs for other components such as Apache HBase, Cloudera Impala, and Cloudera Search. These components are much more sensitive to latency and random access — they aggressively cache data in memory, and cache misses heavily affect performance. SSDs could potentially act as a cost-effective cache between memory and disk in the storage hierarchy, but we need measurements on real clusters to verify. Overall, SSD economics involves the interplay between ever-improving software and hardware, as well as ever-evolving customer workloads. The precise trade-off between SSDs, HDDs, and memory deserves regular re-examination over time. Karthik Kambatla is a member of the Platform Engineering team at Cloudera and a Hadoop committer. Yanpei Chen is a member of the Performance Engineering team at Cloudera. Appendix Below find collectl data, showing TeraSort and WordCount macro-benchmarks that have non-negligible data in all IO stages. The data confirms that shuffle IO sizes are generally smaller than HDFS IO sizes, and that SSD sequential (HDFS) IO sizes are ~2x that of HDDs. (Map output compression is enabled for these tests.) To confirm that these SSDs have higher sequential IO size than HDDs in general, we copied a series of large files to each storage medium, with collectl showing KB-per-IO nearly identical to the values in the table below. These values are per-node averages across the cluster.</snippet></document><document id="27"><title>HBaseCon 2014: Speakers, Keynotes, and Sessions Announced</title><url>http://blog.cloudera.com/blog/2014/03/hbasecon-2014-speakers-keynotes-and-sessions-announced/</url><snippet>Users of diverse, real-world HBase deployments around the world present at this year’s event. This year’s agenda for HBaseCon, the conference for the Apache HBase community (developers, operators, contributors), looks “Stack-ed” with can’t-miss keynotes and breakouts. Program committee, you really came through (again). It looks like this: A welcome message from event host Cloudera: Michael Stack (HBase PMC Chair/Cloudera Software Engineer, and the HBaseCon Program Chair) and Mike Olson (Cloudera Chief Strategy Officer) open the festivities by welcoming attendees. Keynotes from Google, Facebook, and Salesforce.com engineers: The General Session will begin with a keynote from Google’s Avtandil Garakanidze and Carter Page, who will describe how Bigtable (the original inspiration for HBase) has evolved since it was initially described in public in 2006, as well as its current use cases and roadmap at Google. Subsequently, Liyin Tang of Facebook will deep-dive into his company’s internally deployed HydraBase technology, and Lars Hofhansl will provide an overview of HBase ops and use cases at Salesforce.com. Breakout sessions from operators of the largest HBase deployments globally: Technical sessions will be divided into Operations, Features &amp; Internals, Ecosystem, and Case Studies tracks. Session speakers across those tracks include employees of Bloomberg LP, Facebook, Neilsen, Next Big Sound, Opower, Optimizely, Pinterest, Rocket Fuel, Salesforce.con, Xiamoi, and Yahoo! A repeat performance of the HBase operations panel:�Eric Sammer (Cloudera) will moderate a “Smooth Operators” panel that includes Jeremy Carroll (Pinterest), Adam Frank (Flurry), and Paul Tuckfield (Facebook).� Beginner’s content — an HBaseCon first: For attendees who are relatively new to HBase, Jesse Anderson (a Cloudera University instructor) will lead a special “HBase: Just the Basics” breakout before the General Session opens. On the fence about attending? Your decision should be easier now. :) Remember also that formal HBase training is available to attendees at a discount during the rest of the week (also in San Francisco, May 6-9). Click here�to learn more details. The final agenda grid will publish as soon as the Program Committee has determined session order! Thank you to our sponsors�– Continuuity, Hortonworks, Intel, LSI, MapR, Salesforce.com, Splice Machine,�WibiData (Gold); Facebook (Silver); ASF (Community); O’Reilly Media, The Hive, NoSQL Weekly (Media) — without which HBaseCon would be impossible!</snippet></document><document id="28"><title>Meet the Instructor: Bruce Martin</title><url>http://blog.cloudera.com/blog/2014/03/meet-the-instructor-bruce-martin/</url><snippet>In this installment of “Meet the Instructor”, our interview subject is Bruce Martin. What is your role at Cloudera? I am a Senior Instructor at Cloudera.�I teach all of our courses. I most often teach our Data Science, Developer, and Data Analyst courses, all of which make up the Developer Learning Path. What do you enjoy most about training and/or curriculum development? I enjoy teaching the concepts of Apache Hadoop and helping students get some practical experience using the technology.�I very much enjoy leading organized discussions that allow students to learn from my experiences and the experiences of others. I’ve been architecting, designing, and building advanced software systems for almost 30 years. I worked on distributed, replicated filesystems in the 1980s, distributed objects in the 1990s, and application frameworks in the 2000s. When I first learned about Hadoop, I was impressed how well it solved today’s Big Data business problems by creatively combining the algorithms and frameworks my colleagues and I had worked on in the past. In return for sharing my insights from the field, I get to learn from students about a lot of different verticals�healthcare, education, finance, energy, regulatory agencies, advertising, insurance, banking�their Big Data challenges, and the opportunities presented by Hadoop, the enterprise data hub, and data science. Describe an interesting application you�ve seen or heard about for Hadoop. For me, the most interesting Hadoop applications all have a machine-learning component to them.��I am thrilled by the idea that an application can learn from data in order to predict future events and use those predictions to better our lives. All of the collected experiences of my career in technology, from the toil and joy of writing code to studying computer science to eventually teaching thousands of students in the classroom, feels truly worth it when I see folks building machine learning applications that enhance outcomes in healthcare and education, protect us from fraud, support efficient use of energy, and so on. A common example of machine learning is an email spam filter, which learns how to effectively identify and separate out junk email based on examples of spam. Over time, the filter automatically and independently improves by observing the characteristics of more and more emails categorized as spam or not spam by users. Although the program doesn�t necessarily read or understand emails, it is able to use the data that make up emails to predict which qualify as spam and which do not. What advice would you give to an engineer, system administrator, or analyst who wants to learn more about Big Data? For those who are completely new to Hadoop, first focus on the concepts.� The model of parallel, distributed functional computing is fundamentally different from the sequential computing models most of us learned in school.� Learn the concepts before worrying about the many, many details of Hadoop and its ecosystem.��� After mastering the concepts, get your hands dirty.� If you are a developer, data analyst, or data scientist, find a Big Data set (there are many publicly available data sets if you don�t have access to your own) and build an application.�If you are an administrator, build a cluster. Cloudera Express is free to download, as is the QuickStart VM, which makes it easy to spin up a cluster for experimentation. Cloudera even offers a free trial of the Cloudera Enterprise Data Hub Edition, which provides the full suite of all the state-of-the art tools used in production environments.� Our Cloudera University courses are structured to maximize learning-by-doing while teaching you the best practices leveraged from real-world engagements with customers using Hadoop in all different kinds of environments and with different use cases.� They present concepts, provide lots of hands-on labs, and provide students with the opportunity to benefit from the experiences of the instructor and fellow students. Finally, learn from the experiences of others. Join the Cloudera Community. Read white papers. Watch online tutorials. How did you become involved in technical training and Hadoop? Prior to joining Cloudera, I was the product architect for Student Success Products at SunGard Higher Education (now Ellucian).�Most recently, I worked on a product that used machine learning to identify students at risk of failing or dropping out of a course in a university. The product builds a model from historical data about students who previously took a course. The data include student demographic profiles, academic preparation, performance, effort, and final grades. The model then uses the data to predict the success or risk of currently enrolled students. However, the product didn’t operate on Big Data sets. The amount of available data for students taking a particular course is not that large by today’s standards. But the product was envisioned to evolve to operate on data from students in an entire department, or a school, or a university, or a university system, or a state, or even an entire country.�Those data sets were definitely going to be BIG! As the architect, I was required to prepare our systems, products, and teams for the inevitable future of massive and multi-structured data.�I learned about the emerging Big Data technologies and experimented with how we would apply them in education. I became fascinated with Hadoop and its ecosystem, and I jumped when I had the opportunity to teach at Cloudera. What�s one interesting fact or story about you that a training participant would be surprised to learn? There really are too many to list, but here’s a bunch of teasers: According to a priest in the Azores, I am a descendant of Prince Henry the Navigator. I’ve been speaking Spanish most of my life, and l live part-time in Mexico.� Although most of my classes are taught in English, I’ve conducted Spanish-language courses and workshops in Mexico, Spain, and Chile.� As an undergraduate at University of California, Berkeley, I often encountered a graduate student who pretty much lived in the computer room. Turns out it was Bill Joy (eventual co-founder of Sun Microsystems), who was building Berkeley UNIX at the time.�� In graduate school at University of California, San Diego, back in the 1980s, I built replicated filesystems and experimented with voting algorithms.� I am the author of five CORBA Services specifications at the Object Management Group. I used to have a job title Director, Advanced Concepts � not just concepts, but advanced ones!</snippet></document><document id="29"><title>This Month in the Ecosystem (February 2014)</title><url>http://blog.cloudera.com/blog/2014/03/this-month-in-the-ecosystem-february-2014/</url><snippet>Welcome to our sixth edition of “This Month in the Ecosystem,” a digest of highlights from February 2014 (never intended to be comprehensive; for completeness, see the excellent�Hadoop Weekly). February being a short month, the list is relatively short — but never confuse quantity with quality! Hadoop 2.3.0 was released — with HDFS caching among the most prominent new features. (The imminent Cloudera Enterprise 5 GA release will be based on 2.3.0.) Apache Spark graduated into a Top Level Project. Congrats to Sparkies everywhere! Native support for Parquet, the open source, general-purpose columnar storage format for Apache Hadoop (co-founded by Cloudera and Twitter), became official in Apache Hive. Parquet is well on its way to becoming an ecosystem standard, with support now available in Impala, Hive, Spark, Apache Pig, Apache Crunch, Cascading, and more to come. The speakers and schedule for ApacheCon 2014 (April 7-9, in Denver) were announced. Clouderans representing the Hadoop ecosystem include Jarek Jarcec Cecho, Abe Elmahrek, Colin McCabe, Sean Mackrory, Mark Miller, Brock Noland, and Hari Shreedharan. Apache HBase 0.98.0 was released. Some nice security sugar in there. That’s all for this month, folks! Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="30"><title>A Guide to Checkpointing in Hadoop</title><url>http://blog.cloudera.com/blog/2014/03/a-guide-to-checkpointing-in-hadoop/</url><snippet>Understanding how checkpointing works in HDFS can make the difference between a healthy cluster or a failing one. Checkpointing is an essential part of maintaining and persisting filesystem metadata in HDFS. It�s crucial for efficient NameNode recovery and restart, and is an important indicator of overall cluster health. However, checkpointing can also be a source of confusion for operators of Apache Hadoop clusters. In this post, I�ll explain the purpose of checkpointing in HDFS, the technical details of how checkpointing works in different cluster configurations, and then finish with a set of operational concerns and important bug fixes concerning this feature. Filesystem Metadata in HDFS To start, let�s first cover how the NameNode persists filesystem metadata. HDFS metadata changes are persisted to the edit log. At a high level, the NameNode�s primary responsibility is storing the HDFS namespace. This means things like the directory tree, file permissions, and the mapping of files to block IDs. It�s important that this metadata (and all changes to it) are safely persisted to stable storage for fault tolerance. This filesystem metadata is stored in two different constructs: the fsimage and the edit log. The fsimage is a file that represents a point-in-time snapshot of the filesystem�s metadata. However, while the fsimage file format is very efficient to read, it�s unsuitable for making small incremental updates like renaming a single file. Thus, rather than writing a new fsimage every time the namespace is modified, the NameNode instead records the modifying operation in the edit log for durability. This way, if the NameNode crashes, it can restore its state by first loading the fsimage then replaying all the operations (also called edits or transactions) in the edit log to catch up to the most recent state of the namesystem. The edit log comprises a series of files, called edit log segments, that together represent all the namesystem modifications made since the creation of the fsimage. As an aside, this pattern of using a log for incremental changes on top of another storage format is quite common for traditional filesystems. Log-structured filesystems take this to an extreme and use just a log for persisting data, but more common journaling filesystems like EXT3, EXT4, and XFS support writing changes to a journal before applying them to their final locations on disk. Why is Checkpointing Important? A typical edit ranges from 10s to 100s of bytes, but over time enough edits can accumulate to become unwieldy. A couple of problems can arise from these large edit logs. In extreme cases, it can fill up all the available disk capacity on a node, but more subtly, a large edit log can substantially delay NameNode startup as the NameNode reapplies all the edits. This is where checkpointing comes in. Checkpointing is a process that takes an fsimage and edit log and compacts them into a new fsimage. This way, instead of replaying a potentially unbounded edit log, the NameNode can load the final in-memory state directly from the fsimage. This is a far more efficient operation and reduces NameNode startup time. Checkpointing creates a new fsimage from an old fsimage and edit log. However, creating a new fsimage is an I/O- and CPU-intensive operation, sometimes taking minutes to perform. During a checkpoint, the namesystem also needs to restrict concurrent access from other users. So, rather than pausing the active NameNode to perform a checkpoint, HDFS defers it to either the SecondaryNameNode or Standby NameNode, depending on whether NameNode high-availability is configured. The mechanics of checkpointing differs depending on if NameNode high-availability is configured; we�ll cover both. In either case though, checkpointing is triggered by one of two conditions: if enough time has elapsed since the last checkpoint (dfs.namenode.checkpoint.period), or if enough new edit log transactions have accumulated (dfs.namenode.checkpoint.txns). The checkpointing node periodically checks if either of these conditions are met (dfs.namenode.checkpoint.check.period), and if so, kicks off the checkpointing process. Checkpointing with a Standby NameNode Checkpointing is actually much simpler when dealing with an HA setup, so let�s cover that first. When NameNode high-availability is configured, the active and standby NameNodes have a shared storage where edits are stored. Typically, this shared storage is an ensemble of three or more JournalNodes, but that�s abstracted away from the checkpointing process. The standby NameNode maintains a relatively up-to-date version of the namespace by periodically replaying the new edits written to the shared edits directory by the active NameNode. As a result, checkpointing is as simple as checking if either of the two preconditions are met, saving the namespace to a new fsimage (roughly equivalent to running `hdfs dfsadmin -saveNamespace` on the command line), then transferring the new fsimage to the active namenode via HTTP. Checkpointing with NameNode HA configured Here, Standby NameNode is abbreviated as SbNN and Active NameNode as ANN: SbNN checks whether either of the two preconditions are met: elapsed time since the last checkpoint or number of accumulated edits. SbNN saves its namespace to an a new fsimage with the intermediate name fsimage.ckpt_, where txid is the transaction ID of the most recent edit log transaction. Then, the SbNN writes an MD5 file for the fsimage, and renames the fsimage to fsimage_. While this is taking place, most other SbNN operations are blocked. This means administrative operations like NameNode failover or accessing parts of the SbNN�s webui. Routine HDFS client operations (such as listing, reading, and writing files) are unaffected as these operations are serviced by the ANN. SbNN sends an HTTP GET to the active NN�s GetImageServlet at /getimage?putimage=1. The URL parameters also have the transaction ID of the new fsimage and the SbNN�s hostname and HTTP port. The active NN�s servlet uses the information in the GET request to in turn do its own GET back to the SbNN�s GetImageServlet. Similar to the standby, it first saves the new fsimage with the intermediate name fsimage.ckpt_, creates the MD5 file for the fsimage, and then renames the new fsimage to fsimage_. Checkpointing with a SecondaryNameNode In a non-HA deployment, checkpointing is done on the SecondaryNameNode rather than the standby NameNode. Since there isn�t a shared edits directory or automatic tailing of the edit log, the SecondaryNameNode has to go through a few more steps first to refresh its view of the namespace before continuing down the same basic steps. Time diagram of 2NN and NN with annotated steps Here, the NameNode is abbreviated as NN and the SecondaryNameNode as 2NN: 2NN checks whether either of the two preconditions are met: elapsed time since the last checkpoint or number of accumulated edits. In the absence of a shared edit directory, the most recent edit log transaction ID needs to be queried via an explicit RPC to the NameNode (NamenodeProtocol#getTransactionId). 2NN triggers an edit log roll, which ends the current edit log segment and starts a new one. The NN can keep writing edits to the new segment while the SNN compacts all the previous ones. This also returns the transaction IDs of the current fsimage and the edit log segment that was just rolled. Explicit triggering of an edit log roll is not necessary in an HA configuration, since the standby NameNode periodically rolls the edit log orthogonal to checkpointing. Given these two transaction IDs, the 2NN fetches new fsimage and edit files as needed via GET to the NN�s GetImageServlet. The 2NN might already have some of these files from a previous checkpoint (such as the current fsimage). If necessary, the 2NN reloads its namespace from a newly downloaded fsimage. The 2NN replays the new edit log segments to catch up to the current transaction ID. From here, the rest is the same as in the HA case with a StandbyNameNode. 2NN writes out its namespace to a new fsimage. The 2NN contacts the NN via HTTP GET at /getimage?putimage=1, causing the NN�s servlet to do its own GET to the 2NN to download the new fsimage. Operational Implications The biggest operational concern related to checkpointing is when it fails to happen. We�ve seen scenarios where NameNodes accumulated hundreds of GBs of edit logs, and no one noticed until the disks filled completely and crashed the NN. When this happens, there�s not much to do besides restart the NN and wait for it to replay all the edits. Because of the potential severity of this issue, Cloudera Manager will warn if the current fsimage is out of date, if the checkpointing 2NN or SbNN is down, as well as if NN disks are close to capacity. Checkpointing is a very I/O and network intensive operation and can affect client performance. This is especially true on a large cluster with millions of files and a multi-GB fsimage, since copying a new fsimage to the NameNode can eat up all available bandwidth. In this case, the transfer speed can be throttled with dfs.image.transfer.bandwidthPerSec. If you do adjust this parameter, you might also need to adjust dfs.image.transfer.timeout based on your expected transfer time. If you�re on an older version of CDH, there are also a number of issues related to checkpointing that might make upgrading worthwhile. HDFS-4304 (fixed in CDH 4.1.4, 4.2.1, and 4.3.0). Previously, it was possible to write an edit log operation so big that it couldn�t be read when replaying the edit log. This would cause checkpointing and NameNode startup to fail. This was a problem for files with lots of blocks, since closing a file involved writing all the block IDs to the edit log. The fix was to simply increase the size of the maximum allowable edit log operation. HDFS-4305�(fixed in CDH 4.3.0).�Related to HDFS-4304 above, files with a large number of blocks are typically due to misconfiguration. For example, a user might accidentally set a block size of 128KB rather than 128MB, or might only use a single reducer for a large MapReduce job. This issue was fixed by having the NameNode enforce a minimum block size as well as a maximum number of blocks per file. HDFS-4816 (fixed in CDH 4.5.0). Previously, image transfer from the standby NN to the active NN held the standby NN�s write lock. Thus if the active NN failed during image transfer, the standby NN would not be able to failover until the transfer completed. Since transferring the fsimage doesn�t actually modify any namespace data, the transfer was simply moved outside the critical section. HDFS-4128 (fixed in CDH 4.3.0). If the 2NN hit an out-of-memory (OOM) exception during edit log replay, it could get stuck in an inconsistent state where it would try replaying the edit log from the incorrect offset during future checkpointing attempts. Since it�s likely that this OOM would keep happening even if we fixed log replay, the 2NN now simply aborts if it fails to replay logs a few times. The underlying fix though is to configure your 2NN with the same heap size as your NameNode. HDFS-4300 (fixed in CDH 4.3.0). If the 2NN or SbNN experienced an error while transferring an edits file, it would not retry downloading the complete file later. This process would stall checkpointing, since it�d be impossible to replay the partial edits file. This issue was fixed by first transferring the edits file to a temporary location and then renaming it to its final destination after transfer completes. HDFS-4569 (fixed in CDH4.2.1 and CDH4.3.0). Bumped the image transfer timeout from 1 minute to 10 minutes. The default timeout was causing issues for checkpointing with multi-GB fsimages, especially with throttling turned on. Conclusion Checkpointing is a vital part of healthy HDFS operation. In this post, you learned how filesystem metadata is persisted in HDFS, the importance of checkpointing to this role, how checkpointing works in both HA and non-HA setups, and finally covered a selection of important fixes and improvements related to checkpointing. By understanding the purpose of checkpointing and how it works, you are now equipped with the knowledge to debug these kinds of issues in your production clusters. Andrew Wang is a Software Engineer at Cloudera and a Hadoop committer.</snippet></document><document id="31"><title>Why Apache Spark is a Crossover Hit for Data Scientists</title><url>http://blog.cloudera.com/blog/2014/03/why-apache-spark-is-a-crossover-hit-for-data-scientists/</url><snippet>Spark is a compelling multi-purpose platform for use cases that span investigative, as well as operational, analytics. Data science is a broad church. I am a data scientist — or so I�ve been told — but what I do is actually quite different from what other “data scientists” do. For example, there are those practicing “investigative analytics” and those implementing “operational analytics.” (I�m in the second camp.) Data scientists performing investigative analytics use interactive statistical environments like R to perform ad-hoc, exploratory analytics in order to answer questions and gain insights. By contrast, data scientists building operational analytics systems have more in common with engineers. They build software that creates and queries machine-learning models that operate at scale in real-time serving environments, using systems languages like C++ and Java, and often use several elements of an enterprise data hub, including the Apache Hadoop ecosystem. And there are subgroups within these groups of data scientists. For example, some analysts who are proficient with R have never heard of Python or scikit-learn, or vice versa, even though both provide libraries of statistical functions that are accessible from a REPL (Read-Evaluate-Print Loop) environment. A World of Tradeoffs It would be wonderful to have one tool for everyone, and one architecture and language for investigative as well as operational analytics. If I primarily work in Java, should I really need to know a language like Python or R in order to be effective at exploring data? Coming from a conventional data analyst background, must I understand MapReduce in order to scale up computations? The array of tools available to data scientists tells a story of unfortunate tradeoffs: R offers a rich environment for statistical analysis and machine learning, but it has some rough edges when performing many of the data processing and cleanup tasks that are required before the real analysis work can begin. As a language, it�s not similar to the mainstream languages developers know. Python is a general purpose programming language with excellent libraries for data analysis like Pandas and scikit-learn. But like R, it�s still limited to working with an amount of data that can fit on one machine. It’s possible to develop distributed machine learning algorithms on the classic MapReduce computation framework in Hadoop (see Apache Mahout). But MapReduce is notoriously low-level and difficult to express complex computations in. Apache Crunch offers a simpler, idiomatic Java API for expressing MapReduce computations. But still, the nature of MapReduce makes it inefficient for iterative computations, and most machine learning algorithms have an iterative component. And so on. There are both gaps and overlaps between these and other data science tools. Coming from a background in Java and Hadoop, I do wonder with envy sometimes: why can’t we have a nice REPL-like investigative analytics environment like the Python and R users have? That’s still scalable and distributed? And has the nice distributed-collection design of Crunch? And can equally be used in operational contexts? Common Ground in Spark These are the desires that make me excited about Apache Spark. While discussion about Spark for data science has mostly noted its ability to keep data resident in memory, which can speed up iterative machine learning workloads compared to MapReduce, this is perhaps not even the big news, not to me. It does not solve every problem for everyone. However, Spark has a number of features that make it a compelling crossover platform for investigative as well as operational analytics: Spark comes with a machine-learning library, MLlib, albeit bare bones so far. Being Scala-based, Spark embeds in any JVM-based operational system, but can also be used interactively in a REPL in a way that will feel familiar to R and Python users. For Java programmers, Scala still presents a learning curve. But at least, any Java library can be used from within Scala. Spark�s RDD (Resilient Distributed Dataset) abstraction resembles Crunch�s PCollection, which has proved a useful abstraction in Hadoop that will already be familiar to Crunch developers. (Crunch can even be used on top of Spark.) Spark imitates Scala’s collections API and functional style, which is a boon to Java and Scala developers, but also somewhat familiar to developers coming from Python. Scala is also a compelling choice for statistical computing. Spark itself, and Scala underneath it, are not specific to machine learning. They provide APIs supporting related tasks, like data access, ETL, and integration. As with Python, the entire data science pipeline can be implemented within this paradigm, not just the model fitting and analysis. Code that is implemented in the REPL environment can be used mostly as-is in an operational context. Data operations are transparently distributed across the cluster, even as you type. Spark, and MLlib in particular, still has a lot of growing to do. For example, the project needs optimizations, fixes, and deeper integration with YARN. It doesn�t yet provide nearly the depth of library functions that conventional data analysis tools do. But as a best-of-most-worlds platform, it is already sufficiently interesting for a data scientist of any denomination to look at seriously. In Action: Tagging Stack Overflow Questions A complete example will give a sense of using Spark as an environment for transforming data and building models on Hadoop. The following example uses a dump of data from the popular Stack Overflow Q&amp;A site. On Stack Overflow, developers can ask and answer questions about software. Questions can be tagged with short strings like “java” or “sql“. This example will build a model that can suggest new tags to questions based on existing tags, using the alternating least squares (ALS) recommender algorithm; questions are “users” and tags are “items”. Getting the Data Stack Exchange provides complete dumps of all data, most recently from January 20, 2014. The data is provided as a torrent containing different types of data from Stack Overflow and many sister sites. Only the file stackoverflow.com-Posts.7z needs to be downloaded from the torrent. This file is just a bzip-compressed file. Spark, like Hadoop, can directly read and split some compressed files, but in this case it is necessary to uncompress a copy on to HDFS. In one step, that�s: bzcat stackoverflow.com-Posts.7z | hdfs dfs -put - /user/srowen/Posts.xml
   Uncompressed, it consumes about 24.4GB, and contains about 18 million posts, of which 2.1 million are questions. These questions have about 9.3 million tags from approximately 34,000 unique tags. Set Up Spark Given that Spark�s integration with Hadoop is relatively new, it can be time-consuming to get it working manually. Fortunately, CDH hides that complexity by integrating Spark and managing setup of its processes. Spark can be installed separately with CDH 4.6.0, and is included in CDH 5 Beta 2. This example uses an installation of CDH 5 Beta 2. This example uses MLlib, which uses the jblas library for linear algebra, which in turn calls native code using LAPACK and Fortran. At the moment, it is necessary to manually install the Fortran library dependency to enable this. The package is called libgfortran or libgfortran3, and should be available from the standard package manager of major Linux distributions. For example, for RHEL 6, install it with: sudo yum install libgfortran   This must be installed on all machines that have been designated as Spark workers. Log in to the machine designated as the Spark master with ssh. It will be necessary, at the moment, to ask Spark to let its workers use a large amount of memory. The code in MLlib that is used in this example, in version 0.9.0, has a memory issue, one that is already fixed for the next release. To configure for more memory and launch the shell: 
export SPARK_JAVA_OPTS="-Dspark.executor.memory=8g"
spark-shell   Interactive Processing in the Shell The shell is the Scala REPL. It�s possible to execute lines of code, define methods, and in general access any Scala or Spark functionality in this environment, one line at a time. You can paste the following steps into the REPL, one by one. First, get a handle on the Posts.xml file: 
val postsXML = sc.textFile("hdfs:///user/srowen/Posts.xml")   In response the REPL will print: 
postsXML: org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at :12   The text file is an RDD (Resilient Distributed Dataset) of Strings, which are the lines of the file. You can query it by calling methods of the RDD class. For example, to count the lines: 
postsXML.count   This command yields a great deal of output from Spark as it counts lines in a distributed way, and finally prints 18066983. The next snippet transforms the lines of the XML file into a collection of (questionID,tag) tuples. This demonstrates Scala�s functional programming style, and other quirks. (Explaining them is out of scope here.) RDDs behave like Scala collections, and expose many of the same methods, like map: (You can copy the source for the above from here.) You will notice that this returns immediately, unlike previously. So far, nothing requires Spark to actually perform this transformation. It is possible to force Spark to perform the computation by, for example, calling a method like count. Or Spark can be told to compute and persist the result through checkpointing, for example. The MLlib implementation of ALS operates on numeric IDs, not strings. The tags (“items”) in this data set are strings. It will be sufficient here to hash tags to a nonnegative integer value, use the integer values for the computation, and then use a reverse mapping to translate back to tag strings later. Here, a hash function is defined since it will be reused shortly. def nnHash(tag: String) = tag.hashCode &amp; 0x7FFFFF
var tagHashes = postIDTags.map(_._2).distinct.map(tag =&gt;(nnHash(tag),tag))   Now, you can convert the tuples from before into the format that the ALS implementation expects, and the model can be computed: import org.apache.spark.mllib.recommendation._
// Convert to Rating(Int,Int,Double) objects
val alsInput = postIDTags.map(t =&gt; Rating(t._1, nnHash(t._2), 1.0))
// Train model with 40 features, 10 iterations of ALS
val model = ALS.trainImplicit(alsInput, 40, 10)   This will take minutes or more, depending on the size of your cluster, and will spew a large amount of output from the workers. Take a moment to find the Spark master web UI, which can be found from Cloudera Manager, and will run by default at http://[master]:18080. There will be one running application. Click through, then click “Application Detail UI”. In this view it�s possible to monitor Spark�s distributed execution of lines of code in ALS.scala: When it is complete, a factored matrix model is available in Spark. It can be used to predict question-tag associations by “recommending” tags to questions. At this early stage of MLlib�s life, there is not even a proper recommend method yet, that would give suggested tags for a question. However it is easy to define one:����� def recommend(questionID: Int, howMany: Int = 5): Array[(String, Double)] = {
  // Build list of one question and all items and predict value for all of them
  val predictions = model.predict(tagHashes.map(t =&gt; (questionID,t._1)))
  // Get top howMany recommendations ordered by prediction value
  val topN = predictions.top(howMany)(Ordering.by[Rating,Double](_.rating))
  // Translate back to tags from IDs
  topN.map(r =&gt; (tagHashes.lookup(r.product)(0), r.rating))
}
   And to call it, pick any question with at least four tags, like “How to make substring-matching query work fast on a large table?” and get its ID from the URL. Here, that�s 7122697: recommend(7122697).foreach(println)   This method will take a minute or more to complete, which is slow. The lookups in the last line are quite expensive since each requires a distributed search. It would be somewhat faster if this mapping were available in memory. It�s possible to tell Spark to do this: tagHashes = tagHashes.cache   Because of the magic of Scala closures, this does in fact affect the object used inside the recommend method just defined. Run the method call again and it will return faster. The result in both cases will be something similar to the following: (sql,0.17745152481166354)
(database,0.13526622226672633)
(oracle,0.1079428707621154)
(ruby-on-rails,0.06067207312463499)
(postgresql,0.050933613169706474)
   (Your result will not be identical, since ALS starts from a random solution and iterates.) The original question was tagged “postgresql”, “query-optimization”, “substring”, and “text-search”. It�s reasonable that the question might also be tagged “sql” and “database”. “oracle” makes sense in the context of questions about optimization and text search, and “ruby-on-rails” often comes up with PostgreSQL, even though these tags are not in fact related to this particular question. Something for Everyone Of course, this example could be more efficient and more general. But for the practicing data scientists out there — whether you came in as an R analyst, Python hacker, or Hadoop developer — hopefully you saw something familiar in different elements of the example, and have discovered a way to use Spark to access some benefits that the other tribes take for granted. Learn more about Spark and its availability on CDH, and join the discussion in our brand-new Spark forum. Sean is Director of Data Science for�EMEA�at Cloudera, helping customers build large-scale machine learning solutions on Hadoop. Previously, Sean founded Myrrix Ltd, producing a real-time recommender and clustering product evolved from Apache Mahout. Sean was primary author of recommender components in Mahout, and has been an active committer and�PMC�member for the project. He is co-author of Mahout in Action. Spark Summit 2014 is coming (June 30 – July 2)! Register here to get 20% off the regular conference price.</snippet></document><document id="32"><title>New Hue Demos: Spark UI, Job Browser, Oozie Scheduling, and YARN Support</title><url>http://blog.cloudera.com/blog/2014/02/new-hue-demos-spark-ui-job-browser-oozie-scheduling-and-yarn-support/</url><snippet>Hue users can learn a lot about new features by following a steady stream of new demos. Hue, the open source Web UI that makes Apache Hadoop easier to use, is now a standard across the ecosystem — shipping within multiple software distributions and sandboxes. One of the reasons for its success is an agile developer community behind it that is constantly rolling out new features to its users. Just as important, the Hue team is diligent in its documentation and demonstration of those new features via video demos. In this post, for your convenience, I bring you the most recent examples (released since December): The new Spark Igniter App Using YARN and Job Browser Job Browser with YARN Security � Apache Oozie crontab scheduling � You can stay up to date about new demos by following the Hue team’s Vimeo channel and/or the Hue blog. However, I will bring you similar updates here from time to time. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="33"><title>Secrets of Cloudera Support: Inside Our Own Enterprise Data Hub</title><url>http://blog.cloudera.com/blog/2014/02/secrets-of-cloudera-support-inside-our-own-enterprise-data-hub/</url><snippet>Cloudera’s own enterprise data hub is yielding great results for providing world-class customer support. Here at Cloudera, we are constantly pushing the envelope to give our customers world-class support. One of the cornerstones of this effort is the Cloudera Support Interface (CSI), which we�ve described in prior blog posts (here and here). Through CSI, our support team is able to quickly reason about a customer�s environment, search for information related to a case currently being worked, and much more. In this post, I’m happy to write about a new feature in CSI, which we call Monocle Stack Trace. Stack Trace Exploration with Search Hadoop log messages and the stack traces in those logs are critical information in many of the support cases Cloudera handles. We find that our customer operation engineers (COEs) will regularly search for stack traces they find referenced in support cases to try to determine where else that stack trace has shown up, and in what context it would occur. This could be in the many sources we were already indexing as part of Monocle Search in CSI: Apache JIRAs, Apache mailing lists, internal Cloudera JIRAs, internal Cloudera mailing lists, support cases, Knowledge Base articles, Cloudera Community Forums, and the customer diagnostic bundles we get from Cloudera Manager. It turns out that doing routine document searches for stack traces doesn�t always yield the best results. Stack traces are relatively long compared to normal search terms, so search indexes won�t always return the relevant results in the order you would expect. It�s also hard for a user to churn through the search results to figure out if the stack trace was actually an exact match in the document to figure out how relevant it actually is. To solve this problem, we took an approach similar to what Google does when it wants to allow searching over a type that isn�t best suited for normal document search (such as images): we created an independent index and search result page for stack-trace searches. In Monocle Stack Trace, the search results show a list of unique stack traces grouped with every source of data in which unique stack trace was discovered. Each source can be viewed in-line in the search result page, or the user can go to it directly by following a link. We also give visual hints as to how the stack trace for which the user searched differs from the stack traces that show up in the search results. A green highlighted line in a search result indicates a matching call stack line. Yellow indicates a call stack line that only differs in line number, something that may indicate the same stack trace on a different version of the source code. A screenshot showing the grouping of sources and visual highlighting is below: � The high-level implementation details are as follows: Every data source we fetch as part of standard Monocle Search indexing is marked for stack-trace processing. Every hour, a series of MapReduce jobs run to find and extract stack traces from the sources we�ve fetched. For each stack trace found, we create a unique key using the call stack that we use to uniquely identify an exception, and do a lookup in Apache HBase using that key. If there�s already a row, we append the source we found the new stack trace in to that row. If not, we create a new row in HBase, and insert the new call stack into the Search index. When a search is executed, each unique stack trace that Solr sees as a match in the indexed is returned. For each stack trace returned, we then do a lookup against HBase to find the sources that stack trace has been found in. The UI then does a line by line comparison of the call stack, highlighting each call stack line in each search result appropriately. I�ve left out some details about optimizations we�ve made, such as our use of HBase�s bulk loading functionality for the extraction of stack traces from some of our larger data sources, but those aren�t critical to understanding the high-level data flow. On the initial day of launch, we received feedback that the Monocle Stack Trace search index was especially useful when a COE was presented with a stack trace they know they�ve seen in a prior support case before. This way of grouping and visualizing the matches was bringing our supporters right to the case that was being recalled, instead of them having to sift through the long list of fuzzy search results a regular document search would yield. This resulted in less time searching, and more time focusing on solving the problem. Conclusion We feel like this application shows the power of an enterprise data hub (EDH). By having multiple strategies for storing, accessing, and processing data within our EDH, you can truly execute on building innovative applications that solve problems in new ways. This application goes way beyond simple indexing and searching. We are using Cloudera Search, HBase, and MapReduce to process, store, and visualize stack traces that wouldn�t be possible with just a search index. How Monocle Stack Trace integrates with the larger CSI application goes way beyond that, though. It�s a great feeling when you are able to execute a search in Monocle Stack Trace that links directly to a point in time in a customer log file that an Impala query returned after churning through tens of GBs of data — done interactively from a Web UI on the order of a second or two. At Cloudera, we strongly believe in investing in these kinds of applications in the name of giving our COEs that extra edge to provide world-class support. Adam Warrington is an Engineer Manager on the customer operations team at Cloudera. �</snippet></document><document id="34"><title>Apache Hadoop 2.3.0 is Released (HDFS Caching FTW!)</title><url>http://blog.cloudera.com/blog/2014/02/apache-hadoop-2-3-0-is-released-hdfs-caching-ftw/</url><snippet>Hadoop 2.3.0 includes hundreds of new fixes and features, but none more important than HDFS caching. The Apache Hadoop community has voted to release Hadoop 2.3.0, which includes (among many other things): In-memory caching for HDFS, including centralized administration and management Groundwork for future support of heterogeneous storage in HDFS Simplified distribution of MapReduce binaries via the YARN Distributed Cache You can read the release notes here. Congratulations to everyone who contributed! As noted above, one of the major new features in Hadoop 2.3.0 is HDFS caching, which enables memory-speed reads in HDFS. This feature was developed by two engineers/Hadoop committers at Cloudera: Andrew Wang and Colin McCabe. HDFS caching lets users explicitly cache certain files or directories in HDFS. DataNodes will then cache the corresponding blocks in off-heap memory through the use of mmap and mlock. Once cached, Hadoop applications can query the locations of cached blocks and place their tasks for memory-locality. Finally, when memory-local, applications can use the new zero-copy read API to read cached data with no additional overhead. Preliminary benchmarks show that optimized applications can achieve read throughput on the order of gigabytes per second. Better yet, this feature will be landing in CDH 5.0 (which is based on Hadoop 2.3.0) when it ships alongside corresponding Impala improvements that take advantage of these new APIs for improved performance. So, you can look forward to an even faster Impala in the new release! Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="35"><title>How-to: Index and Search Multilingual Documents in Hadoop</title><url>http://blog.cloudera.com/blog/2014/02/how-to-index-and-search-multilingual-documents-in-hadoop/</url><snippet>Learn how to use Cloudera Search along with RBL-JE to search and index documents in multiple languages. Our thanks to Basis Technology for providing the how-to below! Basis Technology’s Rosette Base Linguistics for Java (RBL-JE) provides a comprehensive multilingual text analytics platform for improving search precision and recall. RBL provides tokenization, lemmatization, POS tagging, and de-compounding for Asian, European, Nordic, and Middle Eastern languages, and has just been certified for use with Cloudera Search. Cloudera Search brings full-text, interactive search, and scalable indexing to Apache Hadoop by marrying SolrCloud with HDFS and Apache HBase, and other projects in CDH. Because it’s integrated with CDH, Cloudera Search brings the same fault tolerance, scale, visibility, and flexibility of your other Hadoop workloads to search, and allows for a number of indexing, access control, and manageability options. In this post, you’ll learn how to use Cloudera Search and RBL-JE to index and search documents. Since Cloudera takes care of the plumbing for distributed search and indexing, the only work needed to incorporate Basis Technology’s linguistics is loading the software and configuring your Solr collections. First, install RBL-JE. This essentially involves unpacking a tar.gz file and copying your license file to the licenses directory. Note the root directory of the installation. We’ll refer to this as RBLJE_ROOT later. Searching and indexing with RBL-JE requires a few additions to the schema.xml and solr.xml files for each Solr collections that you will use. To the solrconfig.xml file, you will add these lines to ensure that the appropriate RBL jar files end up on the class path: 
&lt;lib path="[[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/lib/btrbl-je-[[RBLJE_VER]].jar" /&gt;
&lt;lib path="[[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/lib/btcommon-[[BT_COMMON_VER]].jar" /&gt;
&lt;lib path="[[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/lib/slf4j-api-[[SLF4J_VER]].jar" /&gt;
&lt;lib path="[[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/lib/slf4j-simple-[[SLF4J_VER]].jar" /&gt;
&lt;lib path="[[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/lib/btrbl-je-lucene-solr-[[LUCENE_SOLR_VER]]-[[RBLJE_VER]].jar" /&gt;
   Replace the [[xxx]] text in the pathnames above to match the version of RBL you are using. The version numbers can be determined by looking at the contents your RBLJE_ROOT. Edit the schema.xml file to add field types that use RBL and assign them to fields in your documents. Here is an example field type that specifies using RBL to analyze Chinese data: 
&lt;fieldtype name="chinese-basis" class="solr.TextField"&gt;
            &lt;analyzer&gt;
                &lt;tokenizer class="com.basistech.rosette.lucene.BaseLinguisticsTokenizerFactory"
                           language="zhs"
                           licensePath="[[bt.license.path]]"
                           modelDirectory="[[bt.model.directory]]"
                        /&gt;
                &gt;
                &lt;filter class="com.basistech.rosette.lucene.BaseLinguisticsTokenFilterFactory"
                        language="zhs"
                        licensePath="[[bt.license.path]]"
                        dictionaryDirectory="[[bt.dictionary.directory]]"
                        addLemmaTokens="true"/&gt;
            &lt;/analyzer&gt;
        &lt;/fieldtype&gt;
   Where: bt.license.path is [[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/licenses/rlp-license.xml bt.model.directory is [[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/models bt.dictionary.directory is [[RBLJE_ROOT]]/rbl-je-[[RBLJE_VER]]/dicts And here is an example of using it on a field: 
&lt;field name="text" type="chinese-basis" indexed="true" stored="true" /&gt;
   That’s it! Once this bit of configuration is done, the Cloudera Search framework can be used conventionally for indexing and searching. You’ll find a repository of configuration files, scripts, and sample documents that you can use to configure and test RBL-JE here. It provides working examples of the configuration techniques discussed above. Request an evaluation version of RBL-JE from Basis Technology and try it out in your own Cloudera Search application!</snippet></document><document id="36"><title>Native Parquet Support Comes to Apache Hive</title><url>http://blog.cloudera.com/blog/2014/02/native-parquet-support-comes-to-apache-hive/</url><snippet>Bringing Parquet support to Hive was a community effort that deserves congratulations! Previously, this blog introduced Parquet, an efficient ecosystem-wide columnar storage format for Apache Hadoop. As discussed in that blog post, Parquet encodes data extremely efficiently and as described in Google’s original Dremel paper. (For more technical details on the Parquet format read Dremel made simple with Parquet, or go directly to the open and community-driven Parquet Format specification.) Before discussing the Parquet Hive integration, it’s worth discussing how widely Parquet has been adopted across the Hadoop ecosystem. Parquet integrates with the following engines: Cloudera Impala Apache Crunch Apache Drill Apache Hadoop MapReduce Apache Hive (0.10, 0.11, 0.12, and 0.13) Apache Pig Apache Spark Apache Tajo (planned) Cascading Pivotal HAWQ and the following data description software: Apache Avro Apache Thrift Google Protocol Buffers (in code review) When Parquet was announced, Criteo stepped up to create the Parquet Hive integration. Initially this integration was hosted within the Parquet project and shipped with CDH 4.5. However, as the momentum behind Parquet grew, users wanted to use Parquet with a variety of Hive versions. Therefore, the Parquet team determined that native integration with the Hive project would be easier to maintain, as Hive does not have well defined public/private APIs. Furthermore, as can be seen below, native integration greatly simplifies the CREATE TABLE command. As such, the Parquet team decided to move the Parquet Hive integration into the Hive project via HIVE-5783. A diverse set of Parquet and Hive contributors came together to commit native Parquet support to Hive 0.13. Most notably, Criteo engineers Justin Coffey, Micka�l Lacour, and Remy Pecqueur donated the Hive Parquet integration to the Hive project. The end result of this work is that users of Hive 0.13 and CDH 5 can easily create Parquet tables in Hive: 
CREATE TABLE parquet_test (
 id int,
 str string,
 mp MAP&lt;STRING,STRING&gt;,
 lst ARRAY&lt;STRING&gt;,
 struct STRUCT&lt;A:STRING,B:STRING&gt;)
PARTITIONED BY (part string)
STORED AS PARQUET;
   Users of CDH 4.5 and Hive 0.10, 0.11, and 0.12 can continue to use Parquet Hive from the Parquet project proper, by using the older more verbose CREATE TABLE syntax. To create a table in Hive 0.10, 0.11, or 0.12, use the syntax below: 
CREATE TABLE parquet_test (
 id int,
 str string,
 mp MAP&lt;STRING,STRING&gt;,
 lst ARRAY&lt;STRING&gt;,
 strct STRUCT&lt;A:STRING,B:STRING&gt;)
PARTITIONED BY (part string)
ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'
STORED AS
INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'
OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat';
   Thanks to everyone who contributed to this work! Brock Noland is a Software Engineer at Cloudera and a Hive Committer.</snippet></document><document id="37"><title>How-to: Make Hadoop Accessible via LDAP</title><url>http://blog.cloudera.com/blog/2014/02/how-to-make-hadoop-accessible-via-ldap/</url><snippet>Integrating Hue with LDAP can help make your secure Hadoop apps as widely consumed as possible. Hue, the open source Web UI that makes Apache Hadoop easier to use, easily integrates with your corporation’s existing identity management systems and provides authentication mechanisms for SSO providers. So, by changing a few configuration parameters, your employees can start analyzing Big Data in their own browsers under an existing security policy. In this blog post, you’ll learn details about the various features and capabilities available in Hue for integrating with likely the most popular authentication mechanism, LDAP. (It is also possible to authenticate Hue users via PAM, SPNEGO, OpenID, OAuth, and SAML, but those topics are for another post.) Authentication The typical authentication scheme for Hue takes the following form: In the above diagram, credentials are validated against the Hue database. Often it’s easier to manage identities from a central location; with the Hue LDAP integration, users can use their LDAP credentials to authenticate and inherit their existing groups transparently. There is no need to save or duplicate any employee password in Hue: When authenticating via LDAP, Hue validates login credentials against a directory service if configured with this authentication backend: [desktop]
[[auth]]
backend=desktop.auth.backend.LdapBackend
   The LDAP authentication backend will automatically create users that don’t exist in Hue by default. Hue needs to import users in order to properly perform the authentication. (The password is never imported when importing users.) However, you may want to disable automatic import at times to allow logins only by a predefined list of manually imported users. For those cases, you can use the following configuration to disable automatic import: [desktop]
[[ldap]]
create_users_on_login=false   The case sensitivity of the authentication process is defined in the “Case Sensitivity” section below. There are two different ways to authenticate with a directory service through Hue: Search Bind The search-bind mechanism for authenticating will perform an ldapsearch against the directory service and bind using the found distinguished name (DN) and password provided. This is, by default, used when authenticating with LDAP. The configurations that affect this mechanism are outlined in the “LDAP Search” section below. Direct Bind The direct-bind mechanism for authenticating will bind to the LDAP server using the username and password provided at login. You can choose between two options for how Hue binds: nt_domain – Domain component for User Principal Names (UPN) in active directory. This Active Directory-specific idiom allows Hue to authenticate with Active Directory without having to follow LDAP references to other partitions. This typically maps to the email address of the user or the user’s ID in conjunction with the domain. ldap_username_pattern – Provides a template for the DN that will ultimately be sent to the directory service when authenticating. If nt_domain is provided, Hue will use a UPN to bind to the LDAP service: [desktop]
[[ldap]]
nt_domain=example.com   Otherwise, the ldap_username_pattern configuration is used. (The parameter will be replaced with the username provided at login): [desktop]
[[ldap]]
ldap_username_pattern="uid=&lt;username&gt;,ou=People,DC=hue-search,DC=ent,DC=cloudera,DC=com"
   Typical attributes to search for include: uid sAMAccountName   To enable direct bind authentication, the search_bind_authentication configuration must be set to false: 
[desktop]
[[ldap]]
search_bind_authentication=false
   Importing Users If an LDAP user must belong to a certain group and have a particular set of permissions, you can import this user via the Useradmin interface: As you can see above, there are two options available when importing: Distinguished name – If this option is checked, the username provided must be a full distinguished name (for example: uid=hue,ou=People,dc=gethue,dc=com). Otherwise, the Username provided should be a fragment of a Relative Distinguished Name (rDN). (For example, the username “hue” maps to the rDN “uid=hue”.) Hue will perform an LDAP search using the same methods and configurations as defined in the “LDAP Search” section; essentially, Hue will take the provided username and create a search filter using the user_filter and user_name_attr configurations. Create home directory – If this option is checked, when the user is imported and their home directory in HDFS will automatically be created, if it doesn’t already exist. The case sensitivity of the search and import processes are defined in the “Case Sensitivity” section. Importing Groups Groups are importable via the Useradmin interface. Then, you can add users to this group, which would provide a set of permissions (such as accessing the Impala application). This function works similarly to user importing, but has a couple of extra features. As the above image portrays, not only can groups be discovered via DN and rDN search, but users that are members of the group and members of the group’s subordinate groups can be imported as well. Posix groups and members are automatically imported if the group found has the object class posixGroup. Synchronizing Users and Groups Users and groups can be synchronized with the directory service via the Useradmin interface or via a command-line utility. The images from the previous sections use the words “Sync” to indicate that when a name of a user or group that exists in Hue is added, it will actually be synchronized instead. In the case of importing users for a particular group, new users will be imported and existing users will be synchronized. (Note: Users who have been deleted from the directory service will not be deleted from Hue. You can manually deactivate those users from Hue via the Useradmin interface.) Attributes synchronized Currently, only the first name, last name, and email address are synchronized. Hue looks for the LDAP attributes givenName, sn, and mail when synchronizing.� Also, the user_name_attr config is used to appropriately choose the username in Hue. For example, if user_name_attr is set to “uid”, then the uid returned by the directory service will be used as the username of the user in Hue. Useradmin interface The “Sync LDAP users/groups” button in the Useradmin interface will automatically synchronize all users and groups. Command-line interface Here’s a quick example of how to use the command line interface to synchronize users and groups: &lt;hue root&gt;/build/env/bin/hue sync_ldap_users_and_groups
   LDAP Search There are two configurations for restricting the search process: user_filter – General LDAP filter to restrict the search user_name_attr – The attribute that will be considered the username against which to search Here is an example configuration: [desktop]
[[ldap]]
[[[users]]]
user_filter="objectClass=*"
user_name_attr=uid   With the above configuration, the LDAP search filter will take the form: (&amp;(objectClass=*)(uid=&lt;user entered usename&gt;))   Case Sensitivity You can configure Hue to ignore the case of usernames as well as force usernames to lower case via the ignore_username_case and force_username_lowercase configurations. These two configurations should be used in conjunction with each other. This is useful when integrating with a directory service containing usernames in capital letters and UNIX usernames in lowercase letters (which is a Hadoop requirement). Here is an example of configuring them: [desktop]
[[ldap]]
ignore_username_case=true
force_username_lowercase=true   LDAPS/StartTLS support Secure communication with LDAP is provided via the SSL/TLS and StartTLS protocols. It allows Hue to validate the directory service to which it’s going to converse. Practically speaking, if a Certificate Authority Certificate file is provided, Hue will communicate via LDAPS: [desktop]
[[ldap]]
ldap_cert=/etc/hue/ca.crt   The StartTLS protocol can be used as well (step up to SSL/TLS): 
[desktop]
[[ldap]]
use_start_tls=true
   Conclusion The Hue team is working hard to improve security. Upcoming LDAP features include: Import nested LDAP groups and multi-domain support for Active Directory. We hope this brief overview of LDAP in Hue will help you make your system more secure, more compliant with current security standards, and open up big data analysis to many more users! As always, feel free to contact us at hue-user@ or @gethue! Abe Elmahrek is a Software Engineer at Cloudera.</snippet></document><document id="38"><title>Getting MapReduce 2 Up to Speed</title><url>http://blog.cloudera.com/blog/2014/02/getting-mapreduce-2-up-to-speed/</url><snippet>Thanks to the improvements described here, CDH 5 will ship with a version of MapReduce 2 that is just as fast (or faster) than MapReduce 1. Performance fixes are tiny, easy, and boring, once you know what the problem is. The hard work is in putting your finger on that problem: narrowing, drilling down, and measuring, measuring, measuring. Apache Hadoop is no exception to this rule. Recently, Cloudera engineers set out to ensure that MapReduce performance in Hadoop 2 (MR2/YARN) is on par with, or better than, MapReduce performance in Hadoop 1 (MR1). Architecturally, MR2 has many performance advantages over MR1: Better scalability by splitting the JobTracker into the ResourceManager and Application Masters.� Better cluster utilization and higher throughput through finer-grained resource scheduling.� Less tuning required to avoid over-spilling from smarter sort buffer management. Faster completion times for small jobs through �Uber Application Masters,� which run all of a job�s tasks in a single JVM. While these improvements are important, none of them mean particularly much for well-tuned medium-sized jobs on medium-sized clusters. Whenever a codebase goes through large changes, regressions are likely to seep in. While correctness issues are easy to spot, performance regressions are difficult to catch without rigorous measurement. When we started including MR2 in our performance measurements last year, we noticed that it lagged behind MR1 significantly on nearly every benchmark. Since then, we�ve done a ton of work — tuning parameters in Cloudera Manager and fixing regressions in MapReduce itself — and can now proudly say that CDH 5 MR2 performs equally well, or better than, MR1 on all our benchmarks. In this post, I�ll offer a couple examples of this work as case studies in tracking down the performance regressions of complex (Java) distributed systems. Ensuring a Fair Comparison Ensuring a fair comparison between MR1 and MR2 is tricky. One common pitfall is that TeraSort, the job most commonly used for benchmarking, changed between MR1 and MR2. To reflect rule changes in the GraySort benchmark on which it is based, the data generated by the TeraSort included with MR2 is less compressible. A valid comparison would use the same version of TeraSort for both releases; otherwise, MR1 will have an unfair advantage. Another difficult area is resource configuration. In MR1, each node�s resources must be split between slots available for map tasks and slots available for reduce tasks. In MR2, the resource capacity configured for each node is available to both map and reduce tasks. So, if you give MR1 nodes 8 map slots and 8 reduce slots and give MR2 nodes 16 slots worth of memory, resources will be underutilized during MR1�s map phase. MR2 will be able to run 16 concurrent mappers per node while MR1 will only be able to run 8. If you only give MR2 nodes 8 slots of memory, then MR2 will suffer during the period when the map and reduce phases overlap – it will only get to run 8 tasks concurrently, while MR1 will be able to run more. (See this post for more information about properly configuring MR2.) To circumvent this issue, our benchmarks give full node capacity in MR1 to both map slots and reduce slots.�We then set the mapred.reduce.slowstart.completedmaps parameter in both to .99, meaning that there will be no overlap between the map and reduce phases. This ensures that MR1 and MR2 get full cluster resources for both phases. Case 1: CPU Cache Locality in Sorting Map Outputs A performance fix starts with noticing a performance problem. In this case, I noticed WordCount was lagging: A job that ran in 375 seconds on an MR1 cluster took 472 seconds on an MR2 cluster, more than 25 percent longer. A good start when diagnosing a MapReduce performance issue is to determine in which phase it occurs. In this case, the web UI reported that MR2 map tasks were taking much more time than MR1 map tasks. The next thing is to look for any big differences in the counters; here, they were nearly the same between jobs, so that provided few hints. With little to go on from the counters, the next step is to isolate the problem – reproduce it with as little else going on as possible. The difference in map time showed up when running the same job with a single map and single reduce task in the LocalJobRunner.� However, with the reduce task cut out, the times evened out.�Because the sort is skipped when no reduce tasks run, it seemed likely that there was some kind of regression in the map-side sort phase. The Map-Side Sort The next step requires a little bit of background on how the map-side sort works. Map output data is placed in an in-memory buffer. When the buffer fills up, the framework sorts it and then writes it to disk (�spills� it). A separate thread merges the sorted on-disk files into a single larger sorted file. The buffer consists of two parts: a section with contiguous raw output data and a metadata section that holds pointers for each record into the raw data section. In MR1, the sizes of these sections were fixed, controlled by io.sort.record.percent, which could be configured per job.�This meant that, without proper tuning of this parameter, if a job had many small records, the metadata section could fill up much more quickly than the raw data section. The buffer would be spilled to disk before it was entirely full. MAPREDUCE-64 fixed this issue in MR2 by allowing the two sections to share the same space and vary in size, meaning that manual tuning of io.sort.record.percent is no longer required to minimize the number of spills. With all this in mind, I realized that we had not yet tuned io.sort.record.percent for the job, and therefore the MR1 map tasks were spilling 10 times as many times as the MR2 map tasks. When I did tune the parameter for MR1 so that it would spill as many times as MR2, MR1 performance actually suffered — spilling fewer larger chunks meant slower map tasks. I had a theory that CPU cache latency was involved. A smaller chunk of output data might fit into a CPU cache, meaning that all the memory accesses when sorting it would be extremely fast. A larger chunk would not fit, meaning that memory accesses would have to go into a higher-level cache or maybe even all the way to main memory. As memory accesses to each cache level take about an order of magnitude longer, this could cause a large performance hit. Fortunately, there is an extremely powerful Linux profiling tool, called perf, that makes it easy to measure this. Running perf stat -e cache-misses will spit out the number of CPU cache misses encountered by the command. In this case, MR2 job and the tuned MR1 job had a similar number of cache misses, while the untuned MR1 job had half as many. Improving CPU Cache Latency So how to improve CPU cache latency? At the suggestion of Todd Lipcon, I took a look at MAPREDUCE-3235, an unfinished JIRA from a year ago that proposed a couple ways to improve CPU cache performance in exactly this situation. The change suggested was trivial: Instead of sorting indices into the map output meta array, sort the array itself. Previously, to access the nth map output record, I found the nth element in the index into the meta array, followed that to the entry in the meta array, and then followed the position reported there into the raw output data. Now, I just find the nth element in the meta array and follow the position reported there.� This approach removes a layer of indirection and means that I need to access fewer possibly far away memory locations. The drawback is that when the sort does a swap, it moves the full metadata entry (about 20 bytes) instead of the index (4 bytes). A memory access outside the cache is far more expensive than an extra move instruction inside the cache, so it�s worth the cost. The tiny change worked like magic. It cut the number of cache misses in half for the local job and brought the runtime of the MR2 job on the cluster to less than the runtime of the MR1 job.�Win! Case 2: Over-reading in the Shuffle Thanks to a report from a Cloudera partner, we learned that more disk reads were occurring in the shuffle during an MR2 job than during the same one on MR1. To reproduce this issue, I turned to ShuffleText, a benchmark job we run that specifically targets the shuffle. It generates a bunch of data in the mappers, shuffles it, and then throws it away in the reducers. I failed to reproduce the problem with a pseudo-distributed setup, but it immediately reared its head when I ran the jobs on the cluster. The job submitted to MR2 took 30 percent longer than the same job submitted to MR1. Even more dramatic, the average time spent per reduce task fetching map output data was a whopping 60 seconds in MR2 compared to 27 seconds in MR1. While MapReduce counters are helpful in many situations, they don�t provide machine-wide hardware metrics like number of reads that actually hit disk. Cloudera Manager was invaluable for both measuring the problem and allowing us to dig down into what was wrong. I quickly created charts that showed the total bytes read from disk on machines in the cluster while the job was running: 31.2GB for MR2 compared with 6.4GB for MR1. Disk reads happen in a few different places in a MapReduce job: First, when reading the input data from disk; second, when merging map outputs if there are multiple spills; third, when serving data to reducers during the shuffle; and fourth, when merging data on the reduce side. The TaskTracker and NodeManager processes are responsible for serving intermediate data to reducers in MR1 and MR2, respectively. Looking at the disk bytes read by these processes in Cloudera Manager confirmed that the extra reads were occurring when serving data to the reducers. Looking at the code, I noticed that the MR2 shuffle had been rewritten to serve data with Netty async IO instead of the Jetty web server, which used a thread per request — but nothing popped out as to why this specific issue could be occurring. Adding log messages that printed out the total number of bytes read by the shuffle server yielded no useful leads. On top of this I noticed a few facts: In both MR1 and MR2, the sum of bytes being read from disk during the shuffle phase was smaller than the total map output bytes. In MR1, the majority of shuffle disk reads appeared to be occurring on two of the machines, while the rest had nearly 0. These two machines happened to have less physical memory than the other machines on the cluster. The story that best fit all three was that, in MR1, most of the map outputs were able to reside in memory in the OS buffer cache. The two machines were experiencing more disk reads because their smaller physical memory meant they couldn�t fit the map output data in the cache. For some reason, MR2 was not taking advantage of it as well as MR1. fadvise Turning again to the code, I noticed that MR (both 1 and 2) is very careful about its interactions with the OS buffer cache. The Linux fadvise system call allows providing the OS memory subsystem with suggestions about whether or not to cache regions of files. When the shuffle server receives a request for map output data, it fadvises file regions that it is about to read with FADV_WILLNEED so that they will be ready in memory. When done with a file region, the server fadvises it out of memory with FADV_DONTNEED to free up space because it knows that the region likely will not be consumed again. Without any obvious bad logic in the code, the next step was to try figuring out more directly what was going on. I turned to strace, which tracks all the system calls made by a process, and listened for fadvises, file opens, closes, and reads. The amount of data produced by this proved unmanageable to sort by hand. What about simply counting the number of WILLNEEDs and DONTNEEDs? The WILLNEEDs had pretty similar numbers between MR1 and MR2, but seemed to vary per run. For the DONTNEEDs, MR1 was pegged at 768 per node, which corresponded exactly to the number of map tasks that ran on that node multiplied by the number of reducers. But MR2 was always higher than this; and varied between runs. A mere four hours of aggravation later, it all fell into place: In normal operation, reducers will often issue requests for map outputs and then decide they don�t want these outputs yet and terminate the connection. This occurs because reducers have a limited amount of memory into which read map outputs, and they find out how much space a map output will take up from a prefix in the response when they request it from the NodeManager/TaskTracker. (In other words, they don�t know how much space a map output will take up before asking for it, so, when they find out, they may need to abort and come back later when the space is available.) If the reducer terminates a connection, the shuffle server should not evict the file regions being fetched from the OS cache (not fadvise them as DONTNEED) because the reducer will come back and ask for them later. MR1 was doing this right. MR2 was not, meaning that even if a map output was in memory the first time the fetcher came around, it would be evicted if the reducer terminated the connection, and when the reducer came back it would need to be read from disk. The fix merely consisted of the shuffle server not fadvise-ing regions as DONTNEED when a fetch terminates before reading the whole data. This resulted in the average time a reducer spends fetching intermediate data dropping from 60 seconds to 27, the same as MR1. The average job run time also dropped by 30 percent, bringing it in line with MR1 as well. The astute reader will realize that the situation could be improved even further by communicating the size of map output regions to reducers before they try to fetch them. This would allow us to avoid initiating a fetch when the reducer can�t fit the results inside its merge buffer, and reduce unnecessary seeks on the shuffle server side. (We would like to implement this change in future work.) Conclusion I hope that these experiences will assist you in your own performance regression testing, and maybe give you a tiny drop of solace the next time you�re trapped inside an Eclipse window, wondering whether there�s a way to make everything ok. Thanks to these improvements and many others from Cloudera and the rest of the community (the MR2 improvements have gone upstream and will appear in Hadoop 2.3), we are confident that the version of MR2 that will ship inside CDH 5 (in beta at the time of writing; download here) will perform at least as well, and very likely better, than MR1! Many thanks to Yanpei Chen, Prashant Gokhale, Todd Lipcon, and Chris Leroy for their assistance on this project. Sandy Ryza is a Software Engineer at Cloudera and a Hadoop Committer.</snippet></document><document id="39"><title>Best Practices for Deploying Cloudera Enterprise on Amazon Web Services</title><url>http://blog.cloudera.com/blog/2014/02/best-practices-for-deploying-cloudera-enterprise-on-amazon-web-services/</url><snippet>This FAQ contains answers to the most frequently asked questions about the architecture and configuration choices involved. In December 2013, Cloudera and Amazon Web Services (AWS) announced a partnership to support Cloudera Enterprise on AWS infrastructure. Along with this announcement, we released a Deployment Reference Architecture Whitepaper. In this post, you�ll get answers to the most frequently asked questions about the architecture and the configuration choices that have been highlighted in that whitepaper. For what workloads is this deployment model designed? This reference architecture is intended for long-running Cloudera Enterprise Data Hub Edition (EDH) clusters, where the source of data for your workloads is HDFS with S3 as a secondary storage system (used for backups). Different kinds of workloads can run in this environment, including batch processing (MapReduce), fast in-memory analytics (Apache Spark), interactive SQL (Impala), search, and low-latency serving using HBase. This deployment model is not designed for transient workloads such as spinning up a cluster, running a MapReduce job to process some data, and spinning it down; that model involves different considerations and design. Clusters with workflow-defined lifetimes (transient clusters) will be addressed in a future publication of the reference architecture. Why support only VPC deployments? Amazon Virtual Private Cloud (VPC) is the standard deployment model for AWS resources and the default for all new accounts that are being created now. Cloudera recommends deploying in VPC for the following reasons: The easiest way to deploy in AWS, where the AWS resources appear as an extension to the corporate network, is to do so inside a VPC, with a VPN/Direct Connect link to the particular AZ in which you are deploying. VPC has more advanced security options that you can use to comply with security policies. More advanced features, and better network performance for new instance types, are available. Should I have one VPC per cluster? Or should I have one subnet per cluster in a single VPC? What about multiple clusters in a single subnet? Some customers consider having one VPC per environment (dev, QA, prod). Within a single VPC, you can have independent subnets for different clusters — and in some cases, multiple subnets for each cluster, where each subnet is for instances playing a particular role (such as Flume nodes, cluster nodes, and so on). The easiest way to deploy a cluster is to deploy all nodes to a single subnet and use security groups to control ingress and egress in a single VPC. Keep in mind that it�s nontrivial to get instances in different VPCs to interact. What about different subnets for different roles versus controlling access using security groups? You have two models of deployment to consider, depending on your security requirements and policies: Entire cluster within a single subnet — this means that all the different role types that make up a cluster (slaves, masters, Flume nodes, edge nodes) will be deployed within a single subnet. In most cases, the network access rules for these nodes differ.� For example, users will be allowed to login to the edge nodes but not the slave or the master nodes. When deploying in a single subnet, the network rules can be modeled using security groups. Subnet per role per cluster — in this model, each of the different roles will have its own subnet in which to deploy. This is a more complex network topology and allows for finer-grained control over the network rules. In this case, you can use a combination of subnet route tables, security groups, and network ACLs to define your networking rules. However, just using security groups and defining the route tables appropriately is sufficient from a functionality standpoint. Both models are equally valid, but Model #1 is easier to manage. I don�t want my instances to be accessible from the Internet. Do I HAVE to deploy them in a public subnet? Currently, there are two ways an instance can get outbound access to the internet, which is required for it to access other AWS services like S3 (excluding RDS) or external repositories for software updates (find detailed documentation here): By having a public IP address — this allows the instance to initiate outgoing requests. You can block all incoming traffic using Network ACLs or Security Groups. In this case, you have to set up the routing within your VPC to permit traffic between the subnet hosting your instances and the Internet gateway. By having a private IP address only but having a NAT instance in a different subnet through which to route traffic — this allows for all traffic to be routed through the NAT instance. Similar to on-premise configurations, a NAT instance is typically a Linux EC2 instance configured to run as a NAT residing in a subnet that has access to the Internet.� You can direct public Internet traffic from subnets that can’t directly access the Internet to the NAT instance. If you just transfer any sizable amount of data to the public Internet domain (including S3), the recommended method is deployment Model 1. With Model 2, you will bottleneck on the NAT instance. Why only choose cc2.8xlarge and hs1.8xlarge instances as the supported ones? Cloudera Enterprise Data Hub Edition deployments have multiple kinds of workloads running in a long running cluster. To support these different workloads, the individual instances need to provide enough horsepower. The cc2.8xlarge and hs1.8xlarge instances make for the best choices amongst all EC2 instances for such deployments for the following reasons: Individual instance performance does not suffer from the problem of chatty neighboring applications on the same physical host. These instances are on a flat 10G network. They have a good amount of CPU and RAM available. For relatively low storage density requirements, the cc2.8xlarge are the recommended option, and where the storage requirement is high, the hs1.8xlarge are a better choice. Other instance types are reasonable options for specialized workloads and use cases. For example, a memcached deployment would likely benefit from the high-memory instances, and a transient cluster with only batch-processing requirements could probably leverage the m1 family instances (while having a higher number of them). However, as previously explained, those workloads are not addressed by this reference architecture, which is rather intended for long-running EDH deployments where the primary storage is HDFS on the instance stores, supporting multiple different kinds of workloads on the same cluster. Why not EBS-backed HDFS? There are multiple reasons why some people consider Amazon Elastic Block Storage (EBS). They include: Increasing the storage density per node but using smaller instance types You can certainly increase the storage density per node by mounting EBS volumes. Having said that, there are a few reasons why doing so doesn�t help: Not many of the instance types are good candidates for running an EDH that can sustain different kinds of workloads predictably. Adding a bunch of network-attached storage does theoretically increase the storage capacity, but the other resources like CPU, memory, and network bandwidth don�t change. Therefore, it�s undesirable to use small instance types with EBS volumes attached to them. The bandwidth between the EC2 instances and EBS volumes is limited so you�ll likely be bottlenecked on that. EBS shines with random I/O. Sequential I/O, which is the predominant access pattern for Hadoop, is not EBS�s forte. You pay per IOP on EBS, and for workloads that require large amounts of I/O, that can get expensive to a point that having more instances might be more reasonable than adding EBS volumes and keeping the instance footprint small. Allowing expansion of storage on existing instances in an existing cluster, thereby not having to add more instances if the storage requirements increase. The justifications for this requirement are similar to those above. Furthermore, adding storage to a cluster that is predominantly backed by the instance-stores would mean that you have heterogeneous storage options in the same cluster, with different performance and operational characteristics. More EBS volumes means more spindles, and hence better performance. Adding EBS volumes does not necessarily mean better I/O performance. For example, EBS volumes are network attached — therefore, the performance is limited by the network bandwidth between the EC2 instances and EBS. Furthermore, as highlighted previously, EBS shines with random I/O in contrast to sequential I/O, which is the predominant access pattern for Hadoop. Storing the actual files in EBS will enable pausing the cluster and bringing it back on at a later stage. Today, this is a complex requirement from an operational perspective. The only EC2 instances that can be stopped and later restarted are the EBS-backed ones; the others can only be terminated. If you mount a bunch of EBS volumes to the EBS-backed instances and use them as data directories, they remain there when the instances are started up again and the data in them stays intact. From that perspective, you�ll have all your data directories mounted just the way you left them prior to the restart, and HDFS should be able to resume operations. If you mount EBS volumes onto instance-store backed instances, restarting HDFS would mean un-mounting all the EBS volumes when you stop a cluster and then re-mounting them onto a new cluster later. This approach is operationally challenging as well as error-prone. Although both these options are plausible in theory, they are also not very well tested, and HDFS is not designed to leverage these features regardless. EBS has higher durability than instance stores and we can reduce the HDFS replication if we use EBS. This is an interesting proposition and the arguments for and against it are the same as if you were to use NAS to back your HDFS on bare-metal deployments. While certainly doable, there are downsides: By reducing replication of HDFS, you are not only giving up on fault tolerance and fast recoverability but also performance. Because fewer copies of the blocks would be available with which to work, more data will move over the network. All your data will be going over the network between the EC2 instance and EBS volumes, thereby affecting performance. Using EBS to back HDFS certainly looks like an attractive option, but as you look at all the factors mentioned above, it should become clear that it has too many performance, cost, and operational drawbacks. Can I pause my cluster in this deployment model? (Or, Can I stop a cluster when I�m not using it and save money?) Clusters in which HDFS is backed by instance-stores cannot be paused. Pausing a cluster entails stopping the instances, and when you stop instances, the data on the instance-stores is lost. You can find more information about instance lifecycle here. What if I don�t want connectivity back to my data center via VPN or Direct Connect? You don�t have to have connectivity back to your data center if you don�t have to move data between your Hadoop cluster in AWS and other components that may be hosted in your data center. What are �placement groups�, and why should I care? As formally defined by AWS documentation: A placement group is a logical grouping of instances within a single Availability Zone. Using placement groups enables applications to get the full-bisection bandwidth and low-latency network performance required for tightly coupled, node-to-node communication typical of HPC applications. By deploying your cluster in a placement group, you are guaranteeing predictable network performance across your instances. Anecdotally, network performance between instances within a single Availability Zone that are not in a single placement group can be lower than if they were within the same placement group. As of today, our recommendation is to spin up your cluster (at least the slave nodes) within a placement group. Having said that, placement groups are more restrictive in terms of the capacity pool that you can use to provision your cluster, which can make expanding a cluster challenging. The root volume is too small for the logs and parcels. What are my choices? You can resize the root volume on instantiation. However, doing so is more challenging with some AMIs than others. The only reason to resize the root volume is to be able to have enough space to store the logs that Hadoop generates, as well as parcels. For those two purposes, our recommendation is to mount an additional EBS volume and use that instead. You can use the additional EBS volume by sym-linking the /var/logs and /opt/cloudera directories to that. You can also configure Cloudera Manager to use a different path than /var/logs for logs and /opt/cloudera for parcels. In a future post, we�ll cover options for backups, high availability, and disaster recovery in the context of deployments in AWS. Amandeep Khurana is a Principal Solutions Architect at Cloudera and has been heavily involved in Cloudera’s cloud efforts.</snippet></document><document id="40"><title>Cloudera Enterprise 5 Beta 2 is Available: More New Features and Components</title><url>http://blog.cloudera.com/blog/2014/02/cloudera-enterprise-5-beta-2-is-available-more-new-features-and-components/</url><snippet>Cloudera has released the Beta 2 version of Cloudera Enterprise 5 (comprises CDH 5.0.0 and Cloudera Manager 5.0.0).� This release (download) contains a number of new features and component versions including the ones below: New Components Apache Spark 0.9 Apache Crunch 0.9 Parquet 1.2.5 Kite SDK �0.10 Apace Avro 1.7.5 Apache Hadoop Rebase to Hadoop 2.2.0+ HDFS HDFS Caching NFS gateway Cloudera Manager Support for Apache Spark Oozie and YARN Resource Manager High Availability Extensibility to support add-on services Enhancements to Monitoring &amp; Charts Apache HBase Rebase to HBase 0.96.1.1 Impala Rebase to Impala 1.2.3 Apache Flume New Kite Dataset Sink Apache Hive Hive 0.12 Rebase Improved JDBC spec coverage SSL encryption support on non-kerberos authentication Native Parquet support Apache Sqoop Rebase to Sqoop 1.99.3 Apache Oozie Support for CRON like scheduling Hue Single Sign On (SSO) support Graphical facets in Search application Result graphing for Hive and Impala Apache Pig Rebase to Pig 0.12.0 Rebase to DataFu 1.1.0 Cloudera strongly recommends that you install Cloudera Enterprise 5.0 Beta 2 and CDH 5 Beta 2 on test clusters or new clusters only. In the case of existing clusters, Cloudera fully supports upgrade from Cloudera Enterprise 4.x and CDH 4.x to Cloudera Enterprise 5.0 Beta 2 and CDH 5 Beta 2. If you have a particular need to upgrade a cluster running Cloudera Enterprise 5.0 Beta 1 or CDH 5 Beta 1 to Beta 2, customers can contact Cloudera Support for further instructions. If you are not a customer, ask for assistance in the Cloudera Manager forum. Please note that Cloudera Manager 5.0 beta 2 does not support CDH 5.0 beta 1. As part of the open Beta, we encourage the community to try it out. Here is how you can get started: Download Cloudera Enterprise from: cloudera.com/downloads install and try it out.� View the documentation: CDH 5 Beta 2 Documentation Cloudera Manager 5 Beta 2 Documentation Impala Beta 2 Documentation Search Beta 2 Documentation� Once you get started, we encourage you to provide feedback using any of the following methods: Ask questions and provide comments on our Beta community forum. Click here to join.� File a bug through our public Jira at: https://issues.cloudera.org/browse/DISTRO https://issues.cloudera.org/browse/CM We look forward to hearing about your experiences of Cloudera Enterprise 5 Beta 2.</snippet></document><document id="41"><title>Migrating from Hive CLI to Beeline: A Primer</title><url>http://blog.cloudera.com/blog/2014/02/migrating-from-hive-cli-to-beeline-a-primer/</url><snippet>Migrating from the Hive CLI to Beeline isn’t as simple as changing the executable name, but this post makes it easy nonetheless. In its original form, Apache Hive was a heavyweight command-line tool that accepted queries and executed them utilizing MapReduce. Later, the tool split into a client-server model, in which HiveServer1 is the server (responsible for compiling and monitoring MapReduce jobs) and Hive CLI is the command-line interface (sends SQL to the server). Recently, the Hive community (with Cloudera engineers leading the charge) introduced HiveServer2, an enhanced Hive server designed for multi-client concurrency and improved authentication that also provides better support for clients connecting through JDBC and ODBC. Now HiveServer2, with Beeline as the command-line interface, is the recommended solution; HiveServer1 and Hive CLI are deprecated and the latter won’t even work with HiveServer2. Beeline was developed specifically to interact with the new server. Unlike Hive CLI, which is an Apache Thrift-based client, Beeline is a JDBC client based on the SQLLine CLI — although the JDBC driver used communicates with HiveServer2 using HiveServer2�s Thrift APIs. As Hive development has shifted from the original Hive server (HiveServer1) to the new server (HiveServer2), users and developers accordingly need to switch to the new client tool. However, there’s more to this process than simply switching the executable name from �hive� to �beeline�. In this post, you’ll learn how to make this migration as smooth as possible, and learn the differences and similarities between the two clients. While Beeline offers some more non-essential options such as coloring, this post mainly focuses on how to achieve with Beeline what you used to do with Hive CLI. Use Cases: Hive CLI versus Beeline The following section focuses on the common uses of Hive CLI/HiveServer1 and how you can migrate to Beeline/HiveServer2 in each case. Server Connection Hive CLI connects to a remote HiveServer1 instance using the Thrift protocol. To connect to a server, you specify the host name and optionally the port number of the remote server: &gt; hive -h &lt;hostname&gt; -p &lt;port&gt;
   In contrast, Beeline connects to a remote HiveServer2 instance using JDBC. Thus, the connection parameter is a JDBC URL that’s common in JDBC-based clients: &gt; beeline -u  &lt;url&gt; -n &lt;username&gt; -p &lt;password&gt;
   Here are a few URL examples: 
jdbc:hive2://ubuntu:11000/db2?hive.cli.conf.printheader=true;hive.exec.mode.local.auto.inputbytes.max=9999#stab=salesTable;icol=customerID
jdbc:hive2://?hive.cli.conf.printheader=true;hive.exec.mode.local.auto.inputbytes.max=9999#stab=salesTable;icol=customerID
jdbc:hive2://ubuntu:11000/db2;user=foo;password=bar
jdbc:hive2://server:10001/db;user=foo;password=bar?hive.server2.transport.mode=http;hive.server2.thrift.http.path=hs2
   Query Execution Executing queries in Beeline is very similar to that in Hive CLI. In Hive CLI: 
&gt; hive -e &lt;query in quotes&gt;
&gt; hive -f &lt;query file name&gt;
   In Beeline: 
&gt; beeline -e &lt;query in quotes&gt;
&gt; beeline -f &lt;query file name&gt;
   In either case, if no -e or -f options are given, both client tools go into an interactive mode in which you can give and execute queries or commands line by line. Embedded Mode Running Hive client tools with embedded servers is a convenient way to test a query or debug a problem. While both Hive CLI and Beeline can embed a Hive server instance, you would start them in embedded mode in slightly different ways. To start Hive CLI in embedded mode, just launch the client without giving any connection parameters: &gt; hive   To start Beeline in embedded mode, a little more work is required. Basically, a connection URL of jdbc:hive2:// needs to be specified: &gt; beeline -u jdbc:hive2://
   At this point, Beeline enters interactive mode, in which queries and commands against the embedded HiveServer2 instance can be executed. Variables Perhaps the most interesting difference between the clients concerns the use of Hive variables. There are four namespaces for variables: hiveconf for Hive configuration variables system for system variables env for environment variables hivevar for Hive variables (HIVE-1096) A variable is expressed as &amp;namespace&gt;:&lt;variable_name&gt;. For Hive configuration variables, the name space hiveconf can be skipped.�The value of the variable can be referenced using dollar notation, such as ${hivevar:var}. There are two ways to define a variable: as a command-line argument or using the set command in interactive mode. Defining Hive variables in command line in Hive CLI: &gt; hive -d key=value
&gt; hive --define key=value
&gt; hive --hivevar key=value   Defining Hive variables in command line in Beeline: &gt; beeline --hivevar key=value   Defining Hive configuration variables in command line in Hive CLI: &gt; hive --hiveconf key=value
   At the time of this writing, in Beeline it’s not possible to define Hive configuration variables in command line (HIVE-6173). In either Hive CLI and Beeline, you would set variables in interactive mode the same way using the set command: hive&gt; set system:os.name=OS2;
0: jdbc:hive2://&gt; set system:os.name=OS2;   Show the value of a variable: 
hive&gt; set env:TERM;
env:TERM=xterm
0: jdbc:hive2://&gt; set env:TERM;
(Currently display nothing. HIVE-6174)   Note that environment variables cannot be set: hive&gt; set env:TERM=xterm;
env:* variables cannot be set.
0: jdbc:hive2://&gt; set env:TERM=xterm;
env:* variables can not be set.
   The set command without any arguments lists all variables with their values: hive&gt; set;
datanucleus.autoCreateSchema=true
...
0: jdbc:hive2://&gt; set;
+----------------------------------------------------------------+
|                                                                |
+----------------------------------------------------------------+
| datanucleus.autoCreateSchema=true
   Command-Line Help Of course, you can always find help on the command-line arguments: &gt; hive -H
&gt; beeline -h
&gt; beeline --help
   Interactive Mode In Hive CLI interactive mode, you can execute any SQL query that is supported by HiveServer. For example: hive&gt; show databases;
OK
default   Furthermore, you can execute shell command without leaving Hive CLI: hive&gt; !cat myfile.txt;
This is my file!
hive&gt;   In Beeline, you can execute any SQL query as you would in Hive CLI. For example: 0: jdbc:hive2://&gt; show databases;
14/01/31 16:50:47 INFO log.PerfLogger: &lt;PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver&gt;
...
+----------------+
| database_name  |
+----------------+
| default    	|
+----------------+

1 row selected (0.026 seconds)

...
   The above command is equivalent to: 0: jdbc:hive2://&gt; !sql show databases;
   As you can see, you use “!” to execute Beeline commands instead of shell commands. Among Beeline commands, !connect is among the most important; it allows you to connect to a database: beeline&gt; !connect jdbc:hive2://
scan complete in 2ms
Connecting to jdbc:hive2://
Enter username for jdbc:hive2://:
Enter password for jdbc:hive2://:
...
Connected to: Apache Hive (version 0.13.0-SNAPSHOT)
Driver: Hive JDBC (version 0.13.0-SNAPSHOT)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://&gt;
   Another important command is !quit (or !q), which allows you to exit interactive mode: 0: jdbc:hive2://&gt; !quit
Closing: org.apache.hive.jdbc.HiveConnection
   For a list of all Beeline commands, please refer to the SQLLine document here. Conclusion As you can see, the Hive community is working hard to make Beeline as similar to Hive CLI as possible in terms of functionality as well as syntax. The comparisons above should help make your transition relatively painless. Xuefu Zhang is a Software Engineer at Cloudera and a Hive Committer.</snippet></document><document id="42"><title>This Month in the Ecosystem (January 2014)</title><url>http://blog.cloudera.com/blog/2014/02/this-month-in-the-ecosystem-january-2014/</url><snippet>Welcome to our fifth edition of “This Month in the Ecosystem,” a digest of highlights from January 2014 (never intended to be comprehensive; for completeness, see the excellent�Hadoop Weekly). With the close of 2013, we also thought it appropriate to include some high points from across the year (not listed in any particular order): Cloudera announced the general availability of Apache Spark, with a new parcel�tested for use with CDH 4.4 and beyond. This makes Spark, which coincidentally graduated from ASF incubation on nearly the same day, the newest processing framework for enterprise data hubs (in this case, for workloads involving fast, advanced analytics). Speaking of Spark, its original developers at UC Berkeley’s AMPLab have released a new front-end for running R queries on a Spark cluster (RSpark). New benchmark testing revealed that a diverse set of (20) TPC-DS queries ran faster on Impala and Parquet than on a leading analytic DBMS and its own proprietary data store. These tantalizing results indicate that the common view of Apache Hadoop “trade-offs” (that is, the sacrifice of performance for flexibility) are no longer based on fact. Foursquare made it known that it has open-sourced its Hadoop connector for MongoDB, on which it relies to analyze user-generated data captured operationally. HBaseCon 2014 was announced and will occur on May 5, 2014, in San Francisco. Call for Papers and Early Bird registration both close on one week from today! DataFu, the LinkedIn-developed library of Apache Pig UDFs, was accepted into the Apache Incubator. (DataFu is also distributed inside CDH.) InfoWorld named Hadoop�an awardee for “Technology of the Year” for 2014. Another kudo for the community! That’s all for this month, folks! Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="43"><title>How-to: Write and Run Giraph Jobs on Hadoop</title><url>http://blog.cloudera.com/blog/2014/02/how-to-write-and-run-giraph-jobs-on-hadoop/</url><snippet>Create a test environment for writing and testing Giraph jobs, or just for playing around with Giraph and small sample datasets. Apache Giraph is a scalable, fault-tolerant implementation of graph-processing algorithms in Apache Hadoop clusters of up to thousands of computing nodes. Giraph is in use at companies like Facebook and PayPal, for example, to help represent and analyze the billions (or even trillions) of connections across massive datasets. Giraph was inspired by Google’s Pregel framework and integrates well with Apache Accumulo, Apache HBase, Apache Hive, and Cloudera Impala. Currently, the upstream “quick start” document explains how to deploy Giraph on a Hadoop cluster with two nodes running Ubuntu Linux. Although this setup is appropriate for lightweight development and testing, using Giraph with an enterprise-grade CDH-based cluster requires a slightly more robust approach. In this how-to, you will learn how to use Giraph 1.0.0 on top of CDH 4.x using a simple example dataset, and run example jobs that are already implemented in Giraph. You will also learn how to set up your own Giraph-based development environment. The end result will be a setup (not intended for production) for writing and testing Giraph jobs, or just for playing around with Giraph and small sample datasets. (In future posts, I will explain how to implement your own graph algorithms and graph generators as well as how to export your results to Gephi, the “Adobe Photoshop for graphs”, through Impala and JDBC for further inspection.) How Giraph Works As in the core MapReduce framework, in Giraph, all data and workload distribution related details are hidden behind an easy-to-use API. Currently, Giraph API is used on top of MR1 via a slightly unstable mechanism, although a YARN-based implementation is also possible (also to be discussed in a future post). I recommend, to turn off preemption in your cluster if you plan to run Giraph jobs. In Giraph, a worker node or a slave node is a host (either a physical server or even a virtualized server) that performs the computation and stores data in HDFS. Such workers load the graph and keep the full graph or just a part of it (in case of distributed graph analysis) in memory. Very large graphs are partitioned and distributed across many worker nodes. (Note: the term partition has a different meaning in Giraph. Here, the partitioning of the graph is not necessarily the result of the application of a graph-partitioning algorithm. Rather, it is more a way to group data based on a vertex hash value and the number of workers. This approach is comparable to the partitioning that is done during the shuffle-and-sort phase in MapReduce.) A Giraph algorithm is an iterative execution of “super-steps”? that consist of a message exchange phase followed by an aggregation and node or edge property update phase. While vertices and edges are held in memory, the nodes exchange messages in parallel. Therefore, all worker nodes communicate and send each other small messages, usually of very low data volume. After this message-exchange phase, a kind of aggregation is done. This leads to an update of the vertex and/or edge properties and a super-step is finished. Now, the next super-step can be executed and nodes communicate again. This approach is known as Bulk Synchronous Processing (BSP). The BSP model is vertex based and generally works with a configurable graph model G&lt;I,V,E,M&gt;, where I is a vertex ID, V a vertex value, E an edge value, and M a message data type, which all implement the Writable interface (which is well known from the Hadoop API). However, the iterative character of many graph algorithms is a poor fit for the MapReduce paradigm, especially if the graph is a very large one like the full set of interlinked Wikipedia pages. In MapReduce, a data set is streamed into the mappers and aggregation of intermediate results is done in reducers. One MapReduce job can implement one super-step. In a next super-step, the whole graph structure — together with the stored intermediate state of the previous step — have to be loaded from HDFS and stored at the end again. Between processing steps, the full graph is loaded from HDFS and stored there, which is a really large overhead. And let‘s not forget that the intermediate data requires local storage on each worker node while it passes the shuffle-sort phase. For very large graphs, it is inefficient to repeatedly store and load the more or less fixed structural data. In contrast, the BSP approach loads the graph only once, at the outset. The algorithms assume that runtime-only messages are passed between the worker nodes. To support fault-tolerant operation the intermediate state of the graph can be stored from time to time, but usually not after each super-step. MapReduce-based implementations of graph algorithms are demonstrated in the book Data-Intensive Text Processing with MapReduce (J. Lin, C. Dyer). Hands-On The starting point for the hands-on portion of this how-to is Clouder’s QuickStart VM, which contains a virtualized pseudo-distributed Hadoop cluster managed by Cloudera Manager. The VM has many useful modules already pre-installed, including Git client, Maven, and the Eclipse IDE. In this environment, you can deploy and run Giraph benchmarks as functional tests (rather than for performance reasons). Prepare and Test the Virtual Hadoop cluster The Cloudera QuickStart VM requires a 64-bit environment and a minimum of 4GB of RAM allocated to the virtual machine. Our deployment here requires the following software/hardware setup: Hardware: (either a real PC or the Cloudera QuickStart VM, download it here) Dual-core 2 GHz CPU (64-bit architecture) 4GB RAM (6GB would be much better) 25GB hard drive space 100 Mbps NIC CentOS 6.4 (64-bit) Admin account (in Quickstart-VM) username: cloudera password: cloudera Hostname: localhost IP address: 127.0.0.1 Network mask: 255.255.255.0 Oracle JDK (version 1.6.0_32) Apache Hadoop (CDH 4.x) Apache Giraph 1.0.0 An accompanying Github project for this tutorial, called giraphl, contains related material such as a convenient bootstrap script. This script will save you time as you do not have to type all commands provided in this tutorial. Those who want a more manual experience, however, can follow the steps below. Test the Hadoop Deployment First, start the VM and confirm that all required services are up and running. Either you use command sudo jps, which should list at least the following services: NameNode SecondaryNameNode DataNode JobTracker TaskTracker Or, open Cloudera Manager in a browser at http://localhost:7180/. Figure 1: Cloudera Manager shows the service status of a healthy Hadoop cluster in a QuickStart VM. You can deploy the Hadoop client configuration to the local host, which is used during the tutorial. The HDFS and MapReduce services are required at a minimum for this how-to. If all services are not in a healthy state, start troubleshooting now. A prerequisite for running MapReduce jobs in the QuickStart VM (because it uses Cloudera Manager) is to deploy the client configuration. If you skip this step, the hadoop command will use the local job runner by default. So, you need to do this sequence of steps: Go to Cloudera Manager at http://localhost:7180/. Login using “cloudera” for both the username and password. Click the “Actions” menu directly below “Add Cloudera Management Services” and select “Deploy Client Configuration”. (Note: There are several similar dropdowns, so refer to the screenshot in Figure 1 for the correct one.) Click the blue “Deploy Client Configuration”? button to confirm your selection. In less than a minute, you’ll see confirmation that the configuration set up in Cloudera Manager is available to the clients of the different services (such as MapReduce) on the cluster. Running a MapReduce Job Some MapReduce jobs can now be executed. For a first test, use the TeraGen and TeraSort jobs to create some random test data and to sort the data set via MapReduce. As user “cloudera”?, call the following commands: Run TeraGen: $ hadoop jar /usr/lib/hadoop-0.20-mapreduce/hadoop-examples.jar
  teragen 50000 TESTFILE
    Run TeraSort: $ hadoop jar /usr/lib/hadoop-0.20-mapreduce/hadoop-examples.jar
  terasort TESTFILE TESTFILE.sorted
    While the jobs are running you can monitor them via the Web user interface: NameNode daemon: http://127.0.0.1:50070 JobTracker daemon: http://127.0.0.1:50030 Your Hadoop cluster is now ready for action. But before writing a Giraph job, let’s review some Giraph concepts. Anatomy of a Giraph Program A Giraph program consists of a master, many workers, and an Apache ZooKeeper ensemble that maintains the global state of the application or the Giraph job. The active master works as an application coordinator — it synchronizes the super-steps. If the master fails, another master from the ZooKeeper master queue can be activated. (Thus, fault tolerance is built-in.) Workers get a partition of the graph assigned from the master before the super-step starts. Each worker handles the I/O operations, the computational part of the algorithm, and the messaging between the nodes. Figure 2 illustrates the three phases of the data flow in a Giraph application. It starts with loading the graph (Phase 1): The master assigns the InputSplits to workers, which are responsible for loading the data. This process is similar to classical MapReduce processing, where each mapper task is responsible for processing one InputSplit. Phase 2 is the iterative part of the algorithm, which consists of concatenated super-steps. Finally, each worker contributes the data from all partitions for which it was responsible to the OutputFormat to write results to HDFS, HBase, or even Accumulo. Figure 2: The Giraph data flow in a classical MapReduce implementation In classical Hadoop 1.x, a map-only job initializes the BSP based implementation of large-scale graph algorithms. After workers load the initial graph data (Phase 1 in Figure 2), the super-steps are executed (Phase 2 in Figure 2). Within each super-step, the method compute( … ) of all given vertexes is called. A vertex has the data, which describes the state of the node, and its compute method is part of the implementation of a single algorithm. So you need special vertex implementations for different algorithms. Some algorithms require an iterative update of a certain node property or vertex value (for example, the page rank of a node). Therefore, you have to store such data outside the compute method as a vertex attribute. All attributes within a vertex are visible within this vertex only and should be in the private part of the vertex class. Don’t forget to initialize the variable before the program starts! One simple example is the single-source shortest path (SSSP) algorithm. Here, you store the current lowest distance to a selected source vertex as a node property in each vertex. After a new message is received, the values can be updated with the new lowest value at the end of each iteration. Sometimes you need access to node properties from other nodes. To make such values global accessible, use a persistent aggregator. The aggregator has to be initialized before the program starts, and in each super-step, every vertex can send its current value there. Such an aggregator uses a hashmap to store a value for each vertex id. In the case of a persistent aggregator, the data is persistent across all super-steps and the data will be accessible by all vertexes. Developers should be careful to not collect too many large data objects in an aggregator. Deploying Giraph In order to download Giraph from a repository and build it, you need Git and Maven 3 installed. Both packages are preinstalled and configured in QuickStart VM. Giraph can support multiple versions of Hadoop, and here you’ll use the hadoop-2.0.0 profile. First, choose a directory for Giraph. (Most people use /usr/local/giraph, but this is not a strong requirement.) You should define this location as a system variable called GIRAPH_HOME and export it via the .bashrc file in the home directory of the user cloudera. Therefore, you have to edit /home/cloudera/.bashrc. Add the following line: export GIRAPH_HOME=/usr/local/giraph (Note: At the time of this writing, it was not possible to compile giraph-1.1.0. Therefore, the latest stable release, version 1.0.0, is used here.) Let’s define a version and a profile identifier variable by adding: export GV=1.0.0
export PRO=hadoop_2.0.0
   to the file /home/cloudera/.bashrc and refresh the environment variables with: 
$ source ~/.bashrc
   Clone the Giraph project from a Github mirror: $ sudo mkdir /usr/local/giraph
$ cd $GIRAPH_HOME
$ cd ..
$ sudo git clone https://github.com/apache/giraph.git
$ sudo chown -R cloudera:hadoop giraph
   Check out the stable branch, and build Giraph by running the following commands. (Note: there is no space character between -D and skipTests): $ cd $GIRAPH_HOME
$ git checkout release-$GV
$ mvn package -DskipTests -Dhadoop=non_secure -P $PRO
   The argument -DskipTests will skip the testing phase to save some time. As you are not working with a secure cluster setup, you have to select the non-secure mode by using the option -Dhadoop=non_secure. The build procedure may take a while. You may have to execute the build command multiple times. But finally you should see output similar to the following: [INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Apache Giraph Parent .................... 	SUCCESS [0.720s]
[INFO] Apache Giraph Core ....................... 	SUCCESS [21.551s]
[INFO] Apache Giraph Hive I/O ................. 	SUCCESS [10.896s]
[INFO] Apache Giraph Examples ............... 	SUCCESS [7.088s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 40.526s
[INFO] Finished at: Thu Dec 05 08:36:52 PST 2013
[INFO] Final Memory: 43M/569M
[INFO] ------------------------------------------------------------------------   At the end, you should have the distributable Giraph JAR file: giraph-core/target/giraph-$GV-for-$PRO-alpha-jar-with-dependencies.jar
   and the Giraph examples JAR file: giraph-examples/target/giraph-examples-$GV-for-$PRO-alpha-jar-with-dependencies.jar   For convenience, I prefer to create symbolic links to the jar files. Usually such links are managed by the Linux alternatives command. $ cd $GIRAPH_HOME
$ sudo ln -s
giraph-core/target/giraph-$GV-for-$PRO-alpha-jar-with-dependencies.jar
giraph-core.jar
$ sudo ln -s
giraph-examples/target/giraph-examples-$GV-for-$PRO-alpha-jar-with-dependencies.jar
giraph-ex.jar
   Preparing Sample Data The next step is to prepare some example data sets in HDFS. The official Giraph quick-start tutorial uses the tiny_graph.txt file, which contains a small test network. (Later, this network can be loaded via JsonLongDoubleFloatDoubleVertexInputFormat input format.) Figure 3: The tiny sample graph plotted in Gephi. Copy the text below: [0,0,[[1,1],[3,3]]] [1,0,[[0,1],[2,2],[3,1]]] [2,0,[[1,2],[4,4]]] [3,0,[[0,3],[1,1],[4,4]]] [4,0,[[3,4],[2,4]]] into a new empty file named tiny_graph.txt in your QuickStart VM and then copy this file to HDFS into a directory named ginput, which is in your HDFS home directory. Next, create some working directories in HDFS where you will store the input data sets and results: $ hadoop fs -mkdir ginput
$ hadoop fs -mkdir goutput
   To copy the manually created input file to HDFS, you should use the Hadoop filesystem shell. Please make the current directory in your shell the one in which you stored the tiny_graph.txt file and then type: $ hadoop fs -put tiny_graph.txt ginput   The meaning of the data in each line is: [source_id,source_value,[[dest_id, edge_value],...]]   The source_value property describes the node and the edge_value is an edge property (such as the correlation link strength). There are five nodes and 12 directed edges in this graph. Another example data set is available here. Download and decompress the file, then upload it to HDFS. But first you have to install wget via yum package manager. $ sudo yum install wget
$ wget http://ece.northwestern.edu/~aching/shortestPathsInputGraph.tar.gz
$ tar zxvf shortestPathsInputGraph.tar.gz
$ hadoop fs -put shortestPathsInputGraph ginput
   With those files in place, you can now run some Giraph jobs. Running Giraph Jobs The Giraph library offers MapReduce programs, called GiraphJobs, but no additional services for the Hadoop cluster. Therefore, you do not have to deploy any configuration or services on your running Hadoop cluster; rather, just submit the existing map-only jobs. As soon as you do that via the hadoop command-line tool, you have to specify all required libraries. In this case, that will be the giraph-core.jar and the giraph-ex.jar files. The first one contains all Giraph-related code and has to be deployed via the -libjars option. This means that the jar files specified here are added to the distributed cache. During runtime, those libraries are available in the Java classpath on all cluster nodes running a task, which belongs to the submitted job. Giraph jobs are MapReduce jobs. Although mappers usually do not communicate with other mappers, Giraph uses MapReduce only during the initialization phase and mapper-to-mapper communication is actually required. A ZooKeeper quorum provides a global shared memory. You tell Giraph about the configuration of the ZooKeeper servers with the following property: -Dgiraph.zkList=&lt;zknode1&gt;:&lt;zkport1&gt;,&lt;zknode2&gt;:&lt;zkport2&gt; ...
   The Cloudera QuickStart VM has a running HBase cluster with ZooKeeper running on port 2181. So, use that instead of Giraph’s default ZooKeeper port, which is assumed to be port 22181. The rest of this section will show you how to run Giraph jobs and what parameters are required. More details about using other existing algorithms and designing your own implementations will be explained in a subsequent post. PageRankBenchmark There are some benchmarks implemented in the package org.apache.giraph.benchmark. Run the PageRankBenchmark just to verify the setup and the build. $ hadoop jar giraph-ex.jar org.apache.giraph.benchmark.PageRankBenchmark
-Dgiraph.zkList=127.0.0.1:2181 -libjars giraph-core.jar
-e 1 -s 3 -v -V 50 -w 1
   Parameter Name Description -e --edgesPerVertex Number of edges per vertex that are generated -s Number of super-steps to execute before finishing the job -v Run the job in verbose mode. -V --aggregateVertices Aggregate the vertices. -w --workers Number of workers (the number of mappers that have to be started in parallel) SimpleShortestPathsVertex The SimpleShortestPathsVertex job will be started to explain the specific command-line properties, which are slightly different from a classical Hadoop job. It reads a graph from an input file stored in HDFS to compute the length of the shortest paths from one chosen source node to all other nodes. The current implementation of the algorithm will only process the first vertex in the input file. If you need the shortest path for all available vertexes, multiple jobs would have to be started. In each run you would specify the vertex id with a custom argument: -ca SimpleShortestPathsVertex.source=2   Here we will use the JsonLongDoubleFloatDoubleVertexInputFormat and IdWithValue- TextOutputFormat output file formats. The result file is a simple text file where each line consists of target_id length for each vertex in the graph. Length is the shortest path to a target node, starting from the single source node, defined by the custom property. $ hadoop jar giraph-ex.jar org.apache.giraph.GiraphRunner
-Dgiraph.zkList=127.0.0.1:2181 -libjars giraph-core.jar
org.apache.giraph.examples.SimpleShortestPathsVertex
-vif org.apache.giraph.io.formats.JsonLongDoubleFloatDoubleVertexInputFormat
-vip /user/training/ginput/tiny_graph.txt
-of org.apache.giraph.io.formats.IdWithValueTextOutputFormat
-op /user/training/goutput/shortestpathsC2 -ca SimpleShortestPathsVertex.source=2   -w 1
    Parameter Name Description -vif The VertexInputFormat for the job -vip The path in HDFS from which the VertexInputFormat loads the data -of The OutputFormat for the job -op The path in HDFS to which the OutputFormat writes the data -ca A custom argument is defined as a key value pair   Figure 4: The file browser app in Hue shows the content of the result file, which was generated by our first Giraph job. Conclusion Congratulations, you now have a running Hadoop cluster on your desktop and a fresh installation of the latest stable Giraph release, which is version 1.0.0. You should also now know how a Giraph job works and how to run existing algorithms. In future posts, you will get a deep dive into Giraph development and also discover how to export data to Gephi via Hive, Impala (via JDBC) to visualize large graphs. Any feedback is welcome in comments! Mirko K�mpf is the lead instructor for the Cloudera Administrator Training for Apache Hadoop for Cloudera University.  </snippet></document><document id="44"><title>Spark is Now Generally Available for Cloudera Enterprise</title><url>http://blog.cloudera.com/blog/2014/02/spark-is-now-generally-available-for-cloudera-enterprise/</url><snippet>Cloudera is announcing the general availability of support for Spark, bringing interactive machine learning and stream processing to enterprise data hubs. Cloudera is pleased to announce the immediate availability of its first release of Apache Spark�for Cloudera Enterprise (comprising CDH and Cloudera Manager). Spark was created and contributed to the Apache Software Foundation by UC Berkeley, and it has quickly gained adoption for machine learning, interactive analytics, and streaming analytics over large datasets. It features a general programming model for writing applications by composing arbitrary operators, such as mappers, reducers, joins, group-bys, and filters. Spark keeps track of the data that each of the operators produces, enabling applications to reliably store this data in memory, which makes it ideal for low-latency computations and efficient iterative algorithms. Spark applications can be up to 100x faster and require writing 2x to 10x less code than equivalent MapReduce applications.� Cloudera provides enterprise support for Spark through Cloudera Enterprise Flex Edition (as an optional component) and Data Hub Edition (as an included component) subscriptions. This release provides Spark 0.9.0 tested for use with Spark Standalone Mode on CDH 4, from 4.4.0 forward. Expect releases for Cloudera Enterprise 5 (comprising CDH 5 and Cloudera Manager 5) and Spark on YARN in the near future. To get started now, you can follow these instructions to install Spark using parcels with Cloudera Manager. The instructions will also walk you through the basic configuration, and a simple WordCount example on Spark. Once you get going, we would love to hear your feedback: You can ask questions, get help, and share your growing expertise on our community forum for questions about Spark. You can file a bug through our public Jira instances. For issues with Spark please use https://issues.cloudera.org/browse/DISTRO. For issues with the beta integration with Cloudera Manager, please use https://issues.cloudera.org/browse/CM.</snippet></document><document id="45"><title>How Wajam Answers Business Questions Faster With Hadoop</title><url>http://blog.cloudera.com/blog/2014/01/how-wajam-answers-business-questions-faster-with-hadoop/</url><snippet>Thanks to Xavier Clements of Wajam for allowing us to re-publish his blog post about Wajam’s Hadoop experiences below! Wajam is a social search engine that gives you access to the knowledge of your friends. We gather your friends� recommendations from Facebook, Twitter, and other social platforms and serve these back to you on supported sites like Google, eBay, TripAdvisor, and Wikipedia. To do this, we aggregate, analyze and index relevant pieces of shared information on our users� social networks. The challenge for the Business Intelligence team is not so much the storage of the vast number of logs that our millions of users are generating, as our Apache Hadoop cluster is sizeable, rather it is how to quickly answer business questions by running Pig jobs across these entries. As a Business Intelligence analyst who was only introduced to MapReduce (MR) jobs earlier this year I wanted to blog about how frustrating it can be to write Pig jobs to run across our Hadoop cluster containing our users� raw logs � oh how I wanted to write that article. It would have consisted of diatribes about Pig�s horrible compile and run time error messaging, litanies of expletives relating to the sheer number of hours spent waiting for MR jobs to run across our cluster. More recent changes implemented at Wajam by one of our Business Intelligence (BI) gurus have, to a large extent, made such a programming polemic null and void. Thanks to just a little further processing of our logs and storing this rigidly formatted output, our reliance on the raw logs has decreased and with it at least the author�s Pig related profanity has lessened. The primary goal behind these changes was to improve the speed with which we can provide a response to a business query. A secondary goal was to make our data stored on our Hadoop cluster more accessible for everyone. About Our Process The pipeline: Our Hadoop cluster (HC) was and continues to be used for the storage of raw aggregated logs sent to our NameNode from the Scribe servers running on top of our many Web servers and for storing the outputs of the automated Pig jobs that run upon these logs. The reporting pipeline of Pig jobs is scheduled by some Scala scripts, with the overall process summarized below: The logs: Much of the pipeline�s purpose is to aggregate information in the logs and regularly populate our MySQL server so that queries and alerts related to our traffic can be implemented in R. However it is often the raw logs that are queried for our ad hoc requests, as our stored aggregated data may not fit the bill. It is here that our old structure was causing a sticking point. To explain: the raw logs are broken into events, and each event has a certain amount of generic data e.g. date, timestamp, country, user id etc. A further set of fields is populated on a per-event basis. With this flexibility comes an element of complexity, for when querying these events one has to write pig jobs knowing exactly how all of the events are linked. As an example, we have an event for every search that a user makes and one of the fields recorded is a unique search id. That same search id will carry across to other events for when we crawl our partners� servers and for the events generated upon such successful crawls and so on and so forth. Trawling through our memories for how all these events link together when writing a Pig script to answer a particular question regarding our users� search behavior can unnecessarily slow the process down. Recent Changes Our solution to this problem? As there is little new under the sun, we sought inspiration from others who are using Hadoop. LinkedIn, unsurprisingly, was a first port of call e.g. a simple, clean and clever idea like incremental processing to speed up regular Hadoop jobs. With thoughts along similar lines, the BI team added a pre-processing step: store the collective information about every search that could be gathered from our events. As we have recently increased the size of our cluster to just under 100 nodes, the cost in space for storing this extra information was minimal. The positives of moving to this form of granular pre-processing on the other hand were manifold. Two highlights include: 1. Speed : When a business request came in previously, we often had to run Pig jobs which drew directly from the raw logs. The Pig scripts themselves are not all that difficult to write, however the run time was slowed by the sheer size of the files from which we are drawing. This load time issue was compounded when the request required us to load multiple weeks worth of data. Now a month of search data can be loaded in and processed under 2 hours, where previously an equivalent period in raw logs would have taken 3 � 4 hours or the job would simply have hung. 2. Ease of use: Conceptually it is far easier to query a rigid table like structure rather than try to join a myriad of events. This has the added benefit of making newcomers to Wajam less reticent to venture into the land of Hadoop and Pig. Impala Further to point 2 above and to lessen our reliance on MySQL, the next step is to add the Impala feather to our bow. Inspired by Google�s Dremel paper, the Cloudera Impala project aims �to bring real-time, ad hoc query capability to Apache Hadoop� Impala is an “open source Massively Parallel Processing (MPP) query engine that runs natively on Apache Hadoop�. What does this encompass? Essentially multiple clients can be used to issue SQL queries and the Hive metastore provides information as to what databases are available and the schemas associated with same (see diagram below). Then there is an Impala process running on every DataNode in the HDFS which parses up the original SQL queries and runs them on each node. It is the presence of each of these Impala instances on each node and the associate distributed query engine that means MapReduce can be bypassed. As a result of being able to remove MR, there have been instances of 68x speedup as compared to Hive . The extent of the speed improvement is query dependent. In order to leverage the power of Impala, we are incrementally migrating our servers to CDH 4; however some preliminary testing has already begun. Conclusion Coupling our more rigid data structures like our newly created search tables and Impala will likely see an even greater decrease in the turn around time for responding to business requests. The positive effect this will have on our department is hard to quantify � especially as the more approachable SQL like syntax and organized data sets will allow us to explore our data more readily. That is not to say we will entirely rid ourselves of the need to run ad hoc Pig jobs to answer particular requests. However if the bulk of the heavy lifting has been done, the occasional MR job will hopefully keep the colorful language to a minimum. You can find Xavier Clements on Twitter.</snippet></document><document id="46"><title>How-to: Create a Simple Hadoop Cluster with VirtualBox</title><url>http://blog.cloudera.com/blog/2014/01/how-to-create-a-simple-hadoop-cluster-with-virtualbox/</url><snippet>Set up a CDH-based Hadoop cluster in less than an hour using VirtualBox and Cloudera Manager. Thanks to Christian Javet for his permission to republish his blog post below! I wanted to get familiar with the big data world, and decided to test Hadoop. Initially, I used Cloudera�s pre-built virtual machine with its full Apache Hadoop suite pre-configured (called Cloudera QuickStart VM), and gave it a try. It was a really interesting and informative experience. The QuickStart VM is fully functional and you can test many Hadoop services, even though it is running as a single-node cluster. I wondered what it would take to install a small four-node cluster… I did some research and I found this excellent video on YouTube presenting a step by step explanation on how to setup a cluster with VMware and Cloudera. I adapted this tutorial to use VirtualBox instead, and this article describes the steps used. Overview High-level diagram of the VirtualBox VM cluster running Hadoop nodes The overall approach is simple. We create a virtual machine, we configure it with the required parameters and settings to act as a cluster node (specially the network settings). This referenced virtual machine is then cloned as many times as there will be nodes in the Hadoop cluster. Only a limited set of changes are then needed to finalize the node to be operational (only the hostname and IP address need to be defined). In this article, I created a 4 nodes cluster. The first node, which will run most of the cluster services, requires more memory (8GB) than the other 3 nodes (2GB). Overall we will allocate 14GB of memory, so ensure that the host machine has sufficient memory, otherwise this will impact your experience negatively. Preparation The prerequisites for this tutorial is that you should have the latest VirtualBox installed (you can download it for free); We will be using the CentOS 6.5 Linux distribution (you can download the CentOS x86_64bit DVD iso image). Base VM Image creation VM creation Create the reference virtual machine, with the following parameters: Bridge network Enough disk space (more than 40GB) 2 GB of RAM Setup the DVD to point to the CentOS iso image when you install CentOS, you can specify the option �expert text�, for a faster OS installation with minimum set of packages. Network Configuration Perform changes in the following files to setup the network configuration that will allow all cluster nodes to interact. /etc/resolv.conf search example.com
nameserver 10.0.1.1
   /etc/sysconfig/network NETWORKING=yes
HOSTNAME=base.example.com
GATEWAY=10.0.1.1
   /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0
ONBOOT=yes
PROTO=static
IPADDR=10.0.1.200
NETMASK=255.255.255.0
   /etc/selinux/config SELINUX=disabled
   /etc/yum/pluginconf.d/fastestmirror.conf enabled=0
   Initialize the network by restarting the network services: $&gt; chkconfig iptables off
$&gt; /etc/init.d/network restart
   Installation of VM Additions You should now update all the packages and reboot the virtual machine: $&gt; yum update
$&gt; reboot
   In the VirtualBox menu, select Devices, and then Insert Guest�. This insert a DVD with the iso image of the guest additions in the DVD Player of the VM, mount the DVD with the following commands to access this DVD: $&gt; mkdir /media/VBGuest
$&gt; mount -r /dev/cdrom /media/VBGuest
   Follow instructions from this web page. Setup Cluster Hosts Define all the hosts in the /etc/hosts file in order to simplify the access, in case you do not have a DNS setup where this can be defined. Obviously add more hosts if you want to have more nodes in your cluster. /etc/hosts 
10.0.1.201 hadoop1.example.com hadoop1
10.0.1.202 hadoop2.example.com hadoop2
10.0.1.203 hadoop3.example.com hadoop3
10.0.1.204 hadoop4.example.com hadoop4
   Setup SSH To also simplify the access between hosts, install and setup SSH keys and defined them as already authorized $&gt; yum -y install perl openssh-clients
$&gt; ssh-keygen (type enter, enter, enter)
$&gt; cd ~/.ssh
$&gt; cp id_rsa.pub authorized_keys
   Modify the ssh configuration file. Uncomment the following line and change the value to no; this will prevent the question when connecting with SSH to the host. /etc/ssh/ssh_config StrictHostKeyChecking no
   Shutdown and Clone At this stage, shutdown the system with the following command: $&gt; init 0
   We will now create the server nodes that will be members of the cluster. in VirtualBox, clone the base server, using the �Linked Clone� option and name the nodes hadoop1, hadoop2, hadoop3 and hadoop4. For the first node (hadoop1), change the memory settings to 8GB of memory. Most of the roles will be installed on this node, and therefore it is important that it have sufficient memory available. Clones Customization For every node, proceed with the following operations: Modify the hostname of the server, change the following line in the file: /etc/sysconfig/network HOSTNAME=hadoop[n].example.com
   Where [n] = 1..4 (up to the number of nodes) Modify the fixed IP address of the server, change the following line in the file: /etc/sysconfig/network-scripts/ifcfg-eth0 IPADDR=10.0.1.20[n]
   Where [n] = 1..4 (up to the number of nodes) Let�s restart the networking services and reboot the server, so that the above changes takes effect: $&gt; /etc/init.d/network restart
$&gt; init 6
   at this stage we have four running virtual machines with CentOS correctly configured. Four Virtual Machines running on VirtualBox, ready to be setup in the Cloudera cluster. Install Cloudera Manager on hadoop1 Download and run the Cloudera Manager Installer, which will simplify greatly the rest of the installation and setup process. $&gt; curl -O http://archive.cloudera.com/cm4/installer/latest/cloudera-manager-installer.bin
$&gt; chmod +x cloudera-manager-installer.bin
$&gt; ./cloudera-manager-installer.bin
   Use a web browser and connect to http://hadoop1.example.com:7180 (or http://10.0.1.201:7180 if you have not added the hostnames into a DNS or hosts file). To continue the installation, you will have to select the Cloudera free license version. You will then have to define which nodes will be used in the cluster. Just enter all the nodes you have defined in the previous steps(e.g. hadoop1.example.com) separated by a space. Click on the �Search� button. You can then used the root password (or the SSH keys you have generated) to automate the connectivty to the different nodes. Install all packages and services onto the 1st node. Once this is done, you will select additional service components; just select everything by default. The installation will continue and will complete. Using the Hadoop Cluster Now that we have an operational Hadoop cluster, there are two main interfaces that you will use to operate the cluster: Cloudera Manager and Hue. Cloudera Manager Use a web browser and connect to�http://hadoop1.example.com:7180�(or�http://10.0.1.201:7180�if you have not added the hostnames into a DNS or hosts file). Cloudera Manager homepage, presenting cluster health dashboards Hue Similarly to Cloudera Manager, you can access the Hue administration site by accessing: http://hadoop1.example.com:8888, where you will be able to access the different services that you have installed on the cluster. Hue interface, and here more specifically, an Impala saved queries window. Conclusions I have been able to create a small Hadoop cluster in probably less than a hour, largely thanks to the Cloudera Manager Installer, which simplifies the installation to the simplest of operation. It is now possible to execute and use the various examples installed on the cluster, as well as understand the interactions between the nodes. Comments and remarks are welcome!</snippet></document><document id="47"><title>Pro Tips for Pitching an HBaseCon Talk</title><url>http://blog.cloudera.com/blog/2014/01/pro-tips-for-pitching-an-hbasecon-talk/</url><snippet>These suggestions from the Program Committee offer an inside track to getting your talk accepted! With HBaseCon 2014 (in San Francisco on May 5) Call for Papers closing in just over three weeks (on Feb. 14 — sooner than you think), there’s no better time than “now” to start thinking about your proposal. In an effort to help you crank up the creative process, I recently polled the HBaseCon Program Committee for suggestions about crafting a pitch that has “accept” written all over it: Focus on new use cases. If you/your employer is using Apache HBase in a novel or innovative way, that’s an attractive pitch. Tell us your success story. Even if your use case is not strictly “novel”, if you are successfully running HBase in production and few people know that �– especially if it’s outside the typical Web domain — that’s intriguing. Discuss HBase versus X.�Did you select HBase over an alternative data store? If so, what data, configuration, or features justified that decision, and why? Emphasize The Real World. Because science projects are just plain less interesting. As a corollary: Get practical. Provide content and advice that attendees can pack up and use immediately. Go deep. Deep-dives into advanced features are highly valued. At the other end of the spectrum, sessions that can serve as tutorials for beginners/adopters are also attractive. For alumni: Explain recent improvements and optimizations. If you’ve presented your use case or application at a previous HBaseCon, an review of optimizations done since then could be a nice angle. Have you solved hard problems? Tell us how. How does your HBase configuration help you meet your most challenging requirements, such as ensuring low latency for online systems, security, and so on? These are just a few suggestions to get you started. Don’t procrastinate, because�Call for Papers�will close before you know it! Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="48"><title>How-to: Get Started Writing Impala UDFs</title><url>http://blog.cloudera.com/blog/2014/01/how-to-get-started-writing-impala-udfs/</url><snippet>Cloudera provides docs and a sample build environment to help you get easily started writing your own Impala UDFs. User-defined functions (UDFs) let you code your own application logic for processing column values during a Cloudera Impala query. For example, a UDF could perform calculations using an external math library, combine several column values into one, do geospatial calculations, or other kinds of tests and transformations that are outside the scope of the built-in SQL operators and functions. You can use UDFs to simplify query logic when producing reports, or to transform data in flexible ways when copying from one table to another with the�INSERT ... SELECT�syntax. Since release 1.2.0,�Impala�has supported UDFs written in C++. Although existing Apache Hive UDFs written in Java are supported as well, Cloudera recommends using C++ UDFs because the compiled native code can yield higher performance — as illustrated in the chart below (running on a single core; see sample UDF here): � In summary: Native Impala UDFs execute 10x faster than Hive UDFs when run in Impala (resulting in significantly faster queries), and Hive UDFs run faster in Impala than they do in Hive. Impala can run scalar UDFs that return a single value for each row of the result set, and user-defined aggregate functions (UDAFs) that return a value based on a set of rows. Currently, Impala does not support user-defined table functions (UDTFs) or window functions (although this support is on the roadmap). In this post, you will learn how to get started writing your own UDFs in the current Impala release (1.2.3). Sample Build Environment for UDFs The Impala team has made a sample build environment available so that you can create your own UDFs with minimal work. To develop UDFs for Impala, download and install the�impala-udf-devel�package containing header files, sample source, and build configuration files. Start at�http://archive.cloudera.com/impala/�and locate the appropriate�.repo�or list file for your operating system version, such as�the�.repo�file for RHEL 6. Use the familiar�yum,�zypper, or�apt-get�commands depending on your operating system, with�impala-udf-devel�for the package name. (Note:�The UDF development code does not rely on Impala being installed on the same machine. You can write and compile UDFs on a minimal development system, then deploy them on a different one for use with Impala. If you develop UDFs on a server managed by Cloudera Manager through the parcel mechanism, you still install the UDF development kit through the package mechanism; this small standalone package does not interfere with the parcels containing the main Impala code.) When you are ready to start writing your own UDFs, download the sample code and build scripts from�the Cloudera sample UDF GitHub, and see�Examples of Creating and Using UDFs�for how to build and run UDFs. To understand the layout and member variables and functions of the predefined UDF data types, examine the header file�/usr/include/impala_udf/udf.h: // This is the only Impala header required to develop UDFs and UDAs. This header
// contains the types that need to be used and the FunctionContext object. The context
// object serves as the interface object between the UDF/UDA and the impala process.   For the basic declarations needed to write a scalar UDF, see the header file�udf-sample.h�within the sample build environment, which defines a simple function named�AddUdf(): 
#ifndef IMPALA_UDF_SAMPLE_UDF_H
#define IMPALA_UDF_SAMPLE_UDF_H

#include 

using namespace impala_udf;

IntVal AddUdf(FunctionContext* context, const IntVal&amp; arg1, const IntVal&amp; arg2);

#endif
   For sample C++ code for a simple function named�AddUdf(), see the source file�udf-sample.cc�within the sample build environment: #include "udf-sample.h"

// In this sample we are declaring a UDF that adds two ints and returns an int.
IntVal AddUdf(FunctionContext* context, const IntVal&amp; arg1, const IntVal&amp; arg2) {
  if (arg1.is_null || arg2.is_null) return IntVal::null();
  return IntVal(arg1.val + arg2.val);
}

// Multiple UDFs can be defined in the same file
   Conclusion Writing your own UDFs helps you customize an Impala deployment for your particular use case. Also, as you can see from the above, it’s easy to get started! John Russell is a technical writer at Cloudera and the author of the free O’Reilly Media e-book, Cloudera Impala.</snippet></document><document id="49"><title>Meet the Engineer: Romain Rigaux</title><url>http://blog.cloudera.com/blog/2014/01/meet-the-engineer-romain-rigaux/</url><snippet>In this installment of �Meet the Engineer� we speak with Romain Rigaux, a Software Engineer on the Hue team. What do you do at Cloudera, and in which project are you involved? Currently I work on Hue, the open source Web interface that lets users do Big Data analysis directly from their browser. Its goal is to make that process easier, so that more users can get more insights, more quickly. User uptake has been incredibly fast over the past two years, with adoption increasing by 10x. In fact, Hue is now included in all the main Apache Hadoop distributions. It�s the default tool for doing data exploration outside the command line. My days consist of doing feature planning and development, interacting with all the other teams, working with support and customers, collecting feedback, helping the community, spreading the good news on Twitter, writing tutorials, filming videos…you name it. Whatever helps people get started. Why do you enjoy your job? First, the team is awesome. We are agile, like a startup, and moving fast. The life cycle of a new feature/bug is often counted in hours, and each release brings tons of new improvements. We also like to make innovation fun. For example, we do trips to different continents (like the Thailand team retreat) or build new apps for Christmas. Simplifying Hadoop and its ecosystem involves the integration of many moving pieces. We design applications for transferring data in and out of the cluster, query editors for analyzing data, real-time customizable search, workflow scheduling, and big table browsing. It also means working in a high-activity environment with many other people, projects and a huge community with meetups and conferences. Technically, the work is challenging and spans multiple domains: from as high level as HTML, CSS, and JavaScript to as low level as Django, Linux, and application protocols. Last but not least, the company is great! Cloudera is full of talented people with a great attitude trying to solve big problems, and then contributing their work back to open source. What is your favorite thing about Hadoop? Hadoop is like a huge toolbox available freely online. With Hadoop, you can build great systems for understanding more of your data � enterprise data hubs, as we call them — or just learn a lot about complex software projects and human interaction. What is your advice for someone who is interested in participating in any open source project for the first time? Pick a project that you already use or that has your personal interest. Follow and interact on the mailing lists, report some feedback/bugs, write some blog posts, start to send some small patches, and climb up in complexity from there (shameless auto-promotion: check out Hue!). At what age did you become interested in programming, and why? I started in high school, building small websites (with static HTML, banners, and GIFs at that time). I was developing some small games on the side too, like Hunt in QBasic or DarkBasic, then moved on to C. Computers are really interesting to me. With just a keyboard and electricity, you can let your imagination go, create �something� that will work faster than you, and interact with the whole planet.</snippet></document><document id="50"><title>NYU, Analytics, and Cloudera�s QuickStart VM</title><url>http://blog.cloudera.com/blog/2014/01/nyu-analytics-and-clouderas-quickstart-vm/</url><snippet>The Cloudera QuickStart VM is an important platform for learning any Hadoop-related curriculum. In the Fall 2013 semester, more than 30 NYU graduate students completed the Real-time and Big Data Analytics course at the NYU Courant Institute of Mathematical Sciences, for which I served as instructor. In this introductory analytics course, students learn the architectures of the Apache Hadoop storage and compute systems (HDFS and MapReduce respectively). The early part of the course is dedicated to gaining experience with Hadoop and the Hadoop ecosystem projects — with this foundational knowledge in hand, students complete programming assignments in MapReduce, Apache Pig, Apache Hive, and more. This is all groundwork in preparation for the analytics project that each team is required to research, define, and develop. With so much ground to cover, it is critical that the students invest their time in activities directly related to their final projects � not in setting up infrastructure. So, as a course requirement, I asked that students download and install the Cloudera QuickStart VM, which minimizes the time spent configuring a Hadoop environment and maximizes productive time spent on developing analytics � whether for graduate students enrolled in a course, or developers building proof-of-concepts. (This is the second cohort of students to take the course, and the first to use QuickStart VM as their Hadoop learning platform.) Within minutes, students were exploring HDFS commands. This is in direct contrast to the first cohort�s experience, where each student performed the time-consuming chores of downloading, installing, and configuring Hadoop, then Pig, Hive, and so on. Often, this process required several iterations and many hours before a stable Hadoop environment was achieved. By using QuickStart VM, administrative chores were greatly reduced, enabling students to focus on analytics rather than software installation. Cloudera�s QuickStart VM provides another advantage: it facilitates exploration of topics that extend beyond presented course material. With a full complement of Hadoop ecosystem projects already installed, configured, and available on their desktops, students were able to identify the best tools for the job at hand, experiment with them, and employ them in their analytics projects. It was great to see students taking advantage of Cloudera Impala, Apache HBase, Apache Mahout, and other Hadoop ecosystem projects in their final solutions. Best of all, they did so without the distraction of locating, downloading, installing, and configuring �additional software � Cloudera�s QuickStart VM already had it all! Suzanne McIntosh is a Solutions Consultant for Cloudera. Editor’s Note:�The�Cloudera Academic Partnership (CAP)�is another option for colleges and universities who would like to incorporate Big Data and Hadoop into their computer science or data analytics curricula. Cloudera offers free course materials, discounted classroom training, and certification for professors, and a complementary University License for more robust software features. Learn more about becoming a member school at�http://university.cloudera.com/cap.</snippet></document><document id="51"><title>It’s a Three-peat! HBaseCon 2014 Call for Papers and Early Bird Registration Now Open</title><url>http://blog.cloudera.com/blog/2014/01/its-a-three-peat-hbasecon-2014-call-for-papers-and-early-bird-registration-now-open/</url><snippet>The third-annual HBaseCon is now open for business. Submit your paper or register today for early bird savings! Seems like only yesterday that droves of Apache HBase developers, committers/contributors, operators, and other enthusiasts converged in San Francisco for HBaseCon 2013 — nearly 800 of them, in fact.� It was such a good time, in fact, that we’re doing it again: Call for Papers and Early Bird registration are now open for the third annual HBaseCon! This time, the community will converge on Monday, May 5, on its own two floors of the Hilton San Francisco Union Square. And just like the past two years, the event will be complemented by two days of HBase training in San Francisco, for which a discount is available exclusively for HBaseCon attendees. Just to refresh your memory: Why attend HBaseCon? HBaseCon is the annual rallying point for the HBase community. Meet your heroes! HBaseCon is a one-stop shop for learning about the HBase roadmap, as well as other ecosystem projects. Tracks about Development, Operations, Internals, and Ecosystem! HBaseCon is a feast of real-world experiences and use cases. All the most impressive users in one place! HBaseCon is a pageant of HBase engineer rock-stars. If you’re into that! HBaseCon is a blast. Come for the content, stay for the party! We’ll see you there, right? Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="52"><title>Impala Performance Update: Now Reaching DBMS-Class Speed</title><url>http://blog.cloudera.com/blog/2014/01/impala-performance-dbms-class-speed/</url><snippet>Impala�s speed now beats the fastest SQL-on-Hadoop alternatives. Test for yourself! Since the initial beta release of Cloudera Impala more than one year ago (October 2012), we�ve been committed to regularly updating you about its evolution into the standard for running interactive SQL queries across data in Apache Hadoop and Hadoop-based enterprise data hubs. To briefly recap where we are today: Impala is being widely adopted. Impala has been downloaded by people in more than 5,000 unique organizations, and Cloudera Enterprise customers from multiple industries and Fortune 100 companies have deployed it as supported infrastructure, integrated with their existing BI tools, in business-critical environments. Furthermore, MapR and Amazon Web Services (via its Elastic MapReduce service) recently announced support for Impala in their own platforms. Impala has demonstrated production readiness. Impala has proved out its superior performance for interactive queries in a concurrent-workloads environment, while providing reliable resource sharing compared to batch-processing alternatives like Apache Hive. Impala sports an ever-increasing list of enterprise features. Planned new enterprise features are materializing rapidly and according to plan — including the addition of fine-grained, role-based authorization, user-defined functions (UDFs), cost-based optimization, and pre-built analytic functions. In this update, you�ll read about some exciting performance results (based on TPC-DS) that not only prove out Impala�s continuing superiority over the latest releases of Hive (0.12) for interactive queries, but also query performance across open Hadoop data faster than that of a leading proprietary analytic DBMS across data in its native columnar data store. (Yes, you read that right!) Thus far, we consider this milestone the best evidence yet that Impala is unique in its ability to surpass the performance of all SQL-on-Hadoop alternatives, while delivering the value propositions of Hadoop (fractional cost, unlimited scalability, unmatched flexibility) — a combination that the creators of those alternatives (including remote-query approaches and siloed DBMS/Hadoop hybrids) can never hope to achieve. First, let�s take take a look at the latest results compared to those of Hive 0.12 (aka “Stinger”). Impala versus Hive The results below show that Impala continues to outperform all the latest publicly available releases of Hive (the most current of which runs on YARN/MR2). For this analysis, we ran Hive 0.12 on ORCFile data sets, versus Impala 1.1.1 running against the same data set in Parquet (the general-purpose, open source columnar storage format for Hadoop). To obtain the best possible results from Hive, we converted the TPC-DS queries into SQL-92 style joins, manually optimized the join order, and added an explicit partition predicate. (To ensure a direct comparison, the same modifications were run against Impala.) The data set in question comprised 3TB (TPC-DS scale factor 3,000) across five typical Hadoop DataNodes (dual-socket, 8-core, 16-thread CPU; 96GB memory; 1Gbps Ethernet; 12 x 2TB disk drives). As in our previous analysis, the query set was intentionally diverse in order to ensure unbiased results, including a variety of fairly standard joins (from one to seven in number) and aggregations, as well as complex multi-level aggregations and inline views (and categorized into Interactive Exploration, Reports, and Deep Analytics buckets). Five runs were done, and aggregated into the numbers you see below. As you�ll see in the chart below, Impala outperformed Hive on every query, and by very considerable margins: Impala versus Hive 0.12/Stinger (Lower bars are better) In summary, Impala outperformed Hive by 6x to 69x (and by an average of 24x) depending on the category involved: Clearly, Impala�s performance continues to lead that of Hive. Now, let�s move on to the even more exciting results! Impala versus DBMS-Y For this milestone, we tested a popular commercial analytic DBMS (referred to here as �DBMS-Y� due to a restrictive proprietary licensing agreement) against Impala 1.2.2 on a 30TB set of TPC-DS data (scale factor 30,000) — maintaining the same query complexity and unbiased pre-selection characteristics as in the Impala versus Hive analysis above. The queries were not customized for optimal Impala join order; rather, we used the same format as provided in the official TPC-DS kit. In this experiment, we: Pre-selected a diverse set of 20 TPC-DS queries before obtaining any results, to ensure an unbiased comparison. Installed DBMS-Y (with OS filesystem) and Impala (with CDH) on precisely the same cluster (20 nodes with 96GB memory per node). Loaded all the data into the DBMS-Y proprietary columnar data store and compared it to data in Parquet. Ran the set of 20 queries on each of the two systems individually (with the other one completely shut down to prevent any interference). Validated that execution plans from both systems were common-sense and routine — to avoid comparing a �garbage� plan to a �good� plan. Removed currently unsupported analytic functions and added an explicit predicate to the WHERE clauses that expresses a partition filter on the fact table. (Window functions and dynamic partition pruning are both on the Impala near-term roadmap.) To obtain the best possible results from DBMS-Y, we modified the original TPC-DS queries to one-month boundaries to avoid penalizing DBMS-Y for its inability to handle daily partitions on the 30TB data set. We ran the queries against DBMS-Y both with and without an explicit partition filter and chose the best result. (In a few cases, excluding the partition filter led to better DBMS-Y performance.) As you can see from the results below, Impala outperformed DBMS-Y by up to 4.5x (and by an average of 2x), with only three queries performing more slowly on Impala: Impala versus DBMS-Y (Lower bars are better) To repeat: on 17 of 20 examples, Impala queries on open Hadoop data were faster than DBMS-Y queries on its native, proprietary data – with some being nearly 5x as fast. Multiple Impala customers have reported results similar to those above against DBMS-Y and other popular commercial DBMS vendors. We�ve published the benchmarking tests for you to confirm for yourself here! Experiments with Linear Scalability With that extremely impressive outcome, we could have called it a day — but we didn�t stop there. Next, we wanted to document what Impala users have reported with respect to Impala�s linear scalability in response time, concurrency, and data scale. For this set of tests, we used: Two clusters of the same hardware: one of 18 nodes and one of 36 nodes 15TB-scale and 30TB-scale TPC-DS data sets A multi-user workload of TPC-DS queries selected from the �Interactive� bucket described previously The first set of results below demonstrates that you can simply add machines to Impala to achieve the desired latency. For example, if you compare a set of queries on 15TB with 18 nodes to the same queries running against 15TB with 36 nodes, you�ll find that the response time is approximately 2x better with 2x the number of nodes: 2x the Hardware (Expectation: Cut response times in half) � Take-away: When the number of nodes in the cluster is doubled, expect the response time for queries to be reduced by half. The next results demonstrate that you can simply add more machines to the cluster to scale up the number of concurrent users while maintaining latency. Here we ran the same set of queries and 15TB data set against both the 18-node and 36-node clusters, but we doubled the level of concurrency with the 36-node cluster: 2x the Users, 2x the Hardware (Expectation: Constant response times) � Take-away: Assuming constant data, expect that doubling the number of users and hardware will result in constant response time. The final results below demonstrate that you can simply add new machines to your cluster as your Big Data sets grow and expect to achieve the same response times. In this experiment, we ran the same queries on the 18-node and 36-node clusters, but this time we ran against a 15TB data set with the 18-node cluster and a 30TB data set with the 36-node cluster: 2x the Data, 2x the Hardware (Expectation: Constant response times) � Take-away: When both the data and the number of nodes in the cluster are doubled, expect the response time for queries to remain constant. Based on the above, you can safely conclude that Impala�s linear scalability is well established. Conclusion To briefly summarize, based on our latest performance analysis: Impala handily outperformed the latest public release of Hive (0.12 – aka Stinger). Impala (on the open Parquet file format for Hadoop) performance, on average, substantially surpassed that of DBMS-Y (on its proprietary columnar data store). Impala scaled linearly and predictably as users, data, or hardware increased. These results are a milestone in an already exciting year for Impala users. Up until now, users could count on superior performance for BI-style, interactive queries over batch-processing alternatives; even when running multiple workloads simultaneously. Today, with Impala 1.2.x and Parquet, they can also count on performance similar to that of an analytic DBMS on proprietary data, while retaining all the well known advantages of Hadoop — there�s no longer any need to compromise. Clearly, we�re entering a �golden age� for BI users and customers! Although we’re proud of these results, we encourage you to do your own testing — whether utilizing the methodology described above, or your own approach. (It�s unfortunate that this goal is made more difficult to reach by the common use of restrictive licensing agreements that make direct benchmarking impossible.) We welcome any and all outcomes, because in the end, they can only make your Impala experience better. When Impala 2.0 arrives in the first half of 2014, you�ll see further performance improvements, as well as the addition of popular SQL functionality like analytic window functions. Impala is well on its way to becoming one of the main access points to the Enterprise Data Hub. The authors would like to thank Arun Singla (Engineering Manager, Performance at Cloudera) for his contributions to this project. Justin Erickson is a director of product management at Cloudera. Marcel Kornacker is the creator/tech lead of Impala at Cloudera. Greg Rahn and Yanpei Chen are performance engineers at Cloudera.</snippet></document><document id="53"><title>This Month (and Year) in the Ecosystem (December 2013)</title><url>http://blog.cloudera.com/blog/2014/01/this-month-and-year-in-the-ecosystem-december-2013/</url><snippet>Welcome to our sixth edition of “This Month in the Ecosystem,” a digest of highlights from December 2013 (never intended to be comprehensive; for completeness, see the excellent�Hadoop Weekly). With the close of 2013, we also thought it appropriate to include some high points from across the year (not listed in any particular order): The Enterprise Data Hub, a vision for long-term customer value that will forever embed Hadoop into mainstream IT, was effectively articulated and is already being adopted in the form of real deployments. Clouderans were omnipresent on the tech-talk circuit (meetups, user groups, and conferences), interacting with the user community all over the world – with nearly 300 talks in 17 different countries. Furthermore, the first cohort of Cloudera User Groups held their inaugural meetups in Chicago, New York City, and San Francisco. Cloudera customers and users got their very own discussion forums — and the Q&amp;A commenced immediately. Apache Hadoop 2 became A Real Thing — it’s now available to users in the form of a stable release, and will be the default deployment option in CDH 5 and Cloudera Enteprise 5. Cloudera Engineering (@ClouderaEng) shipped a mountain of new product (production-grade software, not just technical previews): Cloudera Impala, Cloudera Search, Cloudera Navigator, Cloudera Development Kit (now Kite SDK), new Apache Accumulo packages for CDH, and several iterative releases of CDH and Cloudera Manager. (And, the Cloudera Enterprise 5 Beta release was made available to the world.). Furthermore, as always, a ton of bug fixes and new features went upstream, with the features notably but not exclusively HiveServer2 and Apache Sentry (incubating). HBaseCon 2013, with nearly 800 attendees, proved out that HBase interest is on a steep incline. (News about HBaseCon 2014 coming very soon!) And Strata + Hadoop World 2013 rocked the ecosystem, with a record-setting 2,500 attendees showing up to learn more about the Hadoop stack. Hadoop deployment options on the cloud moved beyond test and dev; customers can now run production workloads in supported mode on Amazon AWS, SoftLayer, T-Systems, Verizon, and Saviss. Cloudera Impala became a supported offering in MapR’s and AWS’ (via EMR) platforms. Ride Impala! Developers also learned that they can not only run Apache Spark (incubating) jobs alongside CDH (and acquire support from Cloudera for doing that), but that Spark will ship inside CDH 5 when it becomes GA in early 2014. (And, the maiden voyage of Spark Summit was a great one.) Data scientists can breath easier with the availability of new infrastructure for development�(Oryx) and testing�(Gertrude) of machine-learning apps. Via the Cloudera Academic Partnership program, 17 universities adopted Hadoop-related curriculum, helping to ensure that the CS students of today will become the enterprise data hub developers/operators of tomorrow. Solid as a rock! Here’s to an even better 2014. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="54"><title>The Cloudera Developer Newsletter: It’s For You!</title><url>http://blog.cloudera.com/blog/2014/01/the-cloudera-developer-newsletter-its-for-you/</url><snippet>The new Cloudera Developer Newsletter makes its debut in January 2014. Developers and data scientists, we’re realize you’re special – as are operators and analysts, in their own particular ways.� For that reason, we are very happy to kick off 2014 with a new free service designed for you and other technical end-users in the Cloudera ecosystem: the Cloudera Developer Newsletter. This new email-based newsletter contains links to a curated list of new how-to�s, docs, tools, engineer and community interviews, training, projects, conversations, videos, and blog posts to help you get a new Apache Hadoop-based enterprise data hub deployment off the ground, or get the most value out of an existing deployment. Look for a new issue every month! All you have you to do is click the button below, provide your name and email address, tick the “Developer Community” check-box, and submit. Done! (Of course, you can also opt-in to several other communication channels if you wish.) Subscribe Subscribe now to get the first issue near the end of January 2014. We hope you enjoy it! Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="55"><title>Where to Find Cloudera Tech Talks (Through March 2014)</title><url>http://blog.cloudera.com/blog/2014/01/where-to-find-cloudera-tech-talks-through-march-2014/</url><snippet>Find Cloudera tech talks in Berlin, Budapest, London, Stockholm, Tokyo, and across the US during this calendar quarter. Below please find our regularly scheduled quarterly update about where to find tech talks by Cloudera employees – this time, for the first calendar quarter of 2014 (January through March). Note that this list will be continually curated during the period; complete logistical information may not be available yet. And remember, many of these talks are in “free” venues (no cost of entry). As always, we’re standing by to assist your meetup by providing speakers, sponsorships, and schwag! Date City Venue Speaker(s) Jan. 7 Toronto Toronto HUG Mark Grover on Impala Jan. 11 Austin, Tex. Data Day Texas Eric Sammer on Apache Hadoop app development, Josh Wills on data science infrastructure Jan. 13 Cleveland, Oh. Cleveland Big Data &amp; HUG Alex Moundalexis on Hadoop + Solr Jan. 15 Berlin Big Data Beers Marcel Kornacker on Impala Jan. 16 Laurel, Md. Data Science MD Ted Malaska on graph/tree processing Jan. 16 Tokyo NetApp Innovation Tokyo Giuseppe Kobayashi on Hadoop 101 Jan. 17 Hyderabad Hyderabad HUG Harsh Chouraria on running Hadoop in production Jan. 17 London London HBase Meetup Matteo Bertozzi on HBase roadmap, Lars George on Impala/Parquet + HBase Jan. 22 Orlando, Fla. Data Science &amp; Big Data UG Ricky Saltzer on Impala Jan. 23 St. Paul, Min. Twin Cities HUG Brock Noland on security &amp; Apache Sentry (incubating) Feb. 3 Zurich Swiss Big Data UG Sean Owen on Oryx Feb. 4 Stockholm Jfokus Mark Grover on Hadoop app development Feb. 5 San Francisco Hue Users Meetup Romain Rigaux on Hue 3.5 Feb. 6 Orange, Calif. Orange County SQL Server UG Ben White on Impala Feb. 6 Hanover, Va. Cloudera Federal Forum Eric Sammer &amp; Ryan Blue with a Hadoop app development lab Feb. 10 San Francisco SF Java User Group Doug Cutting on Hadoop + Apache Lucene Feb. 10 New York City Big Data Warehousing Meetup Patrick Angeles on Hadoop security Feb. 11-13 Santa Clara, Calif. Strata Santa Clara 2014 Ronan Stokes does an Apache HBase tutorial Marcel Kornacker on Impala Multiple speakers at Cloudera’s exhibition Feb. 12 Budapest Big Data Budapest Wolfgang Hoschek on Search &amp; Morphlines Feb. 17 London London HBase Meetup Matteo Bertozzi on HBase futures; Lars George on HBase + Impala Feb. 17 Santa Clara, Calif. FAST ’14 Mark Grover does a half-day Hadoop tutorial Feb. 20 San Francisco Developer Happy Hour Eric Sammer, Sandy Ryza, and Romain Rigaux on building Hadoop 2 apps Feb. 24 Stockholm Stockholm HUG Wolfgang Hoschek on Search &amp; Morphlines Feb. 25 Mountain View, Calif. She’s Geeky Kathleen Ting with a Hadoop 101 March 3 London QCon London Eva Andreeason on Hadoop use cases, Sean Owen on machine learning March 5 San Francisco SF Django Meetup Abe Elmahrek on Hue March 13 St. Louis, Mo. St. Louis JUG Tom Wheeler on Hadoop 101 April 1 Amsterdam Netherlands HUG Andrew Wang on HDFS caching, Doug Cutting on Hadoop Ecosystem Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="56"><title>Developer Happy Hour with Cloudera: Building Hadoop 2 Applications</title><url>http://blog.cloudera.com/blog/2014/01/developer-happy-hour-with-cloudera-building-hadoop-2-applications/</url><snippet>Join us at Cloudera’s San Francisco office on Feb. 20 for tech talks, T-shirts, and adult refreshments! As an extension of the DeveloperWeek Conf &amp; Festival 2014 experience in San Francisco next month, join us at Cloudera’s San Francisco office for a Developer Happy Hour (beer + tech talks), focusing on Apache Hadoop 2 application development. Anyone (attendees or non) is free to attend, but RSVP now because seats (and “Data is the New Bacon” T-shirts) are limited! Tentative agenda: 6-6:30pm: Networking and munchies 6:30-7pm: Building Hadoop 2 Apps with Kite SDK (by Eric Sammer, Engineering Manager/author of Hadoop Operations) An introduction to Kite SDK, a set of open source libraries, tools, examples, and docs that make it easier to build systems on top of the Hadoop ecosystem. 7-7:30pm: YARN for Developers (by Sandy Ryza, Software Engineer/Hadoop Committer) The new YARN framework promises to make Hadoop a general-purpose platform for Big Data and enterprise data hub applications. In this talk, you’ll learn about writing and taking advantage of applications built on YARN. 7:30-8pm: Hue: The GUI for Hadoop (by Romain Rigaux, Software Engineer) Hue is an open source Hadoop Web UI that lets users be more productive, while also providing a framework for building new apps quickly. Get a tour of Hue features and learn how to re-use the APIs for submitting Hive queries, listing HDFS files, and submitting MapReduce jobs. As we said, now’s the time to click below, because there are only so many seats and tickets are going fast!</snippet></document><document id="57"><title>The Hadoop FAQ for Oracle DBAs</title><url>http://blog.cloudera.com/blog/2014/01/the-hadoop-faq-for-oracle-dbas/</url><snippet>Oracle DBAs, get answers to many of your most common questions about getting started with Hadoop. As a former Oracle DBA, I get a lot of questions (most welcome!) from current DBAs in the Oracle ecosystem who are interested in Apache Hadoop. Here are few of the more frequently asked questions, along with my most common replies. How much does the IT industry value Oracle DBA professionals who have switched to Hadoop administration, or added it to their skill set? Right now, a lot. There are not many experienced Hadoop professionals around (yet)! In many of my customer engagements, I work with the DBA team there to migrate parts of their data warehouse from Teradata or Netezza to Hadoop. They don�t realize it at the time, but while working with me to write Apache Sqoop export jobs, Apache Oozie workflows, Apache Hive ETL actions, and Cloudera Impala reports, they are learning Hadoop. A few months later, I’m gone, but a new team of Hadoop experts who used to be DBAs is left in place. My solutions architect team at Cloudera also hires ex-DBAs as solutions consultants or system engineers. We view DBA experience as invaluable for those roles. What do you look for when hiring people with no Hadoop experience? I strongly believe that DBAs have the skills to become excellent Hadoop experts�– but not just any DBAs. Here are some of the characteristics I look for: Comfort with the command line. Point-and-click DBAs and ETL developers need not apply. Experience with Linux. Hadoop runs on Linux so that�s where much of the troubleshooting will happen. You need to be very comfortable with Linux OS, filesystem, tools, and command line. You should understand OS concepts around memory management, CPU scheduling, and IO. Knowledge of networks. ISO layers, what ssh is really doing, name resolution, basic understanding of switching. Good SQL skills. You know SQL and you are creative in your use of it. Experience with data warehouse basics such as partitioning and parallelism is a huge plus. ETL experience is a plus. Tuning skills are a plus. Programming skills. Not necessarily Java (see below). But, can you write a bash script? Perl? Python? Can you solve few simple problems in pseudo-code? If you can’t code at all, that�s a problem. Troubleshooting skills. This is huge, as Hadoop is far less mature than Oracle. You�ll need to Google error messages like a pro, but also be creative and knowledgeable about where to look when Google isn�t helpful. For senior positions, we look for systems and architecture skills too. Prepare to explain how you’ll design a flight-scheduling system or something similar. And since our team is customer facing, communication skills are a must. Do you listen? Can you explain a complex technical point? How do you react when I challenge your opinion? Is that maybe too much to ask? Possibly. But I can’t think of anything I could remove and still expect success with our team. How do I start learning Hadoop? The first task we give new employees is to set up a five-node cluster in the AWS cloud. That�s a good place to start. Neither Cloudera Manager nor Apache Whirr is allowed; they make things too easy. The next step is to load data into your cluster and analyze it. I recommend following the tutorials here, which show how to load Twitter data using Apache Flume and analyze it using Hive: http://blog.cloudera.com/blog/2012/09/analyzing-twitter-data-with-hadoop/ http://blog.cloudera.com/blog/2012/10/analyzing-twitter-data-with-hadoop-part-2-gathering-data-with-flume/ Also, Cloudera’s QuickStart VM (download here) includes TPC-H data and queries. You can run your own TPC-H benchmarks in the VM. There are also some good books to help you get started. My favorite is Eric Sammer�s Hadoop Operations � it�s concise and practical, and I think DBAs will find it very useful. The chapter on troubleshooting is very entertaining. Other books that DBAs will find useful are Hadoop: The Definitive Guide, Programming Hive, and Apache Sqoop Cookbook�(all of which are authored or co-authored by Clouderans). I also recommend taking a Cloudera University training course or two and perhaps even getting certified. Talking to a live instructor often provides insights that you can’t find on your own. For even more resources, see the �New to Hadoop� page on cloudera.com. Do I need to know Java? Yes and no :) You don’t need to be a master Java programmer. I’m not, and many of my colleagues are not. Some never write Java code at all. You do need to be comfortable reading Java stack traces and error messages. You’ll see many of those. You’ll also need to understand basic concepts like jars and classpath. Being able to read Java source code is useful. Hadoop is open source, and digging into the code often helps you understand why something works the way it does. Even without mastery required, the ability to write Java is often useful. For example, Hive UDFs are typically written in Java (and it�s easier to do that than you think). Conclusion If you�re an Oracle DBA interested in learning Hadoop (or working for Cloudera), this post should get you started. I�m happy to answer any other questions in comments! Gwen Shapira is a Solutions Architect for Cloudera, and a former Oracle DBA.</snippet></document><document id="58"><title>A New Web UI for Spark</title><url>http://blog.cloudera.com/blog/2014/01/a-new-web-ui-for-spark/</url><snippet>The team behind Hue, the open source Web UI that makes Apache Hadoop easier to use, strikes again with a new Spark app. Editor’s note: This post was recently published on the Hue blog. We republish it here for your convenience. Hi Spark Makers! A�Hue application for Apache Spark (incubating) was recently created. It lets users execute and monitor�Spark�jobs directly from their browser and be more productive. We previously released the app with an Apache Oozie submission�backend�but switched to the Spark Job Server (SPARK-818) contributed by�Ooyala�and�Evan�s�team at the last�Spark Summit. This new server will enable a real interactivity with Spark and is closer to the community. We hope to work with the community and have support for Python, Java, direct script submission without compiling/uploading and other improvements in the future! Get Started! Currently only Scala jobs are supported and programs need to implement this�trait�and be packaged into a jar. Here is a WordCount�example. To learn more about Spark Job Server, check its�README. Requirements We assume you have�Scala�installed on your system. Get Spark Job Server Currently on github on this�branch: git clone https://github.com/ooyala/incubator-spark.git spark-server
cd spark-server
git checkout -b jobserver-preview-2013-12 origin/jobserver-preview-2013-12
   Then type: sbt/sbt
project
jobserver re-start
   Get Hue Currently only on github (will be in CDH 5 Beta 2): https://github.com/cloudera/hue#getting-started If Hue and Spark Job Server are not on the same machine, update the�hue.ini�property in desktop/conf/pseudo-distributed.ini: [spark]
  # URL of the Spark Job Server.
  server_url=http://localhost:8090/
   Get a Spark Example to Run Then follow this�walkthrough�and create the example jar that is used in the video demo. As usual, feel free to comment in the Hue community forum�or @gethue! About questions directly related to Job Server, participate on the�pull request,�SPARK-818,�or the Spark�user list!</snippet></document><document id="59"><title>Top 10 Blog Posts of 2013</title><url>http://blog.cloudera.com/blog/2013/12/top-10-blog-posts-of-2013/</url><snippet>From Python, to ZooKeeper, to Impala, to Parquet, blog readers in 2013 were interested in a variety of topics. Clouderans and guest authors from across the ecosystem (LinkedIn, Netflix, Concurrent, Etsy, Stripe, Databricks, Oracle, Tableau, Alteryx, Talend, Twitter, Dell, Concurrent, SFDC, Endgame, MicroStrategy, Hazy Research, Wibidata, StackIQ, ZoomData, Damballa, Mu Sigma) published prolifically on the Cloudera Developer blog in 2013, with more than 250 new posts — basically, averaging one per business day. These were the most popular ones published in 2013: A Guide to Python Frameworks for Apache Hadoop (by Uri Laserson) Uri wrote the definitive guide on this subject, if its ongoing popularity is any guide (which it is). Algorithms Every Data Scientist Should Know: Reservoir Sampling (by Josh Wills) Few people are more likely to Know What Every Data Scientist Should Know than Josh. How-to: Create a CDH Cluster on Amazon EC2 via Cloudera Manager (by Emanuel Buzek) Deploying CDH to the AWS cloud is getting easier and easier – and now, it’s a supported platform for production workloads. How-to: Configure Eclipse for Hadoop Contributions (by Karthik Kambatla) Pragmatic guidance for a pragmatic topic. How-to: Use Apache ZooKeeper to Build Distributed Apps (and Why) (by Sean Mackrory) ZooKeeper is rapidly emerging into the sunlight from relative obscurity as distributed systems move closer to the norm. Cloudera Impala 1.0: It�s Here, It�s Real, It�s Already the Standard for SQL on Hadoop (by Justin Erickson &amp; Marcel Kornacker) The Impala roadmap is so bright, we’re all wearing shades. Since this post was published, yet more significant milestones have been met. How-to: Select the Right Hardware for Your New Hadoop Cluster (by Kevin O’Dell) Some of the most common questions we get involve hardware sizing. This post offers the state of the art in guidance. How-to: Analyze Twitter Data with Hue (by Romain Rigaux) Hue, the open source Web UI for Hadoop, used to be just a diamond in the rough — today, it’s a real gem. Cloudera ML: New Open Source Libraries and Tools for Data Scientists (by Josh Wills) Josh Wills describes the precursor project to Oryx. Still a good guide. Introducing Parquet: Efficient Columnar Storage for Hadoop (by multiple authors from Cloudera, Criteo, and Twitter) Parquet is bringing the performance benefits of columnar data representation to all Hadoop ecosystem projects. It all started with this. We’re just getting started. Looks like 2014 will be even more exciting! Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="60"><title>Accumulo Comes to CDH</title><url>http://blog.cloudera.com/blog/2013/12/accumulo-comes-to-cdh/</url><snippet>Apache Accumulo is now generally available on CDH 4. Cloudera is pleased to announce the immediate availability of its first release of Accumulo packaged to run under CDH, our open source distribution of Apache Hadoop and related projects and the foundational infrastructure for Enterprise Data Hubs. Accumulo is an open source project that provides the ability to store data in massive tables (billions of rows, millions of columns) for fast, random access. Accumulo was created and contributed to the Apache Software Foundation by the National Security Agency (NSA), and it has quickly gained adoption as a Hadoop-based key/value store for applications that require access to sensitive data sets. Cloudera provides enterprise support with the RTD Accumulo add-on subscription for Cloudera Enterprise. This release provides Accumulo 1.4.3 tested for use under CDH 4.3.0. The release includes a significant number of backports and fixes to allow use with CDH 4�s highly available, production-ready packaging of HDFS. As a part of our commitment to the open source community, these changes have been submitted back upstream. To get started now, you can follow these instructions to handle installation using the Cloudera Manager 5.0.0 open beta release with our new extensibility framework. The instructions will also walk you through downloading a single distribution file and installing Accumulo on the current Cloudera QuickStart VM. Once you get going, we would love to hear your feedback: You can ask questions, get help, and share your growing expertise on our community forum for questions about CDH storage components. You can file a bug through our public Jira instances. For issues with Accumulo please use https://issues.cloudera.org/browse/DISTRO. For issues with the beta integration with Cloudera Manager, please use https://issues.cloudera.org/browse/CM. For more information, please visit Cloudera’s Accumulo page. Sean Busbey and Bill Havanki are Solutions Architects at Cloudera. Mike Drob is a Software Engineer at Cloudera and an Accumulo Committer.</snippet></document><document id="61"><title>Doing DevOps with Cloudera Manager</title><url>http://blog.cloudera.com/blog/2013/12/devops-with-cloudera-manager/</url><snippet>More and more customers are using automation/configuration management frameworks alongside Cloudera Manager. As Apache Hadoop clusters continue to grow in size, complexity, and business importance as the foundational infrastructure for an Enterprise Data Hub, the use cases for a robust and mature management console expand.� As those clusters become larger and more complex, many operators are looking to use configuration management/automation frameworks like Ansible, Chef, and Puppet�alongside Cloudera Manager�to help automate the deployment and configuration process. (Others are rolling their own via the open source Cloudera Manager REST API.) Here are some examples of how-to’s that cover these technologies:� Using Cloudera Manager with Ansible Using Cloudera Manager with Dell Crowbar Using Cloudera Manager with StackIQ Blogger James Ruddy recently added to this list with posts about �1) using Puppet to�deploy Cloudera Manager�(based on the Puppet module written by Mike Arnold) and then 2) using Cloudera Manager to deploy Hadoop. Thanks to James for sharing his hard-earned knowledge! If you done something similar with Chef or Salt, let us know in comments. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="62"><title>Cloudera Development Kit is Now "Kite SDK"</title><url>http://blog.cloudera.com/blog/2013/12/cloudera-development-kit-is-now-kite-sdk/</url><snippet>CDK has a new monicker, but the goals remain the same. We are pleased to announce a new name for the Cloudera Development Kit (CDK): Kite. We’ve just released Kite version 0.10.0, which is purely a rename of CDK 0.9.0. The new repository and documentation are here: Kite repository: https://github.com/kite-sdk/kite Kite documentation: http://kitesdk.org/ Kite examples: https://github.com/kite-sdk/kite-examples Why the rename? The original goal of CDK was to increase accessibility to the Apache Hadoop platform by developers. That goal isn’t Cloudera-specific, and we want the name to more forcefully reflect the open, community-driven character of the project.� Will this change break anything? The rename mainly affects dependencies and package names. Once imports and dependencies are updated, almost everything should work the same. However, there are a couple of configuration changes to make for anyone using Apache Flume or Morphlines. The changes are detailed on our migration page. Again, this 0.10.0 release is a rename only. There are no feature changes, and 0.9.0 will be supported as long as 0.10.0, so that there is plenty of time to make a smooth transition. For more information, see the release notes. Ryan Blue is a Software Engineer at Cloudera, working on Kite SDK.</snippet></document><document id="63"><title>How-to: Use Impala on Amazon EMR</title><url>http://blog.cloudera.com/blog/2013/12/how-to-use-impala-on-amazon-emr/</url><snippet>Developers, rejoice: Impala is now available on EMR for testing and evaluation. Very recently, Amazon Web Services announced support for running Cloudera Impala�queries on its Elastic MapReduce (EMR) service. This is very good news for EMR users — as well as for users of other platforms interested in kicking Impala’s tires in a friction-free way. It�s also yet another sign that Impala is rapidly being adopted across the ecosystem as the gold standard for interactive SQL and BI queries on Apache Hadoop. AWS also helpfully provides a tutorial for launching and querying Impala clusters on EMR. It covers: Signing up for Amazon EMR,� Launching a cluster with Impala installed, Connecting to the cluster using SSH, Generating a test data set, Creating Impala tables and populating them with data, and Performing interactive queries on Impala tables You may also want to read the AWS FAQ about Impala on EMR. Developers, start your clusters! Impala is now on EMR for easy testing and evaluation. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="64"><title>How-to: Do Statistical Analysis with Impala and R</title><url>http://blog.cloudera.com/blog/2013/12/how-to-do-statistical-analysis-with-impala-and-r/</url><snippet>The new RImpala package brings the speed and interactivity of Impala to queries from R. Our thanks to Austin Chungath, Sachin Sudarshana, and Vikas Raguttahalli of Mu Sigma, a Decision Sciences and Big Data analytics company, for the guest post below. As is well known, Apache Hadoop traditionally relies on the MapReduce paradigm for parallel processing, which is an excellent programming model for batch-oriented workloads. But when ad hoc, interactive querying is required, the batch model fails to meet performance expectations due to its inherent latency. To overcome this drawback, Cloudera introduced Cloudera Impala, the open source distributed SQL query engine for Hadoop data. Impala brings the necessary speed to queries that were otherwise not interactive when executed by the batch Apache Hive engine; Hive queries that used to take minutes can be executed in a matter of seconds using Impala. Impala is quite exciting for us at Mu Sigma because existing Hive queries can run interactively with few or no changes. Furthermore, because we do a lot of our statistical computing on R, the popular open source statistical computing language, we considered it worthwhile to bring the speed of Impala to R. To meet that goal, we have created a new R package, RImpala, which connects Impala to R. RImpala enables querying the data residing in HDFS and Apache HBase from R, which can be further processed as an R object using R functions. RImpala is now available for download from the Comprehensive R Archive Network (CRAN) under GNU General Public License (GPL3). The RImpala architecture is simple: we used the existing Impala JDBC drivers and wrote a Java program to connect and query Impala, which we then called from R using the rJava package. We put them all together in an R package that you can use to easily query Impala from R. Steps for Installing RImpala Assuming that you have R and Impala already installed, installing the RImpala package is straightforward and is done in a manner similar to any other R package. There are two steps to installing RImpala and getting it working: Step 1: Install the package from CRAN You can install RImpala directly using the install.packages() command in R. 
&gt; install.packages("RImpala")
   Alternatively, if you need to do offline installation of the package, you can download it from here and install using the R CMD INSTALL command: R CMD install RImpala_0.1.1.tar.gz   Step 2: Install the Impala JDBC drivers You need to install Cloudera�s JDBC drivers before you can use the RImpala package that we installed earlier. Cloudera provides JBDC jars on its website that you can download directly. As of this writing, this is the link to zip file containing the JDBC jars. There are two ways to do this: If you have Impala installed on the machine running R, then you will have the necessary JDBC jars already (probably in /usr/lib/impala/lib) and you can use them to initiate the connection to Impala. If the machine running R is a different server than the Impala server, then you need to download the JDBC jars from the above link and extract it to a location that can be accessed by the R user. After you have installed the JDBC drivers you can start using the RImpala package: Load the library. library(RImpala)
   Initialize the JDBC jars. rimpala.init("/path/to/impala/jars")
   Connect to Impala. rimpala.connect("IP or Hostname of Impala server", "port")
   The following is an Rscript showing how to connect to Impala: library(RImpala)
rimpala.init(libs="/tmp/impala/jars/")
rimpala.connect("192.168.10.1","21050")
   Location of JDBC jars = /tmp/impala/jars IP of the server running impalad service = 192.168.10.1 Port where the impalad service is listening = 21050 The default parameter for the rimpala.init() function is �/usr/lib/impala/lib� and the default parameters for rimpala.connect() function are �localhost� and �21050� respectively. To run a query on the impalad instance that the client has connected, you can use the rimpala.query() function. Example: result   All the contents of the sample_table will be stored in the result object as a data frame. This data frame can now be used for further analytical processing in R. You can also install the RImpala package on a client machine running Microsoft Windows. Since the JDBC jars are platform independent, you can extract them into a folder on a Windows machine (such as �C:\Program Files\impala�) and then this location can be passed as a parameter to the rimpala.init() function. The following a simple example that shows you how to use RImpala: &gt; library(RImpala)
Loading required package: rJava

&gt; rimpala.init(libs="/tmp/impala/jars/") # Adds the impala JDBC jars present in the "/tmp/impala/jars/" folder to the classpath
[1] "Classpath added successfully"

&gt; rimpala.connect(IP="192.168.10.1",port="21050")  # Establishes a connection to impalad instance running on the machine 172.25.1.151 on the port 21050
[1] TRUE

&gt; rimpala.invalidate() # Invalidates the metadata of all the tables present in the Hive metastore
[1] TRUE

&gt; rimpala.showdatabases()# Displays all the databases available
# Output #
	name
1	airlines
2	bank
3	default

&gt; rimpala.usedatabase("bank") # Changes the current database to "bank"
Database changed to bank
[1] TRUE

&gt; rimpala.showtables() # Displays all the tables present in the current database
# Output  #
name
1	bank_web_clicks
2	ticker_100m
3	stock_1gb
4	weblog_10gb

&gt; rimpala.describe("bank_web_clicks") # Describes the table "bank_web_clicks"
# Output  #
Name		type		comment
1	customer_id	int     	Customer ID
2	session_id	int		Session ID
3	page		string	Web page name
4	datestamp	timestamp	Date

&gt; result  result
# Output #
customer_id	 session_id	cnt
1	32 		 21		5200
2	34  		 12 		5100
3	35  		 49  		4105
4	32  		 34  		3600
5	36  		 32  		3218
6	37		 67  		3190
7	31  		 45  		2990
8	35		 75  		2300
9	34  		 69  		2113

&gt; rimpala.close() # Closes the connection to the impalad instance
[1] TRUE
   Conclusion Impala is an exciting new technology that is gaining popularity and will probably grow to be an enterprise asset in the Hadoop world. We hope that RImpala will be a fruitful package for all Big Data analysts to leverage the power of Impala from R. Impala is an ongoing and thriving effort at Cloudera and will continue to evolve with richer functionality and improved performance � and so will RImpala. We will continue to improve the package over time and incorporate new features into RImpala as and when they are made available in Impala. � Austin Chungath is a Senior Research Analyst with Mu Sigma�s Innovation &amp; Development Team and maintainer of the RImpala project. He does research on various tools in the Hadoop ecosystem and the possibilities that they bring for analytics. He spends his free time contributing to Open Source projects like Apache Tez or building small robots. Sachin Sudarshana is a Research Analyst with Mu Sigma�s Innovation &amp; Development Team. His responsibilities include researching emerging tools in the Hadoop ecosystem and how they can be leveraged in an analytics context. Vikas Raguttahalli is a Research Lead with Mu Sigma�s Innovation &amp; Development Team. He is responsible for working with client delivery teams and helping clients institutionalize Big Data within their organizations, as well as researching new and upcoming Big Data tools. His expertise includes R, MapReduce, Hive, Pig, Mahout and the wider Hadoop ecosystem.</snippet></document><document id="65"><title>Meet the Book Authors: Flavio Junqueira and Benjamin Reed on ZooKeeper</title><url>http://blog.cloudera.com/blog/2013/12/meet-the-book-authors-flavio-junqueira-and-benjamin-read-on-zookeeper/</url><snippet>Flavio Junqueira (PMC Chair of the Apache ZooKeeper project and a member of the Systems and Networking Group at Microsoft Research) and Benjamin Reed (PMC Member and Software Engineer at Facebook) are the co-authors of the new O’Reilly Media book ZooKeeper: Distributed Process Coordination. We had a chat with Flavio and Ben recently about the rationale for writing the book, and what it will add to the distributed systems conversation. Why did you decide to write this book? It seemed like a natural step. Although ZooKeeper has been around for a while, there was no good starting point for beginners or even a comprehensive reference for advanced developers. The online documentation is ok, but it lacks depth and is a bit scattered.�Also, in some sense, ZooKeeper is a little bit too easy to jump in and use. We found that people were jumping in without fully understanding what they needed to watch out for, ZooKeeper’s limitations, and how to deal with some faulty scenarios. We had been thinking about writing a book for a while, and last year, we decided to move forward with the idea after being contacted by O’Reilly.� Who is your intended reader? We primarily focus on programming with ZooKeeper, so the book targets new and advanced developers. It also has material for sysadmins. The two kinds of content are not incompatible, though, and it is probably a good thing that developers are also aware of administration issues and possibilities. In fact, we cover the internals of ZooKeeper in the part about administration, but we think that this content is valuable for anyone working with ZooKeeper.� When covering the internals, we didn’t write it with distributing computing researchers in mind — quite the opposite. Rather, we tried to explain them in a way that is accessible to the average developer. We also give code references when possible to encourage readers to look into the code and possibly even consider contributing back. We are always looking to grow the community.� � What are your favorite things about ZooKeeper that you want people to know? One of the most interesting aspects of ZooKeeper is that it implements a simple API on top of a difficult core, which makes it possible for most developers to ignore all the deep distributed computing concepts it is implementing internally. Replication protocols like Zab, the one in ZooKeeper, are conceptually simple, but when it comes to implementing them, there are a number of subtle points that can cause headaches. In ZooKeeper, I think we have been able to abstract that away from the developer nicely. As the book says, we didn’t completely hide distributed systems issues from the developer, but we made it easier for developers to reason about them.� In your research, what did you learn that you did not already know? There are two important lessons here, one about protocols and another about everything else around Zab.� We made a few interesting observations about Paxos when contrasting it to Zab, like problems you could run into if you just implemented Paxos alone. Not that Paxos is broken or anything, just that in our setting, there were some properties it was not giving us. Some people still like to map Zab to Paxos, and they are not completely off, but the way we see it, Zab matches a service like ZooKeeper well. The reconfiguration work (ZOOKEEPER-107), which is not yet available in a release, was a challenge to include, mostly because we introduced it so recently. It was a big patch that touched many parts, even if the concept is not very complex. In retrospect, we should have considered it earlier in the project. The other important lesson is related to what developers see. It is a challenge in itself to make a service built on top of� Zab, Paxos, and so on easy to use; a simple API for ZooKeeper made a huge difference for its adoption. Having a pretty cool protocol is not enough, though, if the API does not make it easy to program against the service. Of course, performance is also a concern, and we had to make sure that the system is up to task for our users.�An API with simple operations like ZooKeeper’s helps deliver high throughput and low latency because no operation really blocks the ZooKeeper pipeline for too long. What are some other things that the ZooKeeper community can do to help make distributed systems easier to build? We call ZooKeeper a “coordination system” because a lot of the coordination tasks we need to implement for a distributed system can build on ZooKeeper. Such tasks typically need some processes to agree upon something, which is often hard to implement from scratch because of all the corner cases of things failing and getting disconnected. ZooKeeper makes it easier to deal with these situations. Not everything in a distributed system is directly about coordination, though. For example, reliable messaging and recoverability are present in many systems, and we have worked on Hedwig and BookKeeper respectively to deal with them. They are independent components built on ZooKeeper that make it easier to build other distributed systems. Similar specialized components that possibly build on ZooKeeper would definitely make the task of building distributed systems easier.�Perhaps a shout-out to Apache Curator and similar APIs would be good. We always meant for ZooKeeper to be a generic building block that could be used to implement richer client APIs. Regarding coordination, a few things in ZooKeeper could be improved. It is not uncommon to come across use cases that run across data centers, and ZooKeeper has some features (like observers) that help with such cases. Still, the overall design of ZooKeeper targets a homogeneous set of servers running in a single data center. We can probably do a better job for coordination across data centers with a different design. Also, with new storage technologies (solid state drives, non-volatile RAM, and so on), we could provide backends optimized for them. It is not that ZooKeeper is not going to run if you have SSDs, but the writes to disk are not really optimized for such drives. It would be nice to have better support for these technologies. These two points are just to illustrate that we often come across different requirements and scenarios on the mailing list, so interacting with the community is a good way to get ideas for the Next Cool Thing.�</snippet></document><document id="66"><title>What’s New in Cloudera Manager 5?</title><url>http://blog.cloudera.com/blog/2013/12/whats-new-in-cloudera-manager-5/</url><snippet>Learn the new features and enhancements in Cloudera Manager 5, including support for YARN, management of third-party apps and frameworks, and more. The response to the Oct. 2013 release of Cloudera Enterprise 5 Beta has been overwhelming, and Cloudera is busily working closely with several customers to incorporate their feedback. Cloudera Manager 5 is a key part of this release, and in this post, I will provide a brief overview of some key features in Beta 1 as well as introduce some of those planned for Beta 2 (to be released in early 2014). Workload and Resource Management A major theme of the beta release is the notion of supporting multiple workloads on the same data substrate. Effective resource management becomes an important criterion to make this vision a reality. With Cloudera Manager 5, the plan is to have YARN production ready to support dynamic resource allocation (for different applications that leverage YARN). In addition, we continue to support static partitioning (via cgroups) to divide cluster resources (cpu, memory, and so on) among these stand-alone processes. Cloudera Manager 5 adds several knobs and parameters to manage all these resource management aspects in a simplified and streamlined fashion. Resource Management: static partitioning � Resource Manager: dynamic partitioning Service Extensibility Customers are also asking for an easier way to manage non-CDH services and ISV applications that are deployed on top of, and along with, the CDH stack. Many have already standardized on Cloudera Manager as their Apache Hadoop management platform of choice, so, a natural next step is to use Cloudera Manager to manage these additional/new services. The Service Extensibility mechanism in Cloudera Manager 5 provides different avenues for non-CDH services and ISV applications to be managed via Cloudera Manager. A good example is Cloudera’s recent collaboration with Syncsort to facilitate the deployment of its DMX-h libraries via Parcels. The complete functionality will enable customers to manage the entire lifecycle of new services (such as Apache Accumulo, Apache Spark [incubating], and so on) via this mechanism. The end goal is to have customers write a simple service descriptor (a JSON file along with set of control scripts) for a new service that gets managed by Cloudera Manager. � Accumulo support� � Spark support The plan is to have good set of examples, documentation, and sample code available as part of Beta 2 (or by GA) for customers to try this on their own for any new service they would like to deploy. In the interim, we continue to work with select partners like SAS, 0xData, Syncsort, and others to fine-tune the implementation. Monitoring Enhancements We have also added multiple new monitoring capabilities in Cloudera Manager. More specifically, data visualization has been beefed up, including the ability to chart the time-series metrics as bar graphs, scatter plots, heat maps, and so on. We have also added more thorough monitoring for Cloudera Impala queries and added support for YARN/MR2 monitoring. Impala monitoring � Enhanced charting capabilities Other New Features &amp; Enhancements The above lists only a subset of all new features that are part of Cloudera Enterprise 5. For example, significant work was also done to ease the overall upgrade process from CDH 4 to CDH 5 and MR1 to MR2/YARN. On the security side, we added SAML support for SSO access to Cloudera Manager. We also now support JDK 7. (The entire list of updates and enhancements in Beta 1 is available here.) The team is now busy working on a Beta 2 release, which is currently scheduled to include support for Apache Oozie HA, YARN Resource Management HA workflows, HDFS caching, user-defined triggers, and more. In the meantime, please try out Cloudera Enterprise Beta 1�and give us your feedback via any of the following: A beta specific community forum has been set up.�Click here�to join. File a bug through our public JIRA at: https://issues.cloudera.org/browse/DISTRO https://issues.cloudera.org/browse/CM Bala Venkatrao is a Director, Products at Cloudera.</snippet></document><document id="67"><title>What are HBase Compactions?</title><url>http://blog.cloudera.com/blog/2013/12/what-are-hbase-compactions/</url><snippet>The compactions model is changing drastically with CDH 5/HBase 0.96. Here’s what you need to know. Apache HBase is a distributed data store based upon a log-structured merge tree, so optimal read performance would come from having only one file per store (Column Family). However, that ideal isn’t possible during periods of heavy incoming writes. Instead, HBase will try to combine HFiles to reduce the maximum number of disk seeks needed for a read. This process is called compaction. Compactions choose some files from a single store in a region and combine them. This process involves reading KeyValues in the input files and writing out any KeyValues that are not deleted, are inside of the time to live (TTL), and don�t violate the number of versions. The newly created combined file then replaces the input files in the region. Now, whenever a client asks for data, HBase knows the data from the input files are held in one contiguous file on disk — hence only one seek is needed, whereas previously one for each file could be required. But disk IO isn’t free, and without careful attention, rewriting data over and over can lead to some serious network and disk over-subscription. In other words, compaction is about trading some disk IO now for fewer seeks later. In this post, you will learn more about the use and implications of compactions in CDH 4, as well as changes to the compaction model in CDH 5 (which will be re-based on HBase 0.96). Compaction in CDH 4 The ideal compaction would pick the files that will reduce the most seeks in upcoming reads while also choosing files that will need the least amount of IO. Unfortunately, that problem isn’t solvable without knowledge of the future. As such, it’s just an ideal that HBase should strive for and not something that’s ever really attainable. Instead of the impossible ideal, HBase uses a heuristic to try and choose which files in a store are likely to be good candidates. The files are chosen on the intuition that like files should be combined with like files � meaning, files that are about the same size should be combined. The default policy in HBase 0.94 (shipping in CDH 4) looks through the list of HFiles, trying to find the first file that has a size less than the total of all files multiplied by hbase.store.compaction.ratio. Once that file is found, the HFile and all files with smaller sequence ids are chosen to be compacted. For the default case of the largest files being the oldest, this approach works well: However, this assumption about the correlation between age and size of files is faulty in some cases, leading the current algorithm to choose sub-optimally. Rather, bulk-loaded files can and sometimes do sort very differently from the more normally flushed HFiles, so they make great examples: Compaction Changes in CDH 5 Compactions have changed in significant ways recently. For HBase 0.96 and CDH 5, the file selection algorithm was made configurable via HBASE-7516– so it’s now possible to have user-supplied compaction policies. This change allows more experienced users to test and iterate on how they want to run compactions. The default compaction selection algorithm was also changed to ExploringCompactionPolicy. This policy is different from the old default in that it ensures that every single file in a proposed compaction is within the given ratio. Also, it doesn’t just choose the first set of files that have sizes within the compaction ratio; instead it looks at all the possible sets that don’t violate any rules, and then chooses something that looks to be most impactful for the least amount of IO expected.� To do that, the ExploringCompactionPolicy chooses a compaction that will remove the most files within the ratio, and if there is a tie, preference is given to the set of files that are smaller in size: More changes are planned for future releases, including tiered compaction, striped compaction, and level-based compaction. Conclusion For some use cases, this work won’t have any impact at all. That’s a good thing, as compactions were already pretty well studied. However, for users who have large traffic spikes or that use bulk loads, this work can yield great improvements in IO wait times and in request latency. For a specific bulk-load use case, we have seen a 90% reduction in disk IO due to compactions. Here are results from a test case in HBase�s PerfTestCompactionPolicies: Check out this work in CDH 5 (in beta at the time of this writing) when it comes to a cluster near you. Further Reading: https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.html https://hbase.apache.org/book/regions.arch.html#compaction.file.selection https://issues.apache.org/jira/browse/HBASE-7516 https://issues.apache.org/jira/browse/HBASE-7678 https://issues.apache.org/jira/browse/HBASE-7667 https://issues.apache.org/jira/browse/HBASE-7055 http://www.hbasecon.com/sessions/compaction-improvements-in-apache-hbase/ Elliot Clark is a Software Engineer at Cloudera and an HBase PMC Member/Committer.</snippet></document><document id="68"><title>This Month in the Ecosystem (November 2013)</title><url>http://blog.cloudera.com/blog/2013/12/this-month-in-the-ecosystem-november-2013/</url><snippet>Welcome to our fifth edition of “This Month in the Ecosystem,” a digest of highlights from November 2013 (never intended to be comprehensive; for completeness, see the excellent�Hadoop Weekly). With the holidays upon us, the news in November was sparse. Even so, the ecosystem never stops churning! Continuuity Weave was Proposed as an Apache Incubator Project Weave, an effort to make building new apps on top of YARN much easier for mainstream developers, has been proposed as an incubator project under a new monicker, “Twill.” We think anything that makes developers’ lives easier is a Very Good Thing, and we applaud Continuuity for this proposal. Cloudera’s Tom White and Patrick Hunt were nominated as mentors. Read the Incubator Proposal for Twill Apache Hadoop Got Massively More Accessible Cloudera and Udacity, purveyor of massively open online courses (MOOC) for tech education, have partnered up to bring the first Hadoop-related MOOCs to the world.�Co-developed by Cloudera University faculty and Udacity’s online instructional designers, the new courses help technologists acquire fundamental skills for working with the foundational technology of Enterprise Data Hubs. Enroll in the first new course: “Introduction to Hadoop MapReduce”� The Data Products Toolbox Got Deeper with Oryx and Gertrude Cloudera’s data science department announced two new open source projects to help facilitate the creation of new “data products”: �Oryx (infrastructure for building and deploying machine-learning models) and Gertrude (a framework for testing/experimenting with data products). Both projects are in alpha state so there is still much work to be done. Learn more about/see source code for Oryx |�Learn more about/see source code for Gertrude Facebook Open-Sourced Presto Facebook contributed its distributed SQL-on-Hadoop engine, Presto, to open source. Like Cloudera Impala, Presto dispenses with MapReduce for performance reasons, and Apache Hive data is accessed via CDH. Unlike Impala, no commercial support is yet available. Let the benchmarks fly! Learn more about Presto The next installment of “This Month in the Ecosystem” will publish in early January. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="69"><title>How-to: Get Started with Sentry in Hive</title><url>http://blog.cloudera.com/blog/2013/12/how-to-get-started-with-sentry-in-hive/</url><snippet>A quick on-ramp (and demo) for using the new Sentry module for RBAC in conjunction with Hive One attribute of the Enterprise Data Hub is fine-grained access to data by users and apps. This post about supporting infrastructure for that goal was originally published at blogs.apache.org. We republish it here for your convenience. Apache Sentry (incubating) is a highly modular system for providing fine-grained role-based authorization to both data and metadata stored on an Apache Hadoop cluster. It currently works out of the box with Apache Hive and Cloudera Impala. In this blog post, you will learn how to use Sentry with Hive. Sentry uses a policy provider to define the access control to Hive. Sentry currently ships with a file-based policy provider, see below for an example. A single global policy file can be used to control access to an entire HiveServer2 instance, and multiple dependent per database policy files can be linked to the global one. Lets look at the structure of policy file with an example. Global policy file: 
[groups]
admin_group = admin_role
dep1_admin = uri_role

[roles]
admin_role = server=server1
uri_role = hdfs:///ha-nn-uri/data

[databases]
db1 = hdfs://ha-nn-uri/user/hive/sentry/db1.ini
   Per db policy file: (at hdfs://ha-nn-uri/user/hive/sentry/db1.ini) 
[groups]
dep1_admin = db1_admin_role
dep1_analyst = db1_read_role

[roles]
db1_admin_role = server=server1-&gt;db=db1
db1_read_role = server=server1-&gt;db=db1-&gt;table=*-&gt;action=select
   As you can see above, there are usually three sections in the global policy file: A [groups] section that provides group-to-role mapping A [roles] section that provides role-to-privileges mapping A [databases] (optional) section that provides database-to-per-database policy file mapping. This allows for maintaining per-database privileges separately. Sentry provides authorization through a hook in HiveServer2. When a user makes a connection to HiveServer2, it authenticates the connecting user and persists the user information for the session. For the subsequent operations that user performs, Sentry authorizes the operation by mapping the user to the groups he/she belongs to and determining whether the group(s) have necessary privileges on the relevant objects. Hive Security Landscape with Sentry Next, lets look at how Sentry fits into the security landscape of Hive. The below infographic shows how different authentication and authorization pieces fit together.� Here are the main points to take away: Sentry requires that HiveServer2 be configured to use strong authentication. HiveServer2 supports Kerberos as well as LDAP (and AD) authentication mechanisms. At the Sentry authorization level, there are two supported forms of user-group mappings: HadoopGroup mapping, which uses the underlying�Hadoop groups Hadoop groups in turn support Shell-based mapping as well as LDAP group mapping. Please note that in case of Sentry with Hive, the mapping of users to groups is performed on the HiveServer2 host LocalGroups, where the users and groups can be defined locally in the policy file using [users] section (for testing purposes only) Demo In this demo, we will be using Kerberos authentication for HiveServer2 with HadoopGroups as the Sentry group provider, which by default uses Shell mapping. We briefly go over Sentry and see how to configure and use it in this configuration. (Note: Cloudera Manager 4.7 and CDH 4.4 are shown here; for future versions, the steps will be similar.) Conclusion Sentry brings in fine-grained authorization support for both data and metadata in a Hadoop cluster. It is already being used in production systems to secure the data and provide fine-grained access to its users. It is also integrated with the version of Hive shipping in CDH (upstream contribution is pending), Cloudera Impala, and Cloudera Search. Also, here is a�short demo�if you are interested in using it with Hue.� Sravya Tirukkovalur is a Software Engineer at Cloudera, working on Sentry.</snippet></document><document id="70"><title>Write MapReduce Jobs in Idiomatic Clojure with Parkour</title><url>http://blog.cloudera.com/blog/2013/12/write-mapreduce-jobs-in-idiomatic-clojure-with-parkour/</url><snippet>Thanks to Marshall Bockrath-Vandegrift of advanced threat detection/malware company (and CDH user) Damballa for the following post about his Parkour project, which offers libraries for writing MapReduce jobs in Clojure. Parkour has been tested (but is not supported) on CDH 3 and CDH 4. Clojure is Lisp-family functional programming language which targets the JVM. On the Damballa R&amp;D team, Clojure has become the language of choice for implementing everything from web services to machine learning systems. One of Clojure’s key features for us is that it was designed from the start as an explicitly hosted language, building on rather than replacing the semantics of its underlying platform. Clojure’s mapping from language features to JVM implementation is frequently simpler and clearer even than Java’s. Parkour is our new Clojure library that carries this philosophy to the Apache Hadoop’s MapReduce platform. Instead of hiding the underlying MapReduce model behind new framework abstractions, Parkour exposes that model with a clear, direct interface. Everything possible in raw Java MapReduce is possible with Parkour, but usually with a fraction of the code. Example Every new MapReduce library needs a word-count example, so let�s walk through Parkour’s. 
(ns parkour.examples.word-count
  (:require [clojure.string :as str]
            [parkour (conf :as conf) (mapreduce :as mr) (graph :as pg)
                     (tool :as tool)]
            [parkour.io (text :as text) (seqf :as seqf)])
  (:import [org.apache.hadoop.io Text LongWritable]))   Parkour is designed as a collection of layered APIs in separate namespaces, not an all-or-nothing framework. If you want to use Parkour�s core Clojure-MapReduce integration, but build the actual jobs from Java, Parkour provides the necessary flexibility. 
(defn mapper
  [input]
  (-&gt;&gt; (mr/vals input)
       (mapcat #(str/split % #"s+"))
       (map #(-&gt; [% 1]))))

(defn reducer
  [input]
  (-&gt;&gt; (mr/keyvalgroups input)
       (map (fn [[word counts]]
              [word (reduce + 0 counts)]))))   Parkour mappers and reducers look like Clojure collection functions because they are Clojure collection functions. Parkour treats the entire set of key-value tuples allocated to a task as a literal collection of those tuples. You write Parkour task functions in terms of Clojure’s rich library of lazy sequence and reducers operations, not just an API that looks like them. Parkour mappers and reducers also are their associated Hadoop tasks. They can have direct access to the job configuration, context, counters, and so on, and can do anything you could do in a raw Java MapReduce task. The provided task functions run directly in place of the equivalent Hadoop Java class. The Parkour APIs do require you to use named Clojure vars to specify all functions Hadoop invokes during job execution. Vars are the moral equivalent of Java’s named classes, and make explicit the boundary between local and remote execution. 
(defn word-count
  [conf dseq dsink]
  (-&gt; (pg/input dseq)
      (pg/map #'mapper)
      (pg/partition [Text LongWritable])
      (pg/combine #'reducer)
      (pg/reduce #'reducer)
      (pg/output dsink)
      (pg/execute conf "word-count")
      first))   Parkour recasts job configuration in terms of �configuration step� functions over Hadoop Job objects. These are equivalent to � and frequently invoke � standard job-setup methods like setMapperClass(). This abstraction allows any job to be specified as a simple composition of configuration steps. More important, because functions are first class, they may be passed in to job setup. This inverts the control pattern usually exposed by Java MapReduce job driver methods, allowing callers to inject arbitrarily complex portions of the job configuration. The Parkour job graph API provides helpers for adding all the commonly necessary steps in the right order, while leaving the freedom to add arbitrary additional steps. 
(defn tool
  [conf &amp; args]
  (let [[outpath &amp; inpaths] args
        input (apply text/dseq inpaths)
        output (seqf/dsink [Text LongWritable] outpath)]
    (-&gt;&gt; (word-count conf input output) (into {}) prn)))   Parkour distributed sinks (dsinks) and distributed sequences (dseqs) are extensions of the �configuration step� concept. A distributed sink marries a function configuring a job for particular output with a function configuring a job to consume that output as an input. A distributed sequence automatically takes any function configuring a job for some input and allows local access to the same key-value tuples produced remotely by the backing InputFormat. Job execution returns dseqs for the job output dsinks. Those dseqs may be passed as inputs to additional jobs or processed client-side as reducible collections. This combination of clear local/remote demarcation with seamless composition simplifies many programs involving multiple MapReduce jobs and/or local processing. 
(defn -main
  [&amp; args] (System/exit (tool/run tool args)))   Parkour contains a number of utility namespaces that integrate non-MapReduce facilities with Clojure. As show here, the parkour.tool namespace supports using plain Clojure functions as Hadoop Tools for command-line option parsing. Similarly the parkour.fs namespace allows the Clojure standard I/O functions to work directly on Hadoop Paths, and the parkour.config namespace provides a Clojure map-like API for working with Hadoop Configurations. Next Steps If you like what you see here, Parkour has detailed documentation, example programs, and Apache-licensed source code, all hosted on Github. Check it out and get started simplifying your MapReduce programs with Clojure! Marshall Bockrath-Vandegrift is principal engineer for the Damballa R&amp;D team, where he works to move new botnet-detection research from proof-of-concept to production. He lives in Atlanta with his wife and cats.</snippet></document><document id="71"><title>How-to: Use the HBase Thrift Interface, Part 2: Inserting/Getting Rows</title><url>http://blog.cloudera.com/blog/2013/12/how-to-use-the-hbase-thrift-interface-part-2-insertinggetting-rows/</url><snippet>The second how-to in a series about using the Apache HBase Thrift API Last time, we covered the fundamentals about connecting to Thrift via Python. This time, you’ll learn how to insert and get multiple rows at a time.� Working with Tables Using the Thrift interface, you can create or delete tables.�Let’s take a look at the Python code that creates a table: client.createTable(tablename, [Hbase.ColumnDescriptor(name=cfname)])
   In this snippet, you created a Hbase.ColumnDescriptor object.�In this object, you can set all the different parameters for a Column Family. In this case, you only set the Column Family name.� You may recall from the previous how-to that adding Hbase.thrift file to your project is often useful. This is one of those times: You can open up Hbase.thrift and find the ColumnDescriptor definition with all its parameters and their names. You can confirm a table exists using the following code: tables = client.getTableNames()

found = False

for table in tables:
	if table == tablename:
		found = True   This code gets a list of the user tables, iterates through them, and marks found as true if the table is found. You can delete a table using the following code: client.disableTable(tablename)
client.deleteTable(tablename)   Remember that in HBase, you have to disable a table before deleting it.�This code does just that. Adding Rows with Thrift Thrift gives us a couple of ways to add or update rows:�One row at a time, or multiple rows at a time.�The Thrift interface does not use the same Put object as the Java API.�These changes are called row mutations and use the Mutation and BatchMutation objects. mutations = [Hbase.Mutation(
  column='columnfamily:columndescriptor', value='columnvalue')]
client.mutateRow('tablename', 'rowkey', mutations)   Each Mutation object represents the changes to a single column.�To add or change another column, you would simply add another Mutation object to the mutations list. When you are done adding Mutation objects, you call the mutateRow method.�This method takes the table name, row key, and mutations list as arguments. Adding multiple rows at a time requires a few changes: # Create a list of mutations per work of Shakespeare
mutationsbatch = []

for line in shakespeare:
	rowkey = username + "-" + filename + "-" + str(linenumber).zfill(6)

	mutations = [
			Hbase.Mutation(column=messagecolumncf, value=line.strip()),
			Hbase.Mutation(column=linenumbercolumncf, value=encode(linenumber)),
			Hbase.Mutation(column=usernamecolumncf, value=username)
		]

       mutationsbatch.append(Hbase.BatchMutation(row=rowkey,mutations=mutations))

# Run the mutations for the work of Shakespeare
client.mutateRows(tablename, mutationsbatch)
   In this example, you’re still using the Mutation object but this time you have to wrap them in a BatchMutation object.�The BatchMutation object allows you to specify a different rowkey for each list of Mutations.�You also change to the mutateRows method.�It takes a table name and the BatchMutation object. Getting Rows With Thrift Using the getRow method, you can retrieve a single row based on its row key.�This call returns a list of TRowResult objects.�Here is the code for getting and working with the output: rows = client.getRow(tablename, "shakespeare-comedies-000001")

for row in rows:
     message = row.columns.get(messagecolumncf).value
     linenumber = decode(row.columns.get(linenumbercolumncf).value)

     rowKey = row.row   Start the code with a getRow request.�This get will return the row with the key “shakespeare-comedies-000001″.� These rows will come back as a list of TRowResult.�Using a row loop, you go through the list of rows that were returned. To get the value of a column, use columns.get(“COLUMNFAMILY:COLUMDESCRIPTOR”).�Be sure to use the proper naming syntax. Remember that when dealing with binary data like integers, you will need to convert it from a Python string to whatever type it should be. In this case, you are taking the string and making it an integer with the decode method. Getting multiple rows at a time is very similar to getting one row.� Here is the code: rowKeys = [ "shakespeare-comedies-000001",
"shakespeare-comedies-000010",
"shakespeare-comedies-000020",
"shakespeare-comedies-000100",
"shakespeare-comedies-000201" ]

rows = client.getRows(tablename, rowKeys)
   Instead of specifying a single row, you pass in a list of rows.�You also change the method to getRows, which takes the table name and list of rows as arguments. A list of TRowResult objects is returned and you then iterate through the list just like in the single-row code. In the next and final how-to, you’ll learn how to use scans and get an introduction to some considerations when choosing between the REST and Thrift APIs for development. Jesse Anderson is an instructor for Cloudera University.</snippet></document><document id="72"><title>Managing Multiple Resources in Hadoop 2 with YARN</title><url>http://blog.cloudera.com/blog/2013/12/managing-multiple-resources-in-hadoop-2-with-yarn/</url><snippet>An overview of some of Cloudera’s contributions to YARN that help support management of multiple resources, from multi resource scheduling in the Fair Schedule to node-level enforcement As Apache Hadoop become ubiquitous, it is becoming more common for users to run diverse sets of workloads on Hadoop, and these jobs are more likely to have different resource profiles. For example, a MapReduce distcp job or Cloudera Impala query that does a simple scan on a large table may be heavily disk-bound and require little memory. Or, an Apache Spark (incubating) job executing an iterative machine-learning algorithm with complex updates may wish to store the entire dataset in memory and use spurts of CPU to perform complex computation on it. For that reason, the new YARN framework in Hadoop 2 allows workloads to share cluster resources dynamically between a variety of processing frameworks, including MapReduce, Impala, and Spark. YARN currently handles memory and CPU and will coordinate additional resources like disk and network I/O in the future.� Accounting for memory, CPU, and other resources separately confers several advantages:� It allows us to treat tenants on a Hadoop cluster more fairly by rationing the resources that are most utilized at a point in time. It makes resource configuration more straightforward, because a single resource does not need to be used as a proxy for others. It provides more predictable performance by not oversubscribing nodes, and protects higher-priority workloads with better isolation. Finally, it can increase cluster utilization because all the above mean that resource needs and capacities can be configured less conservatively. One of Cloudera�s top priorities in Cloudera Enterprise 5 (in beta at the time of writing) is to provide smooth and powerful resource management functionality on Hadoop via YARN. In this post, I�ll describe the work we�ve done recently to allow YARN to support multiple resources, from multi-resource scheduling with Dominant Resource Fairness in the Fair Scheduler to enforcement on the node level with cgroups. The changes discussed below are included in YARN/MR2 in CDH 4.4 and going forward. Background In Hadoop 1, a single dimension, the �slot�, represented resources on a cluster. Each node was configured with a number of slots, and each map or reduce task occupied a single slot regardless of how much memory or CPU it used. This approach offered the benefit of simplicity but had a few disadvantages. Because of the coarse-grained abstraction, it was common for a node�s resources to be over or under subscribed. Initially, YARN improved this situation by switching to memory-based scheduling � in YARN, each node is configured with a set amount of memory and applications (such as MapReduce request containers) for their tasks with configurable amounts of memory. More recently, YARN added CPU as a resource in the same manner: Nodes are configured with a number of �virtual cores� (vcores) and applications give a vcore number when requesting a container. In almost all cases, a node�s virtual core capacity should be set as the number of physical cores on the machine. CPU capacity is configured with the yarn.nodemanager.resource.cpu-vcores, and the mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores properties can be used to change the CPU request for MapReduce tasks from the default of 1. Dominant Resource Fairness Perhaps the most difficult challenge when managing multiple resources is deciding how to share them fairly. With a single resource, moving toward a fair allocation across a set of agents is pretty straightforward. When there�s space to place a container, we give it to the agent that has the least total memory already allocated. The Hadoop Fair Scheduler has been taking this approach for years, allocating containers to the agent (pool or application) with the smallest current allocation. (A recent blog postdescribes how we extended this model to a hierarchy of pools.) But what if you want to schedule both memory and CPU, and you need them in possibly different and changing proportions? If you have 6GB and three cores, and I have 4GB and two cores, it�s pretty clear that I should get the next container. What if you have 6GB and three cores, but I have 4GB and four cores? Sure, you have a larger total number of units, but cores might be more valuable. To complicate things further, I might care about CPU more than you do. To navigate this problem, YARN drew on recent research from Ghodsi et al at UC Berkeley that presents a notion of fairness that works with multiple resources. The researchers chose this notion, called dominant resource fairness (DRF), over some other options because it provides several properties that are trivially satisfied in the single resource case but more difficult in the multi resource case. For example, it is strategy-proof — meaning that agents cannot increase the amount of resources they receive by �lying� about what they need, and it incentivizes sharing, meaning that agents with separate clusters of equal size can only stand to benefit by merging their clusters. The basic idea is as follows: My share of a resource is the ratio between the amount of that resource allocated to me and the total capacity on the cluster of that resource. So if the cluster has 10GB and I have 4GB, then my share of memory is 40%. If the cluster has 20 cores and I have 5 of them, my share of CPU is 25%. My dominant share is simply the max of resource shares, in this case 40%.� With single resource fairness, we try to equalize the shares of that resource across agents. When it�s time to allocate a container, we give it to the agent with the lowest share. With DRF, we try to equalize the dominant shares across agents, where the dominant share can come from a different resource for each agent. If you have 1GB and 10 cores, I get the next container, because my 40% dominant share of memory is less than your 50% dominant share of CPU. We can use DRF when working with hierarchical queues similarly to how we do so with a single resource. We assign containers by starting at the root queue and traversing the queue tree. At each node, we explore the nodes below it in the order given by our fairness algorithm (in order of smallest dominant share). Enforcing CPU Allocations with CGroups So, DRF gives us a way to schedule multiple resources. But what about enforcing the allocations? Because memory is for the most part inelastic, we enforce memory limits by killing container processes when they go over their memory allocation. While we could do the same for CPU, this approach is a little bit harsh because the application doesn�t really have much say in the matter unless we expect it to insert a millisecond sleep after every five lines of code. What we really want is a way to control what proportion of CPU time is allotted to each process.� Luckily, the Linux kernel provides us exactly this in a feature called cgroups (control groups).� With cgroups, you can place a YARN container process and all the threads it spawns in a control group. You then allocate a number of �CPU shares� to the process and place it in the cgroups hierarchy next to the other YARN container processes. Available CPU cycles will then be allotted to these processes in proportion to the number of shares given to them. Thus, if the node is not fully scheduled or containers are not using their full allotment, other containers will get to use the extra CPU.� cgroups also provides similar controls for disk and network I/O that we will probably use in the future for managing these resources. We implemented cgroups CPU enforcement in YARN-3. To turn on cgroups CPU enforcement from Cloudera Manager: Go to the configuration for the YARN service check the boxes for “Use CGroups for Resource Management” and “Always use Linux Container Executor”. Go to the hosts configuration and check the box for�”Enable CGroup-based Resource Management”. If you are not using Cloudera Manager: Use the LinuxContainerExecutor. This requires setting yarn.nodemanager.container-executor.class to org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor in all NodeManager configs and setting certain permissions, as described here. Set yarn.nodemanager.linux-container-executor.resources-handler.class to org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler Want to Learn More? There are a ton of details we didn�t cover regarding how multiple resources interact with Fair Scheduler features like per-queue minimums and maximums, preemption, and fair share reporting. For more info, check out the Fair Scheduler documentation. Sandy Ryza is a Software Engineer at Cloudera and a Hadoop Committer.</snippet></document><document id="73"><title>Things For Which We Are Thankful</title><url>http://blog.cloudera.com/blog/2013/11/things-for-which-we-are-thankful/</url><snippet>Some things for which we are thankful, the 2013 edition (not listed in order): 1. The entire Apache Hadoop community for its constant and hard work to Make the Platform Better, 2. Cloudera’s users, customers, and partners for their continual and helpful feedback to help guide us through #1, 3. Luminaries like Doug Cutting, Tom White, Eric Sammer, Josh Wills, and Todd Lipcon, whose first instinct is always to share their hard-earned knowledge and experience with others, 4. User group/meetup organizers and Champions of Big Data around the world, for their commitment to facilitating conversations (and keeping pizza providers in business), 5. @BigDataBorat, @BigDataGrumpCat, @HipsterHacker, and other tweeps for helping us all keep things in perspective with humor. What and who are on your “thankful” list?</snippet></document><document id="74"><title>How-to: Index and Search Data with Hue’s Search App</title><url>http://blog.cloudera.com/blog/2013/11/how-to-index-and-search-data-with-hues-search-app/</url><snippet>You can use Hue and Cloudera Search to build your own integrated Big Data search app. In a previous post, you learned how to analyze data using Apache Hive via Hue’s Beeswax and Catalog apps. This time, you’ll see how to make Yelp Dataset Challenge data searchable by indexing it and building a customizable UI with the Hue Search app. Indexing Data in Cloudera Search Indexing data in Cloudera Search involves : Setting up SolrCloud to partition your dataset into multiple indexes and processes Configuring SolrCloud collections to hold indexes Specifying the schema by which indexes will be created Feeding relevant data into the SolrCloud First, install Cloudera Search using this guide. Then, deploy and configure Solr Cloud. Next, create a new collection and index named "reviews". You can use the predefined schema available here. 
cp solr_local/conf/schema.xml solr_configs/conf/schema.xml
solrctl instancedir --create reviews solr_local
solrctl collection --create reviews -s 1
   Replace the field definitions in the schema with a mapping corresponding to the Yelp data. The schema defines each data field that will be available in the search index. You can read more about schema.xml in the Solr wiki. 
&lt;field name="business_id" type="text_en" indexed="true" stored="true" /&gt;
&lt;field name="cool" type="tint" indexed="true" stored="true" /&gt;
&lt;field name="date" type="text_en" indexed="true" stored="true" /&gt;
&lt;field name="funny" type="tint" indexed="true" stored="true" /&gt;
&lt;field name="id" type="string" indexed="true" stored="true" required="true" multiValued="false" /&gt;
&lt;field name="stars" type="tint" indexed="true" stored="true" /&gt;
&lt;field name="text" type="text_en" indexed="true" stored="true" /&gt;
&lt;field name="type" type="text_en" indexed="true" stored="true" /&gt;
&lt;field name="useful" type="tint" indexed="true" stored="true" /&gt;
&lt;field name="user_id" type="text_en" indexed="true" stored="true" /&gt;
&lt;field name="name" type="text_en" indexed="true" stored="true" /&gt;
&lt;field name="full_address" type="text_en" indexed="true" stored="true" /&gt;
&lt;field name="latitude" type="tfloat" indexed="true" stored="true" /&gt;
&lt;field name="longitude" type="tfloat" indexed="true" stored="true" /&gt;
&lt;field name="neighborhoods" type="text_en" indexed="true" stored="true" /&gt;
&lt;field name="open" type="text_en" indexed="true" stored="true" /&gt;
&lt;field name="review_count" type="tint" indexed="true" stored="true" /&gt;
&lt;field name="state" type="text_en" indexed="true" stored="true" /&gt;
   Then, retrieve and clean a subset of the Yelp data with a Hive query, download it as a CSV, and index it with the indexer tool and this command: hadoop jar /usr/lib/solr/contrib/mr/search-mr-*-job.jar org.apache.solr.hadoop.MapReduceIndexerTool -D 'mapred.child.java.opts=-Xmx500m' --log4j /usr/share/doc/search*/examples/solr-nrt/log4j.properties --morphline-file solr_local/reviews.conf --output-dir hdfs://localhost:8020/tmp/load --verbose --go-live --zk-host localhost:2181/solr --collection reviews hdfs://localhost:8020/tmp/query_result.csv   The command will use a morphline file to map the Yelp data to the fields defined in our index schema.xml. (Cloudera Morphlines, which is bundled with the Cloudera Developer Kit, is an interesting new tool for data transformations that facilitates the indexing of your data.) When debugging morphlines, the –dry-run option will save you some time. Finally, the Administration panel lets you tweak the look and feel and features of the search page. View a demo of this entire process here: Troubleshooting If you see this error: 
org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException
:Error CREATEing SolrCore 'reviews_shard1_replica1': Unable to
create core: reviews_shard1_replica1 Caused by: Could not find
configName for collection reviews found:null
   You may have forgotten to create the collection: 
solrctl instancedir --create review solr_configs   If you see this error: 
ERROR - 2013-10-10 20:01:21.383; org.apache.solr.servlet.SolrDispatchFilter;
   Could not start Solr. Check solr/home property and the logs
ERROR - 2013-10-10 20:01:21.409; org.apache.solr.common.SolrException;
   null:org.apache.solr.common.SolrException: solr.xml not found in ZooKeeper
   at org.apache.solr.core.ConfigSolr.fromSolrHome(ConfigSolr.java:109)

Server is shutting down
   You might need to force Solr to reload the configuration. Beware, this might break Apache ZooKeeper and you might need to read Error #3. 
solrctl init --force
   If you see this error: 
KeeperErrorCode = NoNode for /overseer/collection-queue-work

org.apache.zookeeper.KeeperException$NoNodeException:
KeeperErrorCode = NoNode for /overseer/collection-queue-work
   It probably comes from Error #2. You might need to re-upload the config and recreate the collection. Conclusion Cloudera Search is great for opening your user base to Hadoop and do quick data retrieval. Other how-to’s describe other use cases, like email or customer data search. As usual feel free to comment on the hue-user list, community discussion forum, or @gethue!  </snippet></document><document id="75"><title>Approaches to Backup and Disaster Recovery in HBase</title><url>http://blog.cloudera.com/blog/2013/11/approaches-to-backup-and-disaster-recovery-in-hbase/</url><snippet>Get an overview of the available mechanisms for backing up data stored in Apache HBase, and how to restore that data in the event of various data recovery/failover scenarios With increased adoption and integration of HBase into critical business systems, many enterprises need to protect this important business asset by building out robust backup and disaster recovery (BDR) strategies for their HBase clusters. As daunting as it may sound to quickly and easily backup and restore potentially petabytes of data, HBase and the Apache Hadoop ecosystem provide many built-in mechanisms to accomplish just that. In this post, you will get a high-level overview of the available mechanisms for backing up data stored in HBase, and how to restore that data in the event of various data recovery/failover scenarios. After reading this post, you should be able to make an educated decision on which BDR strategy is best for your business needs. You should also understand the pros, cons, and performance implications of each mechanism. (The details herein apply to CDH 4.3.0/HBase 0.94.6 and later.) Note: At the time of this writing, Cloudera Enterprise 4 offers production-ready backup and disaster recovery functionality for HDFS and the Hive Metastore via Cloudera BDR 1.0 as an individually licensed feature. HBase is not included in that GA release; therefore, the various mechanisms described in this blog are required. (Cloudera Enterprise 5, currently in beta, offers HBase snapshot management via Cloudera BDR.) Backup HBase is a log-structured merge-tree distributed data store with complex internal mechanisms to assure data accuracy, consistency, versioning, and so on. So how in the world can you get a consistent backup copy of this data that resides in a combination of HFiles and Write-Ahead-Logs (WALs) on HDFS and in memory on dozens of region servers?� Let’s start with the least disruptive, smallest data footprint, least performance-impactful mechanism and work our way up to the most disruptive, forklift-style tool: Snapshots Replication Export CopyTable HTable API Offline backup of HDFS data The following table provides an overview for quickly comparing these approaches, which I’ll describe in detail below.   Performance Impact Data Footprint Downtime Incremental Backups Ease of Implementation Mean Time To Recovery (MTTR) Snapshots Minimal Tiny Brief (Only on Restore) No Easy Seconds Replication Minimal Large None Intrinsic Medium Seconds Export High Large None Yes Easy High CopyTable High Large None Yes Easy High API Medium Large None Yes Difficult Up to you Manual N/A Large Long No Medium High Snapshots As of CDH 4.3.0, HBase snapshots are fully functional, feature rich, and require no cluster downtime during their creation. My colleague Matteo Bertozzi covered snapshots very well in his blog entry and subsequent deep dive.�Here I will provide only a high-level overview.� Snapshots simply capture a moment in time for your table by creating the equivalent of UNIX hard links to your table’s storage files on HDFS (Figure 1).� These snapshots complete within seconds, place almost no performance overhead on the cluster, and create a minuscule data footprint.�Your data is not duplicated at all but merely cataloged in small metadata files, which allows the system to roll back to that moment in time should you need to restore that snapshot. Creating a snapshot of a table is as simple as running this command from the HBase shell: 
hbase(main):001:0&gt;  snapshot 'myTable', 'MySnapShot'
   After issuing this command, you’ll find some small data files located in /hbase/.snapshot/myTable (CDH4) or /hbase/.hbase-snapshots (Apache 0.94.6.1) in HDFS that comprise the necessary information to restore your snapshot. Restoring is as simple as issuing these commands from the shell: 
hbase(main):002:0&gt;  disable 'myTable'
hbase(main):003:0&gt;  restore_snapshot 'MySnapShot'
hbase(main):004:0&gt;  enable 'myTable'
   Note: As you can see, restoring a snapshot requires a brief outage as the table must be offline.�Any data added/updated after the restored snapshot was taken will be lost. If your business requirements are such that you must have an offsite backup of your data, you can utilize the exportSnapshot command to duplicate a table’s data into your local HDFS cluster or a remote HDFS cluster of your choosing.� Snapshots are a full image of your table each time; no incremental snapshot functionality is currently available. HBase Replication HBase replication is another very low overhead backup tool.�(My colleague Himanshu Vashishtha covers replication in detail in this blog post.) In summary, replication can be defined at the column-family level, works in the background, and keeps all edits in sync between clusters in the replication chain. Replication has three modes: master-&gt;slave, master&lt;-&gt;master, and cyclic.�This approach gives you flexibility to ingest data from any data center and ensures that it gets replicated across all copies of that table in other data centers.�In the event of a catastrophic outage in one data center, client applications can be redirected to an alternate location for the data utilizing DNS tools. Replication is a robust, fault-tolerant process that provides “eventual consistency," meaning that at any moment in time, recent edits to a table may not be available in all replicas of that table but are guaranteed to eventually get there. Note: For existing tables, you are required to first manually copy the source table to the destination table via one of the other means described in this post. Replication only acts on new writes/edits after you enable it. (From Apache’s Replication page) Export HBase’s Export tool is a built-in HBase utility that enables the easy exporting of data from an HBase table to plain SequenceFiles in an HDFS directory. It creates a MapReduce job that makes a series of HBase API calls to your cluster, and one-by-one, gets each row of data from the specified table and writes that data to your specified HDFS directory.�This tool is more performance-intensive for your cluster because it utilizes MapReduce and the HBase client API, but it is feature rich and supports filtering data by version or date range � thereby enabling incremental backups.� Here is a sample of the command in its simplest form: 
hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt;
   Once your table is exported, you can copy the resulting data files anywhere you’d like (such as offsite/off-cluster storage).�You can also specify a remote HDFS cluster/directory as the output location of the command, and Export will directly write the contents to the remote cluster.�Please note that this approach will introduce a network element into the write path of the export, so you should confirm your network connection to the remote cluster is reliable and fast. CopyTable The CopyTable utility is covered well in Jon Hsieh’s blog entry, but I will summarize the basics here. Similar to Export, CopyTable creates a MapReduce job that utilizes the HBase API to read from a source table.�The key difference is that CopyTable writes its output directly to a destination table in HBase, which can be local to your source cluster or on a remote cluster.� An example of the simplest form of the command is: 
hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name=testCopy test
   This command will copy the contents of a table named “test” to a table in the same cluster named “testCopy.”� Note that there is a significant performance overhead to CopyTable in that it uses individual “puts” to write the data, row-by-row, into the destination table. If your table is very large, CopyTable could cause memstore on the destination region servers to fill up, requiring memstore flushes that will eventually lead to compactions, garbage collection, and so on.� In addition, you must take into account the performance implications of running MapReduce over HBase. With large data sets, that approach might not be ideal. HTable API (such as a custom Java application) As is always the case with Hadoop, you can always write your own custom application that utilizes the public API and queries the table directly.�You can do this through MapReduce jobs in order to utilize that framework’s distributed batch processing advantages, or through any other means of your own design.�However, this approach requires a deep understanding of Hadoop development and all the APIs and performance implications of using them in your production cluster. Offline Backup of Raw HDFS Data The most brute-force backup mechanism — also the most disruptive one — involves the largest data footprint.�You can cleanly shut down your HBase cluster and manually copy all the data and directory structures residing in /hbase in your HDFS cluster. Since HBase is down, that will ensure that all data has been persisted to HFiles in HDFS and you will get an accurate copy of the data. However, incremental backups will be nearly impossible to obtain as you will not be able to ascertain what data has changed or been added when attempting future backups.� It is also important to note that restoring your data would require an offline meta repair because the .META. table would contain potentially invalid information at the time of restore. This approach also requires a fast reliable network to transfer the data offsite and restore it later if needed. For these reasons, Cloudera highly discourages this approach to HBase backups. Disaster Recovery HBase is designed to be an extremely fault-tolerant distributed system with native redundancy, assuming hardware will fail frequently. Disaster recovery in HBase usually comes in several forms: Catastrophic failure at the data center level, requiring failover to a backup location Needing to restore a previous copy of your data due to user error or accidental deletion The ability to restore a point-in-time copy of your data for auditing purposes As with any disaster recovery plan, business requirements will drive how the plan is architected and how much money to invest in it.�Once you’ve established the backups of your choice, restoring takes on different forms depending on the type of recovery required: Failover to backup cluster Import Table/Restore a snapshot Point HBase root directory to backup location If your backup strategy is such that you’ve replicated your HBase data to a backup cluster in a different data center, failing over is as easy as pointing your end-user applications to the backup cluster with DNS techniques.� Keep in mind, however, that if you plan to allow data to be written to your backup cluster during the outage period, you will need to make sure that data gets back to the primary cluster when the outage is over. Master-to-master or cyclic replication will handle this process automatically for you, but a master-slave replication scheme will leave your master cluster out of sync, requiring manual intervention after the outage. Along with the Export feature described previously, there is a corresponding Import tool that can take the data previously backed up by Export and restore it to an HBase table. The same performance implications that applied to Export are in play with Import as well. If your backup scheme involved taking snapshots, reverting back to a previous copy of your data is as simple as restoring that snapshot. You can also recover from a disaster by simply modifying the hbase.root.dir property in hbase-site.xml and pointing it to a backup copy of your /hbase directory if you had done the brute-force offline copy of the HDFS data structures. However, this is also the least desirable of restore options as it requires an extended outage while you copy the entire data structure back to your production cluster, and as previously mentioned, .META. could be out of sync. Conclusion In summary, recovering data after some form of loss or outage requires a well-designed BDR plan. I highly recommend that you thoroughly understand your business requirements for uptime, data accuracy/availability, and disaster recovery.�Armed with detailed knowledge of your business requirements, you can carefully choose the tools that best meet those needs.� Selecting the tools is only the beginning, however. You should run large-scale tests of your BDR strategy to assure that it functionally works in your infrastructure, meets your business needs, and that your operations teams are very familiar with the steps required before an outage happens and you find out the hard way that your BDR plan will not work. If you’d like to comment on or discuss this topic further, use our community forum for HBase. Further reading: Jon Hsieh’s Strata + Hadoop World 2012 presentation HBase: The Definitive Guide (Lars George) HBase In Action (Nick Dimiduk/Amandeep Khurana) Clint Heath is a community manager for community.cloudera.com and the former HBase team lead for Cloudera Support.</snippet></document><document id="76"><title>Putting Spark to Use: Fast In-Memory Computing for Your Big Data Applications</title><url>http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/</url><snippet>Our thanks to Databricks, the company behind Apache Spark (incubating), for providing the guest post below. Cloudera and Databricks recently announced that Cloudera will distribute and support Spark in CDH. Look for more posts describing Spark internals and Spark + CDH use cases in the near future. Apache Hadoop has revolutionized big data processing, enabling users to store and process huge amounts of data at very low costs. MapReduce has proven to be an ideal platform to implement complex batch applications as diverse as sifting through system logs, running ETL, computing web indexes, and powering personal recommendation systems. However, its reliance on persistent storage to provide fault tolerance and its one-pass computation model make MapReduce a poor fit for low-latency applications and iterative computations, such as machine learning and graph algorithms. Apache Spark addresses these limitations by generalizing the MapReduce computation model, while dramatically improving performance and ease of use. Fast and Easy Big Data Processing with Spark At its core, Spark provides a general programming model that enables developers to write application by composing arbitrary operators, such as mappers, reducers, joins, group-bys, and filters. This composition makes it easy to express a wide array of computations, including iterative machine learning, streaming, complex queries, and batch. In addition, Spark keeps track of the data that each of the operators produces, and enables applications to reliably store this data in memory. This is the key to Spark’s performance, as it allows applications to avoid costly disk accesses. As illustrated in the figure below, this feature enables: Low-latency computations by caching the working dataset in memory and then performing computations at memory speeds, and Efficient iterative algorithm by having subsequent iterations share data through memory, or repeatedly accessing the same dataset Spark’s ease-of-use comes from its general programming model, which does not constrain users to structure their applications into a bunch of map and reduce operations. Spark’s parallel programs look very much like sequential programs, which make them easier to develop and reason about. Finally, Spark allows users to easily combine batch, interactive, and streaming� jobs in the same application. As a result, a Spark job can be up to 100x faster and requires writing 2-10x less code than an equivalent Hadoop job. Using Spark for Advanced Data Analysis and Data Science Interactive Data Analysis One of Spark’s most useful features is the interactive shell, bringing Spark’s capabilities to the user immediately – no IDE and code compilation required. The shell can be used as the primary tool for exploring data interactively, or as means to test portions of an application you’re developing. The screenshot below shows a Spark Python shell in which the user loads a file and then counts the number of lines that contain “Holiday”. As illustrated in this example, Spark can read and write data from and to HDFS. Thus, as soon as Spark is installed, a Hadoop user can immediately start analyzing HDFS data. Then, by caching a dataset in memory, a user can perform a large variety of complex computations interactively! Spark also provides a Scala shell, and APIs in Java, Scala, and Python for stand-alone applications. Faster Batch Some of the earliest deployments of Spark have focused on how to improve performance in existing MapReduce applications. Remember that MapReduce is actually a generic execution framework and is not exclusive to it’s most well-known implementation in core Hadoop. Spark provides MapReduce as well, and because it can efficiently use memory (while using lineage to recover from failure if necessary), some implementations are simply faster in Spark’s MapReduce as compared to Hadoop’s MapReduce right off the bat, before you even get in to leveraging cache for iterative programs. The example below illustrates Spark’s implementation of MapReduce’s most famous example, word count. You can see that Spark supports operator chaining. This becomes very useful when doing a bit of pre- or post-processing on your data, such as filtering data prior to running a complex MapReduce job. 
val file = sc.textFile("hdfs://.../pagecounts-*.gz")
val counts = file.flatMap(line =&gt; line.split(" "))
                   .map(word =&gt; (word, 1))
                   .reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://.../word-count")   Spark’s batch capabilities have been proven in real-world scenarios. A very large Silicon Valley Internet company did a plain-vanilla port of a single MR job implementing feature extraction in a model training pipeline, and saw a 3x speedup. Iterative Algorithms Spark allow users and applications to explicitly cache a dataset by calling the cache() operation. This means that your applications can now access data from RAM instead of disk, which can dramatically improve the performance of iterative algorithms that access the same dataset repeatedly. This use case covers an important class of applications, as all machine learning and graph algorithms are iterative in nature. Two of the world’s largest Internet companies leverage Spark’s efficient iterative execution to provide content recommendations and ad targeting. Machine-learning algorithms such as logistic regression have run 100x faster than previous Hadoop-based implementations (see the plot to the right), while other algorithms such as collaborative filtering or alternating direction method of multipliers have run over 15x faster. The following example uses logistic regression to find the best hyperplane that separates two sets of points in a multi-dimensional feature space. Note the cached dataset “points” is accessed repeatedly from memory, whereas in MapReduce, each iteration will read data from the disk, which incurs a huge overhead. 
val points = sc.textFile("...").map(parsePoint).cache()
var w = Vector.random(D) //current separating plane
for (i &lt;- 1 to ITERATIONS) {
    val gradient = points.map(p =&gt;
      (1 / (1 + exp(-p.y*(w dot p.x))) - 1) * p.y * p.x
    ).reduce(_ + _)
    w -= gradient
  }
  println("Final separating plane: " + w)
   Real-Time Stream Processing With a low-latency data analysis system at your disposal, it’s natural to extend the engine towards processing live data streams. Spark has an API for working with streams, providing exactly-once semantics and full recovery of stateful operators. It also has the distinct advantage of giving you the same Spark APIs to process your streams, including reuse of your regular Spark application code. The code snippet below shows a simple job processing a network stream, filtering for words beginning with a hashtag and performing a word count on every 10 seconds of data. Compare this to the previous word-count example and you’ll see how almost the exact same code is used, but this time processing a live data stream. 
val ssc = new StreamingContext(args(0), "NetworkHashCount", Seconds(10),
      System.getenv("SPARK_HOME"), Seq(System.getenv("SPARK_EXAMPLES_JAR")))

val lines = ssc.socketTextStream("localhost", 9999)
val words = lines.flatMap(_.split(" ")).filter(_.startsWith("#"))
val wordCounts = words.map(x =&gt; (x, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()   Although the Spark Streaming API was released less than a year ago, users have deployed it in production to provide monitoring and alerting against stateful, aggregated data from system logs, achieving very fast processing with only seconds of latency. Faster Decision-Making Many companies use big data to make or facilitate user’s decisions in the form of recommendation systems, ad targeting, or predictive analytics. One of the key properties of any decision is latency — that is, the time it takes to make the decision from the moment the input data is available. Reducing decision latency can significantly increase their effectiveness, and ultimately increase the company’s return on investment. Since many of these decisions are based on complex computations (such as machine learning and statistical algorithms), Spark is an ideal fit to speed up decisions. Not surprisingly, Spark has been deployed to improve decision quality as well as to reduce latency. Examples range from ad targeting, to improving the quality of video delivery over the Internet. Unified Pipelines Many of today’s Big Data deployments go beyond MapReduce by integrating other frameworks for streaming, batch, and interactive computation. Users can dramatically reduce the complexity of their data processing pipelines by replacing several systems with Spark. For instance, today, many companies use MapReduce to generate reports and answer historical queries, and deploy a separate system for stream processing to follow key metrics in real-time. This approach requires one to maintain and manage two different systems, as well as develop applications for two different computation models. It would also require one to make sure the results provided by the two stacks are consistent (for example, a count computed by the streaming application and the same count computed by MapReduce). Recently, users have deployed Spark to implement stream processing as well as batch processing for providing historical reports. This not only simplifies deployment and maintenance, but dramatically simplifies application development. For example, maintaining the consistency of real-time and historical metrics is no longer a problem as they are computed using the same code. A final benefit of the unification is improved performance, as there is no need to move the data between different systems: once in-memory, the data can be shared between the streaming computations and historical (or interactive) queries.�� Your Turn: Go Get Started Spark is very easy to get started writing powerful Big Data applications. Your existing Hadoop and/or programming skills will have you productively interacting with your data in minutes. Go get started today: Download: http://spark.incubator.apache.org/downloads.html Quick Start: http://spark.incubator.apache.org/docs/latest/quick-start.html Spark Summit 2013 (Dec. 2, 2013): http://spark-summit.org</snippet></document><document id="77"><title>How-to: Shorten Your Oozie Workflow Definitions</title><url>http://blog.cloudera.com/blog/2013/11/how-to-shorten-your-oozie-workflow-definitions/</url><snippet>While XML is very good for standardizing the way Apache Oozie workflows are written, it�s also known for being very verbose. Unfortunately, that means that for workflows that have many actions, your workflow.xml can easily become quite long and difficult to manage and read. Cloudera is constantly making improvements to address this issue, and in this how-to, you’ll get a quick look at some of the current features and tricks that you can use to help shorten your Oozie workflow definitions. The Sub-Workflow Action One of the more interesting action types that Oozie has is the Sub-Workflow Action; it allows you to run another workflow from your workflow. Suppose you have a workflow where you�d like to use the same action multiple times; this is not usually allowed because Oozie workflows are Direct Acyclic Graphs (DAG) and so actions cannot be executed more than once as part of a workflow. However, if you put that action into its own workflow, you can actually call it multiple times from within the same workflow by using the Sub-Workflow Action. So, instead of copying and pasting the same action to be able to use it multiple times (and taking up a lot of extra space), you can just use the Sub-Workflow Action, which could be shorter; it is also easier to maintain because if you ever want to change that action, you only have to do it in one place. You also get the advantage of being able to use that action in other workflows. Of course, you can still put multiple actions in your sub-workflow. We�re always looking for new ways to improve the usability of Oozie and of the workflow format. I�ve created a simple example workflow showing how the Sub-Workflow Action can be used to run the same action twice from another workflow. Additional details are provided in the readme. (IMPORTANT: Be very careful when using the Sub-Workflow Action. While it can be used to create loops, if you are not careful you can easily create an infinite recursion! OOZIE-1550 and OOZIE-1583 will add safeguards to protect against this risk.) The Sub-Workflow Action has been available since Oozie workflow schema 0.1. Including Other XML Configuration Files Another reason why workflows can become quite long is in specifying all of the necessary properties in the &lt;configuration&gt; section of an action. Oozie allows you to “include” the contents of a separate file (from somewhere in HDFS) as part of the &lt;configuration&gt; section by using the &lt;job-xml&gt; element. This approach is really helpful if you have multiple actions that need to have the same properties. You can put them all in a separate file and simply use &lt;job-xml&gt; instead of copying and pasting them everywhere, thus keeping your workflow shorter. You also get the benefit of only having to maintain one copy of those properties. Another common use case for &lt;job-xml&gt; is for the Hive action, where you would typically point it at a copy of your hive-site.xml. Also, your actions can have multiple &lt;job-xml&gt; elements, so you can include multiple files. If you specify a property in a &lt;job-xml&gt; file and in the &lt;configuration&gt; section, the latter has priority, which provides a convenient way of using this feature while still being able to override common options for just a specific action. One thing to watch out for is that currently any EL variables used in a file referenced by a &lt;job-xml&gt; element are not resolved. OOZIE-1580 aims to improve this situation. The &lt;job-xml&gt; element has has been around since workflow schema 0.1, but was originally limited to only one instance. In workflow schema 0.4 and in newer versions of the extension action schemas, it was modified to allow multiple &lt;job-xml&gt; elements. Using the Global Section The Global Section is another solution for the problem of having to specify the same properties in multiple actions. The Global Section goes at the top of a workflow and looks like this: 
&lt;global&gt;
   &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
   &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
   &lt;job-xml&gt;job1.xml&lt;/job-xml&gt;
   &lt;configuration&gt;
        &lt;property&gt;
            &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
            &lt;value&gt;${queueName}&lt;/value&gt;
        &lt;/property&gt;
    &lt;/configuration&gt;
&lt;/global&gt;
   Most workflows use the same JobTracker and NameNode for all actions, so you can easily remove those two lines from all your actions by simply specifying them once at the top in the Global Section! And if you do have some actions that use a different JobTracker or NameNode, you can override the &lt;global&gt; section in that specific action as well. The &lt;job-xml&gt; element and &lt;configuration&gt; elements here work just like they usually do, except that they’re applied to all actions in your workflow. As with the &lt;job-tracker&gt; and &lt;name-node&gt; elements, you can override their properties by specifying them in the individual actions. The Global Section is a great way to reduce the number of repeated properties, such as mapred.job.queue.name. Also, each of the elements in the &lt;global&gt; section are optional, so if you only want to use one of them, you can do so. The Global Section is a relatively new feature, and was added in Oozie workflow schema 0.4. Conclusion Now that you are aware of these tips and tricks for shortening your workflows, hopefully you can find ways of taking advantage of them. We�re always looking at new ways to improve the usability of Oozie and of the workflow format; that said, if you have any ideas that you think would be helpful in this regard, or for anything, please let us know on the on the cdh-user or oozie-user mailing lists, or the Cloudera Community Forums. Further Reading: Workflow Functional Specification Documentation Sub-Workflow Action Documentation Global Section Documentation Sub-Workflow Action Repeat Example Robert Kanter is a Software Engineer and a Committer/PMC Member for Oozie.</snippet></document><document id="78"><title>How-to: Add Cloudera Search to Your Cluster using Cloudera Manager</title><url>http://blog.cloudera.com/blog/2013/11/how-to-add-cloudera-search-to-your-cluster-using-cloudera-manager/</url><snippet>Cloudera Manager 4.7 added support for managing Cloudera Search 1.0. Thus Cloudera Manager users can easily deploy all components of Cloudera Search (including Apache Solr) and manage all related services, just like every other service included in CDH (Cloudera�s distribution of Apache Hadoop and related projects). In this how-to, you will learn the steps involved in adding Cloudera Search to a Cloudera Enterprise (CDH + Cloudera Manager) cluster. Installing the SOLR Parcel In our example, the cluster uses a CDH 4.4 parcel and is running Apache ZooKeeper, HDFS, and Apache HBase services. (Parcels are a really useful way to deploy new software and do painless upgrades via Cloudera Manager.) If you would like to download the SOLR parcel directly from Cloudera, you can use the default settings for �Remote Parcel Repository URLs� (under the Parcels section in the Administration tab) as shown below: Setting the Parcel repository URL If you want to use a local repository (that is, first download the parcel from Cloudera and then install from the local copy), you can follow the instructions here. The next steps are to �Download�, �Distribute,� and �Activate� the parcel from the Parcels page on the Hosts tab. Deploying the SOLR parcel Once the parcel is activated, you have all components of Cloudera Search (Solr, Lily HBase Indexer, and Apache Flume�s Morphlines Sink) ready to be used along with CDH. The next step is to add the Apache Solr service to your cluster. In the �Actions� menu of your cluster on the Services tab, choose �Add a Service,� which takes you to the �Add Service Wizard� in Cloudera Manager. Once you follow the steps in the wizard and choose where the Solr servers should run, you�ll land on a workflow page that will initialize the Solr service and start all Solr servers. Getting the Solr service up and running That�s it — the Solr service is now ready for use! Follow the instructions in the Cloudera Search User Guide to create collections and add documents to them for indexing. The screenshot below shows how to create a collection using the default Solr schema. � Creating the first collection Adding Lily HBase Indexer Cloudera Manager 4.7 also provides support for the Lily HBase Indexer included with the SOLR parcel. The Lily HBase Indexer Service is a flexible, scalable, fault tolerant, transactional, near-real time system for processing a continuous stream of HBase cell updates into live search indexes. To use it, add the �Keystore Indexer� service via the �Add Service Wizard.� Before you can use the Lily HBase Indexer however, you need to ensure that replication and indexing are enabled in the HBase service in the cluster. You can change these properties on the HBase service configuration page under the �Backup� section. Setting HBase properties for Lily HBase Indexer Also, note that Cloudera Manager includes a default Cloudera Morphlines file that can be used by the Lily HBase Indexer. To modify that file to use your own functions, you should navigate to the Keystore Indexer service and modify the Morphlines configuration as shown below: Editing Cloudera Morphlines for Lily HBase Indexer Once these changes are made, you can start using the Lily HBase Indexer to index any data coming into HBase by following the instructions in the Lily HBase Indexer User Guide. This blog post also provides a great example of how to index emails using HBase and Cloudera Search. Conclusion Now you know how easy it is to deploy, configure, and manage a Cloudera Search service to your CDH cluster using Cloudera Manager. Starting with Cloudera Enterprise 5 (in beta at the time of writing), Cloudera Search and Lily HBase indexer will install and start by default � making this process even easier. Vikram Srivastava is a Software Engineer at Cloudera.</snippet></document><document id="79"><title>It’s "Hello World" for Cloudera User Groups!</title><url>http://blog.cloudera.com/blog/2013/11/its-hello-world-for-cloudera-user-groups/</url><snippet>Since its inception, Cloudera has been an enthusiastic supporter of user groups and meetups worldwide. And now, we’re extending that support yet further, by incubating new Cloudera User Groups (CUGs) in the San Francisco Bay Area, Chicago area, and New York City. Unlike grass-roots user groups, which are inherently community-oriented and have no particular vendor preference, CUGs are designed and intended for users of Cloudera Standard (our free offering containing CDH and Cloudera Manager) and customers of Cloudera Enterprise (our paid, supported offering containing CDH, Cloudera Manager, and enterprise functionality such as rolling upgrades and Cloudera Navigator). For that reason, I predict that CUG conversations will tend to focus on the differentiated aspects of the Cloudera platform. The first meetings of these groups are occurring within the next few weeks, so jump on board by joining and RSVPing today (anyone and everyone is welcome to attend): Dec. 3: Midwest CUG (@ Chicago Mercantile Exchange) – RSVP Dec. 5: New York CUG (@ Conductor Inc. in NYC) – RSVP Dec. 10: SF Bay Area CUG (@ Cloudera HQ in San Francisco) – RSVP Although Cloudera is incubating/organizing these initial meetings, our expectation is that users and customers will eventually grab the car keys and take things from there (with our full and ongoing support). And, of course, any passionate users interested in jump-starting a new CUG in their own home city is welcome to get in touch! Just drop us a note at CUGs@cloudera.com. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="80"><title>BinaryPig: Scalable Static Binary Analysis Over Hadoop</title><url>http://blog.cloudera.com/blog/2013/11/binarypig-scalable-static-binary-analysis-over-hadoop/</url><snippet>Our thanks to Telvis Calhoun, Zach Hanif, and Jason Trost of Endgame for the guest post below about their BinaryPig application for large-scale malware analysis on Apache Hadoop. Endgame uses data science to bring clarity to the digital domain, allowing its federal and commercial partners to sense, discover, and act in real time. Over the past three years, Endgame received 40 million samples of malware equating to roughly 19TB of binary data. In this, we�re not alone. McAfee reports that it currently receives roughly 100,000 malware samples per day and received roughly 10 million samples in the last quarter of 2012. Its total corpus is estimated to be about 100 million samples. VirusTotal receives between 300,000 and 600,000 unique files per day, and of those roughly one-third to half are positively identified as malware (as of April 9, 2013). This huge volume of malware offers both challenges and opportunities for security research, especially applied machine learning. Endgame performs static analysis on malware in order to extract feature sets used for performing large-scale machine learning. Since malware research has traditionally been the domain of reverse engineers, most existing malware analysis tools were designed to process single binaries or multiple binaries on a�single computer and are unprepared to confront terabytes of malware simultaneously. There is no easy way for security researchers to apply static analysis techniques at scale; companies and individuals that want to pursue this path are forced to create their own solutions. Our early attempts to process this data did not scale well with the increasing flood of samples. As the size of our malware collection increased, the system became unwieldy and hard to manage, especially in the face of hardware failures. Over the past two years we refined this system into a dedicated framework based on Hadoop so that our large-scale studies are easier to perform and are more repeatable over an expanding dataset. To address this problem, we created an open source framework, BinaryPig, built on Hadoop and Apache Pig (utilizing CDH, Cloudera�s distribution of Hadoop and related projects) and Python. It addresses many issues of scalable malware processing, including dealing with increasingly large data sizes, improving workflow development speed, and enabling parallel processing of binary files with most pre-existing tools. It is also modular and extensible, in the hope that it will aid security researchers and academics in handling ever-larger amounts of malware. Major Design Points One of the most significant design principles of BinaryPig is the ability to ingest, store, and process large numbers of small files without affecting performance � an area of potential improvement for Hadoop. BinaryPig�s ingest tool combines thousands of small binary files into one or more large Hadoop SequenceFiles (each file typically ranging from 10GB to 100GB in size), an internal data storage format that enables Hadoop to take advantage of sequential disk read speeds as well as split these files for parallel processing. The SequenceFiles created by BinaryPig are a collection of key/value pairs where the key represents the ID of the binary file (typically MD5 checksum) and the value is the contents of the file represented as raw bytes. The malware data is properly distributed across computing nodes in the cluster, and is resilient to failures, via HDFS. BinaryPig�s data processing architecture Within Pig, analysis scripts and their supporting libraries are uploaded to Hadoop�s DistributedCache before job execution and destroyed from the processing nodes afterward. This functionality allows users to quickly publish analytical scripts and libraries to the processing nodes without the general concerns of ensuring deployment operated as expected or the time associated with a manual deployment. We also developed a series of Pig loader functions that read SequenceFiles, copy binaries to the local filesystem, and enable scripts to process them. These load functions were designed to allow these analysis scripts to read the files dropped onto the local filesystem. Thus BinaryPig users can utilize pre-existing malware analysis scripts and tools.� We have built in support for hashing (md5, sha1, sha256, etc), pehash, yara, clamav, and generic script execution.� We have used this with some open source tools such as peframe as well as several internally developed malware analysis scripts. In essence, we are using HDFS to distribute our malware binaries across our cluster and using MapReduce to push the processing to the node where the data is stored. This approach prevents needless data copying as seen in many legacy cluster computing environments. Data emitted from the system is also stored into HDFS for additional batch processing as needed. We also typically load analysis results into ElasticSearch indices. This distributed index is used for manual exploration by individual users, as well as automated extraction by analytical processes. BinaryPig has enabled us to conduct several research studies across our entire malware dataset that were either not possible or too time consuming to be useful before.�Some of these studies include: Leveraging our Hadoop cluster to perform virus scanning of all our malware samples periodically as virus signature files are updated. This is useful since most of the malware we receive is not detected by commercial AV systems for several months. Extracting ICO image files from all our malware in order to cluster malware by embedded icon files. Creating the ability to perform a repeatable malware census and comparisons at scale in just a matter of hours. Scanning our entire malware set with newly developed/released Yara signatures in order to determine if any of our old malware matches new signatures. Frequently the open source community releases Yara signatures for interesting malware before AV vendors release signatures so we can leverage these cutting edge signatures to detect threats faster. A sampling of real icons extracted from our malware set Conclusion As you can see, scalable malware analysis is another good example of a Hadoop use case premised on the ingestion, storage, and processing of huge amounts of data in parallel (in this case, even for data in the form of numerous small files � traditionally, not a strong point for Hadoop). For more details about BinaryPig�s architecture and design, read our paper from Black Hat USA 2013 or check out our presentation slides. BinaryPig is an open source project under the Apache 2.0 License, and all code is available on Github.</snippet></document><document id="81"><title>This Month in the Ecosystem (October 2013)</title><url>http://blog.cloudera.com/blog/2013/11/this-month-in-the-ecosystem-october-2013/</url><snippet>Welcome to our fourth edition of “This Month in the Ecosystem,” a digest of highlights from October 2013 (never intended to be comprehensive; for completeness, see Hadoop Weekly). For generating sheer excitement, that month installed a high bar to meet in the future: Hadoop 2 Became Stable with the 2.2 (“GA”) Release The idea that this news could be done justice in a few sentences strains credulity. (Furthermore, it has been covered elsewhere widely and thoroughly at this point.) Suffice to say here that this release will transform the Big Data ecosystem by enabling Hadoop to serve as DNA for a wide variety of new Big Data applications and use cases. The community deserves sincere thanks and congratulations for working together to achieve this result. Read more about the Hadoop 2 release | See advice for migrating (for users)�|�See advice for migrating (for operators) Strata + Hadoop World 2013 Rocked the Big Data Ecosystem More than 3,000 people attended the conference this year — making the conference now bigger than several other mainstream tech conferences. Of particular note was Cloudera CSO Mike Olson’s prophecy of an Enterprise Data Hub premised on an open source data platform (Hadoop) and complementary security/data/systems management infrastructure. (Protip: Mike hasn’t been wrong yet.) Read Mike Olson’s description of the Enterprise Data Hub |�See Cloudera keynotes + presenter slides Cloudera Released Cloudera Enterprise 5 Beta The perfect companion announcement to that prophecy, of course, was the release of a Cloudera Enterprise 5 (in which Hadoop 2 is default) beta — which when released into GA will allow users to breathe life into their own Enterprise Data Hub. Read more about/download the Cloudera Enterprise 5 Beta Cloudera Announced Support for Apache Spark on CDH Spark, the in-memory processing framework that sits on top of HDFS (complementing or replacing MapReduce), is getting a lot of looks from Hadoop users who depend on fast data analytics. As part of a new program in which Cloudera will partner with companies commercializing interesting new open source projects (in this case, with Databricks), Spark will be formally supported on CDH. Learn more about the Cloudera + Databricks announcement | See the Spark Summit 2013 Agenda Cascading and Spring for Apache Hadoop Were Verified for Compatibility/Certified (Respectively) with CDH The more mainstream developers who have access to CDH as a platform, and the more APIs that can help them get it, the better. +1. Read more about the new certifications Apache HBase Became an 0.96 As HBase VP/0.96 release manager Michael Stack describes in the blog post referenced below, this new release is packed with new functionality for improved scalability, availability, operability, and more. Massive props to the diverse HBase community for reaching this milestone. Read all the details about HBase 0.96�|�Read the 0.94-0.96 upgrade guide� The Cloudera Impala�E-book from O’Reilly Hit the Internets If you feel the need to digest the use cases for and inner workings of Impala via a relatively quick read — and yet get enough examples and technical detail to make it stick — you can’t do better than this 30-page e-book from O’Reilly Media (authored by Cloudera’s John Russell). Read John Russell’s blog post�| Download the e-book Apache Curator Became a Top-Level Apache Project Curator is an interesting effort from some Netflix engineers to dis-intermediate developers from the gory details of building distributed systems on top of Apache ZooKeeper. In October, it joined the ranks of top-level Apache projects. Congrats to the Curator team! Learn more about Curator� The next installment of “This Month in the Ecosystem” will publish in early December. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="82"><title>HBase Training: Demystifying Real-Time Big Data Storage</title><url>http://blog.cloudera.com/blog/2013/11/hbase-training-demystifying-real-time-big-data-storage/</url><snippet>We at Cloudera University have been busy lately, building and expanding our courses to help data professionals succeed. We’ve expanded the�Hadoop Administrator course and created a new Data Analyst course. Now we’ve updated and relaunched our course on Apache HBase�to help more organizations adopt Hadoop’s real-time Big Data store as a competitive advantage. The course is designed to make sure developers and administrators with an HBase use case can start realizing value from day one. We doubled the length of the curriculum to four days, allowing a deep dive into HBase operations as well as development. As the primary course author, I had the pleasure of interviewing some of the most notable members of the HBase community. People like Michael Stack, Lars George, and Amandeep Khurana have written the books, contributed code, and deployed and supported huge clusters in production. I also tried to capture many of the key insights that otherwise only exist in HBase�s tribal knowledge, some of which I discuss in my recent blog posts on the REST Interface and the Thrift Interface, as well as in the Simple User Access chapter of the Apache HBase Reference Guide. Beyond the Tribe The main theme of the four-day course is that effective HBase requires an understanding of both programming and operations topics, typically constituting DevOps, but here positioned for a variety of roles and goals. With our core audiences of developers and administrators in mind, we built the course with the objective of every participant both learning from and contributing to the learning of people from other jobs. Developers will learn how their code can affect operations. Administrators will learn common code and architecture issues that may influence their strategies. Business intelligence analysts and quality assurance engineers will learn how to interact with HBase and what challenges to anticipate. To use HBase effectively, you need to understand programming as well as operations topics. Adding two more days to the HBase course allowed us to increase the number of hands-on exercises and ramp up the focus on real-world scenarios culled from customer engagements. � The curriculum dives deep into using the HBase API and coding with both Java and Python, including hints and solutions for each language in the exercises. Non-developers can also look through these examples to see how the HBase API works. The administration-focused sections show how to add HBase to an existing cluster, covering both the installation and configuration. HBase configuration has long been considered a black art, but the insights we�ve captured from our experts in the field and writing the code cast new light on the methods and best practices required for success. Real-Time Access for the Real World We strive to make the learnings from the course as relevant as possible and cover some of the other ecosystem projects that work with or augment HBase. One exercise explores Apache Hive, which is a great tool for developers and analysts to access HBase using a SQL-like syntax instead of writing code. We also cover the Kiji Project�and how it makes developing real-time Big Data applications with HBase easier. Finally, we cover designing solutions with HBase. These chapters bring together and activate the lessons about HBase architecture and API programming from earlier in the course. Engineering an HBase solution is different from working with an RDBMS. This chapter covers how to develop and execute design according to particular access patterns and performance requirements. If you want to learn HBase, are evaluating or starting an HBase project, or are standing up an HBase cluster, I highly recommend you take this course and start working towards the CCSHB HBase certification. We’ve taken the full HBase knowledge base, condensed it down to the most useful insights and best practices, and made it relevant for data professionals from any type of organization. You can even start by watching our recent Introduction to Apache HBase Training webinar�to get a taste of the course content, understand the audience and prerequisites, and grab a promotion code for a discount on the live course. Jesse Anderson is a Curriculum Developer and Instructor at Cloudera.</snippet></document><document id="83"><title>How-to: Use Cascading Pattern with R and CDH</title><url>http://blog.cloudera.com/blog/2013/11/how-to-use-cascading-pattern-with-r-and-cdh/</url><snippet>Our thanks to Concurrent Inc. for the how-to below about using Cascading Pattern with CDH. Cloudera recently tested CDH 4.4 with the Cascading Compatibility Test Suite verifying compatibility with Cascading 2.2. Cascading Pattern�is a machine-learning project within the Cascading development framework used to build enterprise data workflows. Cascading provides an abstraction layer on top of Apache Hadoop and other computing topologies that allows enterprises to leverage existing skills and resources to build data processing applications on Hadoop, without the need for specialized Hadoop skills. Pattern, in particular, leverages an industry standard called Predictive Model Markup Language (PMML), which allows data scientists to leverage their favorite statistical and analytics tools (such as R, SAS, Oracle, and so on) to export predictive models and quickly run them on data sets stored in Hadoop. Pattern�s benefits include reduced development costs, time savings, and reduced licensing issues at scale � all while leveraging Hadoop clusters, core competencies of analytics staff, and existing intellectual property in the predictive models. By using Cascading Pattern, predictive modeling can now be exported as PMML from a variety of analytics frameworks, then run on Hadoop at scale. This approach saves licensing costs, allows for applications to scale-out, and directly integrates predictive modeling — expressed as Cascading apps — within other business logic. In this how-to, you will learn how to create a simple example model using Cascading, R, and CDH. Step 1: Set Up Your Environment In this section, we will go through the steps needed to set up your environment. To set up Java for your environment, download�Java�and follow the installation instructions. Version 1.6.x was used to create the examples used here. Get the JDK, not the JRE. Install according to vendor instructions. Be sure to set the�JAVA_HOME�environment variable correctly. To set up Gradle for your environment, download�Gradle�and follow the installation instructions. Version 1.4 and later is required for some examples in this tutorial. Install according to vendor instructions. Be sure to set the�GRADLE_HOME�environment variable correctly. Install CDH 4.4�in standalone mode. Set up R and RStudio for your environment by visiting: http://cran.r-project.org/ http://www.rstudio.com/ide/download/ Step 2: Get the Source Code Navigate to the�Pattern Github project and in the bottom right corner of the screen, click Download ZIP�to download a ZIP compressed archive of the source code. When complete, unzip and move the directory �pattern� to a location on your filesystem where you have space available to work. Step 3: Create the Model Navigate to the pattern directory, and then into its pattern-examples subdirectory. There is an example R script in examples/r/rf_pmml.R that creates a Random Forest model. This is representative of a predictive model for an anti-fraud classifier used in e-commerce apps. 
## train a RandomForest model
f &lt;- as.formula("as.factor(label) ~ .")
fit &lt;- randomForest(f, data_train, ntree=50)

## test the model on the holdout test set
print(fit$importance)
print(fit)

predicted &lt;- predict(fit, data)
data$predicted &lt;- predicted
confuse &lt;- table(pred = predicted, true = data[,1])
print(confuse)

## export predicted labels to TSV
write.table(data, file=paste(dat_folder, "sample.tsv", sep="/"),
  quote=FALSE, sep="t", row.names=FALSE)

## export RF model to PMML
saveXML(pmml(fit), file=paste(dat_folder, "sample.rf.xml", sep="/"))
   Load the �rf_pmml.R� script into RStudio using the File menu and Open File..�option. Click the Source button in the upper middle section of the screen. That will execute the R script and create the predictive model. The last line saves the predictive model into a file called sample.rf.xml as PMML. PMML is XML-based and thus not optimal for humans to read, but it is efficient for machines to parse: 
&lt;?xml version="1.0"?&gt;
&lt;PMML version="4.0" xmlns="http://www.dmg.org/PMML-4_0"
 xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
 xsi:schemaLocation="http://www.dmg.org/PMML-4_0

http://www.dmg.org/v4-0/pmml-4-0.xsd"&gt;

 &lt;Header copyright="Copyright (c)2012 Concurrent, Inc."
  description="Random Forest Tree Model"&gt;
  &lt;Extension name="user" value="ceteri" extender="Rattle/PMML"/&gt;
  &lt;Application name="Rattle/PMML" version="1.2.30"/&gt;
  &lt;Timestamp&gt;2012-10-22 19:39:28&lt;/Timestamp&gt;
 &lt;/Header&gt;
 &lt;DataDictionary numberOfFields="4"&gt;
  &lt;DataField name="label" optype="categorical" dataType="string"&gt;
   &lt;Value value="0"/&gt;
   &lt;Value value="1"/&gt;
  &lt;/DataField&gt;
  &lt;DataField name="var0" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="var1" optype="continuous" dataType="double"/&gt;
  &lt;DataField name="var2" optype="continuous" dataType="double"/&gt;
 &lt;/DataDictionary&gt;
 &lt;MiningModel modelName="randomForest_Model" functionName="classification"&gt;
  &lt;MiningSchema&gt;
   &lt;MiningField name="label" usageType="predicted"/&gt;
   &lt;MiningField name="var0" usageType="active"/&gt;
   &lt;MiningField name="var1" usageType="active"/&gt;
   &lt;MiningField name="var2" usageType="active"/&gt;
  &lt;/MiningSchema&gt;
  &lt;Segmentation multipleModelMethod="majorityVote"&gt;
   &lt;Segment id="1"&gt;
    &lt;True/&gt;
    &lt;TreeModel modelName="randomForest_Model" functionName="classification"
     algorithmName="randomForest" splitCharacteristic="binarySplit"&gt;
     &lt;MiningSchema&gt;
      &lt;MiningField name="label" usageType="predicted"/&gt;
      &lt;MiningField name="var0" usageType="active"/&gt;
      &lt;MiningField name="var1" usageType="active"/&gt;
      &lt;MiningField name="var2" usageType="active"/&gt;
     &lt;/MiningSchema&gt;
...
   Cascading Pattern supports additional models, as well as ensembles, of the following models: General Regression Regression Clustering Tree Mining Step 4: Build Cascading Now that we have a model created and exported as PMML, let�s work on running it at scale atop CDH. In the pattern-examples directory, execute the following Bash shell commands: 
&gt; gradle clean jar
   That line invokes Gradle to run the build script build.gradle, and compile the Cascading Pattern example app. After that compiles, look for the built app as a JAR file in the build/libs subdirectory: 
&gt; ls -lts build/libs/pattern-examples-*.jar
   Now we�re ready to run this Cascading Pattern example app on CDH. First, we make sure to delete the output results (required by Hadoop). Then we run Hadoop: we specify the JAR file for the app, the PMML file using a --pmmlcommand line option, along with sample input data data/sample.tsv and the location of the output results: 
&gt; rm -rf out
&gt; hadoop jar build/libs/pattern-examples-*.jar data/sample.tsv out/classify --pmml data/sample.rf.xml
   After that runs, check the out/classify subdirectory. Look at the results of running the PMML model, which will be in the part-* partition files: 
&gt; less out/classify/part-*
   Let�s take a look at what we just built and ran. The source code for this example is located in the src/main/java/cascading/pattern/Main.java file: 
public class Main
  {
  /** @param args  */
  public static void main( String[] args ) throws RuntimeException
    {
    String inputPath = args[ 0 ];
    String classifyPath = args[ 1 ];

    // set up the config properties
    Properties properties = new Properties();
    AppProps.setApplicationJarClass( properties, Main.class );

    HadoopFlowConnector flowConnector = new HadoopFlowConnector( properties );

    // create source and sink taps
    Tap inputTap = new Hfs( new TextDelimited( true, "t" ), inputPath );
    Tap classifyTap = new Hfs( new TextDelimited( true, "t" ), classifyPath );

    // handle command line options
    OptionParser optParser = new OptionParser();
    optParser.accepts( "pmml" ).withRequiredArg();

    OptionSet options = optParser.parse( args );

    // connect the taps, pipes, etc., into a flow
    FlowDef flowDef = FlowDef.flowDef()
      .setName( "classify" )
      .addSource( "input", inputTap )
      .addSink( "classify", classifyTap );

    // build a Cascading assembly from the PMML description
    if( options.hasArgument( "pmml" ) )
      {
      String pmmlPath = (String) options.valuesOf( "pmml" ).get( 0 );

      PMMLPlanner pmmlPlanner = new PMMLPlanner()
        .setPMMLInput( new File( pmmlPath ) )
        .retainOnlyActiveIncomingFields()
        .setDefaultPredictedField( new Fields( "predict", Double.class ) );
      // default value if missing from the model

      flowDef.addAssemblyPlanner( pmmlPlanner );
      }

    // write a DOT file and run the flow
    Flow classifyFlow = flowConnector.connect( flowDef );
    classifyFlow.writeDOT( "dot/classify.dot" );
    classifyFlow.complete();
    }
  }
   Most of the code is the basic plumbing used for Cascading apps. The portions that are specific to Cascading Pattern and PMML are the few lines involving the pmmlPlanner object.</snippet></document><document id="84"><title>Migrating to MapReduce 2 on YARN (For Operators)</title><url>http://blog.cloudera.com/blog/2013/11/migrating-to-mapreduce-2-on-yarn-for-operators/</url><snippet>Cloudera Manager lets you add a YARN service in the same way you would add any other Cloudera Manager-managed service. In Apache Hadoop 2, YARN and MapReduce 2 (MR2) are long-needed upgrades for scheduling, resource management, and execution in Hadoop. At their core, the improvements separate cluster resource management capabilities from MapReduce-specific logic. They enable Hadoop to share resources dynamically between MapReduce and other parallel processing frameworks, such as Cloudera Impala; allow more sensible and finer-grained resource configuration for better cluster utilization; and permit Hadoop to scale to accommodate more and larger jobs. In this post, operators of Cloudera’s distribution of Hadoop and related projects (CDH) who want to upgrade their existing setups to run MR2 on top of YARN will get a guide to the architectural and user-facing differences between MR1 and MR2. (MR2 is the default processing framework in CDH 5, although MR1 will continue to be supported.) Hadoop users/MapReduce programmers can read a similar post designed for them here. Terminology and Architecture In Hadoop 2, MapReduce is split into two components: The cluster resource management capabilities have become YARN, while the MapReduce-specific capabilities remain MapReduce. In the former MR1 architecture, the cluster was managed by a service called the JobTracker. TaskTracker services lived on each node and would launch tasks on behalf of jobs. The JobTracker would serve information about completed jobs. In MR2, the functions of the JobTracker are divided into three services. The ResourceManager is a persistent YARN service that receives and runs applications (a MapReduce job is an application) on the cluster. It contains the scheduler, which, as in MR1, is pluggable. The MapReduce-specific capabilities of the JobTracker have moved into the MapReduce Application Master, one of which is started to manage each MapReduce job and terminated when the job completes. The JobTracker�s function of serving information about completed jobs has been moved to the JobHistoryServer. The TaskTracker has been replaced with the NodeManager, a YARN service that manages resources and deployment on a node. NodeManager is responsible for launching containers, each of which can house a map or reduce task. MR2 architecture is illustrated below. The new architecture has a couple advantages.�First, by breaking up the JobTracker into a few different services, it avoids many of the scaling issues facing MR1.�Most important, it makes it possible to run frameworks other than MapReduce on a Hadoop cluster. For example, Impala can also run on YARN and share resources on a cluster with MapReduce. Configuring and Running MR2 Clusters Migrating Configurations Because MR1 functionality has been split into two components in Hadoop 2, MapReduce cluster configuration options have been split into YARN configuration options, which go in yarn-site.xml; and MapReduce configuration options, which go in mapred-site.xml.�Many have been given new names to reflect the shift.�As JobTrackers and TaskTrackers no longer exist in MR2, all configuration options pertaining to them no longer exist, although many have corresponding options for the ResourceManager, NodeManager, and JobHistoryServer.�We�ll follow up with a full translation table in a future post. A minimal configuration required to run MR2 jobs on YARN is:� yarn-site.xml: &lt;!--?xml version="1.0" encoding="UTF-8"?--&gt;

    yarn.resourcemanager.hostname
    your.hostname.com

    yarn.nodemanager.aux-services
    mapreduce_shuffle
   mapred-site.xml: &lt;!--?xml version="1.0" encoding="UTF-8"?--&gt;

    mapreduce.framework.name
    yarn
   Resource Configuration One of the larger changes in MR2 is the way that resources are managed. In MR1, each node was configured with a fixed number of map slots and a fixed number of reduce slots. Under YARN, there is no distinction between resources available for maps and resources available for reduces – all resources are available for both. Second, the notion of slots has been discarded, and resources are now configured in terms of amounts of memory (in megabytes) and CPU (in �virtual cores�, which are described below). Resource configuration is an inherently difficult topic, and the added flexibility that YARN provides in this regard also comes with added complexity. Cloudera Manager will pick sensible values automatically, but if you are setting up your cluster manually or just interested in the details, read on. Resource Requests From the perspective of a developer requesting resource allocations for a job�s tasks, nothing needs to be changed. Map and reduce task memory requests still work and, furthermore, tasks that will use multiple threads can request more than one core with the mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores properties. Configuring Node Capacities In MR1, the mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum properties dictated how many map and reduce slots each TaskTracker had. These properties no longer exist in YARN. Instead, YARN uses yarn.nodemanager.resource.memory-mb and yarn.nodemanager.resource.cpu-vcores, which control the amount of memory and CPU on each node, both available to both maps and reduces. If you were using Cloudera Manager to configure these properties automatically, Cloudera Manager will take care of it in MR2 as well. If configuring these properties manually, simply set these to the amount of memory and number of cores on the machine after subtracting out resources needed for other services. Virtual Cores To better handle varying CPU requests, YARN supports virtual cores (vcores), a resource meant to express parallelism. The �virtual� in the name is somewhat misleading: Rather, on the NodeManager, vcores should be configured equal to the number of physical cores on the machine. Tasks should be requested with vcores equal to the number of cores they can saturate at once. Currently vcores are very coarse; tasks will rarely want to ask for more than one of them, but a complementary axis that represents processing power will likely be added in the future to enable finer-grained resource configuration. Rounding Request Sizes Also noteworthy are the yarn.scheduler.minimum-allocation-mb, yarn.scheduler.minimum-allocation-vcores, yarn.scheduler.increment-allocation-mb, and yarn.scheduler.increment-allocation-vcores properties, which default to 1024, 1, 512, and 1, respectively. If you submit tasks with resource requests lower than the minimum-allocation values, their requests will be set to these values. If you submit tasks with resource requests that are not multiples of the increment-allocation values, their requests will be rounded up to the nearest increments. To make all of this more concrete, let�s explore an example. Let’s say each node in your cluster has 24GB of memory and 6 cores. Other services running on the nodes require 4GB and 1 core, so you set yarn.nodemanager.resource.memory-mb to 20480 and yarn.nodemanager.resource.cpu-vcores to 5. If you leave the map and reduce task defaults of 1024MB and 1 virtual core intact, you will have at most 5 tasks running at the same time. If you want each of your tasks to use 5GB, you would set their mapreduce.(map|reduce).memory.mb to 5120, which would limit you to 4 tasks running at the same time. Scheduler Configuration Cloudera supports the use of the Fair and FIFO schedulers in MR2. Fair Scheduler allocation files require changes in light of the new way that resources work. The minMaps, maxMaps, minReduces, and maxReduces queue properties have been replaced with a minResources property and a maxProperties. Instead of taking a number of slots, these properties take a value like �1024MB, 3 vcores�. By default, the MR2 Fair Scheduler will attempt to equalize memory allocations in the same way it attempted to equalize slot allocations in MR1. The MR2 Fair Scheduler contains a number of new features including hierarchical queues and fairness based on multiple resources. Administration Commands The jobtracker and tasktracker commands, which start the JobTracker and TaskTracker, are no longer supported because these services no longer exist. They are replaced with yarn resourcemanager and yarn nodemanager, which start the ResourceManager and NodeManager respectively. hadoop mradmin is no longer supported; instead, use yarn rmadmin. The new admin commands mimic the functionality of the old ones, allowing nodes, queues, and ACLs to be refreshed while the ResourceManager is running. Security The following section outlines the additional changes needed to migrate a secure cluster. New YARN Kerberos service principals should be created for the ResourceManager and NodeManager, using the pattern used for other Hadoop services (yarn@). The mapred principal should still be used for the JobHistoryServer. If you’re using Cloudera Manager to configure security, that will be taken care of automatically. As in MR1, a configuration must be set to have the user that submits a job own its task processes. The equivalent of MR1�s LinuxTaskController is the LinuxContainerExecutor. In a secure setup, NodeManager configurations should set yarn.nodemanager.container-executor.class to org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor. Properties set in the taskcontroller.cfg configuration file should be migrated to their analogous properties in the container-executor.cfg file. In secure setups, configuring hadoop-policy.xml allows administrators to set up access control lists on internal protocols. The following is a table of MR1 options and their MR2 equivalents: MR1 MR2 Comments security.task.umbilical.protocol.acl security.job.task.protocol.acl As in MR1, this should never be set to anything other than * security.inter.tracker.protocol.acl security.resourcetracker.protocol.acl   security.job.submission.protocol.acl security.applicationclient.protocol.acl   security.admin.operations.protocol.acl security.resourcemanager-administration.protocol.acl     security.applicationmaster.protocol.acl No MR1 equivalent   security.containermanagement.protocol.acl No MR1 equivalent   security.resourcelocalizer.protocol.acl No MR1 equivalent   security.job.client.protocol.acl No MR1 equivalent   Queue access control lists (ACLs) are now placed in the Fair Scheduler configuration file instead of the JobTracker configuration.�A list of users and groups that can submit jobs to a queue can be placed in aclSubmitApps in the queue�s configuration.�The queue administration ACL is not supported in CDH 5 Beta 1, but will be in a future release. Ports The following is a list of default ports used by MR2 and YARN, as well as the configuration properties used to configure them. Port Use Property 8032 ResourceManager Client RPC yarn.resourcemanager.address 8030 ResourceManager Scheduler RPC (for ApplicationMasters) yarn.resourcemanager.scheduler.address 8033 ResourceManager Admin RPC yarn.resourcemanager.admin.address 8088 ResourceManager Web UI and REST APIs yarn.resourcemanager.webapp.address 8031 ResourceManager Resource Tracker RPC (for NodeManagers) yarn.resourcemanager.resource-tracker.address 8040 NodeManager Localizer RPC yarn.nodemanager.localizer.address 8042 NodeManager Web UI and REST APIs yarn.nodemanager.webapp.address 10020 Job History RPC mapreduce.jobhistory.address 19888 Job History Web UI and REST APIs mapreduce.jobhistory.webapp.address 13562 Shuffle HTTP mapreduce.shuffle.port   High Availability (HA) YARN supports ResourceManager HA to make a YARN cluster highly-available; the underlying architecture of Active/Standby pair is similar to JobTracker HA in MR1. A major improvement over MR1 is: in YARN, the completed tasks of in-flight MapReduce jobs are not re-run on recovery after the ResourceManager is restarted or failed over. Further, the configuration and setup has also been simplified. The main differences are: Failover controller has been moved from a separate ZKFC daemon to be a part of the ResourceManager itself. So, there is no need to run an additional daemon. Clients, Applications, and NodeManagers do not require configuring a proxy-provider to talk to the active ResourceManager. Below is a table with HA-related configs used in MR1 and their equivalents in YARN: MR1 YARN/MR2 Comments mapred.jobtrackers. yarn.resourcemanager.ha.rm-ids   mapred.ha.jobtracker.id yarn.resourcemanager.ha.id Unlike in MR1, this must be configured in YARN. mapred.jobtracker… yarn.resourcemanager.. YARN/ MR2 has different RPC ports for different functionalities. Each port-related config must be suffixed with an id. Note that there is noin YARN. mapred.ha.jobtracker. rpc-address.. yarn.resourcemanager.ha.admin.address   mapred.ha.fencing.methods yarn.resourcemanager.ha.fencer Not required to be specified mapred.client.failover.* None Not required   yarn.resourcemanager.ha.enabled Enable HA mapred.jobtracker.restart.recover yarn.resourcemanager.recovery.enabled Enable recovery of jobs after failover   yarn.resourcemanager.store.class org.apache.hadoop.yarn.server. resourcemanager.recovery.ZKRMStateStore mapred.ha.automatic-failover.enabled yarn.resourcemanager.ha.auto-failover.enabled Enable automatic failover mapred.ha.zkfc.port yarn.resourcemanager.ha.auto-failover.port   mapred.job.tracker yarn.resourcemanager.cluster.id Cluster name   Upgrading an MR1 Installation with Cloudera Manager Cloudera Manager enables adding a YARN service in the same way that you would add any other Cloudera Manager-managed service.�No further steps are required. Manually Upgrading MR1 Installation The following packages are no longer used in MR2 and should be uninstalled: hadoop-0.20-mapreduce hadoop-0.20-mapreduce-jobtracker hadoop-0.20-mapreduce-tasktracker hadoop-0.20-mapreduce-zkfc hadoop-0.20-mapreduce-jobtrackerha The following additional packages must be installed: hadoop-yarn hadoop-mapreduce hadoop-mapreduce-historyserver hadoop-yarn-resourcemanager hadoop-yarn-nodemanager The next step is to look at all the service configs placed in mapred-site.xml and replace them with their corresponding YARN configs. Configs starting with �yarn� should be placed inside yarn-site.xml, not mapred-site.xml.�Refer to the “Resource Configuration” section above for best practices on how to convert TaskTracker slot capacities (mapred.tasktracker.map.tasks.maximum and mapred.tasktracker.reduce.tasks.maximum) to NodeManager resource capacities (yarn.nodemanager.resource.memory-mb and yarn.nodemanager.resource.cpu-vcores), as well as how to convert configs in the Fair Scheduler allocations file, fair-scheduler.xml. Finally, you can start the ResourceManager, and NodeManagers, and JobHistoryServer. Web UI In MR1, the JobTracker web UI served detailed information about the state of the cluster and the jobs currently and recently running on it.�It also contained the job history page, which served information from disk about older jobs. The MR2 web UI provides the same information structured in the same way, but has been revamped with a new look and feel.�The ResourceManager�s UI, which includes information about running applications and the state of the cluster, is now located by default at:8088. The job history UI is now located by default at:19888. You can search and view jobs there just as you could in MR1. Because the ResourceManager is meant to be agnostic to many of the concepts in MapReduce, it cannot host job information directly. Instead, it proxies to a web UI that can. If the job is running, this is the relevant MapReduce Application Master; if it has completed, this is the JobHistoryServer.�In this sense, the user experience is similar to that of MR1, but the information is coming from different places. Conclusion I hope you now have a good understanding of the differences between MR1 and MR2, as well as how to make a seamless transition to the latter. Although Cloudera Manager makes many of the necessary configurations transparent for you, for operators who prefere a manual approach, this post contains everything you need to know to migrate your cluster to YARN/MR2. Sandy Ryza is a Software Engineer at Cloudera and a Hadoop Committer.</snippet></document><document id="85"><title>Migrating to MapReduce 2 on YARN (For Users)</title><url>http://blog.cloudera.com/blog/2013/11/migrating-to-mapreduce-2-on-yarn-for-users/</url><snippet>In Apache Hadoop 2, YARN and MapReduce 2 (MR2) are long-needed upgrades for scheduling, resource management, and execution in Hadoop. At their core, the improvements separate cluster resource management capabilities from MapReduce-specific logic. They enable Hadoop to share resources dynamically between MapReduce and other parallel processing frameworks, such as Cloudera Impala; allow more sensible and finer-grained resource configuration for better cluster utilization; and permit Hadoop to scale to accommodate more and larger jobs. In this post, users of CDH (Cloudera’s distribution of Hadoop and related projects) who program MapReduce jobs will get a guide to the architectural and user-facing differences between MapReduce 1 (MR1) and MR2. (MR2 is the default processing framework in CDH 5, although MR1 will continue to be supported.) Operators/administrators can read a similar post designed for them here. Terminology and Architecture In Hadoop 2, MapReduce is split into two components: The cluster resource management capabilities have become YARN, while the MapReduce-specific capabilities remain MapReduce. In the former MR1 architecture, the cluster was managed by a service called the JobTracker. TaskTracker services lived on each node and would launch tasks on behalf of jobs. The JobTracker would serve information about completed jobs. In MR2, the functions of the JobTracker are divided into three services. The ResourceManager is a persistent YARN service that receives and runs applications (a MapReduce job is an application) on the cluster. It contains the scheduler, which, as in MR1, is pluggable. The MapReduce-specific capabilities of the JobTracker have moved into the MapReduce Application Master, one of which is started to manage each MapReduce job and terminated when the job completes. The JobTracker�s function of serving information about completed jobs has been moved to the JobHistoryServer. The TaskTracker has been replaced with the NodeManager, a YARN service that manages resources and deployment on a node. NodeManager is responsible for launching containers, each of which can house a map or reduce task. MR2 architecture is illustrated below. The new architecture has a couple advantages.�First, by breaking up the JobTracker into a few different services, it avoids many of the scaling issues facing MR1.�Most important, it makes it possible to run frameworks other than MapReduce on a Hadoop cluster. For example, Impala can also run on YARN and share resources on a cluster with MapReduce. Writing and Running Jobs Virtually every job compiled against MR1 in CDH 4 will be able to run without any modifications on an MR2 cluster.�We won�t pay much attention here to jobs written/compiled against CDH 3 or Apache�s Hadoop 1 releases, but in general they will require a recompile to run on CDH 5, just as they do for CDH 4. Java API Compatibility from CDH 4 MR2 supports both the old (�mapred�) and new (�mapreduce�) MapReduce APIs used for MR1, with a few caveats. The difference between the old and new APIs, which concerns user-facing changes, should not be confused with the difference between MR1 and MR2, which concerns changes to the underlying framework. CDH 4 and CDH 5 support the new and old MapReduce APIs as well as both MR1 and MR2. (Now, go back and read this paragraph again, because the naming is often a source of confusion.) Most applications that use @Public/@Stable APIs will be binary-compatible from CDH 4, meaning that compiled binaries should be able to run without modification on the new framework. Source compatibility may be broken for applications that use a few obscure APIs that are technically public, but rarely needed and primarily exist for internal use. These APIs are detailed below.   Binary Incompatibilities Source Incompatibilities CDH 4 MR1 -&gt; CDH 5 MR1 None None CDH 4 MR1 -&gt; CDH 5 MR2 None Rare CDH 5 MR1 -&gt; CDH 5 MR2 None Rare “Source incompatibility” means that code changes will be required to compile.�Source incompatibility is orthogonal to binary compatibility — binaries for an application that is binary-compatible, but not source-compatible, will continue to run fine on the new framework, but code changes will be required to regenerate those binaries. The following are the known source incompatibilities. KeyValueLineRecordReader#getProgress and LineRecordReader#getProgress now throw IOExceptions in both the old and new APIs. Their superclass method, RecordReader#getProgress, already did this, but source compatibility will be broken for the rare code that used it without a try/catch block. FileOutputCommitter#abortTask now throws an IOException.�Its superclass method always did this, but source compatibility will be broken for the rare code that used it without a try/catch block. This was fixed in CDH 4.3 MR1 to be compatible with MR2. Job#getDependentJobs, an API marked @Evolving, now returns a List instead of an ArrayList. Compiling Jobs Against MR2 If you’re using Maven, compiling against MR2 requires including the same artifact, hadoop-client. Changing the version to Hadoop 2 version (for example, using 2.2.0-cdh5.0.0 instead of 2.2.0-mr1-cdh5.0.0) should be enough. If you’re not using Maven, compiling against all the Hadoop jars is recommended.�A comprehensive list of Hadoop Maven artifacts is available at here. Job Configuration As in MR1, job configuration options can be specified on the command line, in Java code, or in the mapred-site.xml on the client machine in the same way they previously were.�Most job configuration options, with rare exceptions, that were available in MR1 work in MR2 as well.�For consistency and clarity, many options have been given new names. The older names are deprecated, but will still work for the time being.� The exceptions are mapred.child.ulimit and all options relating to JVM reuse, which are no longer supported. Submitting and Monitoring Jobs with the Command Line The MapReduce command-line interface remains entirely compatible.Use of the hadoop command-line tool to run MapReduce-related commands (pipes, job, queue, classpath, historyserver, distcp, archive) is deprecated, but still works.�The mapred command-line tool is preferred for these commands. Requesting Resources An MR2 job submission includes the amount of resources to reserve for each map and reduce task. As in MR1, the amount of memory requested is controlled by the mapreduce.map.memory.mb and mapreduce.reduce.memory.mb properties. MR2 also adds additional parameters that control how much processing power to reserve for each task. The mapreduce.map.cpu.vcores and mapreduce.reduce.cpu.vcores properties express how much parallelism a map or reduce task can utilize. These should be kept at their default of 1 unless your code is explicitly spawning extra compute-intensive threads. Web UI In MR1, the JobTracker web UI served detailed information about the state of the cluster and the jobs currently and recently running on it.�It also contained the job history page, which served information from disk about older jobs. The MR2 web UI provides the same information structured in the same way, but has been revamped with a new look and feel. The ResourceManager UI, which includes information about running applications and the state of the cluster, is now located by default at:8088. The job history UI is now located by default at:19888.�Jobs can be searched and viewed there just as they could in MR1. Because the ResourceManager is meant to be agnostic to many of the concepts in MapReduce, it cannot host job information directly. Instead, it proxies to a web UI that can. If the job is running, this is the relevant MapReduce Application Master; if it has completed, this is the JobHistoryServer.�In this sense, the user experience is similar to that of MR1, but the information is coming from different places. Conclusion I hope you now have a good understanding of the differences between MR1 and MR2, as well as how to make a seamless transition to the latter. As you can see, the fact that both APIs, as well as both frameworks, are supported across CDH 4 and CDH 5 means that MapReduce programmers can move forward to MR2 with very few concerns. Sandy Ryza is a Software Engineer at Cloudera and a Hadoop Committer.</snippet></document><document id="86"><title>Sqooping Data with Hue</title><url>http://blog.cloudera.com/blog/2013/11/sqooping-data-with-hue/</url><snippet>Hue, the open source Web UI that makes Apache Hadoop easier to use, has a brand-new application that enables transferring data between relational databases and Hadoop. This new application is driven by Apache Sqoop 2�and has several user experience improvements, to boot. Sqoop is a batch data migration tool for transferring data between traditional databases and Hadoop. The first version of Sqoop is a heavy client that drives and oversees data transfer via MapReduce. In Sqoop 2, the majority of the work was moved to a server that a thin client communicates with. Also, any client can communicate with the Sqoop 2 server over its JSON-REST protocol. Sqoop 2 was chosen instead of its predecessors because of its client-server design. Importing from MySQL to HDFS The following is the canonical import job example sourced from�http://sqoop.apache.org/docs/1.99.2/Sqoop5MinutesDemo.html. In Hue, this can be done in three easy steps. Environment CDH 4.4 or CDH 5 Beta Hue 3.0.0 MySQL 5.1 1. Create a Connection In the Sqoop app, the connection manager is available from the �New Job� wizard. To get to the new job wizard, click�New Job. There may be a list of connections available if a few have been created before. For the purposes of this demo, we�ll go through the process of creating a new connection. Click�Add a new connection and fill in the blanks with the data below. Then click Save to return to the New Job wizard! Connection Parameter Value Name mysql-connection-demo JDBC Driver Class com.mysql.jdbc.Driver JDBC Connection String jdbc:mysql://hue-demo/demo Username demo Password demo � 2. Create a Job After creating a connection, follow the wizard and fill in the blanks with the information below. Job Wizard Parameter Value Name mysql-import-job-demo Type IMPORT Connection mysql-connection-demo Table name test Storage Type HDFS Output format TEXT_FILE Output directory /tmp/mysql-import-job-demo � 3. Save and Submit the Job At the end of the Job wizard, click Save and Run! The job will auto-magically start and the job dashboard will be displayed. As the job is running, a progress bar below the job listing will be dynamically updated. Links to the HDFS output via the File Browser and Map Reduce logs via Job Browser will be available on the left hand side of the job edit page. Conclusion The new Sqoop application enables batch data migration from a more traditional databases to Hadoop and vice versa through Hue. Using Hue, a user can move data between storage systems in a distributed fashion with the click of a button. I�d like to send out a big thank you to the Sqoop community for the new client-server design! Both projects are undergoing heavy development and are welcoming external contributions. Have any suggestions? Feel free to tell us what you think through hue-user or our community forums. Abraham Elmahrek is a Software Engineer at Cloudera, working on the Hue team.</snippet></document><document id="87"><title>Cloudera Slides and Video from Strata + Hadoop World 2013</title><url>http://blog.cloudera.com/blog/2013/11/cloudera-slides-and-video-from-strata-hadoop-world-2013/</url><snippet>In the wake of the Strata + Hadoop World 2013 afterglow, speaker slides and video have been posted. For your convenience, they are aggregated below: Keynotes Hadoop’s Impact on the Future of Data Management (Mike Olson):�Video The Future of Hadoop: What Happened &amp; What’s Possible? (Doug Cutting):�Video Sessions From Promise to a Platform: Next Steps in Bringing Workload Diversity to Hadoop (Henry Robinson): Slides Practical Performance Analysis and Tuning for Cloudera Impala�(Greg Rahn):�Slides How to Leverage Mainframe Data with Hadoop: Bridging the Gap Between Big Iron &amp; Big Data (Matt Brandwein – co-presenter): Slides Parquet: An Open Columnar Storage for Hadoop (Nong Li – co-presenter): Slides Securing the Apache Hadoop Ecosystem (Aaron Myers, Shreepadma Venugopalan): Slides Trickery and Tooling for Distributed System Diagnosis and Debugging (Philip Zeyliger): Slides Unifying Your Data Management Platform with Hadoop: Batch and Real-time Machine Data Ingest, Alerts, and Analytics (Jayant Shekhar): Slides What�s Next for Apache HBase: Multi-tenancy, Predictability, and Extensions (Jon Hsieh): Slides Working with Geospatial Data Using Hadoop and HBase and How Monsanto Used It to Help Farmers Increase Their Yield (Amandeep Khurana – co-presenter): Slides If you were an attendee, thanks for coming! If not, we hope to see you next year at the biggest conference yet! Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="88"><title>Email Indexing Using Cloudera Search and HBase</title><url>http://blog.cloudera.com/blog/2013/11/email-indexing-using-cloudera-search-and-hbase/</url><snippet>In my previous post you learned how to index email messages in batch mode, and in near real time, using Apache Flume with MorphlineSolrSink. In this post, you will learn how to index emails using Cloudera Search with Apache HBase and Lily HBase Indexer, maintained by NGDATA and Cloudera. (If you have not read the previous post, I recommend you do so for background before reading on.) Which near-real-time method to choose, HBase Indexer or Flume MorphlineSolrSink, will depend entirely on your use case, but below are some things to consider when making that decision: Is HBase an optimal storage medium for the given use case? Is the data already ingested into HBase? Is there any access pattern that will require the files to be stored in a format other than HFiles? If HBase is not currently running, will there be enough hardware resources to bring it up? There are two ways to configure Cloudera Search to index documents stored in HBase: to alter the configuration files directly and start Lily HBase Indexer manually or as a service, or to configure everything using Cloudera Manager. This post will focus on the latter, because it is by far the easiest way to enable Search on HBase — or any other service on CDH, for that matter. Understanding HBase Replication and Lily HBase Indexer When designing this solution, Cloudera identified four major requirements to make HBase indexing effective: Indexing latency must be in near-real-time (seconds) and tunable The Solr Index must eventually be consistent with the HBase table while inserts, updates, and deletes are applied to HBase The indexing mechanism must be scalable and fault tolerant The indexing process cannot slow down HBase writes To meet these requirements, Cloudera Search uses HBase�s native replication mechanism. For those unfamiliar with HBase replication, here is a brief and very high-level summary: As updates are applied to the write-ahead-log (WAL), HBase RegionServer listens to these updates on a separate thread. When that thread�s buffer is filled or it hits the end of the file, it sends the batches with all the replicated updates to a peer RegionServer running on a different cluster. The WAL, therefore, is essential for indexing to work. � Cloudera Search uses the HBase replication mechanism, which listens for HBase row mutation events and, instead of sending updates to a different RegionServer, sends them to Lily HBase Indexer. In turn, Lily HBase Indexer applies Cloudera Morphlines transformation logic, breaking up the events into Solr fields and forwarding them into Apache Solr Server. There are major advantages to using HBase replication versus implementing the same functionality in HBase coprocessors. First, replication works in parallel and asynchronously with the data being ingested into HBase. Therefore, Cloudera Search indexing does not add any latency or operational instability to routine HBase operation. Second, using the replication method allows for seamless on-the-fly changes to transformation logic. Conversely, to effect a change through coprocessor modification requires a RegionServer restart, which would make data unavailable to HBase users. Perhaps most important is that implementing coprocessors is fairly intrusive and, if not tested properly, can disrupt HBase performance. This flow is illustrated below: Installing Cloudera Search and Deploying Lily HBase Indexer Cloudera Manager downloads and deploys Cloudera Search as a single package automatically. All you have to do is to click the �Packages� icon in the top nav, choose the Solr version, and download, distribute, and activate it: As mentioned previously, Cloudera Search depends on HBase replication, and, therefore, that will be enabled next. Activate replication by clicking HBase Service-&gt;Configuration-&gt;Backupand ensuring �Enable HBase Replication� and �Enable Indexing� are both checked. If necessary, save the changes and restart the HBase service. To add Lily HBase Indexer, go to Services-&gt;Add Service, choose �Keystore Indexer�, and add it, pointing it to the HBase instance that will be used for email processing: Configuring Solr Next, configure Solr exactly as described in the previous post here. Generate a sample schema.xml configuration file: 
$ solrctl --zk localhost:2181/solr
instancedir --generate $HOME/emailSearchConfig
   Edit the schema.xml file in $HOME/emailSearchConfig, with the config file that will define fields relevant to email processing. A full copy of the file can be found at this link. Upload the Solr configurations to ZooKeeper: 
$ solrctl --zk localhost:2181/solr instancedir
--create email_collection $HOME/emailSearchConfig
   Generate the Solr collection: 
$ solrctl --zk localhost:2181/solr collection
--create email_collection -s 1
     Registering the Indexer This step is needed to add and configure the indexer and HBase replication. The command below will update ZooKeeper and add myindexer as a replication peer for HBase. It will also insert configurations into ZooKeeper, which Lily HBase Indexer will use to point to the right collection in Solr. 
$ hbase-indexer add-indexer -n myindexer -c indexer-config.xml
       -cp solr.zk=localhost:2181/solr
       -cp solr.collection=collection1
   � Arguments: -n myindexer�- specifies the name of the indexer that will be registered in ZooKeeper -c indexer-config.xml – configuration file that will specify indexer behavior -cp solr.zk=localhost:2181/solr �- specifies the location of ZooKeeper and Solr config.� This should be updated with the environment specific location of ZooKeeper. -cp solr.collection=collection1 – specifies which collection to update.� Recall the Solr Configuration step where we created collection1. � The index-config.xml file is relatively straightforward in this case; all it does is specify to the indexer which table to look at, the class that will be used as a mapper (com.ngdata.hbaseindexer.morphline.MorphlineResultToSolrMapper), and the location of the Morphline configuration file. The mapping-type is set to column because we want to get each cell as an individual Solr document. By default mapping-type is set to row, in which case the Solr document becomes the full row. Param name=�morphlineFile� specifies the location of the Morphlines configuration file. The location could be an absolute path of your Morphlines file, but since you are using Cloudera Manager, specify the relative path: �morphlines.conf�. 
&lt;indexer table="inbox" mapper="com.ngdata.hbaseindexer.morphline.MorphlineResultToSolrMapper" mapping-type="column" &gt;
   &lt;!-- The relative or absolute path on the local file system to the morphline configuration file. --&gt;
   &lt;!-- Use relative path "morphlines.conf" for morphlines maaged by Cloudera Manager --&gt;
&lt;param name="morphlineFile" value="morphlines.conf"/&gt;

   &lt;!-- The optional morphlineId identifies a morphline if there are multiple morphlines in morphlines.conf --&gt;
   &lt;!-- --&gt;
&lt;/indexer&gt;
   The contents of the hbase-indexer configuration file can be found at this link. For the full-reference of hbase-indexer command, it is sufficient to execute the command without any arguments: 
$ hbase-indexer

Usage: hbase-indexer &lt;command&gt;
where &lt;command&gt; an option from one of these categories:

TOOLS
  add-indexer
  update-indexer
  delete-indexer
  list-indexers

PROCESS MANAGEMENT
  server           run the HBase Indexer server node

REPLICATION (EVENT PROCESSING) TOOLS
  replication-status
  replication-wait

PACKAGE MANAGEMENT
  classpath        dump hbase CLASSPATH
  version          print the version

 or
  CLASSNAME        run the class named CLASSNAME
Most commands print help when invoked w/o parameters.
   Configuring and Starting Lily HBase Indexer If you recall, when you added Lily HBase Indexer, you specified the instance of HBase with which it�s associated. Therefore, you do not need to do that in this step. You do, however, need to specify the Morphlines transformation logic that will allow this indexer to parse email messages and extract all the relevant fields. Go to Services and choose Lily HBase Indexer that you added previously. Select Configurations-&gt;View and Edit-&gt;Service-Wide-&gt;Morphlines. Copy and paste the morphlines file. The email morphlines library will perform the following actions: 1.���� Read the HBase email events with the extractHBaseCells command 2.���� Break up the unstructured text into fields with the grok command 3.���� If Message-ID is missing from the email, generate it with the generateUUID command 4.���� Convert the date/timestamp into a field that Solr will understand, with the convertTimestamp command 5.���� Drop all of the extra fields that we did not specify in schema.xml, with the sanitizeUknownSolrFieldscommand � The extractHBaseCells command deserves more attention, as it is the only thing different about the HBase Indexer�s morphlines configuration. The parameters are: inputColumn – specifies columns to which to subscribe (can be wild card) outputFied – the name of the field where the data is sent type – the type of the field (it is string in the case of email body) source – could be value or qualified; value specifies that the cell value should be indexed 
extractHBaseCells {
       mappings : [
        {
          inputColumn : "messages:*"
          outputField : "message"
          type : string
          source : value
          }
        ]
      }
   Download a copy of this morphlines file from here. � One important note is that the id field will be automatically generated by Lily HBase Indexer. That setting is configurable in the index-config.xml file above by specifying the unique-key-field attribute.� It is a best practice to leave the default name of id — as it was not specified in the xml file above, the default id field was generated and will be a combination of RowID-Column Family-Column Name. At this point save the changes and start Lily HBase Indexer from Cloudera Manager. Setting Up the Inbox Table in HBase There are many ways to create the table in HBase programmatically (Java API, REST API, or a similar method). Here you will use the HBase shell to create the inbox table (intentionally using a descriptive column family name to make things easier to follow). In production applications, the family name should always be short, since it is always stored with every value as a part of a cell key. The following command will do that and enable replication on a column family called �messages�: 
hbase(main):003:0&gt;  create 'inbox', {NAME =&gt; 'messages', REPLICATION_SCOPE =&gt; 1}
   To check that the table was created properly run the following command: 
hbase(main):003:0&gt; describe 'inbox'
DESCRIPTION                                                                ENABLED
 {NAME =&gt; 'inbox', FAMILIES =&gt; [{NAME =&gt; 'messages', DATA_BLOCK_ENCODING =&gt; ' true
 NONE', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '1', VERSIONS =&gt; '3',
 COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', TTL =&gt; '2147483647', KEEP_DEL
 ETED_CELLS =&gt; 'false', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', ENCODE
 _ON_DISK =&gt; 'true', BLOCKCACHE =&gt; 'true'}]}
   From this point any email put into table �inbox� in column family �messages� will trigger an event to Lily HBase Indexer, which will process the event, break it up into fields, and send it to Solr for indexing. � The schema of inbox table is simple: The row ID is the name of the person to whom this inbox belongs. Each cell is an individual message with the column being a unique integer ID. Below is snapshot of a sample table as displayed by Hue�s HBase interface: Accessing the Data You have the choice of many visual tools to access the indexed emails. Hue and Solr GUI are both very good options. HBase also enables a number of access techniques, not only from a GUI but also via the HBase shell, API, and even simple scripting techniques. Integration with Solr gives you great flexibility and can also provide very simple as well as advanced searching options for your data. For example, configuring the Solr schema.xml file such that all fields within the email object are stored in Solr allows users to access full message bodies via a simple search, with the trade-off of storage space and compute complexity. Alternatively, you can configure Solr to store only a limited number of fields, such as the id, sender, and subject.� With these elements, users can quickly search Solr and retrieve the message ID(s) which in turn can be used to retrieve the full message from HBase itself. The example below stores only the message ID in Solr but indexes on all fields within the email object. Searching Solr in this scenario retrieves email IDs, which you can then use to query HBase. This type of setup is ideal for Solr as it keeps storage costs low and takes full advantage of Solr�s indexing capabilities. The shell script below issues a query to the Solr Rest API for a keyword �productId� and returns the field �id� in CSV format.� The result is a list of document IDs that match the query. The script then loops through the ids and breaks them up into Row Id, Column Family, and Column Name, which are used to access HBase through the standard HBase REST API. 
#!/bin/bash

#  Query SOLR and return the id field for every document
#  that contains the word resign
query_resp=$(curl -s 'http://spark:8983/solr/collection1_shard1_replica1/select?q=productId&amp;amp;fl=id&amp;amp;wt=csv')

# Loop through results of the previous command,
# and use the id to retrieve the cells from HBase via the HBase REST API
for i in  $query_resp
do
            if [ "$i" != "id" ]; then
            cmd=$(echo $i |awk -F'-' '{print "curl -s http://spark:20550/inbox/" $1 "/" $2 ":"  $3}')
            $cmd -H "Accept: application/x-protobuf "
            fi
done
   Conclusion In this post you have seen how easy it is to index emails that are stored in HBase — in near real time and completely non-intrusively to the main HBase flow. In summary, keep these main steps in mind: Enable replication in HBase Properly configure Lily HBase Indexer Use Morphlines in Lily HBase Indexer to help with transformations (no coding required!) If you have had the opportunity to read the previous post, you can see that the morphlines.conf file is practically identical in all three cases. This means that it is very easy to grow the search use cases over the Hadoop ecosystem. If the data is already in HDFS, use MapReduceIndexerTool to index it. If the data is arriving through Flume, use SolrMorphlineSink with an identical morphlines file. If later you decide HBase fits the use case, only a minimal change is required to start indexing cells in HBase: Just add the extractHBaseCells command to the morphlines file.� Although this example concentrates on emails as a use case, this method can be applied in many other scenarios where HBase is used as a storage and access layer. If your enterprise uses HBase already for a specific use case, consider implementing Cloudera Search on top of it.� It requires no coding and can really open up the data to a much wider audience in the organization. Jeff Shmain is a solutions architect at Cloudera.</snippet></document><document id="89"><title>Cascading, Spring, and Spark: Development Choices for CDH Users Expand</title><url>http://blog.cloudera.com/blog/2013/11/cascading-spring-and-spark-development-choices-for-cdh-users-expand/</url><snippet>In software development, there is no substitute for having choices. Furthermore, freedom of choice – between frameworks, APIs, and languages — is a major fuel source for platform adoption across any successful ecosystem. In the case of development on CDH, the open source core of Cloudera�s Big Data platform containing Apache Hadoop and related ecosystem projects, the choices have expanded dramatically in the past three weeks: Spark + CDH Cloudera has announced direct support for Apache Spark (incubating) with CDH. Spark, the in-memory data processing framework designed at UC Berkeley�s AMPLab that complements MapReduce for analytic workloads (and runs on top of HDFS), is well known by developers for its highly consumable APIs – particularly for Java, Scala, and Python. (This support is occurring through the Cloudera Connect: Innovators program, in which Databricks, which is commercializing Spark, is the first partner.) So, for those exploring Spark-based in-memory processing for certain workloads (more to come on that in future posts), the range of development options is rich. Cascading + CDH Cloudera has certified�Cascading 2.2, the popular open source Java-based framework for building data pipelines in Hadoop, with CDH4. Thus community-developed Cascading offshoots for JVM languages like Scala, (Scalding), Clojure (Cascalog) and Groovy (cascading.groovy) should work with CDH4 as well. Spring + CDH Spring for Apache Hadoop, which is bundled inside the ubiquitous open source Spring IO framework for enterprise Java development, is also now certified�for use with CDH4 – making CDH4/Hadoop development accessible by the massive and mainstream Spring community through a single, high-level API. These new options are in addition to familiar ones like Apache Crunch, the Cloudera Development Kit (CDK), and Hadoop�s native APIs – and of course, none of them are necessarily mutually exclusive, depending on the use case involved (although developers do tend to stick with their favorite toys). It�s clear: Making CDH your platform for Hadoop application development gives you the flexibility to choose the right framework/API for the job and for your skill set or personal proclivity. That�s what a platform should do. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="90"><title>Tips for Debugging Distributed Systems</title><url>http://blog.cloudera.com/blog/2013/11/tips-for-debugging-distributed-systems/</url><snippet>Among Cloudera’s engineer-presenters at Strata + Hadoop World 2013 this week, Philip Zeyliger (“Tricks for Distributed System Debugging and Diagnosis“) was particularly fortunate to have been interviewed by O’Reilly Media editor Meghan Blanchette on camera. In the following 8-minute interview, Philip�offers an overview of common pain points and failures when debugging distributed systems: And for more detail, you can view his presentation slides here. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="91"><title>Strata + Hadoop World 2013 in Pictures</title><url>http://blog.cloudera.com/blog/2013/10/strata-hadoop-world-2013-in-pictures/</url><snippet>For those of you attending virtually/in spirit, I thought it would be nice to bring you a selection of photos from the week so far. Credit goes to Alex Moundalexis (@technmsg) for the majority of these shots. Kate Ting, Apache Sqoop cookbook co-chef. Julien Le Dem and Nong Li present on Parquet and Impala. Aaron Myers looks relaxed.� The Definitive Handshake with Tom White.� Eric Sammer being Sammer.� I’ll drink to that, Joey Echeverria.� Keynotes are for early risers.� Mike Olson: Enterprise Data Hub is the thing.� Greg Rahn does Impala + Parquet meetup in style.� Lobby Clouderans. � The CDK is your ticket.�</snippet></document><document id="92"><title>How-to: Use MADlib Pre-built Analytic Functions with Impala</title><url>http://blog.cloudera.com/blog/2013/10/how-to-use-madlib-pre-built-analytic-functions-with-impala/</url><snippet>Thanks to Victor Bittorf, a visiting graduate computer science student at Stanford University, for the guest post below about how to use the new prebuilt analytic functions for Cloudera Impala. Cloudera Impala is an exciting project that unlocks interactive queries and SQL analytics on big data. Over the past few months I have been working with the Impala team to extend Impala�s analytic capabilities. Today I am happy to announce the availability of pre-built mathematical and statistical algorithms for the Impala community under a free open-source license. These pre-built algorithms combine recent theoretical techniques for shared nothing parallelization for analytics and the new user-defined aggregations (UDA) framework in Impala 1.2 in order to achieve big data scalability. This initial release has support for logistic regression, support vector machines (SVMs), and linear regression. Having recently completed my masters degree while working in the database systems group at University of Madison Wisconsin, I’m excited to work with the Impala team on this project while I continue my research as a visiting student at Stanford. I’m going to go through some details about what we’ve implemented and how to use it. As interest in data analytics increases, there is growing demand for deploying analytic algorithms in enterprise systems. One approach that has received much attention from researchers, engineers and data scientists is the integration of statistical data analysis into databases. One example of this is MADlib, which leverages the data-processing capabilities of an RDBMS to analyze data. Via model averaging, each Impala node can train a model using only the data that already resides there. UDAs have emerged as a popular way to implement analytic algorithms in an RDBMS. Many popular approaches to data analysis, such as stochastic gradient descent (SGD), train analytic models in an iterative way, which fits the UDA pattern of initialize, update, and finalize. This style of analytics was used by Bismarck as a way to unify in-RDBMS analytics. These kinds of algorithms incrementally improve a model by processing one tuple at a time while scanning over the database. Since Impala allows us to push computation to the data, we employ recent academic work to distribute our computation. By using model averaging, each node in Impala can train a model using only the data that already resides on that node — shared-nothing analytics. Once all of the nodes have finished training their models, all of the models are collected and averaged together. These insights easily enable Impala to train and evaluate analytic models over data stored in Hadoop. This package uses UDAs and UDFs when training and evaluating analytic models. While all of these tasks can be done in pure SQL using the Impala shell, we’ve put together some front-end scripts to streamline the process. The source code for the UDAs, UDFs, and scripts are all on GitHub. We’ll go through some examples of data analytics using Cloudera’s Impala to demonstrate just how easy it is to use. First, I’ll explain how to interact with arrays and models using the Impala shell. Then, you’ll learn how to use simple command-line scripts to model classification tasks. This is a quick and easy way to get started with analytics in Impala. The wiki on GitHub has detailed documentation for developers. Arrays Arrays of doubles are stored as byte strings in Impala. The UDAs and UDFs for training and evaluating models operate on the binary representation of the arrays. For convenience, we’ve put together some UDFs to convert arrays to and from ASCII printable text. Constructing arrays is easy: 
&gt; SELECT printarray(toarray(1, 2, 3));
  +--------------------------------------------------+
  | &lt;1, 2, 3&gt;                                        |
  +--------------------------------------------------+
   The UDF toarray will create a byte string that represents the array. The UDF printarray converts this byte string to a human-readable string. While Impala supports storage of binary strings, it can be useful to ASCII armor these arrays when working with them in a shell: 
&gt; SELECT encodearray(toarray(1, 2, 3));
  +---------------------------------------------------+
  | aaaaaipdaaaaaaaeaaaaaeae                          |
  +---------------------------------------------------+
   This encoding of an array can be easily converted back to a binary encoding: 
&gt; SELECT printarray(decodearray('aaaaaipdaaaaaaaeaaaaaeae'));
  +--------------------------------------------------------------+
  | &lt;1, 2, 3&gt;                                                    |
  +--------------------------------------------------------------+
   This function lets you easily manage arrays using the Impala shell. Classification with SVM and Logistic Regression Classification is a machine-learning task with many applications. Suppose a hospital is interested in preventing heart attacks. The hospital has a record of patients including risk factors such as anxiety and high blood pressure and whether they have had a heart attack. Classification techniques like SVMs and logistic regression can be used to predict the chance of a new patient having a heart attack based on these risk factors. In this case, the training data would be a feature array of the risk factors and a Boolean label indicating if a patient with those risk factors has had a heart attack. After training the SVM, you can use the resulting model to predict if a new patient is at risk. While a detailed explanation can be helpful (see links below), it is easy to get started. - en.wikipedia.org/wiki/Support_vector_machine – en.wikipedia.org/wiki/Logistic_regression Suppose you have a table: 
CREATE TABLE info (lbl boolean, e0 double, e1 double);
   Populate it with some feature vectors and labels: 
INSERT INTO info
  VALUES (True, -3.33139160117, 2.16414460145), (True, -1.18726873679, 0.295350800223), (False, 3.60399430867, 1.45138655128);
   By executing a front-end Python script, you can use Impala to train an SVM with this data: 
python python/impala_svm.py lbl e0 e1 --db toysvm --table info -y mymodel -e 3
   Using the Impala shell, you can see that a table called mymodel has been created and populated with an SVM model: 
&gt; SELECT iter, printarray(decodearray(model)) FROM mymodel;
  +------+--------------------------------+
  | iter | printarray(decodearray(model)) |
  +------+--------------------------------+
  | 3    | &lt;-0.699068, -0.0608898&gt;        |
  | 2    | &lt;-0.591917, -0.0875452&gt;        |
  | 0    | &lt;&gt;                             |
  | 1    | &lt;-0.479126, -0.115604&gt;         |
  +------+--------------------------------+
   By passing ‘-e 3′ to the script, you instructed it to run for three iterations over the table to train the SVM. The results of the iterations are stored in this table. Notice that the first iteration is NULL, meaning you started with an initialized model (which defaults to all zeros). Using the model from the third iteration (which encodes as ”nbgpcdplpjhgjhnl’), you can see how well our SVM did at learning the training examples: &gt; SELECT lbl, svmpredict(decodearray('nbgpcdplpjhgjhnl'), toarray(e0, e1)) FROM info;
  +-------+--------------------------------------------------------------+
  | lbl   | svmpredict(decodearray('nbgpcdplpjhgjhnl'), toarray(e0, e1)) |
  +-------+--------------------------------------------------------------+
  | true  | true                                                         |
  | true  | true                                                         |
  | false | false                                                        |
  +-------+--------------------------------------------------------------+
   The SVM successfully predicts all the training examples. The Python script used to train the SVM, python/impala_svm.py, has options for setting the step size, step decay, and regularizer. These parameters can vary depending on the dataset. A common approach is to use a grid search to try multiple values in parallel to determine which ones work the best. Logistic regression is supported and is as easy to use as an SVM. There is a convenient front end called impala_logr.py and there is a UDF called logrpredict, which is analogous to svmpredict. Linear Regression This release also has a port of linear regression from MADlib. Since this is not an iterative algorithm, you can run it easily from the SQL prompt: &gt; SELECT printarray(linr(toarray(x, y), z)) from lrt;
  +---------------------------------------------------------+
  | toysvm.printarray(toysvm.linr(toysvm.toarray(x, y), z)) |
  +---------------------------------------------------------+
  | &lt;0.6, 1.2&gt;                                              |
  +---------------------------------------------------------+
   This models the relationship between (x, y) and z using MADlib’s linear regression module. You can use this linear regression model in prediction tasks: 
&gt; select x, y, z, linrpredict(decodearray('kjjjjbpdkjjjjjpd'), toarray(x, y)) from lrt;
  +---+---+---+-----------------------------------------------------------+
  | x | y | z |linrpredict(decodearray('kjjjjbpdkjjjjjpd'),toarray(x, y)) |
  +---+---+---+-----------------------------------------------------------+
  | 1 | 2 | 3 | 3.00000011920929                                          |
  | 2 | 4 | 6 | 6.000000238418579                                         |
  | 1 | 2 | 3 | 3.00000011920929                                          |
  +---+---+---+-----------------------------------------------------------+
   Conclusion The above examples are just a few of the data analytic algorithms that can be implemented in Impala and RDBMSs. The first release of this package includes some of the most popular statistical models but many more can be implemented or ported from existing libraries. Further Reading “Towards a Unified Architecture for in-RDBMS Analytics” (SIGMOD 2012), by Xixuan Feng, Arun Kumar, Ben Recht and Christopher R� Victor Bittorf is a third-year graduate student studying computer science. As a member of the Hazy Research Group, he studies scalable data analytics under the direction of Christopher R�. Victor’s work focuses on understanding the data management challenges that arise in large-scale data analysis problems.</snippet></document><document id="93"><title>Cloudera Enterprise 5 Beta is Now Available for Download</title><url>http://blog.cloudera.com/blog/2013/10/cloudera-enterprise-5-beta-is-now-available-for-download/</url><snippet>We are pleased to announce the beta release of Cloudera Enterprise 5 (CDH 5 and Cloudera Manager 5). This release has both Cloudera Impala and Cloudera Search integrated into CDH. It also includes many new features and updated component versions including the ones below: Apache Hadoop Rebase on Hadoop 2.2 GA HDFS HDFS Snapshots HDFS Caching Preview MR MR2 is production ready MR2 Management by Cloudera Manager MR1 and MR2 are both supported Resource Management/YARN Ability to run MR and Impala with resource sharing Resource Management High Availability Cloudera Manager Platform coverage/support for CDH 5 Resource/Workload management Extensibility support for Partner applications Advanced Impala monitoring Apache HBase Significant reductions to Mean time to Recovery (MTTR) Rebase on HBase 0.95.2+ Impala Language extensibility with native UDFs and UDAF plus support for existing Hive UDFs Automatic and coordinated metadata refresh Preview of YARN-integrated resource management Support for HDFS caching Apache Hive Rebase to Hive 0.11 plus most of Hive 0.12 Hive Support for MR2 Cloudera Search Support for CDH 5 (MR1, MR2, HBase 0.95.2) Back-up and Disaster Recovery (BDR) HDFS Snapshots HBase Snapshots Support for MR2 Global replication page Apache Oozie Oozie HA Rebase on Oozie 4.0 Hue Sqoop App ZooKeeper App Hue 3.0 Apache Mahout Rebase on 0.8 As part of the open beta, we encourage the community to try it out. Here is how you can get started: Download Cloudera Enterprise from: cloudera.com/downloads install and try it out.� View the documentation: CDH 5 Beta 1 Documentation Cloudera Manager 5 Beta 1 Documentation Once you get started, we encourage you to provide feedback. We have the following mechanisms set up to do this: A beta specific community forum has been set up. Click here to join. Please use this to ask questions and provide feedback. File a bug through our public Jira. �You can reach it at: https://issues.cloudera.org/browse/DISTRO https://issues.cloudera.org/browse/CM We look forward to hearing about your experiences of Cloudera Enterprise 5 beta.</snippet></document><document id="94"><title>See You Next Week at Strata + Hadoop World 2013!</title><url>http://blog.cloudera.com/blog/2013/10/see-you-next-week-at-strata-hadoop-world-2013/</url><snippet>We are just a weekend away from the Biggest. Strata + Hadoop World. Ever. Not to make non-attendees feel bad — the show sold out a couple weeks ago — but some great things are in store for technical end-users (as well as everyone else): Hear keynotes from Mike Olson and Doug Cutting: Be prepared to hear two of the most influential voices in Big Data make some Big News.� Learn “How to Develop a Hadoop Data Application”�(via the CDK) from Tom White, Eric Sammer, and Joey Echeverria. No pressure! The first two people on that list literally wrote the books on Hadoop app development and Hadoop operations, respectively. Dive into multiple technical sessions�based on real-world use cases led by Cloudera software engineers and Cloudera customers. Because the best way to learn is from the do-ers. (There are copious interesting sessions from other people, as well.) Watch a demo, get a T-shirt. The Cloudera exhibit will be brimming over with demos, books, and schwag, including but not limited to the legendary “Data is the New Bacon” gear. Get your drink on. A pub crawl in the West Village? Who could resist? Meet-up! More than 10 community meetups are planned – covering Apache Accumulo, Apache Hadoop, Cloudera Impala, Apache Hive, Cloudera Manager, and more. We simply can’t wait for the show to start, and we can’t wait to see you when it does! Justin Kestelyn is Cloudera’s developer community outreach director.</snippet></document><document id="95"><title>Collection Aliasing: Near Real-Time Search for Really Big Data</title><url>http://blog.cloudera.com/blog/2013/10/collection-aliasing-near-real-time-search-for-really-big-data/</url><snippet>The rise of Big Data has been pushing search engines to handle ever-increasing amounts of data. While building Cloudera Search, one of the things we considered in Cloudera Engineering was how we would incorporate Apache Solr with Apache Hadoop in a way that would enable near-real-time indexing and searching on really big data. Eventually, we built Cloudera Search on Solr and Apache Lucene, both of which have been adding features at an ever-faster pace to aid in handling more and more data. However, there is no silver bullet for dealing with extremely large-scale data. A common answer in the world of search is �it depends,� and that answer applies in large-scale search as well. The right architecture for your use case depends on many things, and your choice will generally be guided by the requirements and resources for your particular project. We wanted to make sure that one simple scaling strategy that has been commonly used in the past for large amounts of time-series data would be fairly simple to set up with Cloudera Search. By �time-series data,� I mean logs, tweets, news articles, market data, and so on — data that is continuously being generated and is easily associated with a current timestamp. One of the keys to this strategy is a feature that Cloudera recently contributed to Solr: collection aliasing. The approach involves using collection aliases to juggle collections in a very scalable little �dance.� The architecture has some limitations, but for the right use cases, it�s an extremely scalable option. I also think there are some areas of the dance that we can still add value to, but you can already do quite a bit with the current functionality. Inside Collection Aliasing Collection aliasing allows you to setup a virtual collection that actually points to one or more real collections. For example, you could have an alias called �articles� that when queried actually searched �magazines� and �blogs�. Aliases meant for read operations can refer to one or more real collections. Aliases meant for updates should only map to one real collection. You may use a collection alias in any place that you would normally specify a real collection name. One of the main benefits to aliases is that you can use the aliases from any search client applications and then switch which actual collections those aliases refer to on the fly without requiring updates to the clients. Collection aliases are only available when using SolrCloud mode and are manipulated via a simple HTTP API: Create http://localhost:8983/solr/admin/collections?action=CREATEALIAS&amp;name=AliasName&amp;collection=ListOfCollections Delete http://localhost:8983/solr/admin/collections?action=DELETEALIAS&amp;name=AliasName List To be done: SOLR-4968 A Collection Composed of Collections One way to take advantage of Collections Aliases is to make a virtual collection out of multiple real SolrCloud collections. Each real collection will act as a logical shard of the full virtual collection. As you will be putting time-series data into this virtual collection, this approach allows you to treat the collection for different time ranges differently. It also allows you to control the size of the single collection that will be accepting new data and providing near-real-time search. A smaller near-real-time collection will perform better and independently of the total size of the virtual collection. Initially, there will be just one collection. You can use a variety of naming patterns for the real collection names. For illustrative purposes, I�ve simply chosen to use ColN as the collection names. Each of these collections should use the same set of configurations files – they are in fact, many collections acting as a single collection, so this makes sense. Using a single set of config files will ensure you can search across the collections and lets you update settings for the virtual collection in a single place. The first collection is the one that will collect all the incoming time series data. You will likely be using something like Apache Flume with Cloudera Morphlines to load in the data, but the following technique is not specific to any particular data loading method. You will want at least two aliases to control access to your large virtual collection: an update alias and a read alias. Note: This technique is meant for cases where the data is coming in live – not for bulk loading existing data. You can bulk load and use this architecture with further effort, but it is beyond the scope of this post. Update Alias Create alias TSCUpdate -&gt; Col1 http://localhost:8983/solr/admin/collections?action=CREATEALIAS&amp;name=TSCUpdate&amp;collection=Col1 Whatever the manner you are loading in the data, you will want to create a single update alias to use as the configured collection name to send updates to. In the diagram below, I have chosen TSCUpdate as the name of the update alias. You can use this update alias to seamlessly switch which physical collection actually receives the incoming updates. In the above diagram, Col1 is the only physical collection and so that is what the TSCUpdate points to. As the data comes in, it will be directed at the update alias and fill the Col1 collection for X amount of time. At time X, Col2 will be created and the update alias set to it. Col1 is now a static index with older data and Col2 accepts all the incoming data. One of the simplifying features of this strategy is that data updates are not generally expected. Rather than update existing documents, new documents just keep flowing in. Note: To update an alias, you simply use CREATEALIAS again on an existing alias. http://localhost:8983/solr/admin/collections?action=CREATEALIAS&amp;name=TSCUpdate&amp;collection=Col2 After another interval of X, Col3 will be created and the update alias TSCUpdate directed to it. http://localhost:8983/solr/admin/collections?action=CREATEALIAS&amp;name=TSCUpdate&amp;collection=Col3 The newest data that is coming in will often also be the most searched data. With this strategy, you can keep the most recent data in a relatively small collection while older data is kept in static collections that can be packed onto fewer machines and/or merged together. Static data is very friendly to caching. The collections with more recent data might have higher replication than older data to support a higher throughput. You could remove extra replicas for older collections. In general, you can tune different time regions by collection for your use case. You can also explicitly and flexibly leave large time regions out of queries. Note: It may take a while to create and delete collections when it�s time to update the update collection alias. You might want to schedule the heavy lifting to happen before your time interval is up, and then simply do the alias updating (which is very fast) on the interval. Read Alias Create Alias - TSC -&gt; Col1. http://localhost:8983/solr/admin/collections?action=CREATEALIAS&amp;name=TSC&amp;collection=Col1 You will also want to use read side aliases. With read size aliases, you can explicitly leave large sections of the total data out of your queries as well as search all of the data using a single virtual collection name. You might set up a few of these aliases to allow clients to search over different time windows of the data. To search Time Series Collection in the figure below, you would have to search over Col1, Col2, and Col3. With our read side alias, you could easily do this by simply specifying a collection of TSC. http://localhost:8983/solr/admin/collections?action=CREATEALIAS&amp;name=TSC&amp;collection=Col2,Col3 Because Col1, Col2, and Col3 will hold essentially ordered time series data, you can also search a smaller index by ignoring older data. For example, you could only search over Col2 and Col3, ignoring the data that first filled up Col1 for most queries. If you think about something like Twitter or a large news aggregator, you can imagine that most inquires will be directed at newer data, and so most searches might not need to touch data from 2005. For queries that must go back further, you can explicitly search over Col1, Col2, and Col3. http://localhost:8983/solr/admin/collections?action=CREATEALIAS&amp;name=TSC&amp;collection=Col1,Col2,Col3 The most common read side alias would probably be a rolling window that was always updated to reference the most recent N collections. Note: If your use case is very sensitive to the boundary of the time intervals of your collections, you may have to include another collection on either end of your search window and use range queries against the timestamp field to narrow in on the small number of updates that may have missed the boundary window. Merging Because all collections but the current one are static, merging collections becomes relatively straightforward. You might do this so that older data is hosted on less hardware or so that you won�t have to search across so many collections. Older data might not see the same load as more recent data, and a slightly worse response time for older data might be reasonable in any case. For example, you might have a single collection for the most recent day, then weekly collections for the past year, then monthly or yearly collections for data older than that. To turn the single day collections into collections that span multiple days, you will have to periodically merge indexes in the background. You can use static indexes and collection aliasing to do this. How to Merge Solr has a feature for merging�each shard in a collection via an HTTP API call. Typically, the merge has to use local indexes. With Cloudera Search, the command is more powerful because collections live in HDFS, and so merges can easily be done with remote collections. You can easily move collections by creating an empty matching collection and merging into it, and you can easily combine collections by merging two into a single new matching empty collection. We are currently missing some sugar in that you will have to manually merge each shard in a collection. I�m sure Solr and Cloudera Search will eventually offer the ability to merge by collection name and automatically figure out and execute the correct shard merges. Some useful Java example code to look at is our GoLive feature in the Solr-MR contrib, which Cloudera is contributing to Solr. It looks up the right shard urls for a collection and individually merges each shard. MapReduceIndexerTool is one such informative class to explore. https://github.com/cloudera/search/blob/master/search-mr/src/main/java/org/apache/solr/hadoop/MapReduceIndexerTool.java and ZooKeeperInspector https://github.com/cloudera/search/blob/master/search-mr/src/main/java/org/apache/solr/hadoop/ZooKeeperInspector.java Optimizing Collections perform better when each shard of the collection is made up of as few index segments as possible. When you add data to a Solr index, new segments are created and merged together over time. If you have a static index, it often makes a lot of sense to merge each shard down to one segment. This is what an optimize does. One strategy to handle this transparently is to merge the collection into a new collection that is located on separate machines and then optimize that new collection while still serving requests against the first collection. You can optimize in place, but it can be a fairly resource intensive operation. When the optimize is done, update the read alias to reference the new collection and remove the first collection. Re-indexing Collection aliases are also useful for re-indexing – especially when dealing with static indices. You can re-index in a new collection while serving from the existing collection. Once the re-index is complete, you simply swap in the new collection and then remove the first collection using your read side aliases. Scheduling This strategy begs for you to schedule collection creation and deletion, collection alias updates, and possibly an optimize and/or merging step. The scheduled logic that is kicked off can range from very simple to very complicated, but in either case, it�s probably best to contain it in it a small driver program or two. The driver(s) can be either very simple or very sophisticated depending on your needs. You may want to write the driver(s) in Java as you can reuse Solr�s Java code to read from ZooKeeper about the cluster. However, for the simplest setups, anything will do: a shell script, python, perl, whatever you have that can execute basic http requests against your cluster. You can then use your favorite scheduler to kick off �intervals� of your driver – cron is a reliable and popular option for scheduling on UNIX/Linux, and there’s always Apache Oozie. Conclusion Here you have learned a simple but very effective architecture for handling huge amounts of some near-real-time data. The techniques are not necessarily specific to time series data either; that is just one of the simpler applications. Keep an eye out for enhanced support for this strategy in the future – perhaps even the ability to accomplish something similar within a single collection using custom sharding. Finally, for someone willing to dig in a little, these techniques can also be used in a wide range of use cases beyond those described above. Mark Miller (@heismark) is a Software Engineer at Cloudera and a Committer/PMC Member of the Solr project.</snippet></document><document id="96"><title>Download the New Impala e-Book from O’Reilly Media</title><url>http://blog.cloudera.com/blog/2013/10/download-the-new-impala-e-book-from-oreilly-media/</url><snippet>As a delicious appetizer for the Strata Conference + Hadoop World next week (sold out!), O’Reilly Media has partnered with us to create and publish a new e-book specifically intended for technical end-users of Cloudera Impala, the open source distributed query engine for Apache Hadoop. Authored by Cloudera’s own John Russell, the e-book provides a 30-page tour of Impala’s internals and architecture, as well as common usage patterns intended for mainstream (SQL) users. As John explains in his introductory post on O’Reilly’s Strata blog: “I wanted to give an overview that didn�t rely on already being an expert with Hadoop, Hive, Java, some particular database system, and so on. With Impala, a little SQL and UNIX experience is all you really need. The patterns are familiar, even if the terminology is a little different. An end user doesn�t need to concern themselves with the underlying plumbing. But depending on where they�re coming from, they might have definite ideas about which logical or physical aspects are important.” You can download this new e-book from cloudera.com right now (registration required). But for those of you lucky enough to be attending the Impala + Parquet meetup on Tuesday evening, we have another treat: a box of hardcopies!</snippet></document><document id="97"><title>What are HBase znodes?</title><url>http://blog.cloudera.com/blog/2013/10/what-are-hbase-znodes/</url><snippet>Apache ZooKeeper is a client/server system for distributed coordination that exposes an interface similar to a filesystem, where each node (called a znode) may contain data and a set of children. Each znode�has a name and can be identified using a filesystem-like path (for example, /root-znode/sub-znode/my-znode). In Apache HBase, ZooKeeper coordinates, communicates, and shares state between the Masters and RegionServers. HBase has a design policy of using ZooKeeper only for transient data (that is, for coordination and state communication). Thus if the HBase�s ZooKeeper data is removed, only the transient operations are affected –�data can continue to be written and read to/from HBase. In this blog post, you will get a short tour of HBase znodes usage. The version of HBase used for reference here is 0.94 (shipped inside CDH 4.2 and CDH 4.3), but most of the znodes are present in�previous versions and also likely to be so in future versions. The HBase root znode path is configurable using hbase-site.xml, and by default the location is �/hbase�. All the znodes referenced below will be prefixed using the default /hbase location, and the configuration property that lets you rename the particular znode will be listed next to the default znode name and highlighted with bold type. ZooKeeper provides an interactive shell that allows you to explore the ZooKeeper state — run it by using hbase zkcli and walk through the znode via ls, as in a typical filesystem. You can also get some information about the znode content by using the get command. $ hbase zkcli
[zk: localhost:2181(CONNECTED) 0] ls /
[hbase, zookeeper]
[zk: localhost:2181(CONNECTED) 1] ls /hbase
[splitlog, online-snapshot, unassigned, root-region-server, rs, backup-masters, draining, table, master, shutdown, hbaseid]
[zk: localhost:2181(CONNECTED) 2] get /hbase/root-region-server
3008@u1310localhost,60020,1382107614265
dataLength = 44
numChildren = 0
...
   Operations The znodes that you’ll most often see are the ones that coordinate operations like Region Assignment, Log Splitting, and Master Failover, or keep track of the cluster state such as the ROOT table location, list of online RegionServers, and list of unassigned Regions. /hbase (zookeeper.znode.parent) The root znode that will contain all the znodes created/used by HBase /hbase/hbaseid (zookeeper.znode.clusterId) Initialized by the Master with the UUID that identifies the cluster. The ID is also stored on HDFS in hdfs:/&lt;namenode&gt;:&lt;port&gt;/hbase/hbase.id. /hbase/root-region-server (zookeeper.znode.rootserver) Contains the location of the server hosting the ROOT region. It is queried by the client to identify the RegionServer responsible for ROOT and ask for the META locations. (In 0.96, the ROOT table was removed as part of HBASE-3171, and this znode is replaced by /hbase/meta-region-server [zookeeper.znode.metaserver] that contains the location of the server hosting META.) /hbase/rs (zookeeper.znode.rs) On startup each RegionServer will create a sub-znode (e.g. /hbase/rs/m1.host) that is supposed to describe the �online� state of the RegionServer. The master monitors this znode to get the �online� RegionServer list and use that during Assignment/Balancing. /hbase/unassigned (zookeeper.znode.unassigned) Contains a sub-znode for each unassigned region (e.g. /hbase/unassigned/&lt;region name&gt;). This znode is used by the Assignment Manager to discover the regions to assign. (Read this to learn more about the Assignment Manager.) /hbase/master (zookeeper.znode.master) The �active� master will register its own address in this znode at startup, making this znode the source of truth for identifying which server is the Master. /hbase/backup-masters (zookeeper.znode.backup.masters) Each inactive Master will register itself as backup Master by creating a sub-znode (hbase/backup-master/m1.host). This znode is mainly used to track which machines are available to replace the Master in case of failure. /hbase/shutdown (zookeeper.znode.state) Describes the cluster state, �Is the cluster up?� It is created by the Master on startup and deleted by the Master on shutdown. It is watched by the RegionServers. /hbase/draining (zookeeper.znode.draining.rs) Used to decommission more than one RegionServer at a time by creating sub-znodes with the form serverName,port,startCode (for example, /hbase/draining/m1.host,60020,1338936306752). This lets you decommission multiple RegionServers without having the risk of regions temporarily moved to a RegionServer that will be decommissioned later. Read this to learn more about /hbase/draining. /hbase/table (zookeeper.znode.masterTableEnableDisable) Used by the master to track the table state during assignments (disabling/enabling states, for example). /hbase/splitlog (zookeeper.znode.splitlog) Used by the log splitter to track the pending log to replay and its assignment. (Read this to learn more about log splitting). � Security The Access Control List (ACL) and the Token Provider coprocessors add two more znodes: one to synchronize access to table ACLs and the other to synchronize the token encryption keys across the cluster nodes. /hbase/acl (zookeeper.znode.acl.parent) The acl znode is used for synchronizing the changes made to the _acl_ table by the grant/revoke commands. Each table will have a sub-znode (/hbase/acl/tableName) containing the ACLs of the table. (Read this for more information about the access controller and the ZooKeeper interaction.) /hbase/tokenauth (zookeeper.znode.tokenauth.parent) The token provider is usually used to allow a MapReduce job to access the HBase cluster. When a user asks for a new token the information will be stored in a sub-znode created for the key (/hbase/tokenauth/keys/key-id). � Replication As general rule, all znodes are ephemeral, which means they are describing a �temporary� state — so, even if you remove everything from ZooKeeper, HBase should be able to recreate them. Although the Replication znodes do not describe a temporary state, they are meant to be the source of truth for the replication state, describing the replication state of each machine. (Read this to learn more about replication). /hbase/replication (zookeeper.znode.replication) Root znode that contains all HBase replication state information /hbase/replication/peers (zookeeper.znode.replication.peers) Each peer will have a sub-znode (e.g. /hbase/replication/peers/&lt;ClusterID&gt;) containing the ZK ensemble’s addresses that allows the peer to be contacted. /hbase/replication/peers/&lt;ClusterId&gt;/peer-state (zookeeper.znode.replication.peers.state) Mirror of the /hbase/replication/peers znode, but here each sub-znode�(/hbase/replication/peer-state/&lt;ClusterID&gt;) will track the peer enabled/disabled state. /hbase/replication/state (zookeeper.znode.replication.state) Indicates whether replication is enabled. Replication can be enabled by setting the hbase.replication configuration to true, or can be enabled/disabled by using the start/stop command in the HBase shell. (In 0.96, this znode was removed and the peer-state znode above is used as a reference.) /hbase/replication/rs (zookeeper.znode.replication.rs) Contains the list of RegionServers in the main cluster (/hbase/replication/rs/&lt;region server&gt;). And for each RegionServer znode there is one sub-znode per peer to which it is replicating. Inside the peer sub-znode the hlogs are waiting to be replicated (/hbase/replication/rs/&lt;region server&gt;/&lt;ClusterId&gt;/&lt;hlogName&gt;). � Online Snapshot Procedures Online snapshots are coordinated by the Master using ZooKeeper to communicate with the RegionServers using a two-phase-commit-like transaction. (Read this for more details about snapshots.) /hbase/online-snapshot/acquired The acquired znode describes the first step of a snapshot transaction. The Master will create a sub-znode for the snapshot (/hbase/online-snapshot/acquired/&lt;snapshot name&gt;). Each RegionServer will be notified about the znode creation and prepare the snapshot; when done they will create a sub-znode with the RegionServer name meaning, �I�m done� (/hbase/online-snapshot/acquired/&lt;snapshot name&gt;/m1.host). /hbase/online-snapshot/reached Once each RegionServer has joined the acquired znode, the Master will create the reached znode for the snapshot (/hbase/online-snapshot/reached/&lt;snapshot name&gt;) telling each RegionServer that it is time to finalize/commit the snapshot. Again, each RegionServer will create a sub-znode to notify the master that the work is complete. /hbase/online-snapshot/abort If something fails on the Master side or the RegionServer side, the abort znode will be created for the snapshot telling everyone that something went wrong with the snapshot and to abort the job. � Conclusion As you can see, ZooKeeper is a fundamental part of HBase. All operations that require coordination, such as Regions assignment, Master-Failover, replication, and snapshots, are built on ZooKeeper. (You can learn more about why/how you would use ZooKeeper in your applications here.) Although most znodes are only useful to HBase, some — such as the list of RegionServers (/hbase/rs) or list of Unassigned Regions (/hbase/unassigned) — may be used for debugging or monitoring purposes. Or, as in the case with /hbase/draining, you may interact with them to let HBase know what you�re doing with the cluster. Matteo Bertozzi is a Software Engineer at Cloudera and a Committer on the HBase project.</snippet></document><document id="98"><title>HBase 0.96.0 Released!</title><url>http://blog.cloudera.com/blog/2013/10/hbase-0-96-0-released/</url><snippet>The following post, by Apache HBase 0.96 Release Manager/Cloudera Software Engineer Michael Stack, was published originally at blogs.apache.org and is provided below for your convenience.�Our thanks to the release’s numerous contributors! Note: HBase 0.96 will be packaged in the next release of CDH (CDH 5).� Here are some notes on our recent hbase-0.96.0 release. (For the complete list of over 2k issues addressed in 0.96.0, see Apache JIRA Release Notes.) hbase-0.96.0 was more than a year in the making. It was heralded by three developer releases — 0.95.0, 0.95.1 and 0.95.2 — and it went through six release candidates before we arrived at our final assembly released on Friday, Oct, 18, 2013. The big themes that drove this release gleaned of rough survey of users and our experience with HBase deploys were: Improved stability: A new suite of integration cluster tests (HBASE-6241 HBASE-6201), configurable by node count, data sizing, duration, and �chaos� quotient, turned up loads of bugs around assignment and data views when scanning and fetching. These we fixed in hbase-0.96.0. Table locks added for cross-cluster alterations and cross-row transaction support, enabled on our system tables by default, now have us wide-berth whole classes of problematic states. Scaling: HBase is being deployed on larger clusters. How we kept schema in the filesystem or our archiving a WAL file at a time when done replicating worked fine on clusters of hundreds of nodes but made for significant friction when we moved to the next scaling level up. Mean Time To Recovery (MTTR): A sustained effort in HBase and in our substrate, HDFS, narrowed the amount of time data is offline after node outage. Operability: Many new tools were added to help operators of hbase clusters: from a radical redo of the metrics emissions, through a new UI and exposed hooks for health scripts. It is now possible to trace lagging calls down through the HBase stack (HBASE-9121 Update HTrace to 2.00 and add new example usage) to figure where time is spent and soon, through HDFS itself, with support for pretty visualizations in Twitter Zipkin (See HBase Tracing from a recent meetup). Freedom to Evolve: We redid how we persisted everywhere, whether in the filesystem or up into zookeeper, but also how we carry queries and data back and forth over RPC. Where serialization was hand-crafted when we were on Hadoop Writables, we now use generated Google protobufs. Standardizing serialization on protobufs, with well-defined schemas, will make it easier evolving versions of the client and servers independently of each other in a compatible manner without having to take a cluster restart going forward. Support for hadoop1 and hadoop2: hbase-0.96.0 will run on either. We do not ship a universal binary. Rather you must pick your poison; hbase-0.96.0-hadoop1 or hbase-0.96.0-hadoop2 (Differences in APIs between the two versions of Hadoop forced this delivery format). hadoop2 is far superior to hadoop1 so we encourage you move to it. hadoop2 has improvements that make HBase operation run smoother, facilitates better performance — e.g. secure short-circuit reads — as well as fixes that help our MTTR story. Minimal disturbance to the API: Downstream projects should just work. The API has been cleaned up and divided into user vs developer APIs and all has been annotated using Hadoop�s system for denoting APIs stable, evolving, or private. That said, a load of work was invested making it so APIs were retained. Radical changes in API that were present in the last developer release were undone in late release candidates because of downstreamer feedback. Below we dig in on a few of the themes and features shipped in 0.96.0. Mean Time To Recovery HBase guarantees a consistent view by having a single server at a time solely responsible for data. If this server crashes, data is �offline� until another server assumes responsibility. When we talk of improving Mean Time To Recovery in HBase, we mean narrowing the time during which data is offline after a node crash. This offline period is made up of phases: a detection phase, a repair phase, reassignment, and finally, clients noticing the data available in its new location. A fleet of fixes to shrink all of these distinct phases have gone into hbase-0.96.0. In the detection phase, the default zookeeper session period has been shrunk and a sample watcher script will intercede on server outage and delete the regionservers ephemeral node so the master notices the crashed server missing sooner (HBASE-5844 Delete the region servers znode after a regions server crash). The same goes for the master (HBASE-5926 Delete the master znode after a master crash). At repair time, a running tally makes it so we replay fewer edits cutting replay time (HBASE-6659 Port HBASE-6508 Filter out edits at log split time). A new replay mechanism has also been added (HBASE-7006 Distributed log replay) that speeds recovery by skipping having to persist intermediate files in HDFS. The HBase system table now has its own dedicated WAL, so this critical table can come back before all others (See HBASE-7213 / HBASE-8631). Assignment all around has been speeded up by bulking up operations, removing synchronizations, and multi-threading so operations can run in parallel. In HDFS, a new notion of �staleness� was introduced (HDFS-3703, HDFS-3712). On recovery, the namenode will avoid including stale datanodes saving on our having to first timeout against dead nodes before we can make progress (Related, HBase avoids writing a local replica when writing the WAL instead writing all replicas remote out on the cluster; the replica that was on the dead datanode is of no use come recovery time. See HBASE-6435 Reading WAL files after a recovery leads to time lost in HDFS timeouts when using dead datanodes). Other fixes, such as HDFS-4721 Speed up lease/block recovery when DN fails and a block goes into recovery shorten the time involved assuming ownership of the last, unclosed WAL on server crash. And there is more to come inside the 0.96.x timeline: e.g. bringing regions online immediately for writes, retained locality when regions come up on the new server because we write replicas using the �Favored Nodes� feature, etc. Be sure to visit the Reference Guide for configurations that enable and tighten MTTR; for instance, �staleness� detection in HDFS needs to be enabled on the HDFS-side. See the Reference Guide for how. HBASE-5305 Improve cross-version compatibility &amp; upgrade-ability Rather than continue to hand-write serializations as is required when using Hadoop Writables, our serialization means up through hbase-0.94.x, in hbase-0.96.0 we moved the whole shebang over to protobufs. Everywhere HBase persists we now use protobuf serializations whether writing zookeeper znodes, files in HDFS, and whenever we send data over the wire when RPC�ing. Protobufs support evolving types, if careful, making it so we can amend Interfaces in a compatible way going forward, a freedom we were sorely missing — or to be more precise, was painful to do — when all serialization was by hand in Hadoop Writables. This change breaks compatibility with previous versions. Our RPC is also now described using protobuf Service definitions. Generated stubs are hooked up to a derivative, stripped down version of the Hadoop RPC transport. Our RPC now has a specification. See the Appendix in the Reference Guide. HBASE-8015 Support for Namespaces Our brothers and sisters over at Yahoo! contributed table namespaces, a means of grouping tables similar to mysql�s notion of database, so they can better manage their multi-tenant deploys. To follow in short order will be quota, resource allocation, and security all by namespace. HBASE-4050 Rationalize metrics, metric2 framework implementation New metrics have been added and the whole plethora given a radical edit, better categorization, naming and typing; patterns were enforced so the myriad metrics are navigable and look pretty up in JMX. Metrics have been moved up on to the Hadoop 2 Metrics 2 Interfaces. See Migration to the New Metrics Hotness � Metrics2for detail. New Region Balancer A new balancer using an algorithm similar to Simulated annealing or Greedy Hillclimbingfactors in not only region count, the only attribute considered by the old balancer, but also region read/write load, locality, among other attributes, coming up with a balance decision. Cell In hbase-0.96.0, we began work on a long-time effort to move off of our base KeyValue type and move instead to use a CellInterface throughout the system. The intent is to open up the way to try different implementations of the base type; different encodings, compressions, and layouts of content to better align with how the machine works. The move, though not yet complete, has already yielded performance gains. The Cell Interface shows through in our hbase-0.96.0 API with the KeyValue references deprecated in 0.96.0. All further work should be internal-only and transparent to the user. Incompatible Changes HBASE-3171 ROOT table removed HBASE-6706 Remove total order partitioner HBASE-4451 improved zk node naming (0.96.0 namings are not compat with 0.94) Protobuf wire and durable data encoding New directory layout to support namespaces HBASE-7660 removed support for HFile V1 HBASE-6553 Removed Avro Gateway HBASE-7315 HBASE-7263 Remove support for client-side RowLocks. It was a bad idea. HBASE-4336 Convert source tree into maven modules. Downstream projects can use hbase-client and hbase-testing-utility artifacts independent of the rest of HBase. Miscellaneous You can specify permissions to a finer granularity. For example, see HBASE-7331 Add access control for region open and close, row locking, and stopping the regionserver. Similarly, coprocessors can influence a wider expanse of HBase behaviors with added API for interjection. A bunch of work was done on compactions and flushes to make them pluggable and the triggers which set them off were made smarter. For example, see: HBASE-7678 HBASE-7667 make storefile management pluggable, together with compaction HBASE-7110 refactor the compaction selection and config code similarly to 0.89-fb changes HBASE-7603 HBASE-7519 refactor storefile management in HStore in order to support things like LevelDB-style compactions HBASE-7842 Exploring compactor HBase now runs on Windows. Default configurations have had an extensive edit so the out-of-the-box experience should be much improved. You are encouraged to make use of HDFS short-circuit reads — it makes for a big speedup in random reads — and to make use of checksums in HBase. See the reference guide for how to enable. Replication has been hardened and been made more efficient as more and more users have come to depend upon its operation. Bugs in Master/master replication have been fixed. Table snapshots, a useful primitive for taking backups for working with the HBase data offline, are available in hbase-0.96.0. (They were also backported to hbase-0.94.x.) A new client-side library was added to HBase to facilitate common serialization strategies for common types preserving sort-order. HBASE-8693 HBASE-8089 DataType: provide extensible type API HBASE-8201 HBASE-8089 OrderedBytes: an ordered encoding strategy Support for online region merging: HBASE-8219 Align Offline Merge with Online Merge HBASE-4391 Add ability to start RS as root and call mlockall An experimental HBASE-7404 Bucket Cache:A solution about CMS, Heap Fragment and Big Cache on HBASE was added. You will need to restart your cluster to come up on hbase-0.96.0. After deploying the binaries, run a checker script that will look for the existence of old format HFiles no longer supported in hbase-0.96.0. The script will warn you of their presence and will ask you to compact them away. This can be done without disturbing current serving. Once all have been purged, stop your cluster, and run a small migration script. The HBase migration script will upgrade the content of zookeeper and rearrange the content of the filesystem to support the new table namespaces feature. The migration should take a few minutes at most. Restart. See Upgrading from 0.94.x to 0.96.x for details. From here on out, 0.96.x point releases with bug fixes only will start showing up on a roughly monthly basis after the model established in our hbase-0.94 line. hbase-0.98.0, our next major version, is scheduled to follow in short order (months). You will be able to do a rolling restart off 0.96.x and up onto 0.98.0. Guaranteed. A big thanks goes out to all who helped make hbase-0.96.0 possible. This release is dedicated to Shaneal Manek, HBase contributor.</snippet></document><document id="99"><title>Parquet at Salesforce.com</title><url>http://blog.cloudera.com/blog/2013/10/parquet-at-salesforce-com/</url><snippet>The following Parquet blog post was originally published by Salesforce.com Lead Engineer and Apache Pig Committer Prashant Kommireddi (@pRaShAnT1784). Prashant has kindly given us permission to re-publish below. Parquet is an open source columnar storage format co-founded by Twitter and Cloudera. Parquet is a columnar storage format for Apache Hadoop that uses the concept of repetition/definition levels borrowed from�Google Dremel. It provides efficient encoding and compression schemes, the efficiency being improved due to application of aforementioned on a per-column basis (compression is better as column values would all be the same type, encoding is better as values within a column could often be the same and repeated).�Here�is a nice blog post from Julien Le Dem of Twitter describing Parquet internals. Parquet can be used by any project in the Hadoop ecosystem, there are integrations provided for MR, Pig, Hive, Cascading, and Cloudera Impala. I am by no means an expert at this, and a lot of what I write here is based on my conversations with a couple of key contributors on the project (@J_�and @aniket486). Also, most of the content mentioned on this post is based on Pig+Parquet integration. We at Salesforce.com have started using Parquet for application logs processing with Pig and are encouraged with the preliminary performance results. Writing a Parquet File There is�parquet.hadoop.ParquetWriter. You need to decide which ObjectModel you want to use. It could be Thrift, Avro, Pig, or the�example model. Here is a function for writing a file using the Pig model (TupleWriteSupport): private void write(String pigSchemaString, String writePath) throws Exception {
     Schema pigSchema = Utils.getSchemaFromString(pigSchemaString);
     TupleWriteSupport writeSupport = new TupleWriteSupport(pigSchema);
     FileSystem fs = FileSystem.get(new Configuration());
     Path path = new Path(writePath);
     if(fs.exists(path)) {
         fs.delete(path, true);
     }
     ParquetWriter writer = new ParquetWriter&lt;Tuple&gt;&gt;(path, writeSupport,       CompressionCodecName.UNCOMPRESSED,  ParquetWriter.DEFAULT_BLOCK_SIZE, ParquetWriter.DEFAULT_PAGE_SIZE, false, false);

     TupleFactory tf = TupleFactory.getInstance();
     for (int i = 0; i &lt; NUM_RECORDS; i++) {
         Tuple t = tf.newTuple();
         for(int j=0; j&lt;pigSchema.size(); j++) {
         t.append(i+j);
         }
         writer.write(t);

     }
     writer.close();
}
   �pigSchemaString� is the schema for the parquet file. This could be any valid pig schema, such as �a:int, b:int, c:int�. Note that I insert integer values in the tuple and hence schema fields are defined to be int. So what exactly happened during the write? I use�TupleWriteSupport�which is a�WriteSupport�implementation that helps us write parquet files compatible with Pig. I then use ParquetWriter passing in a few�arguments: path�� file path to write to writeSupport�� TupleWriteSupport in this case compressionCodecName�� could be UNCOMPRESSED, GZIP, SNAPPY, LZO blockSize�� block size which is 128M by default. Total size used by a block pageSize�� �from the parquet docs: �pages should be considered indivisible so smaller data pages allow for more fine grained reading (e.g. single row lookup). Larger page sizes incur less space overhead (less page headers) and potentially less parsing overhead (processing headers). Note: for sequential scans, it is not expected to read a page at a time; this is not the IO chunk. �We recommend 8KB for page sizes.� Default page size is 1MB enableDictionary�� turn on/off dictionary encoding At the end, I create tuples with a few elements and write to the parquet file. Reading a Parquet File There is a bug due to which trying to read Parquet files written using the Pig model (TupleWriteSupport) directly with ParquetReader fails. See https://github.com/Parquet/parquet-mr/issues/195 However, you can use Pig to read the file. A = load �parquet_file� USING parquet.pig.ParquetLoader();

describe A;

{a: int, b: int, c: int}

B = foreach A generate a;

dump B;
   Initial Findings I ran the PerfTest and noticed it takes longer to read 1 column and takes progressively less for more columns. Parquet file read took : 1790(ms) for 1 columns
Parquet file read took : 565(ms) for 2 columns
Parquet file read took : 560(ms) for 3 columns
Parquet file read took : 544(ms) for 4 columns
Parquet file read took : 554(ms) for 5 columns
Parquet file read took : 550(ms) for 10 columns
Parquet file read took : 545(ms) for 20 columns
Parquet file read took : 563(ms) for 30 columns
Parquet file read took : 1653(ms) for 40 columns
Parquet file read took : 1049(ms) for 50 columns
   That�s the JVM warmup phase! The�JIT compiler will inline methods�based on how often they are called. So it waits before doing so to see what gets called often. Julien suggested I run it twice in a row (in the same process) and compare the times. This is generic and nothing in particular to Parquet, but I wanted to highlight in case you happen to run into similar perf numbers. Voila! The 2nd iteration did provide better results. Parquet file read took : 1809(ms) for 1 columns, iteration 0
Parquet file read took : 563(ms) for 2 columns, iteration 0
Parquet file read took : 562(ms) for 3 columns, iteration 0
Parquet file read took : 548(ms) for 4 columns, iteration 0
Parquet file read took : 548(ms) for 5 columns, iteration 0
Parquet file read took : 554(ms) for 10 columns, iteration 0
Parquet file read took : 548(ms) for 20 columns, iteration 0
Parquet file read took : 550(ms) for 30 columns, iteration 0
Parquet file read took : 1603(ms) for 40 columns, iteration 0
Parquet file read took : 1054(ms) for 50 columns, iteration 0
Parquet file read took : 536(ms) for 1 columns, iteration 1
Parquet file read took : 531(ms) for 2 columns, iteration 1
Parquet file read took : 527(ms) for 3 columns, iteration 1
Parquet file read took : 532(ms) for 4 columns, iteration 1
Parquet file read took : 527(ms) for 5 columns, iteration 1
Parquet file read took : 533(ms) for 10 columns, iteration 1
Parquet file read took : 537(ms) for 20 columns, iteration 1
Parquet file read took : 534(ms) for 30 columns, iteration 1
Parquet file read took : 538(ms) for 40 columns, iteration 1
Parquet file read took : 1966(ms) for 50 columns, iteration 1
Parquet file read took : 523(ms) for 1 columns, iteration 2
Parquet file read took : 525(ms) for 2 columns, iteration 2
Parquet file read took : 691(ms) for 3 columns, iteration 2
Parquet file read took : 529(ms) for 4 columns, iteration 2
Parquet file read took : 530(ms) for 5 columns, iteration 2
Parquet file read took : 531(ms) for 10 columns, iteration 2
Parquet file read took : 532(ms) for 20 columns, iteration 2
Parquet file read took : 532(ms) for 30 columns, iteration 2
Parquet file read took : 1032(ms) for 40 columns, iteration 2
Parquet file read took : 1044(ms) for 50 columns, iteration 2
   I ran another test � reading a regular text file vs parquet file. The file contained 20M rows and 6 columns. The results didn�t seem right, both storage and processing-wise. Was I missing something? Text file size : 513M
Parquet file size : 895M

Time taken to read, Text : 13784ms
Time taken to read, Parquet: 19551ms
   By default compression is not enabled, which is why the Parquet file is larger (footers, headers, summary files take up additional space. Note Parquet stores information regarding each page, column chunk, file to be able to determine the exact pages that need to be loaded by a query. You can find additional info�here). Also if you are reading all the columns, it is expected that the columnar format will be slower as row storage is more efficient when you read all the columns. Project fewer columns and you should find a difference. You should see a projection pushdown message in the logs. Yes you need bigger files to get benefits from the columnar storage! At this point, I wanted to try out encoding and see how that plays out on the overall storage. My next question � Are different encoding (RLE, dictionary) to be provided by the client, or does Parquet figure out the right one to use based on the data? Turns out Parquet will use the dictionary encoding if it can but right now you need to turn that on: parquet.enable.dictionary=true
   Finally did get some nice results after enabling dictionary encoding and filtering on a single column. It was a lot better storage wise too once Dictionary Encoding was enabled. (The following was run on a larger dataset) 1st iteration:

Text file read took : 42231ms
Parquet file read took : 27853ms
   Here are the numbers from 2nd iteration – just to negate the effects of JVM warmup, and to be fair to text-row format :) 2nd iteration:

Text file read took : 36555ms
Parquet file read took : 27669ms
   Schema Management Parquet can handle multiple schemas. This is important for our use-case at SFDC for log processing. We have several different types of logs, each with its own schema, and we have a few hundred of them. Most pig queries run against a few log types. Parquet merges schema and provides the ability to parse out columns from different files. LogType A : organizationId, userId, timestamp, recordId, cpuTime LogType V : userId, organizationId, timestamp, foo, bar A query that tries to parse the organizationId and userId from the 2 logTypes should be able to do so correctly, though they are positioned differently in the schema. With Parquet, it�s not a problem. It will merge �A� and �V� schemas and project columns accordingly. It does so by maintaining a file schema in addition to merged schema and parsing the columns by referencing the 2. Projection Pushdown One of the advantages of a columnar format is the fact that it can read only those parts from a file that are necessary. The columns not required are never read, avoiding unnecessary and expensive I/O. For doing this in Pig, just pass in the required schema in to the constructor of ParquetLoader. A = LOAD �/parquet/file� USING parquet.pig.ParquetLoader.('a:int, b:int');
   The above query loads columns �a� and �b� only. When you do so, you should find a message similar to the following in logs Assembled and processed 4194181 records from 2 columns in 2124 ms: 1974.6615 rec/ms, 1974.6615 cell/ms If you hadn�t done that, a file containing 16 columns would all be loaded Assembled and processed 4194181 records from 16 columns in 6313 ms: 664.3721 rec/ms, 10629.953 cell/ms Summary Files Parquet generates a summary file for all part files generated under a directory (job output). The summary file reduces the number of calls to the namenode and individual slaves while producing the splits which reduces the latency to start a job significantly. Otherwise it will have to open the footer of every part file which occasionally is slowed down by the namenode or a bad slave that we happen to hit. Reading one summary file reduces the risks to hit a slow slave and the load on the namenode. For example, if the output directory to which Parquet files are written by a Pig script is �/user/username/foo�. STORE someAlias INTO �/user/username/foo� using parquet.pig.ParquetStorer();
   This will create part files under �foo�, the number of these part files depends on the number of reducers. /user/username/foo/part-r-00000.parquet

/user/username/foo/part-r-00001.parquet

/user/username/foo/part-r-00002.parquet

/user/username/foo/part-r-00003.parquet
   The summary file is generated when the hadoop job writing the files is finished as it is in the outputCommitter of the output format (ParquetOutputCommitter.commitJob). It reads all footers in parallel and creates the summary file so all subsequent �LOAD� or reads on the directory �foo� could be more efficient. There is one summary file for all the part files output by the same job. That is, one per directory containing multiple part files. Hadoop Compatibility Anyone who has been a part of a major hadoop upgrade should be familiar with how painful the process can be. At SFDC, we moved from a really old version 0.20.2 to 2.x (recently declared�GA). This involved upgrading a ton of dependencies, making client side changes to use the newer APIs, bunch of new configurations, and eliminating a whole lot of deprecated stuff. Though this was a major upgrade and most upgrades here on should be smooth(er), it always helps if dependent and 3rd party libraries don�t need to be recompiled. With Parquet, you should not need to re-compile for hadoop 2. It hides all the hadoop 2 incompatibilities behind reflective calls so the same jars will work. And Finally… We at Salesforce.com have been early adopters of several big data open source technologies. Hadoop, Pig, HBase, Kafka, Zookeeper, Oozie to name a few either have been or are in the process of making it to production. Phoenix, a SQL layer on top of HBase, is a project that was homegrown and is now open-sourced. Parquet is the latest addition, and we are looking forward to using it for more datasets (Oracle exports for example) in the near future and not just application logs. The Parquet community is helpful, open to new ideas and contributions, which is great for any open source project. Thanks, Prashant!</snippet></document><document id="100"><title>Guide to Special Users in the Hadoop Environment</title><url>http://blog.cloudera.com/blog/2013/10/guide-to-special-users-in-the-hadoop-environment/</url><snippet>There are a number of special “users” with roles to play in the Apache Hadoop environment. For your reference, we have summarized them below as of CDH 4.4. Kerberos principals (used for authentication in a secure cluster) are not covered here. The specific user IDs listed are the ones created by default on installation but they are configurable unless otherwise indicated. Project User Group Notes HDFS hdfs hdfs The NameNode and DataNodes run as this user, and the HDFS root directory as well as the directories used for edit logs should be owned by it. Superusers are defined by a group named in hdfs-site.xml, dfs.permissions.superusergroup, which is the UNIX group containing users that will be treated as superusers by HDFS. The default is supergroup if installing with Cloudera Manager (can be changed in the Cloudera Manager UI), hadoop otherwise. The hdfs, yarn, and mapred users belong to the hadoop group. To give users root privileges in HDFS, create a UNIX group with the same name as this group (or change the value of the configuration to correspond to an existing UNIX group) and add them to the group. The impala user also belongs to the hdfs group. httpfs httpfs httpfs The httpfs service runs as this user. MapReduce mapred mapred Without Kerberos, the JobTracker and tasks run as this user. The LinuxTaskController binary is owned by this user for Kerberos. It would be complicated to use a different user ID. YARN yarn mapred, yarn Without Kerberos, all YARN services and applications run as this user. The LinuxContainerExecutor binary is owned by this user for Kerberos. It would be complicated to use a different user ID. HBase hbase hbase The Master and the RegionServer processes run as this user. Hive hive hive The HiveServer2 process and the Hive Metastore processes run as this user. A user must be defined for Hive access to its Metastore DB (e.g. MySQL or Postgres) but it can be any identifier and does not correspond to a Unix uid. This is javax.jdo.option.ConnectionUserName in hive-site.xml. HCatalog hive hive The WebHCat service (for REST access to Hive functionality) runs as the hive user. It is not configurable. Sentry No special users Pig No special users Oozie oozie oozie The Oozie service runs as this user. Flume flume����������������������� flume The sink that writes to HDFS as this user must have write privileges. Sqoop1 sqoop sqoop This user is only for the Sqoop1 Metastore, a configuration option that is not recommended. Sqoop2 sqoop2 sqoop The Sqoop2 service runs as this user. Hue hue hue Hue runs as this user. It is not configurable. ZooKeeper zookeeper zookeeper The zookeeper process runs as this user. It is not configurable. Search solr solr The solr process runs as this user. It is not configurable. Impala impala impala The impala user also belongs to the hive and hdfs groups. Whirr � No special users Mahout � No special users Cloudera Manager cloudera-scm cloudera-scm Cloudera Manager processes such as the CM Server and the monitoring daemons run as this user. It is not configurable. � Rob Weltman is Director of Engineering at Cloudera.</snippet></document><document id="101"><title>Enabling SSO Authentication in Hue</title><url>http://blog.cloudera.com/blog/2013/10/enabling-sso-authentication-in-hue/</url><snippet>There�s good news for users of Hue, the open source web UI that makes Apache Hadoop easier to use: A new SAML 2.0-compliant backend, which is scheduled to ship in the next release of the Cloudera platform, will provide a better authentication experience for users as well as IT. With this new feature, single sign-on (SSO) authentication can be achieved instead of using Hue credentials � thus, user credentials can be managed centrally (a big benefit for IT), and users needn�t log in to Hue if they have already logged in to another Web application sharing the SSO (a big benefit for users). Here�s a demo that shows this experience: Next, we�ll explore some details about how this new infrastructure works and then offer a quick example that illustrates the authentication process. The Basics There are two basic components in SAML 2.0: the service provider (SP) and identity provider (IdP). The typical flow from SP to IdP is illustrated by the following image. Figure 1. SAML architecture (from http://en.wikipedia.org/wiki/SAML_2.0; available under public domain) In this process, Hue acts as an SP with an assertion consumer service (ACS). An assertion indicates that the IdP has authenticated a user. It optionally contains information about the user�s privileges and other attributes. Thus, Hue communicates with the IdP to authenticate users. Also, Hue provides two URLs that enable communication with the IdP: /saml2/metadata /saml2/acs The IdP will contact the metadata URL for information on the SP. For example, the ACS URL is described in metadata. The ACS URL is the consumer of assertions from the IdP. The IdP will redirect users to the ACS URL after it has authenticated them. Creating Users When a user logs into Hue through the SAML backend, a new user is created in Hue if it does not already exist. This logic is almost identical to that used by the LdapBackend. It is also configurable via the create_users_on_login parameter. Example Next, let�s review a how-to based on making Hue communicate via SAML with a Shibboleth IdP. Shibboleth is an open source project that implements federated identity support through SAML 2.0. Environment This example runs on CentOS 6.4 and assumes the following projects are installed and configured: Shibboleth 2.4.0 – IdP OpenDS 2.2.1 – Authentication service Tomcat 6 – Server for IdP Shibboleth IdP is installed to /opt/shibboleth-idp and has the following custom configurations: Releases the UID attribute with assertions (makes it available to requesters) Available over SSL on port 8443 Provides authentication via LDAP through OpenDS Connects to a relying party that contains metadata about the SP. In this case, the relying party is Hue and its metadata URL is /saml2/metadata. Uses the UsernamePassword handler and provides very obvious feedback that all components are configured appropriately Accessible from all IP addresses OpenDS was installed and 2,000 users were automatically generated. Then, a user �test� was added with the password �password�. Preparing Hue The libraries that support SAML in Hue must be installed: build/env/bin/pip install -e git+https://github.com/abec/pysaml2@HEAD#egg=pysaml2
build/env/bin/pip install -e git+https://github.com/abec/djangosaml2@HEAD#egg=djangosaml2
   The above commands will also install: decorator python-memcached repoze.who zope.interface Note: The SAML libraries are dependent on xmlsec1 being installed and available on the machine. For RHEL or CentOS: yum install xmlsec1 xmlsec1-openssl
   Hue must be configured as an SP and use the SAML authentication backend. Because Hue acts as the SP, you must configure it to communicate with the IdP in hue.ini: [libsaml]
xmlsec_binary=/opt/local/bin/xmlsec1
metadata_file=/tmp/metadata.xml
key_file=/tmp/key.pem cert_file=/tmp/cert.pem
   You can copy the key_file and cert_file from the Shibboleth IdP credentials directory (/opt/shibboleth-idp/credentials/). The files idp.crt and idp.key correspond to cert_file and key_file, respectively. These files should already be in PEM format, so for the purpose of this how-to, they are renamed to cert.pem and key.pem. The metadata_file is set to the file containing the IdP metadata (/tmp/metadata.xml). You can create it from the XML response of http://:8443/idp/shibboleth/. The XML itself may require some massaging. For example, in some fields, the port 8443 is missing from certain URLs. The table below describes the available parameters for SAML in hue.ini. Parameter Description xmlsec_binary Xmlsec1 binary path; should be executable by the user running Hue create_users_on_login Create users received in assertion response upon successful authentication and login required_attributes Required attributes to ask for from the IdP optional_attributes Optional attributes to ask for from the IdP metadata_file IdP metadata in the form of a file; generally an XML file containing metadata that the IdP generates key_file Private key to encrypt metadata cert_file Signed certificate to send along with encrypted metadata user_attribute_mapping Mapping from attributes in the response from the IdP to django user attributes SAML Backend for Logging in You have to use the SAML authentication backend so that users can login and be created: [desktop] [[auth]] backend=libsaml.backend.SAML2Backend
   SAML and Hue in Action Now that Hue has been setup to work with the SAML IdP, attempting to visit any page redirects to Shibboleth�s login screen: � Figure 2. Shibboleth login screen after attempting to access /about After logging in, Hue is readily available and visible! Conclusion As you can probably appreciate, providing SSO support through SAML helps IT by enabling centralized authentication. From a user�s perspective, life is easier because it removes the burden of password management. After a user has logged in, they get the same permissions and adhere to the same rules as other users across the enterprise. Have any suggestions? Feel free to tell us what you think through hue-user, at @gethue, or via our community discussion forum for Hue.</snippet></document><document id="102"><title>Apache Hadoop 2 is Here and Will Transform the Ecosystem</title><url>http://blog.cloudera.com/blog/2013/10/apache-hadoop-2-is-here-and-will-transform-the-ecosystem/</url><snippet>The release of Apache Hadoop 2, as announced today by the Apache Software Foundation, is an exciting one for the entire Hadoop ecosystem. Cloudera engineers have been working hard for many months with the rest of the vast Hadoop community to ensure that Hadoop 2 is the best it can possibly be, for the users of Cloudera�s platform as well as all Hadoop users generally. Hadoop 2 contains many major advances, including (but not limited to): High availability for the HDFS NameNode, which eliminates the previous SPOF in HDFS. Support for filesystem snapshots in HDFS, which brings native backup and disaster recovery processes to Hadoop. Support for federated NameNodes, which allows for horizontal scaling of the filesystem namespace. Support for NFS access to HDFS, which allows HDFS to be mounted as a standard filesystem. Native network encryption, which secures data while in transit. The YARN resource management system, which provides infrastructure for the creation of new Hadoop computing paradigms beyond MapReduce. This new flexibility will serve to expand the use cases for Hadoop, as well as improve the efficiency of certain types of processing over data already stored there. Several performance-related enhancements, including more efficient (and secure) short-circuit local reads in HDFS. Furthermore, a great deal of work has gone into stabilizing and maturing Hadoop’s APIs in preparation for this release, which should give all users and projects building on top of Hadoop confidence that what they’re creating today will work for years to come. With the continuing growth of the Hadoop development community, the myriad advances in this release in particular highlight the benefits that the entire ecosystem receives from participating in collaborative open source development. Thanks in part to the testing and packaging standardization provided through the Apache Bigtop project, customers now have the luxury of choosing roughly the same core software from no fewer than eight different vendors — and thus can focus on selecting the platform with the best applications, best data access frameworks, and best support on top of this ubiquitous core. As for CDH, Cloudera�s distribution including Hadoop and related projects, we have already delivered several stable, high-value parts of Hadoop 2 in the current release (such as HDFS 2.0, network encryption, and performance improvements), and the next release (CDH 5) will be based entirely on Hadoop 2 — using YARN for resource coordination between MapReduce and other components. We look forward to continuing to work with the entire community to push Hadoop forward at a rapid clip, in the Hadoop 2 release line and beyond! Further Reading: Migrating to MapReduce 2 on YARN (For Users) Migrating to MapReduce 2 on YARN (For Operators)� Aaron T. Myers is a Software Engineer at Cloudera and a Committer/PMC Member on the Hadoop project.</snippet></document><document id="103"><title>Writing Hadoop Programs That Work Across Releases</title><url>http://blog.cloudera.com/blog/2013/10/writing-hadoop-programs-that-work-across-releases/</url><snippet>In a fast-moving project like Apache Hadoop, there are always exciting new features introduced in each release. While it is tempting to make the most of these new features by upgrading to the latest release, users are often concerned about their code continuing to run. In this post, you’ll get an overview of the the Hadoop API annotations and compatibility policies. Hadoop annotates specific APIs to be safe for use by end-users. By using these APIs, users can ensure their code works across a set of releases and be aware of what releases it might not work against. Hadoop Releases and Compatibility Before discussing the API-specific aspects, let us quickly look at the types of releases and the kinds of changes allowed in them. Hadoop releases (CDH or stock Apache Hadoop) belong to one of three categories, and adopt an�x.y.z form of release numbering. The changes in a release determine the release type. The releases can be roughly characterized as follows (see Hadoop Roadmap for details): Major release (x): Major releases are vehicles to ship new features that might require API/wire incompatible change. These releases are infrequent and shipped as needed. Minor release (y): Minor releases are more frequent (every few months) and ship features and bug fixes that are API/wire compatible to previous minor versions in the same major version. Point release (z): Point releases are meant to fix critical bug fixes and should not introduce any incompatibilities. To help maintain compatibility, Hadoop interfaces and classes are annotated to describe the intended audience and stability. For example, post the initial CDH 4.0.0 release, the following CDH4.y.z releases introduced several features, all of which maintain API and wire compatibility with other CDH4.y.z releases. Similarly, Apache Hadoop 1.y.z releases are API-compatible releases within the Hadoop-1 major release line. Recently, the Hadoop community has formulated a compatibility guide that describes the various types of compatibility, including API source and binary compatibility, between releases. Hadoop developers (committers) follow the guide to determine the changes that can be included in a release. For end-users, the guide outlines potential incompatibilities to expect when upgrading to a release. Java API Annotations and Compatibility Hadoop interfaces and classes are annotated to describe the intended audience and stability in order to maintain compatibility with previous releases. (See Hadoop Interface Classification for details.) InterfaceAudience — captures the intended audience, possible values are: Public – for end users and external projects LimitedPrivate – for other Hadoop components like MapReduce, and closely related projects like Apache HBase Private – for intra-component use InterfaceStability –describes what types of interface changes are permitted. Possible values are: Stable – interfaces that adhere to the compatibility rules outlined in the compatibility guide Evolving – interfaces that might change incompatibly across minor releases Unstable – interfaces that might change incompatibly across any release, including point releases Deprecated- for interfaces that might be removed in a later release, typically with information on alternative APIs to be used As you can infer from the above, end-users are expected to use Public-Stable APIs. According to the compatibility guide, the Public-Stable APIs need to be deprecated in a major release before being removed in a subsequent major release. For instance, an API that is annotated “Public-Stable” in CDH 4.x release needs to be deprecated in CDH 5.x before being removed in CDH 6.x release. Note that two major releases span years, and even these deprecations/removals are expected to be rare. Essentially, by using Public-Stable APIs, users can ensure their programs work for several years. Here are some examples: org.apache.hadoop.mapred.JobClient class, that is used to submit MapReduce jobs, is annotated Public-Stable. It is safe to use and has to be deprecated in one major release before being incompatibly changed in the next major release. org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler class (the scheduler implementation for fair sharing of resources) is annotated LimitedPrivate(�yarn�) and is not intended to be used outside of YARN. org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy class (the policy to use with a particular queue in the FairScheduler) is annotated Public-Evolving. Hence, it is available for Public use, but can change in the future in an incompatible way. API Semantics The Hadoop community strives to ensure that the behavior of APIs remains consistent over versions, although changes for correctness may result in changes in behavior. Tests and javadocs specify the API’s behavior. The API implementation may be changed to fix incorrect behavior; in such a case, the change should be accompanied by updating existing buggy tests or adding tests in cases there were none prior to the change. REST APIs Hadoop REST APIs are specifically meant for stable use by end-users across releases, even major releases. WebHDFS is a good example of such a REST API. The YARN-specific REST APIs (including MapReduce on YARN) are expected to be stable starting with the GA release of Hadoop 2 (and the correponding CDH release). Apache Hadoop 2.2.0 GA Release The recent Apache Hadoop 2.2.0 release is the result of significant amount of API stabilization work by the community. Starting with this release, the community aims to preserve API compatibility in the Hadoop 2 releases to follow. We encourage end-users and downstream projects to try out the latest APIs and report (through JIRAs or cdh-user/hadoop-user mailing lists) any issues encountered. Karthik Kambatla is a Software Engineer at Cloudera in the scheduling and resource management team and works primarily on MapReduce and YARN.</snippet></document><document id="104"><title>Hello, Starbase: A Python Wrapper for the HBase REST API</title><url>http://blog.cloudera.com/blog/2013/10/hello-starbase-a-python-wrapper-for-the-hbase-rest-api/</url><snippet>The following guest post is provided by Artur Barseghyan, a web developer currently employed by Goldmund, Wyldebeast &amp; Wunderliebe in The Netherlands. Python is my personal (and primary) programming language of choice and also happens to be the primary programming language at my company. So, when starting to work with a new technology, I prefer to use a clean and easy (Pythonic!) API. After studying tons of articles on the web, reading (and writing) white papers, and doing basic performance tests (sometimes hard if you’re on a tight schedule), my company recently selected Cloudera for our Big Data platform (including using Apache HBase as our data store for Apache Hadoop), with Cloudera Manager serving a role as �one console to rule them all.� However, I was surprised shortly thereafter to learn about the absence of a working Python wrapper around the REST API for HBase (aka Stargate). I decided to write one in my free time, and the result, ladies and gentlemen, was Starbase (GPL). In this post, I will provide some code samples and briefly explain what work has been done on Starbase. I assume that reader of this blog post already has some basic understanding of HBase (that is, of tables, column families, qualifiers, and so on). Installation Next, I�ll show you some frequently used commands and use cases. But first, install the current version of Starbase from CheeseShop (PyPi). $ pip install starbase
   Do required imports: &gt;&gt;&gt; from starbase import Connection
   �and create a connection instance. Starbase defaults to 127.0.0.1:8000; if your settings are different, specify them here. &gt;&gt;&gt; c = Connection()
   Use Cases and Examples Show Tables Assuming that there are two existing tables named table1 and table2, the following would be printed out. &gt;&gt;&gt; c.tables()
['table1', 'table2']
   Table Schema Operations Whenever you need to operate with a table, you need to create a table instance first. Create a table instance (note, that at this step no table is created): &gt;&gt;&gt; t = c.table('table3')
   Create a new table: Create a table with columns ‘column1′, ‘column2′, ‘column3′ (here the table is actually created): &gt;&gt;&gt; t.create('column1', 'column2', 'column3')
201
   Check if table exists: &gt;&gt;&gt; t.exists()
True
   Show table columns: &gt;&gt;&gt; t.columns()
['column1', 'column2', 'column3']
   Add columns to the table, given (�column4�, �column5�, �column6�, �column7�): &gt;&gt;&gt; t.add_columns('column4', 'column5', 'column6', 'column7')
200
   Drop columns from table, given (�column6�, �column7�): &gt;&gt;&gt; t.drop_columns('column6', 'column7')
201
   Drop entire table schema: &gt;&gt;&gt; t.drop()
200
   Table Data Operations Insert data into a single row: &gt;&gt;&gt; t.insert(
&gt;&gt;&gt;���� 'my-key-1',
&gt;&gt;&gt;���� {
&gt;&gt;&gt;�������� 'column1': {'key11': 'value 11', 'key12': 'value 12', 'key13': 'value 13'},
&gt;&gt;&gt;�������� 'column2': {'key21': 'value 21', 'key22': 'value 22'},
&gt;&gt;&gt;�������� 'column3': {'key32': 'value 31', 'key32': 'value 32'}
&gt;&gt;&gt;���� }
&gt;&gt;&gt; )
200
   Note that you may also use the �native� means of naming the columns and cells (qualifiers). The result of the following would be equal to the result of the previous example. &gt;&gt;&gt; t.insert(
&gt;&gt;&gt;���� 'my-key-1a',
&gt;&gt;&gt;���� {
&gt;&gt;&gt;�������� 'column1:key11': 'value 11', 'column1:key12': 'value 12', 'column1:key13': 'value 13',
&gt;&gt;&gt;�������� 'column2:key21': 'value 21', 'column2:key22': 'value 22',
&gt;&gt;&gt;�������� 'column3:key32': 'value 31', 'column3:key32': 'value 32'
&gt;&gt;&gt;���� }
&gt;&gt;&gt; )
200
   Update row data: &gt;&gt;&gt; t.update(
&gt;&gt;&gt;���� 'my-key-1',
&gt;&gt;&gt;���� {'column4': {'key41': 'value 41', 'key42': 'value 42'}}
&gt;&gt;&gt; )
200
   Remove a row cell (qualifier): &gt;&gt;&gt; t.remove('my-key-1', 'column4', 'key41')
200
   Remove a row column (column family): &gt;&gt;&gt; t.remove('my-key-1', 'column4')
200
   Remove an entire row: &gt;&gt;&gt; t.remove('my-key-1')
200
   Fetch a single row with all columns: &gt;&gt;&gt; t.fetch('my-key-1')
  {
  ��� 'column1': {'key11': 'value 11', 'key12': 'value 12', 'key13': 'value 13'},
  ��� 'column2': {'key21': 'value 21', 'key22': 'value 22'},
  ��� 'column3': {'key32': 'value 31', 'key32': 'value 32'}
  }
   Fetch a single row with selected columns (limit to ‘column1′ and ‘column2′ columns): &gt;&gt;&gt; t.fetch('my-key-1', ['column1', 'column2'])
  {
  ��� 'column1': {'key11': 'value 11', 'key12': 'value 12', 'key13': 'value 13'},
  ��� 'column2': {'key21': 'value 21', 'key22': 'value 22'},
  }
   Narrow the result set even more (limit to cells ‘key1′ and ‘key2′ of column `column1` and cell ‘key32′ of column ‘column3′): &gt;&gt;&gt; t.fetch('my-key-1', {'column1': ['key11', 'key13'], 'column3': ['key32']})
  {
  ��� 'column1': {'key11': 'value 11', 'key13': 'value 13'},
  ��� 'column3': {'key32': 'value 32'}
  }
   Note that you may also use the native means of naming the columns and cells (qualifiers). The example below does exactly the same thing as the example above. &gt;&gt;&gt;� t.fetch('my-key-1', ['column1:key11', 'column1:key13', 'column3:key32'])
  {
  ��� 'column1': {'key11': 'value 11', 'key13': 'value 13'},
  ��� 'column3': {'key32': 'value 32'}
  }
   If you set the perfect_dict argument to False, you’ll get the native data structure: &gt;&gt;&gt;� t.fetch('my-key-1', ['column1:key11', 'column1:key13', 'column3:key32'], perfect_dict=False)
{
  ��'column1:key11': 'value 11', 'column1:key13': 'value 13',
  ��'column3:key32': 'value 32'
}
   Batch Operations with Table Data Batch operations (insert and update) work similarly to routine insert and update, but are done in a batch. You are advised to operate in batch as much as possible. In the example below, we will insert 5,000 records in a batch: � &gt;&gt;&gt; data = {
&gt;&gt;&gt;���� 'column1': {'key11': 'value 11', 'key12': 'value 12', 'key13': 'value 13'},
&gt;&gt;&gt;��� �'column2': {'key21': 'value 21', 'key22': 'value 22'},
&gt;&gt;&gt; }
&gt;&gt;&gt; b = t.batch()
&gt;&gt;&gt; for i in range(0, 5000):
&gt;&gt;&gt;���� b.insert('my-key-%s' % i, data)
&gt;&gt;&gt; b.commit(finalize=True)
{'method': 'PUT', 'response': [200], 'url': 'table3/bXkta2V5LTA='}
   In the example below, we will update 5,000 records in a batch: &gt;&gt;&gt; data = {
&gt;&gt;&gt;���� 'column3': {'key31': 'value 31', 'key32': 'value 32'},
&gt;&gt;&gt; }
&gt;&gt;&gt; b = t.batch()
&gt;&gt;&gt; for i in range(0, 5000):
&gt;&gt;&gt;���� b.update('my-key-%s' % i, data)
&gt;&gt;&gt; b.commit(finalize=True)
{'method': 'POST', 'response': [200], 'url': 'table3/bXkta2V5LTA='}
   Note: The table batch method accepts an optional size argument (int). If set, an auto-commit is fired each the time the stack is full. Table Data Search (Row Scanning) A table scanning feature is in development. At the moment it’s only possible to fetch all rows from a table. The result set returned is a generator. &gt;&gt;&gt; t.fetch_all_rows()
   Conclusion I hope you learned a little about Starbase here and will put it to good use. You are welcome to report any issues via the project�s issue tracker. Editor’s note: This post should not be taken as an indication that Starbase is recommended for production or will be supported in CDH. We just thought you might be interested.</snippet></document><document id="105"><title>Explore the Impala App in Hue</title><url>http://blog.cloudera.com/blog/2013/10/explore-the-impala-app-in-hue/</url><snippet>The following post was originally published by the Hue Team at the�Hue blog�in a slightly different form. Hue, the open source web GUI that makes Apache Hadoop easy to use, has supported Cloudera Impala since its inception to enable fast, interactive SQL queries from within your browser. In this post, you’ll see a demo of Hue’s Impala app in action and explore its impressive query speed for yourself. Impala App Demo The demo below compares some queries across Hue’s Apache Hive and Impala applications. (Impala supports a broad range of SQL and HiveQL commands.) Although this comparison is not scientific, it does reflect general user experience across common cases. In many ways, using Impala through the Hue app is easier than using it through the command-line impala-shell. For example, table names, databases, columns, and built-in functions are auto-completable, and a syntax highlighting feature shows potential typos in your queries. Multiple queries or a selected portion of a query can be executed from the editor. Parameterized queries are supported and you will be prompted for values at submission time. You can also save Impala queries, share them with other users, or delete them (and then restore them in case of mistakes). Impala uses the same Metastore as Hive so you can browse tables with the Metastore app. You can also pick a database with a drop-down in the editor. After submission, progress and logs are reported and you can browse the result with infinite scroll or download the data with your browser. Comparing Query Speeds If you have Hue installed and want to compare query speed between Impala and Hive for yourself, start with the Hue examples. Although the queries are very small, they nonetheless illustrate the lightning speed of Impala. To do that, confirm that the Hive and Impala examples are installed in Hue and then in each app, go to Saved Queries, copy the query Sample: Top salaries, and submit. You can also try queries on more complex data — for example, from Yelp�restaurants and reviews: SELECT r.business_id, name, SUM(cool) AS coolness
FROM review r JOIN business b
ON (r.business_id = b.business_id)
WHERE categories LIKE '%Restaurants%'
AND `date` = '$date'
GROUP BY r.business_id, name
ORDER BY coolness DESC
LIMIT 10
   You can see the benefits of Impala�s architecture and optimization first-hand! Results come back fast overall, and in the Yelp data case, instantaneously. Conclusion Impala and Hue combined are a recipe for fast analytics. Moreover, Hue�s Python API can also be reused if you want to build your own client. Cloudera�s QuickStart VM with its Hadoop tutorials is a great way to get started with Impala and Hue. An upcoming blog post will describe how to use more efficient file formats in Impala. As usual, feel free to try the app at�gethue.com. Let us know what you think on the�Hue user group�or�community forum!</snippet></document><document id="106"><title>Customer Spotlight: Learn How Edo Closes the Advertising Loop with Hadoop at Cloudera Sessions Milwaukee</title><url>http://blog.cloudera.com/blog/2013/10/customer-spotlight-learn-how-edo-closes-the-advertising-loop-with-hadoop-at-cloudera-sessions-milwaukee/</url><snippet>The Cloudera Sessions fall series is coming to a close next week, but first we�ll make a final stop in Milwaukee, Wisconsin (on Oct. 17), where attendees will hear about edo — a company that is revolutionizing the advertising space by closing the loop between promotions and point-of-sale transactions. In Milwaukee, edo CTO�Jeff Sippel�will engage in a fireside chat with Cloudera�s VP of marketing, Alan Saldich. At edo, Jeff is responsible for the strategy, planning, and execution for the systems — including Apache Hadoop — that power the edo offer platforms. edo is a venture-backed startup that sits at the intersection of payments and advertising.�It analyzes credit-card data from banking transactions to produce targeted offers for card-linked loyalty programs that are relevant and incredibly easy to redeem. Offers are preloaded on your credit card so savings happen instantly without a coupon or separate loyalty card.� For merchants, edo leverages data to provide insightful retailer metrics — tracking new customers, the increase in order size, and repeat visits — for a better understanding of customer behavior.� I�m sure I�m not the only one who�s found myself time and again in line at a retail store, rummaging through my purse only to discover that I left my coupon at home. Imagine having your purchase automatically discounted — no coupon required!� Hadoop allows edo to capture large volumes of detailed transaction data to ensure the offers delivered to consumers are highly targeted and relevant, while also meeting the advertiser�s goal. If you�re based in the Milwaukee area and want to learn more, be sure to attend next week�s Cloudera Sessions event!� Learn more about The Cloudera Sessions. Learn more about edo (@edointeractive). Learn more about Jeff Sippel (@jsippel). �Karina Babcock is Cloudera�s customer programs and marketing manager.�</snippet></document><document id="107"><title>Where to Find Cloudera Tech Talks Through December 2013</title><url>http://blog.cloudera.com/blog/2013/10/where-to-find-cloudera-tech-talks-through-december-2013/</url><snippet>Below please find our regularly scheduled quarterly update about where to find tech talks by Cloudera employees this year – this time, for October through December 2013. Note that this list will be continually curated during the period; complete logistical information may not be available yet. As always, we’re standing by to assist your meetup by providing speakers, sponsorships, and schwag! Date City Venue Speaker(s) Oct. 1 Aarhus, Denmark GOTO Aarhus Eva Andreeason on Hadoop use cases Oct. 8 Sunnyvale, Calif. Hadoop Happy Hour Kathleen Ting and Jarek Cecho sign books! Oct. 9 Santa Clara, Calif. IEEE BigData Conference Amr Awadallah on Hadoop use cases Oct. 9 San Francisco SF Hadoop Users Eric Sammer on Hadoop app development (panelist) Oct. 10 Sydney DataCon Sean Owen on data science Oct. 15 Durham, NC TriHUG Mark Miller on Solr+Hadoop Oct. 15 Mountain View, Calif. Oracle NoSQL &amp; Big Data Meetup Mike Olson on virtues of key-value stores Oct. 15-17 Burlingame, Calif. Big Data TechCon Apache Hive workshop with Mark Grover Doug Cutting on the Hadoop revolution Hadoop app development (CDK) workshop with Ryan Blue Jonathan Seidman on extending data infrastructure with Hadoop Jonathan Seidman on the Hadoop ecosystem Himanshu Vashishtha on HBase use cases Kate Ting on Apache ZooKeeper Kate Ting on 7 Deadly Hadoop Misconfigurations Oct. 16 Dallas, Tex. DFW Big Data John Ringhofer on Impala Oct. 17 Milwaukee, Wis. Cloudera Sessions Hadoop app development lab (on CDK) with Ryan Blue Oct. 17 St. Louis, Mo. St. Louis HUG Tom Wheeler on Parquet Oct. 18 Munich HUG Munich Lars George on Impala Oct. 22 London UK HUG Sean Owen on Scalable Big learning Oct. 23 Seattle Seattle Scalability Meetup Ronan Stokes on Cloudera Search Oct. 24 Palo Alto, Calif. Bay Area HBase User Group Michael Stack on HBase 0.96 Oct. 24 Raleigh, NC All Things Open Josh Wills on open source innovation Oct. 28-30 New York Strata Conference + Hadoop World 2013 Mike Olson on Hadoop’s impact on data management Doug Cutting on the future of Hadoop Henry Robinson on workload diversity in Hadoop Hadoop app development (CDK) workshop with Eric Sammer Matt Brandwein on leveraging mainframe data with Hadoop Aaron T. Myers and Shreepadma Venugopalan on Hadoop security Jayant Shekar on machine data analytics Amandeep Khurana on Monsanto’s use case for Hadoop &amp; HBase Philip Zeyliger on debugging distributed systems Greg Rahn on Impala performance tuning Jon Hsieh on HBase roadmap Oct. 28 New York NYC HUG Arvind Prabhakar on Apache Sentry (incubating) Oct. 28 New York Sqoop User Meetup Abe Elmahrek on the Sqoop2 app for Hue Oct. 29 New York Impala + Parquet Meetup Greg Rahn on Impala+Parquet performance tuning Oct. 29 New York Cloudera Manager Meetup Aditya Achara on Cloudera Manager success stories Oct. 30 New York Apache Sentry User Meetup Arvind Prabhakar and Shreepadma Venugopalan with a Sentry overview Oct. 30 Philadelphia Chariot Data IO Conference Lars George on HBase sizing as well as on Parquet Nov. 6 Chantilly, Va. Open Source Search Conference Alex Moundalexis on Search+Hadoop Nov. 6 Munich JAX Munich Lars George on HBase and Impala Nov. 7 Tokyo Cloudera World Tokyo Kiyoshi Mizumaru on CDH Sho Shimauchi on Cloudera Manager Tatsuo Kawasaki witha Hadoop 101 Daisuke Kobayashi on Hadoop ops Nov. 11 London UK HUG Marcel Kornacker on Impala Nov. 12-13 London Strata London Sean Owen on Scalable Big Learning; Tom White on Hadoop app development with CDK Nov. 12 San Francisco QCon SF Josh Wills on machine learning Nov. 13 Washington DC LISA 2013 John Ridley on Hadoop 101 for sysadmins Nov. 14 Seoul Tech Planet Korea Michael Stack on HBase roadmap Nov. 14 Tokyo Cloudera Manager Meetup Sho Shimauchi, Kiyoshi Mizumaru: What is Cloudera Manager? Nov. 14 Antwerp Devoxx Belgium Tom White on building Hadoop apps with CDK Nov. 16 Los Angeles Big Data Camp LA Alex Behm on Impala Nov. 20 Boulder, Colo. Boulder/Denver Big Data Meetup John Darrah on Hadoop 101 Dec. 2 Tokyo Cloudera Manager Meetup Sho Shimauchi, Kiyoshi Mizumaru: What is Cloudera Manager?</snippet></document><document id="108"><title>Let a Thousand Hadoop How-Tos Bloom</title><url>http://blog.cloudera.com/blog/2013/10/let-a-thousand-hadoop-how-tos-bloom/</url><snippet>History teaches us that ecosystem growth is fueled by enthusiasm, tools (including frameworks and APIs), and knowledge in roughly equal measures. To this point, the Apache Hadoop ecosystem has been blessed with the first two ingredients � thanks to the magic of open source � but in the third category, there is still plenty of work to be done. For Cloudera, our Academic Partnership program is a major part of that effort. Through that program, accredited nonprofit universities around the world get access to Cloudera�s own Hadoop curriculum for their computer science departments, in addition to discounted training and certification for students and instructors. Thus far, Cloudera Academic Partners include (but are not limited to) San Jose State University, DePaul University, Fordham University, Vanderbilt University, Technische Universit�t Berlin, and Rensselaer Polytechnic Institute. Recently, a wonderful side effect has emerged: the contribution of knowledge and experience deriving from an Academic Partnership back to the ecosystem! The University of St. Thomas (UST) in St. Paul, Minnesota, is such a partner through its Center of Excellence for Big Data (CoE4BD). UST�s software engineering faculty are no strangers to Hadoop; they have been doing research, teaching courses, and interacting with local companies using Hadoop ecosystem technologies for several years. That effort has been throttled by a lack of complete documentation in the open ecosystem, however. According to Brad Rubin, an associate professor in UST�s Graduate Programs in Software, �While we have found excellent references available on many topics, we have also found some gaps in short, focused how-to docs for common questions and use cases.� Just recently, the CoE4BD staff and students started to contribute what they have learned back to the community in the form of Hadoop how-tos in a GitHub repository. Furthermore, they are releasing longer-form information derived from faculty research and student projects in a sister repo. Consumers of this content may ask questions and seek advice through Cloudera’s community forum for questions about Hadoop concepts. From time to time, you will also see Cloudera re-publish CoE4BD-produced how-tos in this very blog. (See our most popular how-tos to date here.) Rubin says that UST�s status as a Cloudera Academic Partner has directly led to topic ideas (which will grow over time) and that technical support from Cloudera has helped overcome stumbling blocks. We can think of no better use case for an Academic Partnership! For more information about the CoE4BD, point your browser to www.stthomas.edu/coe4bd or send them an email. Justin Kestelyn is Cloudera’s developer community outreach director.</snippet></document><document id="109"><title>This Month in the Ecosystem (September 2013)</title><url>http://blog.cloudera.com/blog/2013/10/this-month-in-the-ecosystem-september-2013/</url><snippet>Welcome to our third edition of “This Month in the Ecosystem,” a digest of highlights from September 2013 (never intended to be comprehensive; for completeness, see Hadoop Weekly). Note: there were a few other interesting developments this week, but out of respect for the calendar, I’ll address them next month. New Ecosystem Projects Joined the Apache Incubator A trio of Hadoop-related projects were accepted into the Apache Incubator, including Storm (for real-time event processing – contributed by Nathan Marz and championed by Doug Cutting), Samza (for processing of data streamed from publish-subscribe systems – contributed by LinkedIn and championed by Jakob Homan), and Sentry (for role-based authorization and control in Hadoop – contributed by Cloudera and championed by Arvind Prabhakar). The first two newly incubating projects reflect the consensus for enriching the platform to include MapReduce alternatives, whereas the third reflects its evolution toward an enterprise-class security ideal. Read the Storm Proposal | Read the Samza Proposal | Read the Sentry Proposal Python Became a First-Class Citizen in Apache Pig Mortar Data, historically a center of Python + Hadoop evangelism, contributed code that brings CPython support to Apache Pig (obviating the inherent limitations of Jython UDFs or streaming) upstream to the Pig trunk. And Pythonistic data scientists rejoiced! Learn more about CPython support in Pig DataFu Reached a Milestone DataFu, the library of data mining/statistical analysis UDFs for Pig open-sourced by LinkedIn (and shipping inside Cloudera’s platform), became a 1.0 this month. In the blog post referenced below, LinkedIn engineer Matthew Vaughan goes into deep detail about current features. Read about functionality in DataFu 1.0� Big Data Made a Big Splash at Oracle OpenWorld 2013 Oracle OpenWorld turned out to be a bonanza for Big Data enthusiasts, with more sessions on Big Data use cases, and specifically on the architecture of Oracle’s Cloudera software-powered Big Data Appliance, than ever before (with some of the latter delivered by Cloudera engineers). Review the Big Data agenda delivered at Oracle OpenWorld The Community Meetup Schedule for Strata + Hadoop World 2013 Solidified More than 10 community meetups are planned for Strata + Hadoop World week — covering Cloudera Impala + Parquet, Apache Hive, Apache Sqoop, Apache HBase, Apache Flume, Apache Sentry and more — occuring onsite at the show as well as at offsite locations. It being early October, it’s time to RSVP and add at least one to your calendar.� See the meetup schedule at Strata + Hadoop World The next installment of “This Month in the Ecosystem” will publish in early November. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="110"><title>Customer Spotlight: Persado Makes Marketing a Data Science</title><url>http://blog.cloudera.com/blog/2013/10/customer-spotlight-persado-makes-marketing-a-data-science/</url><snippet>It�s common to hear people describe themselves as being �left-brained� or �right-brained� based on their tendency to be more logical and mathematically driven (left-brained), or, conversely, to be intuitive and creatively driven (right-brained). For example, people who prefer math over art are often considered left-brained. People who get a higher verbal score on their SATs than for math are often considered right-brained. In general, language and creative writing are considered right-brained exercises. Many people also associate marketing and advertising as a right-brained function, whereas engineering is considered very left-brained. But Big Data is changing this. Many companies are applying math and engineering to creative writing and marketing in order to optimize marketing campaigns� results. Persado has actually built its business around this idea. Persado, self-described as �the pioneer and global leader in Marketing Language Engineering,� helps brands deliver messages to their target audiences that are fine-tuned to get a response by combining math, computational linguistics, and Big Data. Persado�s development team collaborates with PhD statisticians and data analysts to create new ways to segment audiences, explore content variations, and generate the most relevant and effective marketing messages in real time. And, as you might suspect, collecting and analyzing all this data to engineer marketing language requires a Big Data platform. Persado has adopted Cloudera Standard as its platform of choice, quickly ramping-up its team on how to best use Apache Hadoop with Cloudera University training. Want to learn more? Read the success story Read the press release Read the CIO Magazine article Karina Babcock is Cloudera�s customer programs and marketing manager.</snippet></document><document id="111"><title>Meet the Project Founder: Josh Wills</title><url>http://blog.cloudera.com/blog/2013/10/meet-the-project-founder-josh-wills/</url><snippet>In this installment of �Meet the Project Founder,� we speak with Josh Wills (@josh_wills), Cloudera’s Senior Director of Data Science and founder of Apache Crunch and Cloudera ML. What led you to your project idea(s)? When I first started at Cloudera in 2011, I had a fairly vague job description, no real responsibilities, and wasn’t all that familiar with the Apache Hadoop stack, so I started working on various pet projects in order to learn more about the tools and the use cases in domains like healthcare and energy. My first project, analyzing adverse drug events, involved lots of Apache Pig programming. I liked Pig’s data flow programming model, but I didn’t enjoy writing user-defined functions: I had to step out of vim, switch to an IDE, read a lot of Javadoc about Pig’s internal data model, code for a while, compile something, switch back to vim, find bugs, go back to the IDE, and so on and so on. To this day, I am a big fan of Apache Hive and Pig right up to the point where I have to write a UDF, and then I die a little bit inside. (That said, the StreamingQuantile function that I adapted from Sawzall and contributed to DataFu is arguably the most useful thing I’ve done at Cloudera.) My next project was in a field called reflection seismology, which is the art and science of how to determine where oil and natural gas are located underground. The problems involve transforming and processing time-series data, and I wanted to develop a way for geophysicists to execute MapReduce pipelines over this data using familiar tools and create something that was less like a language and more like an application. I felt that all the pipeline development tools on the Hadoop stack were designed for people who thought about analyzing data using relational techniques, which weren’t really appropriate for time-series analysis. I personally find relational thinking to be limiting when it comes to large-scale data analysis and model building; I like to think in MapReduce and take advantage of its flexibility to help me solve problems more efficiently. I like to think in MapReduce and take advantage of its flexibility. What I really wanted was a library that I had used at Google to develop MapReduce pipelines called FlumeJava, which is much closer to bare-metal MapReduce but supports common design patterns like joins and aggregations. Since I didn’t work at Google anymore, I set out to recreate enough of FlumeJava to help me build my time-series application. Much like Goldilocks, it took me three tries to get it right: The first attempt was too simplistic, the second one was over-engineered, and the third was good enough that I wasn’t completely embarrassed to release it publicly as Crunch (now an Apache project). Aside from doing the initial commit, what is your definition of the project founder�s role across the lifespan of the project? Benevolent dictator, referee, silent partner? I initially posted the code on Github, and for the first few months, I did most of the work on it with help from Tom White and Brock Noland of Cloudera. Those were the “benevolent dictator” days of moving fast and fixing things, and I really enjoyed them. The thing that changed that for me was when Gabriel Reid and Christian Tzolov, who were both working at TomTom at the time, started using Crunch in their own projects and making major contributions. Reading through Gabriel’s first pull request, which fixed a subtle bug in the job planner, was one of the most sublime moments of my life. It’s such a wonderful feeling seeing someone you’ve never met and never talked to explore something you created, understand it, and then improve upon it. I realize that sounds corny, but I hope it’s the sort of thing that every project founder experiences, because it’s pretty awesome. We continued to work together for a number of months, and were joined by some of the folks at WibiData, who did a lot of the work on the Scala code and Apache HBase support. I felt like the project had reached a point where the code was as much Gabriel’s and Christian’s and the Wibis’ as it was mine, and that they should have just as much say in the future of the project as I did. And that was when we got together and decided to take the project to the Apache Software Foundation (ASF). Within the ASF, I think that the founder’s role is one of servant-leader, just as it would be for any other leader on the project. The VP of an Apache project isn’t a technical leadership role; it’s designed for someone who cares enough about the project to do a disproportionate amount of the tedious, bureaucratic work that has to get done in order to ensure that everyone else has the time and space to work on the pieces of the project that they enjoy most. My role model here is Bryan Duxbury, who was VP of Apache Thrift when we were first bringing Crunch to the Apache Incubator. What has surprised you the most about how your project has evolved/matured? What is the most important work TBD? Before I started working on Crunch, I came up with this theory about the best way to develop software. My thought was that the person who wanted to use the software, the eventual end-user, should create a minimally functional version of whatever they wanted to solve their problem. Then, this user should find a bunch of really good engineers and demo the software for them, to show them how it solves the problem. Finally, the user should show these really good engineers the ugly, poorly commented, and obviously sub-optimal source code that the user wrote in order to solve the problem. My hypothesis was that the really good engineers would be so offended by the ugly, poorly commented, and obviously sub-optimal source code that they would immediately set to work fixing it and making it awesome. The biggest surprise for me was that this theory turned out to actually kind of work, at least in the case of Crunch. Anyone who makes the time, and has the courage, to submit a patch is a hero in my book. As far as what’s TBD, I think that finding the right metaphors to extend Crunch’s abstractions to streaming data systems like Apache Storm (newly incubating) or Apache Samza (also newly incubating) in order to make it easier to implement systems based on the lambda architecture are the most interesting new frontier to explore. I have some ideas around the best way to do this, and have been trying to convince some really good engineers to implement them for me, but I suspect it would be better for me to whip up another ugly, poorly commented, and obviously sub-optimal prototype. What is your philosophy, if you have one, for balancing quality versus quantity with respect to contributions? I am strongly biased in favor of quantity. Anyone who makes the investment in time and has the courage to submit a patch is a hero in my book, and I always try to accept it or work with them to find an even better way to solve the use case that their patch is trying to solve. Do you have any other advice for potential project founders? The key to a successful open source project is recruiting developers, and I there are two very basic things you should do to help you succeed. First, you should be the end-user of the software you’re creating; Build something for yourself and people like you. You probably know other people like you, and if you build something for yourself, you’re building it for them as well. Second, be humble: You aren’t the best software engineer in the world, and acting like it isn’t going to make people want to work with you. If you’re not a naturally humble person, self-identify as something other than a software engineer (a data scientist, for example), so that your ego can take it when other people make really obvious improvements to your code. Meet other project founders: - Doug Cutting (Apache Hadoop, Apache Lucene, Apache Avro)� - Roman Shaposhnik (Apache Bigtop) - Alejandro Abdelnur (Apache Oozie) - Tom White (Apache Whirr)�</snippet></document><document id="112"><title>Hadoop Administrator Training Gets Hands-On</title><url>http://blog.cloudera.com/blog/2013/10/hadoop-administrator-training-gets-hands-on/</url><snippet>I’ve always held a strong bias that education is most effective when the student learns by doing. As a developer of technical curricula, my goal is to have training participants engage with real and relevant problems as much as possible through hands-on exercises. The high rate at which Apache Hadoop is changing, both as a technology and as an ecosystem, makes developing Cloudera training courses not only demanding but also seriously fun and rewarding. I recently undertook the challenge of upgrading the Cloudera Administrator Training for Apache Hadoop. I more than quadrupled the amount of hands-on exercises from the previous version, adding a full day to the course. At four days, it�s now the most thorough training for Hadoop administrators and truly the best way to start building expertise. While developing the course, I collaborated with some of the most knowledgeable Hadoop administrators I could find, including Eric Sammer, Amandeep Khurana, Kathleen Ting, Romain Rigaux,�and many other smart folks at Cloudera. The upgrades to the curriculum and exercises are based on best practices used to resolve our customers� biggest problems. These insights resonate throughout the course, including the determination that administrators should learn installation, configuration, maintenance, monitoring, and troubleshooting using the standard Hadoop tools. Although we certainly hope that Hadoop users take advantage of Cloudera Manager to simplify and streamline many of these tasks, we believe that every good administrator needs to first take a look under the hood and tinker with Hadoop�s guts. There�s no replacement for get-your-hands-dirty experience to achieve expertise. Cluster in the Cloud In addition to making changes to the course materials and adding a bunch of exercises, I really got my geek on and built a new training environment.�Students in the Cloudera Administrator Training class still start by working in a�pseudo-distributed�environment — a fully operational Hadoop cluster running on a single machine — but very quickly graduate to working with a full, four-instance cluster. Each student gets his or her own cluster to build up, configure, mess up, fix, mess up again, and explore. Hands-on training is the best first step toward unlocking the opportunities Hadoop offers. The new class environment is really cool. Students are given four Amazon EC2 instances to connect when they start doing their exercises. The EC2 instances are generated from public AMIs so that students can recreate the instances on their own AWS accounts and work their way through the exercises again after class is over. A local environment with four virtual machines is also available for situations where it is not possible to connect to the cloud from a classroom. The local environment operates identically to the cloud-based version. Combined with a catch-up script — a powerful tool that lets students automatically set their clusters to the starting point for any exercise within the course — the environment encourages students to perform exercises on their own, at their desired pace, and as many times as they wish. The catch-up script is very helpful for students who get called out of class because of pressing issues at work or for students who simply felt rushed trying to get exercises working during class time. Every student has the option to go through the hands-on exercises again later and at his or her own pace to gain greater understanding through repeat experience and to focus on the exercises that are most relevant, regardless of where they fall in the stack. The ability to perform hand-on exercises at one�s own pace, both during and after class, can really personalize the learning experience. My goal was to convey the sense that training does not end when the class ends, and that the course is �just in time and just for you.� Hands On, Hands On, Hands On I firmly believe hands-on training is the best first step toward achieving the efficiencies and unlocking the opportunities Hadoop offers. Over the course of four days, there are lots of highlights in the new Cloudera Administrator Training. Students get to: Install CDH on the cluster’s four instances Get Hive and Impala working and see if Impala queries can run as fast as Cloudera says they can Install Hue and configure it so that users in different roles have access to different Hadoop functionality in the Hue UI Configure Apache Hadoop for HDFS high availability Configure the Hadoop Fair Scheduler As an added bonus, there’s a troubleshooting challenge at the conclusion of the course in which students test their newly acquired skills by diagnosing and then attempting to fix a messy, but altogether common, misconfiguration.� It�s rewarding being a curriculum developer at Cloudera because so many administrators from different types of companies rely on the curriculum to learn how to effectively store, manage, and access their Big Data. It’s fun, too, because Cloudera�s customers and engineers are advancing the Hadoop platform and ecosystem so rapidly that I have a whole new set of challenges in front of me every day. I’ll certainly be updating the Cloudera Administrator Training exercises and course content to work with CDH 4.4, which was recently released. And I’ve recently started taking a look at some exciting new technology, so we’ll see where that takes me! Learn More If you want to learn more about the updated Cloudera Administrator Training course, watch this free on-demand webinar to get a sense of the prerequisites, intended audience, and outline of the live training. Ian Wrigley, Cloudera�s Senior Curriculum Manager and an extraordinary instructor as well, shares two sections of the course, including an overview of HDFS high availability and settings for some of Hadoop’s more advanced configuration options. I highly recommend this webinar for anyone considering moving into Hadoop administration. David Goldsmith is Senior Curriculum Developer at Cloudera.</snippet></document><document id="113"><title>Secrets of Cloudera Support: Impala and Search Make the Customer Experience Even Better</title><url>http://blog.cloudera.com/blog/2013/09/secrets-of-cloudera-support-impala-and-search-make-the-customer-experience-even-better/</url><snippet>In December 2012, we described how an internal application built on CDH called Cloudera Support Interface (CSI), which drastically improves Cloudera�s ability to optimally support our customers, is a unique and instructive use case for Apache Hadoop. In this post, we�ll follow up by describing two new differentiating CSI capabilities that have made Cloudera Support yet more responsive for customers: How Cloudera Impala has turbo-charged CSI with support for real-time log file analysis and visualization How Cloudera Search enables interactive data exploration of multiple sources simultaneously from within CSI So, let�s explore these use cases in detail. Log File Visualization with Impala As you may recall from our previous blog post on this topic, Cloudera Manager has a feature that allows a user to send diagnostic bundles (which are fairly large files) to Cloudera Support to help us diagnose issues more quickly and comprehensively. Cloudera Support ingests these bundles into HDFS, processes them via a thorough and robust data pipeline, and then visualizes the data for our customer operation engineers (COEs) via the CSI GUI. (COEs are Cloudera engineers who work full-time on technical support issues.). Before the availability of Impala, COEs would often ask the customer to manually provide individual logs to facilitate troubleshooting. This process often involved some back-and-forth with the customer, lengthening overall resolution time. Furthermore, without an easy way to view multiple logs together in a single view, or to window logs by timeframe or other variables, the log analysis process lengthened overall resolution time that much more. With the addition of Impala to the CSI suite in early 2013, logs are now included in bundles and ingested into HDFS by default, and that data is accessible via interactive SQL queries. Adding those queries to the back-end of the CSI GUI, just like a BI tool, enables a range of new log visualization features: Each log has a timeline (with histogram) that the COE can narrow into a specific time span as needed. Logs load on scroll, exposing 500 lines at a time, and as the user scrolls the log window, more data loads automatically. COEs can view logs side-by-side to compare two logs across a set time span. A search function that creates Impala queries on the back-end lets COEs do free-text searches for�specific words in the logs they are viewing, or across all logs in the bundle. The side-by-side window function in CSI. COEs can adjust date/time ranges via slider. A given diagnostic bundle, typically up to 80GB in size uncompressed, can be processed for visualization at interactive speeds. Using an intelligent partitioning strategy, and a columnar file format, our 30+ billion row Impala table can be sliced and diced within seconds. With that short processing time and the new ability to do interactive visualization, COEs can analyze and compare log information much more easily and quickly than before. Data Exploration with Search The other significant enhancement to our internal support processes is a new application inside CSI called Monocle, which lets COEs do keyword searches of multiple information sources simultaneously. Furthermore, thanks to Cloudera Search, all the data involved is ingested, processed, and indexed on the same CDH cluster as everything else. COEs have a wealth of information as reference material — including JIRAs, support cases, mailing lists, the support knowledge base, and most recently, discussion forums. But because this information lives in different places, prior to Monocle, COEs doing exploratory research had no choice but to do a series of cumbersome, independent searches using different tools. Today, with the Cloudera Search-based Monocle in place, COEs can work smarter and more efficiently by searching across all those sources from a single UI. Monocle helps our support team find relevant content quickly and easily, ultimately leading to faster response for support issues. Monocle lets COEs do keyword searches across multiple sources from a single GUI. For those of you interested in implementation details, here�s how the Monocle data flow works: Data is fetched from multiple sources, with some stored in HBase. Replication is enabled in HBase and the Lily HBase Indexer registers as a peer cluster. When updates are replicated, Lily HBase Indexer creates Solr documents and indexes them in Cloudera Search. For data that is not available in HBase (such as PDFs), sources are periodically scanned, and then data is extracted using Apache Tika and indexed using Cloudera Morphlines. Monocle features include: High availability, scalability, and extensibility backed by Solr Cloud, using HDFS for storage and Cloudera Manager for monitoring Near real-time indexing of content sources Automatic support for new collections and dynamic reflection of changes in schema Automatic faceting of all indexed data, which provides a quick overview for search-result classification and filtering of results Presentation of relevant search terms based on indexed content, offering quick context with links back to original source Range filtering of search results Cross-collected queries and interspersed results along with matching scores Similar to the new Impala-powered log file analysis capabilities in CSI, Monocle helps COEs get to the bottom of customer issues much more quickly than in the past by radically streamlining internal support processes. Conclusion As you can see, Cloudera Support�s ability to internally take advantage of differentiating features in our platform via CSI – in this case, Impala and Search – directly translates into a better customer experience. And as the number of differentiating features expands, that experience will only get better. Krista Mizusaki is a program manager in Cloudera Support.</snippet></document><document id="114"><title>How-to: Use HBase Bulk Loading, and Why</title><url>http://blog.cloudera.com/blog/2013/09/how-to-use-hbase-bulk-loading-and-why/</url><snippet>Apache HBase is all about giving you random, real-time, read/write access to your Big Data, but how do you efficiently get that data into HBase in the first place? Intuitively, a new user will try to do that via the client APIs or by using a MapReduce job with TableOutputFormat, but those approaches are problematic, as you will learn below. Instead, the HBase bulk loading feature is much easier to use and can insert the same amount of data more quickly. This blog post will introduce the basic concepts of the bulk loading feature, present two use cases, and propose two examples. Overview of Bulk Loading If you have any of these symptoms, bulk loading is probably the right choice for you: You needed to tweak your MemStores to use most of the memory. You needed to either use bigger WALs or bypass them entirely. Your compaction and flush queues are in the hundreds. Your GC is out of control because your inserts range in the MBs. Your latency goes out of your SLA when you import data. Most of those symptoms are commonly referred to as �growing pains.� Using bulk loading can help you avoid them. � In HBase-speak, bulk loading is the process of preparing and loading HFiles (HBase�s own file format) directly into the RegionServers, thus bypassing the write path and obviating those issues entirely. This process is similar to ETL and looks like this: 1. Extract the data from a source, typically text files or another database.�HBase doesn�t manage this part of the process. In other words, you cannot tell HBase to prepare HFiles by directly reading them from MySQL — rather, you have to do it by your own means. For example, you could run mysqldump on a table and upload the resulting files to HDFS or just grab your Apache HTTP log files. In any case, your data needs to be in HDFS before the next step. 2. Transform the data into HFiles.�This step requires a MapReduce job and for most input types you will have to write the Mapper yourself. The job will need to emit the row key as the Key, and either a KeyValue, a Put, or a Delete as the Value. The Reducer is handled by HBase; you configure it using HFileOutputFormat.configureIncrementalLoad() and it does the following: Inspects the table to configure a total order partitioner Uploads the partitions file to the cluster and adds it to the DistributedCache Sets the number of reduce tasks to match the current number of regions Sets the output key/value class to match HFileOutputFormat’s requirements Sets the reducer up to perform the appropriate sorting (either KeyValueSortReducer or PutSortReducer) At this stage, one HFile will be created per region in the output folder. Keep in mind that the input data is almost completely re-written, so you will need at least twice the amount of disk space available than the size of the original data set. For example, for a 100GB mysqldump you should have at least 200GB of available disk space in HDFS. You can delete the dump file at the end of the process. 3. Load the files into HBase by telling the RegionServers where to find them.�This is the easiest step. It requires using LoadIncrementalHFiles (more commonly known as the completebulkload tool), and by passing it a URL that locates the files in HDFS, it will load each file into the relevant region via the RegionServer that serves it. In the event that a region was split after the files were created, the tool will automatically split the HFile according to the new boundaries. This process isn�t very efficient, so if your table is currently being written to by other processes, it�s best to get the files loaded as soon as the transform step is done.� Here�s an illustration of this process. The data flow goes from the original source to HDFS, where the RegionServers will simply move the files to their regions� directories. Use Cases Original dataset load: All users migrating from another datastore should consider this use case. First, you have to go through the exercise of�designing the table schema and then create the table itself, pre-split. The split points have to take into consideration the row-key distribution and the number of RegionServers. I recommend reading my colleague Lars George�s presentation on advanced schema design for any serious use case. The advantage here is that it is much faster to write the files directly than going through the RegionServer�s write path (writing to both the MemStore and the WAL) and then eventually flushing, compacting, and so on. It also means you don�t have to tune your cluster for a write-heavy workload and then tune it again for your normal workload. Incremental load: Let�s say that you have some dataset currently being served by HBase, but now you need to import more data in batch from a third party or you have a nightly job that generates a few gigabytes that you need to insert. It�s probably not as large as the dataset that HBase is already serving, but it might affect your latency�s 95th percentile. Going through the normal write path will have the adverse effect of triggering more flushes and compactions during the import than normal. This additional IO stress will compete with your latency-sensitive queries. Examples You can use the following examples in your own Hadoop cluster but the instructions are provided for the Cloudera QuickStart VM, which is a single-node cluster, guest OS, and sample data and examples baked into a virtual machine appliance for your desktop. � Once you start the VM, tell it, via the web interface that will automatically open, to deploy CDH and then make sure that the HBase service is also started. Built-in TSV Bulk Loader HBase ships with a MR job that can read a delimiter-separated values file and output directly into an HBase table or create HFiles for bulk loading. Here we are going to: Get the sample data and upload it to HDFS. Run the ImportTsv job to transform the file into multiple HFiles according to a pre-configured table. Prepare and load the files in HBase. The first step is to open a console and use the following command to get sample data: � curl -O
https://people.apache.org/~jdcryans/word_count.csv
   I created this file by running a word count on the original manuscript of this very blog post and then outputting the result in csv format, without any column titles. Now, upload the file to HDFS: � hdfs dfs -put word_count.csv   The extraction part of the bulk load now being complete, you need to transform the file. First you need to design the table. To keep things simple, call it �wordcount� — the row keys will be the words themselves and the only column will contain the count, in a family that we�ll call �f�. The best practice when creating a table is to split it according to the row key distribution but for this example we�ll just create five regions with split points spread evenly across the key space. Open the hbase shell: � hbase shell   And run the following command to create the table: � create 'wordcount', {NAME =&gt; 'f'},   {SPLITS =&gt; ['g', 'm', 'r', 'w']}   The four split points will generate five regions, where the first region starts with an empty row key. To get better split points you could also do a quick analysis to see how the words are truly distributed, but I�ll leave that up to you. If you point your VM�s browser to http://localhost:60010/ you will see our newly created table and its five regions all assigned to the RegionServer. Now it�s time to do the heavy lifting. Invoking the HBase jar on the command line with the �hadoop� script will show a list of available tools. The one we want is called importtsv and has the following usage: hadoop jar /usr/lib/hbase/hbase-0.94.6-cdh4.3.0-security.jar importtsv
 ERROR: Wrong number of arguments: 0
 Usage: importtsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;inputdir&gt;
   The command line we are going to use is the following one: hadoop jar   /usr/lib/hbase/hbase-0.94.6-cdh4.3.0-
security.jar importtsv
-Dimporttsv.separator=,
-Dimporttsv.bulk.output=output
-Dimporttsv.columns=HBASE_ROW_KEY,f:count wordcount word_count.csv
   Here�s a rundown of the different configuration elements: -Dimporttsv.separator=, specifies that the separator is a comma. -Dimporttsv.bulk.output=output is a relative path to where the HFiles will be written. Since your user on the VM is �cloudera� by default, it means the files will be in /user/cloudera/output. Skipping this option will make the job write directly to HBase. -Dimporttsv.columns=HBASE_ROW_KEY,f:count is a list of all the columns contained in this file. The row key needs to be identified using the all-caps HBASE_ROW_KEY string; otherwise it won�t start the job. (I decided to use the qualifier �count� but it could be anything else.) The job should complete within a minute, given the small input size. Note that five Reducers are running, one per region. Here�s the result on HDFS: � -rw-r--r--�� 3 cloudera cloudera�� ����� 4265   2013-09-12 13:13 output/f/2c0724e0c8054b70bce11342dc91897b
-rw-r--r--�� 3 cloudera cloudera�� ����� 3163   2013-09-12 13:14 output/f/786198ca47ae406f9be05c9eb09beb36
-rw-r--r--�� 3 cloudera cloudera�� ����� 2487   2013-09-12 13:14 output/f/9b0e5b2a137e479cbc978132e3fc84d2
-rw-r--r--�� 3 cloudera cloudera�� ����� 2961   2013-09-12 13:13 output/f/bb341f04c6d845e8bb95830e9946a914
-rw-r--r--�� 3 cloudera cloudera�� ����� 1336   2013-09-12 13:14 output/f/c656d893bd704260a613be62bddb4d5f
   As you can see, the files currently belong to the user �cloudera�. In order to load them we need to change the owner to �hbase� or HBase won�t have the permission to move the files. Run the following command: sudo -u hdfs hdfs dfs -chown -R   hbase:hbase/user/cloudera/output
   For the final step, we need to use the completebulkload tool to point to where the files are and which tables we are loading to: � hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles output wordcount   Going back into the HBase shell, you can run the count command that will show you how many rows were loaded. If you forgot to chown, the command will hang. Custom MR Job The TSV bulk loader is good for prototyping, but because it interprets everything as strings and doesn�t support manipulating the fields at transformation time, you will end up having to write your own MR job. My colleague James Kinley, who works as a solutions architect in Europe, wrote such a job that we�re going to use for our next example. The data for the job contains public Facebook and Twitter messages related to the 2010 NBA Finals (game 1) between the Lakers and the Celtics.�You can find the code�here. (The Quick Start VM comes with git and maven installed so you can clone the repository on it.) � Looking at the Driver class, the most important bits are the following: � job.setMapOutputKeyClass(ImmutableBytesWritable.class);
    job.setMapOutputValueClass(KeyValue.class);
�
	// Auto configure partitioner and reducer
    HFileOutputFormat.configureIncrementalLoad(job, hTable);
      First, your Mapper needs to output a ImmutableBytesWritable that contains the row key, and the output value can be either a KeyValue, a Put, or a Delete. The second snippet shows how to configure the Reducer; it is in fact completely handled by HFileOutputFormat. configureIncrementalLoad() as described in the �Transform� section previously. � The HBaseKVMapper class only contains the Mapper that respects the configured output key and values: � public class HBaseKVMapper extends
   Mapper&lt;LongWritable,   Text, ImmutableBytesWritable,
KeyValue&gt; {   In order to run it you�ll need to compile the project using maven and grab the data files following the links in the README. (It also contains the shell script to create the table.) Before starting the job, don�t forget to upload the files to HDFS and to set your classpath to be aware of HBase because you�re not going to use its jar this time: � export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/etc/hbase/conf/:/usr/lib/hbase/*
   You�ll be able to start the job using a command line similar to this one: � hadoop jar hbase-examples-0.0.1-SNAPSHOT.jar
com.cloudera.examples.hbase.bulkimport.Driver -libjars
/home/cloudera/.m2/repository/joda-time/joda-time/2.1/joda-time-2.1.jar,
/home/cloudera/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar
RowFeeder\ for\ Celtics\ and\ Lakers\ Game\ 1.csv output2 NBAFinal2010
   As you can see, the job�s dependencies have to be added separately. Finally, you can load the files by first changing their owner and then running the completebulkload tool: � sudo -u hdfs hdfs dfs -chown -R hbase:hbase/user/cloudera/output2
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles output2 NBAFinal2010   Potential Issues Recently deleted data reappearing. This issue happens when a Delete is inserted via a bulk load and is major compacted while the corresponding Put is still in a MemStore. The data will be considered deleted when the Delete is in an HFile but, once it�s removed during the compaction, the Put will become visible again. If you have such a use case, consider configuring your column families to keep the deleted cells with KEEP_DELETED_CELLS in the shell or HColumnDescriptor.setKeepDeletedCells(). � Bulk-loaded data cannot be overwritten by another bulk load. This issue occurs when two bulk-loaded HFiles loaded at different times try to write a different value in the same cell, meaning that they have the same row key, family, qualifier, and timestamp. The result is that the first inserted value will be returned instead of the second one. This bug will be fixed in HBase 0.96.0 and CDH 5 (the next CDH major version) and work is being done in HBASE-8521 for the 0.94 branch and CDH 4. Bulk loading triggers major compactions. This issue comes up when you’re doing incremental bulk loads and there are enough bulk-loaded files to trigger a minor compaction (the default threshold being 3). The HFiles are loaded with a sequence number set to 0 so they get picked up first when the RegionServer is selecting files for a compaction, and due to a bug it will also select all the remaining files. This issue will seriously affect those who already have big regions (multiple GBs) or who bulk load often (every few hours and less) as a lot of data will be compacted. HBase 0.96.0 has the proper fix and so will CDH 5; HBASE-8521 fixes the issue in 0.94 as the bulk-loaded HFiles are now assigned a proper sequence number. HBASE-8283 can be enabled with hbase.hstore.useExploringCompation after 0.94.9 and CDH 4.4.0 to mitigate this issue by just being a smarter compaction-selection algorithm. Bulk-loaded data isn�t replicated. As bulk loading bypasses the write path, the WAL doesn�t get written to as part of the process. Replication works by reading the WAL files so it won�t see the bulk loaded data � and the same goes for the edits that use Put.setWriteToWAL(true). One way to handle that is to ship the raw files or the HFiles to the other cluster and do the other processing there. Conclusion The goal of this blog post was to introduce you to Apache HBase bulk loading�s basic concepts. We explained how the process is like doing ETL, and that it is much better for big data sets than using the normal API since it bypasses the write path. The two examples were included to show how simple TSV files can be bulk loaded to HBase and how to write your own Mapper for other data formats. � Now you can try doing the same using a graphical user interface via Hue. Jean-Daniel (JD) Cryans is a software engineer at Cloudera and an HBase Committer/PMC Member. �</snippet></document><document id="115"><title>Email Indexing Using Cloudera Search</title><url>http://blog.cloudera.com/blog/2013/09/email-indexing-using-cloudera-search/</url><snippet>Why would any company be interested in searching through its vast trove of email? A better question is: Why wouldn�t everybody be interested?� Email has become the most widespread method of communication we have, so there is much value to be extracted by making all emails searchable and readily available for further analysis. Some common use cases that involve email analysis are fraud detection, customer sentiment and churn, lawsuit prevention, and that�s just the tip of the iceberg. Each and every company can extract tremendous value based on its own business needs.� A little over a year ago we described how to archive and index emails using HDFS and Apache Solr. However, at that time, searching and analyzing emails were still relatively cumbersome and technically challenging tasks. We have come a long way in document indexing automation since then — especially with the recent introduction of Cloudera Search, it is now easier than ever to extract value from the corpus of available information. In this post, you�ll learn how to set up Apache Flume for near-real-time indexing and MapReduce for batch indexing of email documents. Note that although this post focuses on email data, there is no reason why the same concepts could not be applied to instant messages, voice transcripts, or any other data (both structured and unstructured).� Where to Start Every document that you would like to make searchable must live in an index. In addition, the documents themselves must be broken up to provide the capability of searching on specific fields. The diagram below provides a high-level view of how indexes, documents, and fields are related: The first thing to do is to break down the emails into fields that are of particular interest to the use case.� The fields will be used for building the indexes and facets. To understand the fields of the email, we will use the Mime 1.0 standard, which is defined here. Below is the text from a sample email, with relevant fields highlighted (This email came from a public archive.) Cloudera Search is based on Solr, which includes multiple other components such as Apache Lucene, SolrCloud, Apache Tika, and Solr Cell. Let�s begin by editing Solr�s schema.xml file to incorporate the fields we have identified in the previous exercise. Throughout this document, you will extensively use the solrctl command to manage SolrCloud deployments. (See the full command reference here.) The command below will generate a template schema.xml file that you will update to fit our email use case. � $ solrctl --zk localhost:2181/solr instancedir --generate $HOME/emailSearchConfig
   (Please note that –zk localhost:2181 should be replaced with the address and port of your own Apache ZooKeeper quorum.) � Next, edit the schema.xml file (which can be found here). What follows is a brief overview of what was changed from the template generated above.� First, completely replace the &lt;fields&gt; and &lt;fieldTypes&gt; section�with the one below and add the additional “to” line. The email addresses need to be tokenized differently from the rest of the text — therefore, we created email_general field type below, using the solr.UAX29URLEmailTokenizerFactory tokenizer. Similarly, use a new field type, names_general, for fields that contain names. Date fields use the tdate field type, which allows for easy date range faceting. All other fields are just set up as text_general, which is supplied in the default Solr schema.xml file. � &lt;fields&gt;
  � &lt;field name="message_id" type="string" indexed="true" stored="true" required="true" multiValued="false" /&gt;
  � &lt;field name="date" type="tdate" indexed="true" stored="true"/&gt;
  � &lt;field name="from" type="email_general" indexed="true" stored="true"/&gt;
  � &lt;field name="to" type="email_general" indexed="true" stored="true"/&gt;
  � &lt;field name="subject" type="text_general" indexed="true" stored="true"/&gt;
  � &lt;field name="from_names" type="names_general" indexed="true" stored="true"/&gt;
  � &lt;field name="to_names" type="names_general" indexed="true" stored="true"/&gt;
  � &lt;field name="cc" type="email_general" indexed="true" stored="true"/&gt;
  � &lt;field name="bcc" type="text_general" indexed="true" stored="true"/&gt;
  � &lt;field name="cc_names" type="names_general" indexed="true" stored="true"/&gt;
  � &lt;field name="bcc_names" type="names_general" indexed="true" stored="true" /&gt;
  � &lt;field name="x_folder" type="text_general" indexed="true" stored="false" /&gt;
  � &lt;field name="x_origin" type="text_general" indexed="true" stored="false" /&gt;
  � &lt;field name="x_filename" type="text_general" indexed="true" stored="false"/&gt;
  � &lt;field name="message" type="text_general" indexed="false" stored="false"/&gt;
  � &lt;field name="body" type="text_general" indexed="true" stored="true"/&gt;
  � &lt;field name="text" type="text_general" indexed="false" stored="false" /&gt;
  � &lt;field name="_version_" type="long" indexed="true" stored="true"/&gt;
  &lt;/fields&gt;
  &lt;uniqueKey&gt;message_id&lt;/uniqueKey&gt;

&lt;fieldType name="names_general" class="solr.TextField" &gt;
  � &lt;analyzer type="index"&gt;
  ��� &lt;charFilter class="solr.HTMLStripCharFilterFactory" /&gt;
  ��� &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
  ��� &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
  � &lt;/analyzer&gt;
  � &lt;analyzer type="query"&gt;
  ��� &lt;charFilter class="solr.HTMLStripCharFilterFactory" /&gt;
  ��� &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt;
  ��� &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
  � &lt;/analyzer&gt;
  &lt;/fieldType&gt;
  &lt;fieldType name="email_general" class="solr.TextField" &gt;
  � &lt;analyzer type="index"&gt;
  ��� &lt;tokenizer class="solr.UAX29URLEmailTokenizerFactory" /&gt;
  ��� &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
  � &lt;/analyzer&gt;
  � &lt;analyzer type="query"&gt;
  ��� &lt;tokenizer class="solr.UAX29URLEmailTokenizerFactory" /&gt;
  ��� &lt;filter class="solr.LowerCaseFilterFactory"/&gt;
  � &lt;/analyzer&gt;
  &lt;/fieldType&gt;
   One very important concept in SolrCloud deployments is the notion of collections. A �collection� is a single index that spans multiple Solr Instances. For example, if your email index is distributed across two Solr Instances, they all add up to form one collection. Let�s call our collection email_collection and set it up using the commands below: $ solrctl --zk localhost:2181/solr instancedir --create email_collection $HOME/emailSearchConfig
$ solrctl --zk localhost:2181/solr collection --create email_collection -s 2
 Again, replace –zk localhost:2181 with your own ZooKeeper quorum configuration in both statements. � Note that the “-s 2″ argument defines the number of shards. A �shard� is a very important concept in Solr and refers to a slice of an index. For example, if you have a corpus of 1 million emails, you may want to split it into two shards for scalability and to improve query performance. The first shard will handle all the documents that have a message-id between 0 – 500,000, and the second shard will handle documents with message id between 500,000 – 1,000,000. This logic is all handled by Solr internally; you need only specify the number of shards you would like to create with the -s option. The number of shards will depend on many factors and should be determined carefully.�The following table describes some of the considerations that should go into choosing the optimal number: How Email Parsing Happens Email parsing is achieved with the help of Cloudera Morphlines.� The Morphlines library is an open source framework, available through the Cloudera Development Kit (CDK), that defines a transformation chain without a single line of code needed. The Morphlines configuration file defines the transformation chain. We will define a Morphlines configuration file and use it later for both batch and near real-time indexing. The Email morphlines library will perform the following actions: Read the email events with the readMultiLine command Break up the unstructured text into fields with the grok command If Message-ID is missing from the email, generate it with the generateUUID command Convert the date/timestamp into a field that Solr will understand, with the convertTimestamp command Drop all of the extra fields that we did not specify in schema.xml, with the sanitizeUknownSolrFields command Load the record into Solr for HDFS write, with the loadSolr command To view the full Morphline configuration file for this example, please visit this link. Which Tool to Use for Indexing Cloudera Search can index emails in various ways. In a near-real-time scenario, Cloudera Search will utilize Flume to index messages on their way into HDFS.� As the data passes through Flume, relevant fields will be extracted using MorphlineSolrSink. Solr will then index the event and write the indexes to HDFS. For the batch-oriented scenarios, Cloudera Search provides MapReduceIndexerTool, which will read the data out of HDFS, build the indexes, and write them out back to HDFS. There are multiple options in the tool, including one that will merge the generated indexes into live Solr Servers. The appropriate indexing method will be dictated entirely by your use case and data ingestion strategy. For example, if you are doing customer churn analysis, you may initiate a periodic search query for words that signify a negative sentiment. As emails from customers come into the system, you may need to persist and index them right away. In that case, the indexing must be done in near-real-time and the data would be ingested through a Flume Agent. But suppose you already have five years of emails already stored on HDFS?� If you need to index them, the only scalable option is to run MapReduceIndexerTool. In some use cases, both methods may be needed. For example, if most of the emails are indexed in real time but later on you identify a new field that was not indexed before, you may need to go back and run MapReduceIndexerTool to reindex the whole corpus. (There is an HBase near-real time indexer as well, but we�ll address that in a future post.) The table below describes some question that may help you identify optimal indexing tools:� How to Run Batch Indexing The flow of the MapReduceIndexerTool is straightforward: � Read the HDFS directory with the files that need to be indexed. Pass them through the morphline, which will perform the following transformation chain: Break the text up into fields with grok command Change the date field into Date Time that Solr will understand Drop all of the extra fields that you did not specify in the schema.xml file Generate the indexes and store them to HDFS �Merge the indexes into live SOLR servers To run the MapReduceIndexerTool, execute the following command: /contrib/mr/search-mr-0.9.2-cdh4.3.0-SNAPSHOT-job.jar \

org.apache.solr.hadoop.MapReduceIndexerTool \

--morphline-file &lt;morphlines file&gt; \

--output-dir &lt;hdfs URI for indexes&gt; --go-live --zk-host clust2:2181/solr \ --collection email_collection &lt;HDFS URI with the files to index&gt;
 A few notes: &lt;cloudera search directory&gt; is the directory where search tools are installed. If you installed via packages, this would be /usr/lib/solr. If you installed via parcels, it would be the directory where the Search parcel was deployed.� &lt;morphlines file&gt; is the location of the morphlines config file that was created above. The –go-live option will actually merge the indexes into Solr Server. For a full description of the above command, read this doc. How to Index in Near-Real Time Using Flume The flow for near-real-time indexing looks more complex than batch indexing, but it is just as simple: As the email files are dropped into the spooling directory, Flume will pick them up. Flume will replicate them into two different memory channels. The first channel is read by HDFS Sink and rolled directly into HDFS. The second channel is read by MorphlineSolrSink, which will perform the same transformation chain as described previously: Break the text up into fields with the grok command Change the date field into Date Time that Solr will understand Drop all of the extra fields that you did not specify in the schema.xml file Generate the Indexes and send them to Solr One powerful feature of Cloudera Search is that the morphline file, which encodes the transformation logic, is identical in both examples. For completeness, here is the section in Flume configuration�that defines Solr Sink: # SOLR Sink
tier1.sinks.solrSink.type=org.apache.flume.sink.solr.morphline.MorphlineSolrSink
tier1.sinks.solrSink.channel=solrChannel
tier1.sinks.solrSink.morphlineFile=/tmp/morphline.conf
   How to Search and View Indexed Data Once the data is indexable and accessible by Cloudera Search, there are many ways for users to interact with it, including the Solr GUI or the via Hue�s Search application, which provides a very rich and configurable interface with faceting, date ranges and much more.� Solr GUI � Hue�s Search app I could spend a significant amount of time describing Search visualization tools, but that is a matter for another post.� Conclusion To summarize, here are the important steps an enterprise can take to get more value from its email data: Decide which email fields are to be extracted and indexed. Use Morphlines to help with transformations. (No coding required!) Use MapReduceIndexerTool to index the emails in HDFS. Use Flume with MorphlineSolrSink to index the emails in near real time. The big news is that with Cloudera Search, indexing and searching various types of data over small and large data sets has become much easier and much more flexible. We have all learned to search the internet for information by using popular tools like Google and Yahoo!. Now, this tool is available for data in your Hadoop and HBase environments – and conveniently provides a more integrated and unified process. You will be able to find your data and process it on the same platform, without having to switch context or move your data.� Jeff Shmain is a solution architect at Cloudera.</snippet></document><document id="116"><title>What�s Next for Impala After Release 1.1</title><url>http://blog.cloudera.com/blog/2013/09/whats-next-for-impala-after-release-1-1/</url><snippet>In December 2012, while Cloudera Impala was still in its beta phase, we provided a roadmap�for planned functionality in the production release. In the same spirit of keeping Impala users, customers, and enthusiasts well informed, this post provides an updated roadmap for upcoming releases later this year and in early 2014. But first, a thank-you: Since the initial beta release, we�ve received a tremendous amount of feedback and validation about Impala — copious in its quality as well as quantity. At least one person in approximately 4,500 unique organizations around the world have downloaded the Impala binary, to date. And even after only a few months of GA, we�ve seen Cloudera Enterprise customers from multiple industries deploy Impala 1.x in business-critical environments with support via a Cloudera RTQ (Real-Time Query) subscription — including leading organizations in insurance, banking, retail, healthcare, gaming, government, telecom, and advertising. Furthermore, based on the reaction from other vendors in the data management space, few observers would dispute the notion that Impala has made low-latency, interactive SQL queries for Hadoop as important a customer requirement as the high-latency, batch-oriented SQL queries enabled by Apache Hive. That�s a great development for Hadoop users everywhere! What Was Delivered in Impala 1.0/1.1 Let�s begin with a report card on the previously published Impala 1.0/1.1 roadmap. Here�s the feature list, grouped by delivery status:� Delivered: Support for Parquet format, Apache Avro file format, and LZO-compressed TextFiles Support for the same 64-bit OS platforms as supported for CDH JDBC driver DDL support Faster, bigger, more memory efficient joins Faster, bigger, more memory efficient aggregations More SQL performance optimizations Postponed based on customer feedback: Straggler handling Automatic metadata refresh   Furthermore, thanks to the addition of the Apache Sentry module�(incubating), Impala 1.1 and later now also provide granular, role-based authorization, ensuring that the right users and applications have access to the right data. (With the recent contribution of Sentry to the Apache Incubator and of HiveServer2 to Hive by Cloudera, Hive 0.11 and later have that functionality, as well.) A lot of work was done, but there is still plenty of work to do. Now, on to the Impala 2.0 wave. Near-Term Roadmap The following new Impala functionality will be released incrementally across near-term future releases, starting with Impala 1.2 in late 2013 and ending with Impala 2.0 in the first third of 2014. In addition, you�ll see more performance gains and SQL functionality enhancements in each release � with the goal of expanding Impala�s performance lead over the alternative SQL-on-Hadoop approaches of legacy relational database vendors as well as Hadoop distro vendors. Please note, as is always the case with roadmaps, that timelines and features are always subject to change. What you see below captures our current plan-of-record, however. Impala 1.2 UDFs and extensibility � enables users to add their own custom functionality; Impala will support existing Hive Java UDFs as well as high-performance native UDFs and UDAFs Automatic metadata refresh � enables new tables and data to seamlessly be available for Impala queries as they are added without having to issue a manual refresh on on each Impala node In-memory HDFS caching – allows access to frequently accessed Hadoop data at in-memory speeds Cost-based join order optimization � frees the user from having to guess the correct join order Preview of YARN-integrated resource manager — allows prioritization of workloads at a finer granularity than the service-level isolation currently provided in Cloudera Manager Impala 2.0 The list below captures only the bigger, most frequently requested features; it�s by no means complete. SQL 2003-compliant analytic window functions (aggregation OVER PARTITION) � to provide more advanced SQL analytic capabilities Additional authentication mechanisms � including the ability to specify username/passwords in addition to the already supported Kerberos authentication UDTFs (user-defined table functions) � for more advanced user functions and extensibility Intra-node parallelized aggregations and joins � to provide even faster joins and aggregations on on top of the performance gains of Impala Nested data � enables queries on complex nested structures including maps, structs, and arrays Enhanced, production-ready, YARN-integrated resource manager Parquet enhancements � continued performance gains including index pages Additional data types � including Date and Decimal types ORDER BY without LIMIT clauses Beyond Impala 2.0 The following list of features are those that we currently anticipate will be present in 2.1 or a release soon thereafter: Additional analytic SQL functionality � ROLLUP, CUBE, and GROUPING SET Apache HBase CRUD � allows use of Impala for inserts and updates into HBase External joins using disk � enables joins between tables to spill to disk for joins that require join tables larger than the aggregate memory size Subqueries inside WHERE clauses As we learn more about customer and partner requirements, this list will expand. Conclusion As you can see, Impala has evolved considerably since its beta release, and it will continue to evolve as we gather more feedback from users, customers, and partners. Ultimately, we believe that Impala has already enabled our overall goal of allowing users to store all their data in native Hadoop file formats, and simultaneously run all batch, machine learning, interactive SQL/BI, math, search, and other workloads on that data in place. From here, it�s just a matter of continuing to build upon that very solid foundation with richer functionality and improved performance. Justin Erickson is a director of product management at Cloudera.</snippet></document><document id="117"><title>Customer Spotlight: ISS� Wes Caldwell Speaks at Cloudera Sessions in Denver</title><url>http://blog.cloudera.com/blog/2013/09/customer-spotlight-iss-wes-caldwell-speaks-at-cloudera-sessions-in-denver/</url><snippet>This week�s Cloudera Sessions roadshow will make it to Denver, Colo., on Thursday, where the customer Fireside Chat will feature Intelligent Software Solutions (ISS) Chief Architect of Global Enterprise Solutions, Wes Caldwell. ISS helps many government organizations�– including several within the U.S. Department of Defense — deploy next-generation data management and analytic solutions using a combination of systems integration expertise and custom-built software. During the Fireside Chat, Cloudera�s COO Kirk Dunn will engage Wes in a conversation to discuss the business use cases for Hadoop that ISS sees most often in the field, primarily within two buckets: batch analytics and real-time applications. Wes will also share his thoughts on some of the more recent innovations within the Apache Hadoop ecosystem, such as Cloudera Impala and Solr integrations. If you are local to the Denver area, it�s not too late to register for Thursday�s event. If you�re planning to attend and would like to suggest topics or questions for discussion, particularly during the Fireside Chat with Wes, comment here or tweet using hashtag #ClouderaSessions. Learn more about The Cloudera Sessions Learn more about ISS See the organizations ISS supports We hope to see you Thursday! Karina Babcock is Cloudera�s customer programs and marketing manager.�</snippet></document><document id="118"><title>How-to: Use the HBase Thrift Interface, Part 1</title><url>http://blog.cloudera.com/blog/2013/09/how-to-use-the-hbase-thrift-interface-part-1/</url><snippet>There are various way to access and interact with Apache HBase. Most notably, the Java API�provides the most functionality. But some people want to use HBase without Java. Those people have two main options: One is the Thrift interface�(the more lightweight and hence faster of the two options), and the other is the REST interface�(aka Stargate). A REST interface uses HTTP verbs to perform an action. By using HTTP, a REST interface offers a much wider array of languages and programs that can access the interface. (If you’d like more information about the REST interface, you can go to my series of how-to�s about it.) In this series of how-to�s you�ll learn your way around the Thrift interface and explore Python code samples for doing that. This first post will cover HBase Thrift, working with Thrift, and some boilerplate code for connecting to Thrift. The second post will show how to insert and get multiple rows at a time. The third post will explain how to use scans and some considerations when choosing between REST and Thrift. The full code samples can be found on my GitHub account. HBase Thrift Thrift is a software framework that allows you to create cross-language bindings. In the context of HBase, Java is the only first-class citizen. However, the HBase Thrift interface allows other languages to access HBase over Thrift by connecting to a Thrift server that interfaces with the Java client. For both Thrift and REST to work, another HBase daemon needs to be running to handle these requests. These daemons can be installed with the hbase-thrift and hbase-rest packages.� The diagram below shows how Thrift and REST are placed in the cluster.� Note that the Thrift and REST client hosts usually don’t run any other services (such as DataNodes or RegionServers) to keep the overhead low and responsiveness high for REST or Thrift interactions. Make sure to install and start these daemons on nodes that have access to both the Hadoop cluster and the application that needs access to HBase. The Thrift interface doesn’t have any built-in load balancing, so all load balancing will need to be done with external tools such a DNS round-robin, a virtual IP address, or in code. Cloudera Manager also makes it really easy to install and manage the HBase REST and Thrift services. You can download and try it out for free in Cloudera Standard! The downside to Thrift is that it’s more difficult to set up than REST. You will need to compile Thrift and generate the language-specific bindings. These bindings are nice because they give you code for the language you are working in — there’s no need to parse XML or JSON like in REST; rather, the Thrift interface gives you direct access to the row data. Another nice feature is that the Thrift protocol has native binary transport; you will not need to base64 encode and decode data. To start using the Thrift interface, you need to figure out which port it’s running on. The default port for CDH is port 9090.� For this post, you’ll see the host and port variables used, here are the values we’ll be using: host = "localhost"
port = "9090"
   You can�set up�the Thrift interface�to use Kerberos credentials for better security. For your code, you’ll need to use the IP address or fully qualified domain name of the node and the port running the Thrift daemon. I highly recommend making this URL a variable as it could change with network changes. Language Bindings Before you can create Thrift bindings, you must download and compile Thrift. There are no binary packages for Thrift that I could find, except on Windows. You will have to follow Thrift’s instructions for the installation on your platform of choice. Once Thrift is installed, you need to find the Hbase.thrift file. To define the services and data types in Thrift, you have to create an IDL file.� Fortunately, the HBase developers already created one for us. Unfortunately, the file isn’t distributed as part of the CDH binary packages. (We will be fixing that in a future CDH release.) You will need to download the source package of the HBase version you are using. Be sure to use the correct version of HBase as this IDL could change.� In the compressed file, the path to the IDL is hbase-VERSION/src/main/resources/org/apache/hadoop/hbase/thrift/Hbase.thrift. Thrift supports generating language bindings for more than 14 languages including Java, C++, Python, PHP, Ruby, and C#.� To generate the bindings for Python, you would use the following command: thrift -gen py /path/to/hbase/source/hbase-VERSION/src/main/resources/org/apache/hadoop/hbase/thrift/Hbase.thrift
   Next, you will need to get the Thrift code for your language that contains all the classes for connection to Thrift and its protocols. This code can be found at /path/to/thrift/thrift-0.9.0/lib/py/src/. Here are the commands I ran to create a Python project to use HBase Thrift: $ mkdir HBaseThrift
$ cd HBaseThrift/
$ thrift -gen py ~/Downloads/hbase-0.94.2-cdh4.2.0/src/main/resources/org/apache/hadoop/hbase/thrift/Hbase.thrift
$ mv gen-py/* .����������
$ rm -rf gen-py/
$ mkdir thrift
$ cp -rp ~/Downloads/thrift-0.9.0/lib/py/src/* ./thrift/
   I like to keep a copy of the Hbase.thrift file in the project to refer back to.� It has a lot of “Javadoc” on the various calls, data objects, and return objects. $ cp ~/Downloads/hbase-0.94.2-cdh4.2.0/src/main/resources/org/apache/hadoop/hbase/thrift/Hbase.thrift
     Boilerplate Code You’ll find that all your Python Thrift scripts will look very similar. Let’s go through each part. from thrift.transport import TSocket
from thrift.protocol import TBinaryProtocol
from thrift.transport import TTransport
from hbase import Hbase
   These will import the Thrift and HBase modules you need. # Connect to HBase Thrift server
transport = TTransport.TBufferedTransport(TSocket.TSocket(host, port))
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)
   This creates the socket transport and line protocol and allows the Thrift client to connect and talk to the Thrift server. # Create and open the client connection
client = Hbase.Client(protocol)
transport.open()
   These lines create the Client object you will be using to interact with HBase. From this client object, you will issue all your Gets and Puts. Next, open the socket to the Thrift server. # Do Something
   Next you�ll actually work with the HBase client. Everything is constructed, initialized, and connected.� First, start using the client.� transport.close()
   Finally, close the transport. This closes up the socket and frees up the resources on the Thrift server. Here is the code in its entirety for easy copying and pasting: from thrift.transport import TSocket
from thrift.protocol import TBinaryProtocol
from thrift.transport import TTransport
from hbase import Hbase

# Connect to HBase Thrift server
transport = TTransport.TBufferedTransport(TSocket.TSocket(host, port))
protocol = TBinaryProtocol.TBinaryProtocolAccelerated(transport)

# Create and open the client connection
client = Hbase.Client(protocol)
transport.open()

# Do Something

transport.close()
   In HBase Thrift’s Python implementation, all values are passed around as strings. This includes binary data like an integer. All column values are held in the TCell object. Here is the definition in the Hbase.thrift file: struct TCell{
� 1:Bytes value,
� 2:i64 timestamp
}
   Notice the change to a string when the Python code is generated: thrift_spec = (
��� None, # 0
��� (1, TType.STRING, 'value', None, None, ), # 1
��� (2, TType.I64, 'timestamp', None, None, ), # 2
)
   I wrote a helper method�to make it easier to deal with 32-bit integers. To change an integer back and forth between a string, you use these two methods. # Method for encoding ints with Thrift's string encoding
def encode(n):
���� return struct.pack("i", n)

# Method for decoding ints with Thrift's string encoding
def decode(s):
���� return struct.unpack('i', s)[0]
   Keep this caveat in mind as you work with binary data in Thrift. You will need to convert binary data to strings and vice versa. Erroring Out It�s not as easy as it could be to understand errors in the Thrift interface. For example, here’s the error that comes out of Python when a table is not found: Traceback (most recent call last):
� File "./get.py", line 17, in &lt;module&gt;
��� rows = client.getRow(tablename, "shakespeare-comedies-000001")
� File "/mnt/hgfs/jesse/repos/DevHivePigHBaseVM/training_materials/hbase/exercises/python_bleets_thrift/hbase/Hbase.py", line 1038, in getRow
��� return self.recv_getRow()
� File "/mnt/hgfs/jesse/repos/DevHivePigHBaseVM/training_materials/hbase/exercises/python_bleets_thrift/hbase/Hbase.py", line 1062, in recv_getRow
��� raise result.io
hbase.ttypes.IOError: IOError(_message='doesnotexist')
   All is not lost though because you can look at the HBase Thrift log file. On CDH, this file is located at /var/log/hbase/hbase-hbase-thrift-localhost.localdomain.log. In the missing table example, you would see an error in the Thrift log saying the table does not exist. It�s inconvenient, but you can debug from there. In the next installment, I’ll cover inserting and getting rows. Jesse Anderson is an instructor for Cloudera University.</snippet></document><document id="119"><title>Get Hired as a Certified Data Scientist</title><url>http://blog.cloudera.com/blog/2013/09/get-hired-as-a-certified-data-scientist/</url><snippet>To paraphrase Nate Silver: “There is lots of data coming. Who will speak for all this data?” Nearly every day, I read new articles about how Big Data is “changing everything.” Data scientists are unlocking new approaches that help researchers find the cure for cancer, banks fight fraud, the police fight drug-related crimes, and fantasy sports leaguers fight each other. It seems like all I need is an analytics platform like Apache Hadoop and a big pile of data, and actionable insights will just leap out at me, right? Well… not quite.�Hadoop makes the difficult easy and the impossible merely difficult. However, we still have to know what we�re looking for and, once we’ve found it, understand what the results mean. The volume, velocity, and variety of Big Data make it hard to know where to focus and even harder to represent insights in a way that is consumable without sacrificing detail. Finding meaningful patterns and converting them into actionable insights requires plenty of computers, sophisticated software, and experts who can use these tools to coax answers from all our information. That is the realm of data science. Data Science Defined An aspiring data scientist needs a highly sought but difficult-to-attain combination of skills. Like other scientists, a data scientist produces a hypothesis, runs an experiment, and looks at the results to determine whether the hypothesis holds true. In the Big Data space, though, the underlying processes are not quite so straightforward for three main reasons: Gathering enough perspective on a massive data set to generate a hypothesis can be a significant endeavor itself.� Data science is most often analytical, not experimental, meaning the data has already been gathered as the very first step.�This fact makes the notion of a controlled experiment impossible. Instead, data scientists have to do a form of experimental reverse engineering through careful modeling. The real work only begins after a data scientist has proven a hypothesis and discovered a useful pattern in the data.� The true challenge lies in turning that pattern into a data product that can be used to analyze new data or perform ongoing predictive analysis. To be successful, an aspiring data scientist needs a highly sought but difficult-to-attain combination of skills: statistics, programming, machine learning, and multiple technologies (such as Hadoop, R, and visualization tools).�Moreover, the best data scientists distinguish themselves and create value for their companies by applying softer skills like domain expertise (life sciences, behavior classification, climate science), storytelling, and personal qualities like curiosity, resourcefulness, persistence, and mental dexterity. It�s a lot to ask for, and that’s why the likes of the McKinsey Global Institute, Harvard Business Review, and Gartner Group project a shortage in the hundreds of thousands of individuals with data science skills over the next few years. Signal to Noise and Wheat from Chaff Further complicating the supply/demand imbalance for data scientists is the absence of data scientist professional accreditations to verify capabilities.�A small handful of universities have begun to offer degrees in advanced analytics and data science, but these programs are works-in-progress, have yet to graduate substantial numbers, and do not certify the mix of skills and experience required of a working data scientist beyond the classroom.�There is no “International Board of Data Science” or “Data Science Institute,” and the vast majority of managers responsible for hiring data scientists have no data science experience themselves, so a r�sum� and interview alone will prove little.�The dual problems of talent gap and talent non-verifiability will only become more pronounced as smaller businesses begin to accumulate Big Data and seek firepower in building sophisticated tools for it. The dual problems of talent gap and talent non-verifiability will only become more pronounced. One part of the solution is a formalized data science curriculum built by actual data scientists.�Cloudera offers an excellent three-day Introduction to Data Science course that teaches the fundamentals and trains participants to build their own recommender systems based on insights from data science stars like Jeff Hammerbacher and Josh Wills.�Another part of the solution is public data science competitions, through which individuals build experience and demonstrate their chops in a realistic setting.� A Challenge to Shape the Industry But how much education and practice is enough when it comes to a job whose starting salary is regularly reported around $300,000 per year?�This is where a formal industry certification would be most valuable, giving businesses a known yardstick by which to measure practitioners of the trade.�At Cloudera, we�re drawing on our industry leadership and early corpus of real-world experience to address this gap.�We recently introduced a two-part Cloudera Certified Professional: Data Scientist (CCP:DS) program, consisting of a traditional multiple-choice exam and a Web Analytics Challenge, focusing on classification, clustering, and collaborative filtering. The challenge runs for three months and will be followed by a new challenge one quarter later.�Beyond the shadow of a doubt, the participants who successfully complete any CCP:DS Challenge will be verifiably among the world�s most employable (and extremely sexy) data scientists.� The first CCP:DS Challenge is open to participants until the end of September 2013, following which we will offer a variety of learning tools to understand the solutions and prepare for future challenges and exams. We�re even planning a Data Scientist Takeover of the Cloudera booth at Strata + Hadoop World 2013 on the evening of Mon., Oct. 28, to celebrate the inaugural CCP:DS Challenge, so please plan to join us in New York City for drinks, announcements, and a toast by Josh Wills. Sarah Sproehnle is a vice president at Cloudera, responsible for training and certification programs.</snippet></document><document id="120"><title>Understanding Connectors and Drivers in the World of Sqoop</title><url>http://blog.cloudera.com/blog/2013/09/understanding-connectors-and-drivers-in-the-world-of-sqoop/</url><snippet>Note: This post was originally published at blogs.apache.org in a slightly different form. Apache Sqoop is a tool for doing highly efficient data transfers between relational databases and the Apache Hadoop ecosystem. One significant benefit of Sqoop is that it�s easy to use and can work with a variety of systems inside as well as outside of that ecosystem. Thus, with one tool, you can import or export data from all databases supporting the JDBC interface with the same command-line arguments exposed by Sqoop. Furthermore, Sqoop was designed to be modular, allowing you to plug in specialized additions to optimize transfers for particular database systems. Many users use the words “connector” and “driver” interchangeably, but these words mean completely different things in the context of Sqoop — which�can lead to confusion because both connectors and drivers are needed for every Sqoop invocation. This blog post will explain the difference between them, and how Sqoop uses these concepts when transferring data between Hadoop and other systems. Definitions Driver: In Sqoop, the word �driver� refers to a JDBC Driver. JDBC is a standard Java API for accessing relational databases and some data warehouses. The Java language prescribes only what classes and methods this interface contains and the JDK does not have any default implementation. So, each database vendor is responsible for writing its own implementation that will communicate with the corresponding database with its native protocol. For this reason, JDBC drivers often carry restrictive licenses that prohibit redistribution. As a result, each user needs to download the drivers separately and install them into Sqoop. Connector: Despite existence of a SQL standard, every database vendor has its own dialect. The basics usually work the same way across databases, although some edge conditions might be implemented differently. Also, as SQL is a very general query processing language, it might not always be optimal for importing or exporting data out of the database server. Connectors allow Sqoop to transcend SQL dialects while optimizing data transfer. A �connector� is a pluggable piece that fetches metadata about transferred data (columns, associated data types, and so on) to drive data transfer in the most efficient manner. The most basic connector that ships with Sqoop is Generic JDBC Connector, and as the name suggests, it uses only the JDBC interface for accessing metadata and transferring data. As a result, this connector will work on most databases out of the box — but may not be optimal for your use case. Sqoop also ships with specialized connectors for MySQL, PostgreSQL, Oracle Database, Microsoft SQL Server, IBM DB2, and Netezza, so there is usually no need to download extra connectors. However, there are additional connectors available that can add support for additional database systems or improve the performance of built-in connectors. Example: Connecting to the Database To demonstrate how drivers and connectors are used, let�s take a look at how Sqoop connects to the database. Depending on specified command-line arguments and all available connectors, Sqoop will try to load the one offering the best performance. This process begins with Sqoop scanning all extra manually downloaded connectors to confirm if one can be used. If there are no manually installed connectors present or no installed connectors identify themselves as candidates, Sqoop will check the JDBC URL (usually starting with jdbc:) to see if you are connecting to a database for which a built-in special connector is available. (For example, for the jdbc:mysql:// URL that is used for MySQL, Sqoop will pick up the MySQL Connector, which is optimized for MySQL.) Finally, if no connector is yet confirmed for use, Sqoop will default to the Generic JDBC Connector. If this selection mechanism does not suit your environment, you can use the argument –connection-manager with the class name of any arbitrary connector. However, keep in mind that connectors are usually designed for one specific database vendor and thus, for example, the MySQL Connector won�t work with PostgreSQL. After selecting the connector, the next step is to choose the JDBC driver. As most connectors are specialized for a given database and most databases have only one JDBC driver available, the connector itself determines which driver should be used. (For example, the MySQL connector will always use the MySQL JDBC Driver called Connector/J.) The only exception is the Generic JDBC Connector, which isn�t tied to any database and thus can�t determine what JDBC Driver should be used. In that case, you have to supply the driver name in the –driver parameter on the command line. But be careful: Using the –driver parameter will always force Sqoop to use the Generic JDBC Connector regardless of if a more specialized connector is available. For example, if the MySQL specialized connector would be used because the URL starts with jdbc:mysql://, specifying the –driver option will force Sqoop to use the generic connector instead. As a result, in most cases, you should not need to use the –driver option at all! Finally, after both connector and driver have been determined, the connection between the Sqoop client and the target database can be established, as shown in the diagram below. (Please note that the picture is only describing the process of opening the initial connection and roles of the connector and driver. It�s not describing all connections that Sqoop can open as, for example, the data transfer itself is done inside a MapReduce job and is not executed on the Sqoop client side.) Architecture Overview Now that you know all the pluggable pieces that Sqoop uses to connect to your database server, let�s look at where each piece lives. The connector is a Sqoop-specific plugin, so it�s not surprising that it�s part of the Sqoop installation. In fact, Sqoop ships with a variety of connectors out of the box and various additional extra connectors can be downloaded and installed into Sqoop easily. Even though drivers are database-specific pieces, created and distributed by the various database vendors, they are often not installed along with the database server. Instead, they need to be installed on the machine where Sqoop is executed because they are used by connectors to establish the connection with the database server. (For example, as previously explained, JDBC drivers are not shipped with Sqoop due to incompatible licenses and thus you must download and install one manually.) Appropriate drivers can usually be found on the database vendor�s website or are distributed separately with the database installation itself. Connectors and drivers need to be installed only on the machine where Sqoop is executed; you do not need to install them on all nodes in your Hadoop cluster. Sqoop will propagate them itself whenever necessary. Conclusion I hope that this blog post has clarified the differences between connectors and drivers and why both are needed to transfer data between relational databases and Hadoop ecosystem. Happy Sqooping! Jarek Jarcec Cecho is a Software Engineer on the Platform team, and is a Sqoop Committer/PMC Member.</snippet></document><document id="121"><title>How-to: Manage HBase Data via Hue</title><url>http://blog.cloudera.com/blog/2013/09/how-to-manage-hbase-data-via-hue/</url><snippet>The following post was originally published by the Hue Team at the Hue blog in a slightly different form. In this post, we�ll take a look at the new Apache HBase Browser App added in Hue 2.5 and which has improved significantly since then. To get the Hue HBase browser, grab Hue via CDH 4.4�packages, via Cloudera Manager, or build it directly from GitHub. Prerequisites before starting Hue: Have Hue built or installed. Have HBase and Thrift Service 1 initiated. (Thrift can be configured through Cloudera Manager or manually.) Configure your list of HBase Clusters in hue.ini to point to your Thrift IP/Port. In the demo below, you’ll get a walk-through of the two main features of this app. SmartView The SmartView is where you land when you first enter a table. On the left-hand side are the row keys, and hovering over a row reveals a list of controls on the right. Click a row to select it, and then you can perform batch operations, sort columns, or do multiple common database operations. To explore a row, simple scroll to the right. By scrolling, the row should continue to lazily-load cells until the end. Adding Data To initially populate the table, you can insert a new row or bulk upload CSV files, TSV files, and so on, type data into your table. On the right-hand side of a row is a �+� sign that lets you insert columns into your row. Mutating Data To edit a cell, simply click to edit inline: If you need more control or data about your cell, click Full Editor to edit. In the full editor, you can view cell history or upload binary data to the cell. Binary data of certain MIME types are detected — meaning you can view and edit images, PDFs, JSON, XML, and other types directly in your browser! Hovering over a cell also reveals some more controls (such as the delete button or the timestamp). Click the title to select a few and do batch operations: If you need some sample data to get started and explore, check out this how-to about creating HBase tables. Smart Searchbar The Smart Searchbar is a sophisticated tool that helps you zero-in on your data. Smart Search supports a number of operations; the most basic ones include finding and scanning row keys. The screenshot illustrates selecting two row keys with: domain.100, domain.200
   Submitting this query returns the correct two rows. If you want to fetch rows after one of these, you have to do a scan — which is as easy as writing a �+� followed by the number of rows you want to fetch. Typing in: domain.100, domain.200 +5
   Fetches domain.100 and domain.200 followed by the next five rows. If you�re ever confused about your results, you can look down below and the query bar and also click in to edit your query. The smart search also supports column filtering. On any row, you can specify the specific columns or families you want to retrieve with: domain.100[column_family:]
   I can select a bare family, or mix columns from different families like so: domain.100[family1:, family2:, family3:column_a]
   Doing this will restrict your results from one row key to the columns you specified. If you want to restrict column families only, the same effect can be achieved with the filters on the right. Just click to toggle a filter. Finally, let�s try some more complex column filters. Query for bare columns: domain.100[column_a]
   This will multiply the query over all column families. You can also do prefixes and scans: domain.100[family: prefix* +3]
   This will fetch all columns that start with prefix* limited to three results. Finally, you can filter on range: domain.100[family: column1 to column100]
   This will fetch all columns in �family:� that are lexicographically &gt;= column1 but The smart search also supports prefix filtering on rows. To select a prefixed row, simply type the row key followed by a star *. The prefix should be highlighted like any other searchbar keyword. A prefix scan is performed exactly like a regular scan, but with a prefixed row. domain.10* +10
   Finally, as a new feature, you can take full advantage of the HBase filtering language by typing your filter string between curly braces. HBase Browser autocompletes your filters for you so you don�t have to look them up every time. You can apply filters to rows or scans. domain.1000 {ColumnPrefixFilter('100-') AND ColumnCountGetFilter(3)}
   This post only covers a few basic features of the Smart Search. You can take advantage of the full querying language by referring to the help menu when using the app, including column prefix, bare columns, column range, and so on. Remember that if you ever need help, you can use the help menu that pops up while typing, which will suggest next steps to complete your query. If you want to learn how to create various tables in HBase, read “How to Create Example Tables in HBase”.   Et voila! Feel free to try the app at gethue.com. Let us know what you think on the Hue user group or community forum!</snippet></document><document id="122"><title>How-to: Write an EL Function in Apache Oozie</title><url>http://blog.cloudera.com/blog/2013/09/how-to-write-an-el-function-in-apache-oozie/</url><snippet>When building complex workflows in Apache Oozie, it is often useful to parameterize them so they can be reused or driven from a script, and more easily maintained. The most common method is via ${VAR} variables. For example, instead of specifying the same NameNode for all of your actions in a given workflow, you can specify something like ${myNameNode}, and then in your job.properties file, you would define it like myNameNode=hdfs://localhost:8020. One of the advantages of that approach is that if you want to change the variable (the NameNode in this example), you only have to change it in one place and subsequently all the actions will use the new value. This can be particularly useful when testing in a dev or staging environment where you can simply change a few variables instead of editing the workflow itself. Oozie also provides functions, called EL (Expression Language) functions, for the same purpose. A host of built-in EL functions provide things like the name of the user who submitted the workflow, the external id of a MapReduce job submitted by Oozie, concatenating two strings, and much more. In this blog post, you’ll learn how to write your own EL function. When to Use an EL Function An EL function should be simple, fast, and robust. One of the reasons Oozie uses a MapReduce �launcher� job to run most of its actions is so these large and complicated programs are not executed directly in the Oozie server itself. However, always keep in mind that EL functions are executed in the Oozie server, so you don�t want to overtax or threaten its stability with a poorly conceived one. Some examples of �good� EL functions are ones that perform a simple string operation, arithmetic operation, or return some useful internal value from Oozie. It is also important that your EL function be robust; you don�t want your workflows failing because your function isn�t handling edge cases, is throwing exceptions, or something else undesirable (although doing so won�t bring down Oozie). Always keep in mind that EL functions are executed in the Oozie server. Examples of “bad” EL functions would be ones that create a bunch of threads for doing heavy processing, download gigabytes of data from a remote server, and so on. These actions are far too heavy or brittle, and it would be a bad idea to run them in the Oozie server. If you need to do something like this in a workflow, and possibly use the output for another action, a better alternative is to use the Shell or Java actions with the &lt;capture-output/&gt; element. (See myprevious blog post, “How To: Use Oozie Shell and Java Actions“, for more information on that process.) Conversely, using the Shell or Java actions is overkill for simply concatenating two strings — whereas a simple EL function makes more sense. Writing an EL Function Now, let’s dive into an example of how to write your own extension EL function and how to use it in a workflow. Oozie comes with a number of string-manipulation EL functions, but what if you want to compare two strings to see if they are equal regardless of case? For example, the built-in equality function would consider �Foo� and �fOo� to not be equal — so let’s make a function that would consider them equal. (This is essentially the same as Java�s String#equalsIgnoreCase method. In fact, you�ll be using it in your function.) No specific interface or class needs to be extended; you can simply use a basic class and define static functions in it to be your EL functions. For example, below I’ve created a class named MyELFunctions in the rkanter package and a function named equalsIgnoreCase that takes two Strings and returns a boolean: package rkanter;
public class MyELFunctions {
      public static boolean equalsIgnoreCase(String s1, String s2) {
          if (s1 == null) {
              return (s2 == null);
          }
          return s1.equalsIgnoreCase(s2);
      }
  }
   (You can also find a copy of the above code on GitHub.) As you can see, the function is fairly simple. In order to make the code more robust, it checks that the first String is not null; if it is, then the function returns true if the second string is also null or false if not. This extra check will help prevent NullPointerExceptions or other issues. The next thing to do is compile the code (which would be in rkanter/MyELFunctions.java) into a jar file. There are many ways to compile Java code into a Jar file (Ant, Maven, NetBeans, IntelliJ, Eclipse, and so on), but here you can simply use the javac and jar commands included with the JDK. Compile the java file into a class like this: $ javac rkanter/MyELFunctions.java
   And package the compiled class into a jar file like this: $ jar -cf MyELFunctions.jar rkanter/MyELFunctions.class
   To confirm, our jar file now contains the following: $ jar -tf MyELFunctions.jar
META-INF/
META-INF/MANIFEST.MF
rkanter/MyELFunctions.class
   Configuring Oozie to Use Your EL Function Now that you have an EL function in a jar file, the next step is to configure Oozie to use it. The Oozie server must be restarted to notice the jar file, so first make sure that the Oozie server isn�t running. CDH package/parcel installation: Copy MyELFunctions.jar to /var/lib/oozie/ (or to /usr/lib/oozie/libext/, which is simply a symlink to the former). Tarball installation (CDH or Apache): Copy MyELFunctions.jar to /where/you/deployed/oozie/libext/ (creating libext if it doesn�t exist). Run the �bin/oozie-setup.sh prepare-war� command (in earlier versions of Oozie, this is simply �bin/oozie-setup.sh� with no arguments). Now that Oozie has the jar file, you have to tell Oozie how to use the EL Function. In oozie-site.xml, set the oozie.service.ELService.ext.functions.workflow property (creating it if it doesn�t already exist) as follows: &lt;property&gt;
    &lt;name&gt;oozie.service.ELService.ext.functions.workflow&lt;/name&gt;
    &lt;value&gt;
         equalsIgnoreCase=rkanter.MyELFunctions#equalsIgnoreCase
    &lt;/value&gt;
&lt;/property&gt;
   This property takes a comma-separated list of EL function declarations that you want to add to the original built-in EL functions. The format of a declaration is [PREFIX:]NAME=CLASS#METHOD where PREFIX can be a prefix such as �wf�, NAME is the name that you want to give to your EL function as used in your workflows, CLASS is the class name where you defined your EL Function, and METHOD is the method name of the EL Function in that CLASS. In our example, we didn�t use a prefix, we named the function function �equalsIgnoreCase�, the class is �rkanter.MyELFunctions�, and the method is �equalsIgnoreCase�. Finally, re-start Oozie. Using Your EL Function in a Workflow As an example of using your EL Function in a workflow, let’s take a look at the Shell example workflow included with Oozie: &lt;workflow-app xmlns="uri:oozie:workflow:0.4" name="shell-wf"&gt;
    &lt;start to="shell-node"/&gt;
    &lt;action name="shell-node"&gt;
        &lt;shell xmlns="uri:oozie:shell-action:0.2"&gt;
            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
            &lt;configuration&gt;
                &lt;property&gt;
                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
                    &lt;value&gt;${queueName}&lt;/value&gt;
                &lt;/property&gt;
            &lt;/configuration&gt;
            &lt;exec&gt;echo&lt;/exec&gt;
            &lt;argument&gt;my_output=Hello Oozie&lt;/argument&gt;
            &lt;capture-output/&gt;
        &lt;/shell&gt;
        &lt;ok to="check-output"/&gt;
        &lt;error to="fail"/&gt;
    &lt;/action&gt;
    &lt;decision name="check-output"&gt;
        &lt;switch&gt;
            &lt;case to="end"&gt;
                ${wf:actionData('shell-node')['my_output'] eq 'Hello Oozie'}
            &lt;/case&gt;
            &lt;default to="fail-output"/&gt;
        &lt;/switch&gt;
    &lt;/decision&gt;
    &lt;kill name="fail"&gt;
        &lt;message&gt;Shell action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;
    &lt;/kill&gt;
    &lt;kill name="fail-output"&gt;
        &lt;message&gt;Incorrect output, expected [Hello Oozie] but was [${wf:actionData('shell-node')['my_output']}]&lt;/message&gt;
    &lt;/kill&gt;
    &lt;end name="end"/&gt;
&lt;/workflow-app&gt;
   This workflow simply uses the shell action (�shell-node�) to echo �Hello Oozie�, then goes to a decision node (�check-output�) that will fail if the previous action didn�t actually output �Hello Oozie�. The actual check is done using the following EL Expression: ${wf:actionData('shell-node')['my_output'] eq 'Hello Oozie'}
   If we want to use our new EL function, we can modify the above EL expression using our new caseIgnoresEquals EL Function that won�t care if the shell action outputted �Hello Oozie� or something like �heLLO OoZiE�: ${equalsIgnoreCase('heLLo OoZiE', wf:actionData('shell-node')['my_output'])}
   After making the above change, and submitting the modified shell example workflow, it should still succeed because our equalsIgnoreCase EL Function doesn�t care that the cases don�t match! You can verify that it’s actually working correctly by trying something that truly isn�t equal, such as �foo� and by trying �heLLo OoZiE� in the original EL expression (with the normal equals function) and seeing that the workflow fails in both cases. Conclusion Writing custom EL functions is a powerful way to add new capabilities to make your Oozie workflows more reusable and modular. In this blog post we only saw a very simple example of an EL function but because it’s just Java code, you are free to make much more powerful ones. We also only looked at adding EL functions to workflows, but they can also be added to coordinators. If you think other users might benefit from your EL functions, please feel free to contribute them to the community by creating a patch and uploading it to a new JIRA at http://issues.apache.org/jira/browse/OOZIE. Further Reading: Workflow EL Functions Documentation EL Expression Language Quick Reference Robert Kanter is a Software Engineer on the Platform team and an Apache Oozie Committer/PMC Member.</snippet></document><document id="123"><title>Next Stops for The Cloudera Sessions: Jersey City, Miami, Denver, Milwaukee</title><url>http://blog.cloudera.com/blog/2013/09/next-stops-for-the-cloudera-sessions-jersey-city-miami-denver-milwaukee/</url><snippet>In its first leg of its tour of the United States earlier this year (see photos here), The Cloudera Sessions proved to be an invaluable single-day event for business and technical leaders exploring practical applications of Apache Hadoop. So valuable, in fact, that we’ve extended the tour with dates/cities this September and October. Based on feedback from previous attendees, we’ve customized the agenda to be even more targeted for real-world use cases. Furthermore, we’ve added a new Hadoop application development lab, where developers can get hands-on direction for using the Cloudera Development Kit (CDK) to more easily build apps upon, or integrate existing infrastructure with, the Hadoop platform. Here’s the remaining schedule: Sept. 18: Jersey City Sept. 19: Miami Sept. 26: Denver Oct. 17: Milwaukee You can register here for any of the cities/dates listed above. The Cloudera Sessions are a GREAT day for any LOB owner, IT manager, or developer interested in getting started with Hadoop!</snippet></document><document id="124"><title>How-to: Test HBase Applications Using Popular Tools</title><url>http://blog.cloudera.com/blog/2013/09/how-to-test-hbase-applications-using-popular-tools/</url><snippet>While Apache HBase adoption for building end-user applications has skyrocketed, many of those applications (and many apps generally) have not been well-tested. In this post, you’ll learn some of the ways this testing can easily be done. We will start with unit testing via JUnit, then move on to using Mockito and Apache MRUnit, and then to using an HBase mini-cluster for integration testing. (The HBase codebase itself is tested via a mini-cluster, so why not tap into that for upstream applications, as well?) As a basis for discussion, let’s assume you have an HBase data access object (DAO) that does the following insert into HBase. The logic could be more complicated of course but for the sake of example, this does the job. public class MyHBaseDAO {

        	public static void insertRecord(HTableInterface table, HBaseTestObj obj)
       	throws Exception {
  	              	Put put = createPut(obj);
  	              	table.put(put);
        	}

        	private static Put createPut(HBaseTestObj obj) {
  	              	Put put = new Put(Bytes.toBytes(obj.getRowKey()));
  	              	put.add(Bytes.toBytes("CF"), Bytes.toBytes("CQ-1"),
             	               	Bytes.toBytes(obj.getData1()));
  	              	put.add(Bytes.toBytes("CF"), Bytes.toBytes("CQ-2"),
             	               	Bytes.toBytes(obj.getData2()));
  	              	return put;
        	}
}
   HBaseTestObj is a basic data object with getters and setters for rowkey, data1, and data2. The insertRecord does an insert into the HBase table against the column family of CF, with CQ-1 and CQ-2 as qualifiers. The createPut method simply populates a Put and returns it to the calling method.� Using JUnit JUnit, which is well known to most Java developers at this point, is easily applied to many HBase applications. First, add the dependency to your pom: &lt;dependency&gt;
    &lt;groupId&gt;junit&lt;/groupId&gt;
  ��&lt;artifactId&gt;junit&lt;/artifactId&gt;
  ��&lt;version&gt;4.11&lt;/version&gt;
  ��&lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
   Now, within the test class: public class TestMyHbaseDAOData {
          	@Test
          	public void testCreatePut() throws Exception {
    	  	HBaseTestObj obj = new HBaseTestObj();
    	  	obj.setRowKey("ROWKEY-1");
    	  	obj.setData1("DATA-1");
    	  	obj.setData2("DATA-2");
    	  	Put put = MyHBaseDAO.createPut(obj);
    	  	assertEquals(obj.getRowKey(), Bytes.toString(put.getRow()));
    	  	assertEquals(obj.getData1(), Bytes.toString(put.get(Bytes.toBytes("CF"),
  Bytes.toBytes("CQ-1")).get(0).getValue()));
    	  	assertEquals(obj.getData2(), Bytes.toString(put.get(Bytes.toBytes("CF"),
  Bytes.toBytes("CQ-2")).get(0).getValue()));
          	}
  }
   What you did here was to ensure that your createPut method creates, populates, and returns a Put object with expected values. Using Mockito So how do you go about unit testing the above insertRecord method? One very effective approach is to do so with Mockito. First, add Mockito as a dependency to your pom: &lt;dependency&gt;
  ��&lt;groupId&gt;org.mockito&lt;/groupId&gt;
  ��&lt;artifactId&gt;mockito-all&lt;/artifactId&gt;
  ��&lt;version&gt;1.9.5&lt;/version&gt;
    &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
   Then, in test class: @RunWith(MockitoJUnitRunner.class)
public class TestMyHBaseDAO{
� @Mock�
� private HTableInterface table;
� @Mock
� private HTablePool hTablePool;
� @Captor
� private ArgumentCaptor putCaptor;

� @Test
� public void testInsertRecord() throws Exception {
��� //return mock table when getTable is called
��� when(hTablePool.getTable("tablename")).thenReturn(table);
��� //create test object and make a call to the DAO that needs testing
��� HBaseTestObj obj = new HBaseTestObj();
��� obj.setRowKey("ROWKEY-1");
��� obj.setData1("DATA-1");
��� obj.setData2("DATA-2");
��� MyHBaseDAO.insertRecord(table, obj);
��� verify(table).put(putCaptor.capture());
��� Put put = putCaptor.getValue();
��
��� assertEquals(Bytes.toString(put.getRow()), obj.getRowKey());
��� assert(put.has(Bytes.toBytes("CF"), Bytes.toBytes("CQ-1")));
��� assert(put.has(Bytes.toBytes("CF"), Bytes.toBytes("CQ-2")));
��� assertEquals(Bytes.toString(put.get(Bytes.toBytes("CF"),
Bytes.toBytes("CQ-1")).get(0).getValue()), "DATA-1");
��� assertEquals(Bytes.toString(put.get(Bytes.toBytes("CF"),
Bytes.toBytes("CQ-2")).get(0).getValue()), "DATA-2");
� }
}
   Here you have populated HBaseTestObj with �ROWKEY-1�, �DATA-1�, �DATA-2� as values. You then used the mocked table and the DAO to insert the record. You captured the Put that the DAO would have inserted and verified that the rowkey, data1, and data2 are what you expect them to be. The key here is to manage htable pool and htable instance creation outside the DAO. This allows you to mock them cleanly and test Puts as shown above. Similarly, you can now expand into all the other operations such as Get, Scan, Delete, and so on. Using MRUnit With regular data access unit testing covered, let�s turn toward MapReduce jobs that go against HBase tables. Testing MR jobs that go against HBase is as straightforward as testing regular MapReduce jobs. MRUnit makes it really easy to test MapReduce jobs including the HBase ones. Imagine you have an MR job that writes to an HBase table, “MyTest”, which has one column family, “CF”. The reducer of such a job could look like: public class MyReducer extends TableReducer&lt;Text, Text, ImmutableBytesWritable&gt; {
   public static final byte[] CF = "CF".getBytes();
   public static final byte[] QUALIFIER = "CQ-1".getBytes();
  public void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException {
     //bunch of processing to extract data to be inserted, in our case, lets say we are simply
     //appending all the records we receive from the mapper for this particular
     //key and insert one record into HBase
     StringBuffer data = new StringBuffer();
     Put put = new Put(Bytes.toBytes(key.toString()));
     for (Text val : values) {
         data = data.append(val);
     }
     put.add(CF, QUALIFIER, Bytes.toBytes(data.toString()));
     //write to HBase
     context.write(new ImmutableBytesWritable(Bytes.toBytes(key.toString())), put);
   }
 }
   Now how do you go about unit-testing the above reducer in MRUnit? First, add MRUnit as a dependency to your pom. &lt;dependency&gt;
   &lt;groupId&gt;org.apache.mrunit&lt;/groupId&gt;
   &lt;artifactId&gt;mrunit&lt;/artifactId&gt;
   &lt;version&gt;1.0.0 &lt;/version&gt;
   &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
     Then, within the test class, use the ReduceDriver that MRUnit provides as below: public class MyReducerTest {
��� ReduceDriver&lt;Text, Text, ImmutableBytesWritable, Writable&gt; reduceDriver;
��� byte[] CF = "CF".getBytes();
��� byte[] QUALIFIER = "CQ-1".getBytes();

��� @Before
��� public void setUp() {
����� MyReducer reducer = new MyReducer();
����� reduceDriver = ReduceDriver.newReduceDriver(reducer);
��� }
��
�� @Test
�� public void testHBaseInsert() throws IOException {
����� String strKey = "RowKey-1", strValue = "DATA", strValue1 = "DATA1",�
strValue2 = "DATA2";
����� List&lt;Text&gt; list = new ArrayList&lt;Text&gt;();
����� list.add(new Text(strValue));
����� list.add(new Text(strValue1));
����� list.add(new Text(strValue2));
����� //since in our case all that the reducer is doing is appending the records that the mapper���
����� //sends it, we should get the following back
����� String expectedOutput = strValue + strValue1 + strValue2;
���� //Setup Input, mimic what mapper would have passed
����� //to the reducer and run test
����� reduceDriver.withInput(new Text(strKey), list);
����� //run the reducer and get its output
����� List&lt;Pair&lt;ImmutableBytesWritable, Writable&gt;&gt; result = reduceDriver.run();
����
����� //extract key from result and verify
����� assertEquals(Bytes.toString(result.get(0).getFirst().get()), strKey);
����
����� //extract value for CF/QUALIFIER and verify
����� Put a = (Put)result.get(0).getSecond();
����� String c = Bytes.toString(a.get(CF, QUALIFIER).get(0).getValue());
����� assertEquals(expectedOutput,c );
�� }

}
   Basically, after a bunch of processing in MyReducer, you verified that:   The output is what you expect. The Put that is inserted in HBase has “RowKey-1″ as the rowkey. “DATADATA1DATA2″ is the value for the CF column family and CQ column qualifier.   You can also test Mappers that get data from HBase in a similar manner using MapperDriver, or test MR jobs that read from HBase, process data, and write to HDFS. Using an HBase Mini-cluster Now we’ll look at how to go about integration testing. HBase ships with HBaseTestingUtility, which makes writing integration testing with an HBase mini-cluster straightforward. In order to pull in the correct libraries, the following dependencies are required in your pom: � &lt;dependency&gt;
  ��&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  ��&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;
  ��&lt;version&gt;2.0.0-cdh4.2.0&lt;/version&gt;
  ��&lt;type&gt;test-jar&lt;/type&gt;
  ��&lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt;
  ��&lt;artifactId&gt;hbase&lt;/artifactId&gt;
  ��&lt;version&gt;0.94.2-cdh4.2.0&lt;/version&gt;
  ��&lt;type&gt;test-jar&lt;/type&gt;
  ��&lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
  ������
&lt;dependency&gt;
   �&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  ��&lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
  ��&lt;version&gt;2.0.0-cdh4.2.0&lt;/version&gt;
  ��&lt;type&gt;test-jar&lt;/type&gt;
  ��&lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
  ��&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  ��&lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;
  ��&lt;version&gt;2.0.0-cdh4.2.0&lt;/version&gt;
  ��&lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
   Now, let’s look at how to run through an integration test for the MyDAO insert described in the introduction: public class MyHBaseIntegrationTest {
private static HBaseTestingUtility utility;
byte[] CF = "CF".getBytes();
byte[] QUALIFIER = "CQ-1".getBytes();

@Before
public void setup() throws Exception {
	utility = new HBaseTestingUtility();
	utility.startMiniCluster();
}

@Test
    public void testInsert() throws Exception {
   	 HTableInterface table = utility.createTable(Bytes.toBytes("MyTest"),
   			 Bytes.toBytes("CF"));
   	 HBaseTestObj obj = new HBaseTestObj();
   	 obj.setRowKey("ROWKEY-1");
   	 obj.setData1("DATA-1");
   	 obj.setData2("DATA-2");
   	 MyHBaseDAO.insertRecord(table, obj);
   	 Get get1 = new Get(Bytes.toBytes(obj.getRowKey()));
   	 get1.addColumn(CF, CQ1);
   	 Result result1 = table.get(get1);
   	 assertEquals(Bytes.toString(result1.getRow()), obj.getRowKey());
   	 assertEquals(Bytes.toString(result1.value()), obj.getData1());
   	 Get get2 = new Get(Bytes.toBytes(obj.getRowKey()));
   	 get2.addColumn(CF, CQ2);
   	 Result result2 = table.get(get2);
   	 assertEquals(Bytes.toString(result2.getRow()), obj.getRowKey());
   	 assertEquals(Bytes.toString(result2.value()), obj.getData2());
    }}
   Here you created an HBase mini-cluster and started it. You then created a table called “MyTest” with one column family, “CF”. You inserted a record using the DAO you needed to test, did a Get from the same table, and verified that the DAO inserted records correctly. The same could be done for much more complicated use cases along with the MR jobs like the ones shown above. You can also access the HDFS and ZooKeeper mini-clusters created while creating the HBase one, run an MR job, output that to HBase, and verify the inserted records. Just a quick note of caution: starting up a mini-cluster takes 20 to 30 seconds and cannot be done on Windows without Cygwin. However, because they should only be run periodically, the longer run time should be acceptable.� You can find sample code for the above examples at https://github.com/sitaula/HBaseTest. Happy testing! Sunil Sitaula is a Solutions Architect for Cloudera.  </snippet></document><document id="125"><title>New Webinar Series Featuring Cloudera and Partner Solutions</title><url>http://blog.cloudera.com/blog/2013/09/new-webinar-series-for-cloudera-partners/</url><snippet>Welcome to the Cloudera Connect Webinar Series! Cloudera’s platform touches every part of your data management infrastructure, so it’s critical that it works well with partner technologies to create value beyond simple integration. We need to make sure that 1 + 1 &gt; 2. Join Cloudera and its partners for a new series of webinars to learn how to leverage your existing technologies, or identify new solutions, to create an end-to-end data management solution. We will cover topics including how to: Enable data processing from various data sources Use business intelligence tools on top of Apache Hadoop data Uncover insights from all your data with predictive analytics Each webinar will help organizations find the right track to understand, recognize value, and cost-effectively deploy a Big Data solution. Cloudera Connect Webinar Series Calendar � How to Leverage Mainframe Data with Hadoop: Bridging the Gap Between Big Iron and Big Data Sept. 25 9am PT /12pm ET Join Syncsort and Cloudera to learn the top challenges of ingesting and processing mainframe data in Hadoop � and how to solve them. Mainframe-to-Hadoop connectivity: How to read files directly from the mainframe and load them into HDFS Data translation: How to understand and translate mainframe data into Hadoop-friendly format Data processing: How to easily develop and deploy common data flows with mainframe data in Hadoop Deployment and administration: How to deploy, administer, and monitor your Hadoop environment with Cloudera Manager � Get Started with Hadoop in Less Than 30 Minutes Sept. 26 9am PT /12pm ET Join Global Knowledge for this free, hour-long webinar and get on your way to becoming a Hadoop rock star by deploying CDH in the cloud as a proof of concept. Benefits and value of Cloudera Manager Cloudera Manager (Standard) and additional features enabled with Enterprise support Live demo (installing a 10-node cluster) � R + Hadoop:�Ask Bigger (and New) Questions and Get Better, Faster Answers Oct. 3 11am PT/2pm ET Join Revolution Analytics and Cloudera in this webinar to learn how to: Un-accept the status quo and break away from dependence on legacy tools� Bring the power of R � including Revolution Analytics� ScaleR library of Hadoop-ready Big Data analytics algorithms � to Cloudera�s CDH Apply this joint solution to real-life scenarios to get better and faster results by asking bigger questions, asking new questions, and obtaining and sharing better results faster � How Big Data Analytics &amp; Hadoop Complement Your Existing Data Warehouse Oct. 16 10am PT/1pm ET Join Datameer and Cloudera in this webinar to discuss how Hadoop and Big Data analytics can help to: Get all the data your business needs quickly into one environment Shorten the time to insight from months to days Extend the life of your existing data warehouse investments Enable your business analysts to ask and answer bigger questions   Sandy Lii is Cloudera’s�solution marketing manager.</snippet></document><document id="126"><title>Customer Spotlight: Hear from eBay, LiveRamp, Comcast, and OCLC at Cloudera Sessions</title><url>http://blog.cloudera.com/blog/2013/09/customer-spotlight-hear-from-ebay-liveramp-comcast-and-oclc-at-cloudera-sessions/</url><snippet>We�re kicking off the second leg of our Cloudera Sessions roadshow this week, starting in San Francisco on Wednesday and Philadelphia�on Friday. The spring series of the Cloudera Sessions was a big hit, which is why we�re back with a new and improved agenda for the fall, to offer even more options that will help attendees — ranging from developers to line-of-business managers and executives — navigate the Big Data journey. The expanded fall series agenda includes an application development lab (based on CDK) that coincides with the general session throughout the morning, and two tracks for clinics after lunch. One portion of the general session that was a big hit throughout the spring series and that will return this fall is the Fireside Chat, during which the Cloudera executive host sits with one or two customers to talk about their �real life� experiences and lessons learned with Apache Hadoop. The Fireside Chat gives local customers an opportunity to showcase the work they�re doing, and allows attendees to hear from real users what worked, what didn�t, how they got started with Hadoop, and best practices learned along the way. The Cloudera Sessions� Fireside Chat in San Francisco will feature a conversation among: Jeremy Lizt, LiveRamp�s vice president of engineering Pradeep Sankaranthi, software development manager of eBay�s search back-end team Amr Awadallah, Cloudera�s co-founder and CTO As one of Cloudera�s very first customers, Jeremy brings a breadth of expertise with Hadoop to the conversation. He will talk about LiveRamp�s requirements for a large-scale infrastructure to support what it calls �data onboarding� in the marketing technology space, and how it has deployed Hadoop at the core of its data management infrastructure today. Pradeep represents another mature Hadoop user. He�ll share three core use cases for Hadoop within eBay�s search team and how it isleveraging Hadoop to meet strict SLAs and maintain a reliable, stable system with high site traffic and throughput requirements. Then, on Friday, we�ll take the Cloudera Sessions to Philadelphia, where the Fireside Chat will feature: Edward Niescior, Comcast�s manager of business analytics Ron Buckley, OCLC�s software architect Kirk Dunn, Cloudera�s COO Edward will discuss the infrastructure challenges faced by Comcast with their legacy infrastructures, how it is using Hadoop to complement the existing environment, and best practices learned in the deployment process. Ron, representing the worldwide library cooperative OCLC, will explain how it’s using Apache HBase and Hadoop as the system of record to support very high transaction volumes — such as check-in and check-out of library books — at a global scale. Please feel free to comment here with questions that you�d like our guest speakers to address. We hope to see you at the Cloudera Sessions in a city near you! Karina Babcock is Cloudera�s customer programs &amp; marketing manager.�</snippet></document><document id="127"><title>Community Meetups at Strata Conference + Hadoop World 2013</title><url>http://blog.cloudera.com/blog/2013/09/community-meetups-at-strata-conference-hadoop-world-2013/</url><snippet>Strata Conference + Hadoop World 2013�(Oct. 28-30 in New York City) approaches (register here for an automatic 20% discount), and that means it’s time to get your meetup schedule sorted out! There are a variety of them planned across the week (something for everyone!), onsite at the conference hotel as well as offsite. Use the links below to RSVP. (YES, there will be food, adult refreshments, and T-shirts.) Mon., Oct. 28 Apache Sqoop User Meetup Where: Hilton Hotel, Hudson Suite When: 6:30pm Apache Hive User Group Meeting Where: Hilton Hotel, East Suite When: 6:30pm Apache HBase NYC Where: Facebook offices When: 7pm New York Hadoop User Group Where: AppNexus offices When: 7pm Apache Accumulo User Group Where: Hilton Hotel, Room TBD When: 6pm Tues., Oct. 29 Hue Users and Hadoop UI Meetup Where: Hilton Hotel, Cloudera booth When: 2pm Cloudera Manager Meetup Where: Hilton Hotel, Hudson Suite When: 6:30pm Apache Flume User Meetup Where: Hilton Hotel, East Suite When: 6:30pm Parquet Format + Impala NYC Meetup Where: Projective Space LES When: 6:30pm NYC Pig User Group Where: Hilton Hotel, Room TBD When: 7pm� New York Hadoop User Group (YARN) Where: Hilton Hotel, Beekman Parlor/Sutton North When: 7pm� Weds., Oct. 30 Apache Sentry Meetup Where: Oracle offices When: 6:30pm Now, who’s going? PS: For more information about Cloudera’s investments in user groups and meetups generally, see this page.</snippet></document><document id="128"><title>Cloudera Manager 4.7 Released</title><url>http://blog.cloudera.com/blog/2013/09/cloudera-manager-4-7-released/</url><snippet>Cloudera Manager 4.7 is an update to Cloudera Manager 4 and contains a number of bug fixes and usability improvements. Furthermore, we have introduced new features such as: Support for Sentry (Impala and Hive Authorization) with CDH 4.3.0 or later New Home page to offer a clusterwide status of all issues Support for HBase Indexer for Cloudera Search New Parcel usage page to give a more granular view into parcel usage across the cluster More granular controls around alerting for various health states Improved user experience for Events and Audits pages For more details, refer to the Release Notes. All of the Cloudera Manager 4.7 documentation can be found here. You can download Cloudera Manager 4.7 from here or you can upgrade from an existing installation of Cloudera Manager using the documentation here. As always, we are happy to hear your feedback. Please send your comments and suggestions to scm-users@cloudera.org or through our new�community forums.</snippet></document><document id="129"><title>This Month in the Ecosystem (August 2013)</title><url>http://blog.cloudera.com/blog/2013/09/this-month-in-the-ecosystem-august-2013/</url><snippet>Welcome to our second edition of “This Month in the Ecosystem.” (See the inaugural edition here.) Although August was not as busy as July, there are some very notable highlights to report. Hadoop 2 Goes to Beta Clearly a highlight of the summer: Apache Hadoop 2.1.0 became the first beta release of Hadoop 2 — big group hug for the community! A “GA” is expected within a couple months. Read more about the Hadoop 2 beta Search Becomes a First-Class Citizen on Hadoop With the release of Cloudera Search 1.0 and the contribution of related integration work upstream, Google-style search now joins SQL and Java as the most mainstream options for accessing data in HDFS and HBase. And non-technical end users rejoice. Learn more about the Cloudera Search 1.0 release Strata Conference + Hadoop World 2013 Enters the Viewfinder The program for Strata Conference + Hadoop World, Oct. 28-30 in New York, came into full view. You’ll find that the agenda has a decidedly more “real world” flavor in the past, with greater emphasis on production deployments. (A variety of community meetups are also planned; plan to attend a few of those.) A 20% registration discount is available by using the link below. Register/learn more about Strata + Hadoop World | Explore the Cloudera agenda and meetup schedule Twitter Open-Sources Summingbird As Twitter describes it, Summingbird lets you “write MapReduce programs that look like native Scala or Java collection transformations and execute them on a number of well-known distributed MapReduce platforms, including Storm and�Scalding.” Summingbird carries the Apache License. Explore the Summingbird project The next installment of “This Month in the Ecosystem” will publish in early October. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="130"><title>Cloudera Search is Now Generally Available</title><url>http://blog.cloudera.com/blog/2013/09/cloudera-search-is-now-generally-available/</url><snippet>After three months of public beta, and months of private beta before that, Cloudera Search is now generally available. At this milestone, Cloudera has contributed its innovations and IP around the integration of Apache Solr and Apache Lucene with CDH back to the respective upstream projects. The GA of Cloudera Search also signifies the completion of a vast amount of hardening, integration, simplification, and packaging work. Features of Cloudera Search 1.0 include: Cluster-level access control (Kerberos authentication and impersonation)� Batch indexing via MapReduce Near real-time indexing at ingest via Apache Flume and Apache HBase Cloudera Manager integration for better production visibility and monitoring SolrCloud deployment capabilities on HDFS (such as index creation, storage, and serving from HDFS) Simplified data preparation for indexing across workloads (via Cloudera Morphlines) A simple and configurable search application in Hue� Index aliasing (ability to handle sliding-window data through index mapping) GoLive feature (ability to start serving indexes generated by MapReduce to running Solr services on the cluster as they become readily available) Solr/CDH platform packaging …and much more; read the full list of features in the Release Notes. Cloudera Search is now a first-class citizen in Cloudera’s platform, supported on CDH 4.3 and later and deployed with Cloudera Manager 4.7. Having free text search on your data processing platform opens the door to new use cases and, at the same time, a less costly infrastructure. Although these are worthy achievements, the road doesn�t end here — we’re already looking at new features and further platform integration that will help customers achieve better insights on unified infrastructure. But before we jump into the next phase of search on Apache Hadoop, we would like to thank our supporting beta customers and participants. You have truly been an inspiration. Thanks for all the great feedback, and we look forward to continuing our great technical partnership! Eva Andreasson is a senior product manager at Cloudera.</snippet></document><document id="131"><title>How-to: Use Cloudera Enterprise with StackIQ</title><url>http://blog.cloudera.com/blog/2013/09/how-to-use-cloudera-standard-with-stackiq/</url><snippet>StackIQ takes a �software defined infrastructure� approach to provision and manage cluster infrastructure that sits below Big Data platforms such as Apache Hadoop. In the guest post below, StackIQ co-founder and VP Engineering Greg Bruno explains how to install Cloudera Enterprise on top of StackIQ�s management system so they can work together. The hardware used for this deployment is a small cluster: one node (i.e. one server) for the StackIQ Cluster Manager and four nodes as backend/data nodes. Each node has two disks and all nodes are connected via 1Gb Ethernet on a Private Network. The Cluster Manager node is also connected to a Public Network using its second NIC. (StackIQ Cluster Manager is used in similar deployments between two nodes and 4,000+ nodes in size.) Step 1. Install StackIQ Cluster Manager The StackIQ Cluster Manager node is installed from bare-metal (i.e. there is no prerequisite software and no operating system previously installed) by burning the StackIQ Cluster Core Roll ISO to DVD and booting from it. (Your can download the StackIQ Cluster Core Roll from the �Rolls� section after registering.) The Core Roll leads the user through a few simple forms (e.g., what is the IP address of the Cluster Manager, what is the gateway, DNS server) and then asks for a base OS DVD (for example, Red Hat Enterprise Linux 6.4; other Red Hat-like distributions such as CentOS are supported as well). The installer copies all the bits from both DVDs and automatically creates a new Red Hat distribution by blending the packages from both DVDs. The remainder of the Cluster Manager installation requires no further manual actions and this entire step takes between 30 to 40 minutes. Step 2. Install the CDH Bridge Roll StackIQ has developed software that �bridges� its core infrastructure management solution to Cloudera Enterprise — which comprises CDH and Cloudera Manager — that we�ve named the CDH Bridge Roll. One feature of our management solution is that it records several parameters about each backend node (number of CPUs, networking configuration, disk partitions, etc.) in a local database. After the Cluster Manager is installed and booted, it is time to download and install the CDH Bridge Roll: After the Cluster Manager is installed and booted, it�s time to download and install the CDH Bridge Roll: Log into the frontend as "root", download cdh-bridge iso from StackIQ Downloads. 
# wget http://stackiq-release.s3.amazonaws.com/stack2/cdh-bridge-4-1.x86_64.disk1.iso
   Then execute the following commands: 
# rocks add roll
&lt;path_to_iso&gt;
# rocks enable roll cdh-bridge
# rocks create distro
# rocks run roll cdh-bridge | sh
   The cluster is now configured to install Cloudera packages on all nodes. Step 3. Install Cloudera Manager and Cloudera CDH4 Roll You can download a prepackaged Cloudera Manager here and a prepackaged CDH 4 from here. We will now install these two ISOs.  
# rocks add roll cloudera-cdh4/cloudera-cdh4-6.5-0.x86_64.disk1.iso
# rocks add roll cloudera-manager/cloudera-manager-6.5-0.x86_64.disk1.iso
# rocks enable roll cloudera-cdh4
# rocks enable roll cloudera-manager
# rocks create distro
# rocks run roll cloudera-cdh4 | sh
# rocks run roll cloudera-manager | sh
   Step 4. Install the Backend Nodes Before we install the backend nodes (also known as compute nodes), we want to ensure that all disks in the backend nodes are optimally configured for HDFS. During an installation of a data node, our software interacts with the disk controller to optimally configure it based on the node’s intended role. For data nodes, the disk controller will be configured in “JBOD mode” with each disk configured as a RAID 0, a single partition will be placed on each data disk and a single file system will be created on that partition. For example, if a data node has one boot disk and 4 data disks, after the node installs and boots, you’ll see the following 4 file systems on the data disks: /hadoop01, /hadoop02, /hadoop03 and /hadoop04. For more information on this feature, see our blog post Why Automation is the Secret Ingredient for Big Data Clusters. Now we don’t want to reconfigure the controller and reformat disks on every installation, so we need to instruct the StackIQ Cluster Manager to perform this task the next time the backend nodes install. We do this by setting an attribute (“nukedisks”) with the rocks command line: # rocks set appliance attr compute nukedisks true
# rocks set appliance attr cdh-manager nukedisks true
   Now we are ready to install the backend nodes. First we put the StackIQ Cluster Manager into "discovery" mode using the CLI or GUI and all backend nodes are PXE booted. We will boot the first node as a cdh-manager appliance. The cdh-manager node will run the Cloudera Manager web admin console used to configure, monitor and manage CDH. We will install all the other nodes in the cluster as compute nodes. StackIQ Cluster Manager discovers and installs each backend node in parallel (10 to 20 minutes) – no manual steps are required. (For more information on installing and using the StackIQ Cluster Manager (a.k.a., Rocks+), please visit StackIQ Support or watch the demo video.) After all the nodes in the cluster are up and running you will be ready to install Cloudera Manager. In this example, the StackIQ Cluster Manager node was named “frontend” and the compute nodes were assigned default names of compute-0-0, compute-0-1, compute-0-2 (3 nodes in Rack 0), and compute-1-0 (1 node in Rack 1). Step 5. Install Cloudera Manager To install Cloudera Manager on the frontend, as root, execute: 
# /opt/rocks/sbin/cloudera-manager-installer.bin --skip_repo_package=1
   This will install Cloudera Manager with packages from our local yum repository as opposed to fetching packages over the Internet. Step 6. Select What to Install Login to the frontend http://FQDN:7180 (where ‘’ is the FQDN of your StackIQ Cluster Manager) with username admi’ and password ‘admin’. Cloudera Manager will take you through a full CDH install. When you are done, you will have a working Hadoop instance on your cluster capable of running Hadoop-based analysis. You can see the full step-by-step install guide here.  Step 7. Run a Hadoop Sample Program It is never enough to set-up a cluster and the applications users need and then let them have at it. There are generally nasty surprises all around when this happens. A validation check is a requirement to make sure everything is working as expected. � Do this to confirm the cluster is functional: Log into the frontend as root via SSH or Putty. On the command line, run the following MapReduce program as the �hdfs� user, which runs a simulation to estimate the value of pi based on sampling: # sudo -u hdfs hadoop jar /usr/lib/hadoop-0.20-mapreduce/hadoop-examples.jar pi 10 10000
   The output should look something like this: A complete version of the steps involved to deploy real machines with StackIQ and Cloudera (with pictures!) is available from the StackIQ blog. We�re certain you�ll find this one of the quickest ways to deploy a cluster capable of running Cloudera�s platform. Give a it shot and send us your questions! �</snippet></document><document id="132"><title>New Cloudera and Cisco Reference Architecture Now Available</title><url>http://blog.cloudera.com/blog/2013/09/new-cloudera-and-cisco-reference-architecture-now-available/</url><snippet>Cloudera and Cisco are announcing a joint solution today, the Cisco Validated Design (CVD) for Cloudera Enterprise reference architecture. What�s our reasoning behind this new reference architecture? While the competitive pressure on enterprises vastly increases, the amount of data being ingested and managed has exploded and is accelerating quickly. At the same time, the need for timely and more accurate analytics has also increased. As a result, the need for a cost-effective, flexible and scalable infrastructure to store and process data has never been greater. We�re partnering to deliver tested and certified Hadoop infrastructure solutions and ongoing support that help take the time and risk out of deploying Apache Hadoop. The solutions provide a comprehensive, enterprise-class platform for Hadoop applications powered by Cloudera Enterprise, tested by Cisco and certified by the Cloudera Certified Technology program to streamline deployment and reduce risk. Cloudera Enterprise is a complete enterprise analytic data management platform comprised of Cloudera�s enterprise-grade 100% open source Hadoop distribution, CDH, and comprehensive management software, Cloudera Manager. The software versions used in this CVD are CDH 4.1.3 and Cloudera Manager 4.5. Together with Cisco, Cloudera�s software complements the Cisco UCS Common Platform Architecture for Big Data to accelerate deployment of Hadoop by storing and analyzing more data, ensuring optimal deployment of the infrastructure through UCS Manager and Cloudera Manager, accelerating time to value for our customers. You can download the Cisco Validated Design (CVD) for Cloudera Enterprise, and we�d like to hear your feedback letting us know what you think.</snippet></document><document id="133"><title>Distributed Systems Get Simpler with Apache Helix</title><url>http://blog.cloudera.com/blog/2013/09/distributed-systems-get-simpler-with-apache-helix/</url><snippet>Our thanks to Kishore Gopalakrishna, staff engineer at LinkedIn and one of the original developers of Apache Helix (incubating), for the introduction below. Cloudera�s Patrick Hunt is a mentor for the project. With the trend of exploding data growth and the systems in the NoSQL and Big Data space, the number of distributed systems has grown significantly. At LinkedIn, we have built a number of distributed systems over the years. Such systems run on a cluster of multiple servers and need to handle the problems that come with distributed systems. Fault tolerance � that is, availability in the presence of server failures and network problems — is critical to any such system. Horizontal scalability and seamless cluster expansion to handle increasing workloads are also essential properties. Without a framework that provides these capabilities, developers of distributed platforms have to continually re-implement features that support them. From LinkedIn�s own experiences building various distributed systems and having to solve the same problems repeatedly, we decided to build a generic platform to simply this process � which we eventually called Helix (and which is now in the Apache Incubator). Concept Helix introduces the following terminologies that allow you to define a distributed system and express its behavior. A set of autonomous processes, each of which is referred to as an�instance, collectively constitute a�cluster. The participants collectively perform or support a task, which is referred to as a resource. A�resource�is a logical entity that can span many Instances. For example, a database, topic, index or any computation/serving task can be mapped to a resource. A resource can be further sub-divided into sub-resources or sub-tasks, which are referred to as�partitions. A partition�is the largest part of a resource that can be associated with an instance, which implies a partition�cannot span multiple�instances. For fault tolerance and/or scalability, multiple copies of a partition are possible. Each copy is referred to as a�replica. The above terminology is sufficiently generic to be mapped to any distributed system. But distributed systems vary significantly in their function and behavior. For example, a search system that serves only queries might behave quite differently from a database that supports reads on mutable data. So, to effectively describe the behavior of a distributed system, we need to define various states for the resources and partitions of a distributed system and capture the description of all its valid states and legal state�transitions. Helix accomplishes this task by providing the ability to plug in a Finite-State Machine (FSM). The following figure illustrates Helix terminology. Replicas and nodes (aka instances) are color-coded to indicate the nodes they exist on; for example, P1.r2 is a slave and it sits on Node 4: An FSM by itself is not sufficient to describe all the invariants in the system — we still need a way to capture the desired system�behavior�when the cluster topology changes. For example, when a node fails, we could choose to create new replicas or change the state of existing replicas. When new nodes are added to the cluster, we could re-distribute the partitions to new nodes at once or incrementally. These are nontrivial problems that make it very difficult for programmers to correctly program system behavior for all possible scenarios. Helix implements the concept of constraints and objectives: One can define the constraints on each state and transitions in the FSM. Furthermore, you can specify constraints at various granularities such as partition, resource, instance, and cluster. Here are some examples of constraints and objectives: State constraints: Partition scope: 1 Master, 2 Slave Instance scope: Max 10 partitions on each node Transition constraints: Max number of bootstrap (copy data over network) transitions in a cluster=5 Objectives: Even distribution of partitions Replicas on different nodes/racks On node failure/addition/removal redistribute partitions evenly with minimal movement Based on these concepts, we can define cluster management this way: Given a set of instances and resources, along with some constraints on the system state, compute an assignment of partitions to instances such that all the constraints are satisfied. Based on that definition, we can envision cluster management as a constraint satisfaction problem. In general, multiple solutions can satisfy the given constraints, and in order to reduce the solution space, we can enforce some optimization rules or objectives. IdealState In Helix parlance, a solution that satisfies the constraints and meets the objectives is called the IdealState of the system: "TASK_NAME" : {���� "LOCATION" : "STATE"�� }
   It simply describes the mapping of TASK to an INSTANCE and STATE. For example, if you need one master and two slaves and three nodes are up (N1,N2,N3), the solution can be: "myTask_0" : {������ "N1" : "MASTER",���� },���� "myTask_1" : {������ "N2" : "SLAVE",���� },���� "myTask_2" : {������ "N3" : "SLAVE",���� }
   Modeling a distributed system as a state machine with constraints on state and transitions has the following benefits: It separates cluster management from core functionality. It quickly transforms a single-node system to an operable, distributed system. System components do not have to manage the global cluster. This division of labor makes it easier to build, debug, and maintain your system. See this doc for a detailed explanation of Helix concepts. Roles Helix divides distributed system components into three logical roles: Controller:�The controller is the core component in Helix and serves as the �brain� of the system. It hosts the state machine engine, runs the execution algorithm, and issues transitions against the distributed system. The controller is responsible for computing the IdealState that satisfies the given constraints and objectives. If the IdealState differs from the current state of the system, the controller computes the transitions required to take the system from its CurrentState to IdealState. This process is repeated on every change in the cluster, planned as well as unplanned: starting/stopping of nodes, adding or losing nodes, adding or deleting resources, and adding or deleting partitions, among others. Participant: Each instance in the cluster that performs the core functionality is called a participant. The participants run the Helix library that�invokes callbacks whenever the�controller initiates a state transition on the participant. The distributed system is responsible for implementing those callbacks. In this way,�Helix remains agnostic to the semantics of a state transition or how to actually implement the transition. For example,�in a LeaderStandby model, the participant implements methods OnBecomeLeaderFromStandby and OnBecomeStandbyFromLeader. Spectator:�This component represents the external entities that need to observe the state of the system. A spectator is notified of the state changes in the system, so that they can interact with the appropriate�participant.�For example, a routing/proxy component (a spectator) can be notified when the routing table has changed, such as when a cluster is expanded or a node fails. It�s important to note that these are just logical components and can be physically co-located within the same process. Cluster state�is maintained in Apache ZooKeeper. Use Cases at LinkedIn LinkedIn uses Helix extensively to manage our backend systems, such as Espresso (a distributed NoSQL data store), Databus (a change data capture system), and SEAS (Search as a Service). Espresso is a distributed, timeline consistent, scalable, document store that supports local secondary indexing and�local transactions. Espresso runs on a number of storage node servers that store and index data and answer queries. Espresso�databases are horizontally partitioned into a number of partitions, with each partition having a certain number of replicas�distributed across the storage nodes. Databus provides a common pipeline for transporting events from�LinkedIn�s primary databases (Espresso and Oracle Database) to downstream applications like caches and search systems. Each Databus partition is assigned to a consumer such that partitions are�evenly distributed across consumers and each partition is assigned to exactly one consumer at a time. The set of consumers�may grow over time, and consumers may leave the group due to planned or unplanned outages. In these cases, partitions�must be reassigned, while maintaining balance and the single consumer-per-partition invariant. LinkedIn�s SEAS lets internal customers define custom indexes on a chosen dataset and�then makes those indexes searchable via a service API. The index service runs on a cluster. The index is broken�into partitions and each partition has a configured number of replicas. Distributed System Recipes The Helix team has written a few recipes to demonstrate common usage patterns. Helix comes with pre-defined state models such as MasterSlave, OnlineOffline, and LeaderStandby and a recipe is available for each model. Distributed lock manager (LeaderStandBy model): With this recipe, you can simply define the number of locks needed and Helix will distribute them among the live nodes. Note: even though Helix uses Zookeeper, we never elect the leader using zookeeper ephemeral nodes. Instead, the leader is selected by the controller. Consumer grouping (OnlineOffline model): Consumer grouping is a simple recipe for load balancing the consumption of messages with fault tolerance from a pub/sub system. This recipe uses Rabbit MQ but can be applied to any pub/sub system. Rsync-based replicated file system (MasterSlave model): This recipe demonstrates a system where files written to one server are automatically replicated to another slave using rsync. The client always writes data to the Master and it gets replicated. It also provides fault tolerance — if a Master dies, another Slave becomes the Master. Questions? The Helix concept is a bit different from the traditional way of building distributed systems. But once the concept is clear, we are confident that you will see the same benefits we saw at LinkedIn. We still have a long way to go and are making improvements to design and refactoring our APIs as we see more use cases outside of LinkedIn. We welcome your feedback and contributions via the mailing lists (user@helix.incubator.apache.org,�dev@helix.incubator.apache.org). Further Reading: Paper: �Untangling Cluster Management with Helix�</snippet></document><document id="134"><title>What I Learned During My Summer Internship at Cloudera, Part 2</title><url>http://blog.cloudera.com/blog/2013/08/what-i-learned-during-my-summer-internship-at-cloudera-part-2/</url><snippet>The guest post below is from Wei Yan, a 2013 summer intern at Cloudera. In this post, he helpfully describes his personal projects from this summer. Thanks for your contributions, Wei! As a Ph.D. student at Vanderbilt University, I work on the Apache Hadoop MapReduce framework, with a focus on optimizing data intensive computing tasks. Although I�m very familiar with MapReduce itself, my curiosity about the use cases for MapReduce and where it generally fits in the Big Data are drew me to Cloudera for the summer of 2013. At Cloudera, I mainly worked on two projects: a Hadoop YARN Scheduler Load Simulator and a testing framework called Hadoop MiniKDC. In the remainder of this post, I�ll describe my experiences with each and what I learned from them. Predicting YARN Scheduler Performance My first project was to build a simulator that evaluates Hadoop YARN scheduling algorithms. YARN is a platform for hosting applications in Hadoop 2, and provides resource management for various data processing frameworks, such as Hadoop MapReduce. The YARN scheduler is a major component in Hadoop that matches available resources with resource requests. It has multiple implementations such as FIFO and Capacity and Fair schedulers, with each implementation driving scheduling decisions based on many different factors, such as fairness, capacity guarantee, and so on. Thus, selecting an appropriate implementation and configuring it correctly brings many benefits (better system throughput and resource utilization, for example). Conversely, a poor selection may hurt system performance. Thus it is very important to evaluate scheduler implementation and configuration well before it is deployed in a production cluster. Unfortunately, currently it is nontrivial to evaluate a scheduler implementation. Evaluating in a real cluster is always costly and time consuming, and it is also very hard for researchers to find a sufficiently large cluster. Hence, a simulator that can predict how well a scheduler implementation works for some specific workload would be quite useful. A simulator that can predict how well a YARN scheduler implementation works would be quite useful. The objective of this project was to build a YARN Scheduler Load Simulator to simulate large-scale YARN clusters and application loads in a single machine (YARN-1021). This would be invaluable in furthering YARN by providing a tool for researchers and developers to prototype new scheduler features and predict their behavior and performance with a reasonable amount of confidence, thereby aiding rapid innovation. The first challenge here was to determine the right level of abstraction — that is, to determine which components from Hadoop I would execute and which components I would simulate. It is unrealistic to launch real Node Managers (NMs) and Application Masters (AMs) in the simulator; as each NM and AM requires considerable resources (i.e., CPU and memory), launching several NMs and AMs would quickly use up all the available resources in a single machine. Thus, I needed to define an appropriate abstraction level that could support large-scale clusters and application loads in a single machine, without affecting the scheduler algorithm semantics and behaviors. The simulator exercises the real YARN resource manager and removes the network factor by simulating NMs and AMs via handling and dispatching NM/AM heartbeat events from within the same JVM. By simulating NMs and AMs, the simulator can limit its required CPU and memory. Using the real resource manager where the scheduler algorithm resides, the simulator can largely maintain the original scheduler running semantics. Defining the metrics used for scheduler evaluation was another major challenge because tracking these metrics should not bring much overhead to the scheduler�s own execution. Furthermore, these metrics should provide enough information for developers and researchers to evaluate a scheduler algorithm. Based on the above, the simulator provides three types of metrics, including: Resource usage for the whole cluster and each queue, which can be utilized to configure cluster and queue capacity The detailed application execution trace (recorded in relation to simulated time), which can be analyzed to understand/validate the scheduler behavior (individual jobs turn around time, throughput, fairness, capacity guarantee) Several key metrics of scheduler algorithm, such as time cost of each scheduler operation (allocate, handle, and so on), which can be utilized by Hadoop developers to find the code performance bottlenecks and scalability limits Furthermore, I built a web app inside the simulator that enables users to track these metrics in real time when the simulator is running. Working on the simulator was a great opportunity for me to understand in detail the design and implementation of the YARN framework, including event-trigger architecture, the decoupling of resource management and application, and so on. Another lesson was how to actually deliver the project. The first demo did not have the online tracking service; users could only analyze the metrics hours after the simulator finished. Eventually, I designed and implemented a web app for users to track the simulator in real time. The video below presents a demo of the simulator and the real-time tracking feature: Making Kerberos Security Testing Easier My second project was to build a “mini KDC” (HADOOP-9848) for Kerberos security testing in the Hadoop ecosystem. Currently, lots of security test cases in Hadoop are disabled, as they require running a Kerberos Key Distribution Center (KDC). Setting up a KDC takes time and that process is hard to automate. Working on the simulator was a great opportunity to learn the design and implementation of YARN. The MiniKDC builds an embedded KDC using Apache Directory Server, and allows the creation of principals and keytabs on the fly. Developers can start and stop a KDC inside their Java code directly. It fulfills a detailed requirement in the Hadoop project, and can be integrated with several other projects. An important lesson for me here was how to design APIs. Simple but functional APIs can help developers easily integrate MiniKDC with other projects. Learning How to Contribute At Cloudera, I also learned how to get involved in, and contribute to, an open source project like Hadoop. I started with a simple HDFS JIRA to remove an unnecessary warning message from the logs. This JIRA helped me to get familiar with the development protocol and whole lifecycle, from creating a Hadoop issue, building Hadoop, testing my changes, and posting a patch for others to review. After that, I worked on several JIRAs in different areas of Hadoop (Common, YARN, MapReduce). Working on these JIRAs gave me the opportunity to join and collaborate with the Hadoop open source community, where I can easily publish my own work and gain useful feedback from other contributors. Another benefit of interacting with the community is that you can have discussions with people from different companies, which is a great opportunity to collect requirements from different use case scenarios. Back to School All in all, I had a great time working on these projects, and interacting with all the brilliant people I�ve had the pleasure to meet at Cloudera. I have learned so much in this summer. I saw how a software company operates and learned the art and practice of open source software development. Cloudera has provided me with a more complete picture of the diversity of industry�s data and needs, which will also help me with my research when I return to school. Read about 2013 summer intern Catherine Ray’s experiences here!</snippet></document><document id="135"><title>Meet the Instructor: Nathan Neff</title><url>http://blog.cloudera.com/blog/2013/08/meet-the-instructor-nathan-neff/</url><snippet>In this installment of �Meet the Instructor,� we speak to St. Louis-based Nathan Neff, the Training Lead for Cloudera�s new Data Analyst course.� What is your role at Cloudera? I’m an instructor teaching almost all of Cloudera’s curricula: Developer, Administrator, Data Analyst, HBase, and Hadoop Essentials. I�m currently gearing up to start delivering Cloudera�s Introduction to Data Science training, which, from an instructor�s perspective, is a pretty exciting challenge.� Most of the classes I teach are live and in-person, but I’ve also recorded screencasts and helped design multimedia courseware for Cloudera’s customers, which was a lot of fun. Right now, my primary role is Training Lead for Cloudera’s new Data Analyst course. I worked closely with our curriculum team during design and development to ensure the best possible experience for every student. I taught the train-the-trainer and first public classes, and provided participant feedback and guidance from my own experience. My goal is to help create the best course available to analysts, developers, and administrators who want to work with Big Data at scale and in real time using Hadoop tools. Now that the course has been released, my responsibility is twofold: ensure that Cloudera�s Data Analyst Training stays up-to-date with the rapidly changing ecosystem, and help our instructors with the tools to deliver a fun, challenging, and consistent class. What do you enjoy most about training and/or curriculum development? Like most technical instructors, I enjoy meeting new people who see opportunity in skills advancement. It�s a treat to teach students who are as excited about the technology as I am. My biggest reward is hearing about all the cool new ways people and organizations are using Hadoop and Big Data in practice.� In addition to teaching, curriculum development helps me remain an active and contributing member of the Hadoop community. One of the qualities that typifies Cloudera is that we not only manage the largest Hadoop knowledge base, but also work to broaden access in any way we can. A relevant and state-of-the-art training curriculum is a fantastic way to spread the word on the latest and greatest in the platform.�As often as I can, I provide additional questions, exercises, and challenges to students, and I’ve now started screencasting tips and tricks for our Data Analyst instructors when I find a clever way of teaching a subject or point from our classes. Describe an interesting application you�ve seen or heard about for Apache Hadoop. The applications of Hadoop that I find most interesting are those that hold the potential to positively impact huge numbers of people, particularly projects related to genome sequencing. A student in a recent class discussed the benefits and risks of knowing our genetic makeup: prevention and early treatment of diseases, increased life expectancy, issues surrounding privacy and ownership of all this information.�Hadoop has essentially made possible the storage, management, and analysis of vast quantities of data that help us understand human biology. Over time, Hadoop will enable the collection of all the information stored in the cells and behaviors of every living thing on Earth, which could have amazing implications for humanitarianism, science, and health. What advice would you give to an engineer, system administrator, or analyst who wants to learn more about Big Data? The best advice is to engage with people who are similarly interested Big Data. Cloudera University classes are a great place to meet other people delving into Hadoop-based scenarios and hear first-hand accounts and use cases from other students and our widely experienced instructors. The Cloudera Developer Center has tons of resources for people at any stage of their Hadoop journey, and the newly launched Community Forums are the online water cooler for everyone using CDH. I also recommend finding user groups in your city that specialize in Hadoop, Big Data, machine learning, data science, or ecosystem projects like Cloudera Impala, Apache HBase, and Apache Pig. Start by exploring meetup.com for a Hadoop User Group near you. Finally, I suggest diving in head first! Cloudera has a fantastic QuickStart VM available for download that has Hadoop and the ecosystem components already set up.� Think of some kind of Big Data project you’d like to work on and get to it!� Just remember that you’re never done learning. How did you become involved in technical training and Hadoop? My previous experience as an instructor came at Bworks�in St. Louis, where, for five years, I taught computer classes to kids in fifth through eighth grades. I wrote the curriculum for the classes, including a class where students wrote a video game using M.I.T.�s excellent Scratch programming tool. I learned about Hadoop while working for a startup that specialized in comparing the features of and making recommendations about enterprise software.� We used Hadoop, HBase, and Apache Solr to collect and serve this huge and diverse set of data to customers who wanted objective comparisons and evaluations of the relevant software products available to them. What�s one interesting fact or story about you that a training participant would be surprised to learn? I play guitar in an Irish band called Rusty Nail.� We released an album and gig around the St. Louis area. I even recently found out that an Oakland A’s pitcher liked our album and uses one of our songs as his warm-up and entrance soundtrack during home games.� That’s really cool!</snippet></document><document id="136"><title>How-to: Select the Right Hardware for Your New Hadoop Cluster</title><url>http://blog.cloudera.com/blog/2013/08/how-to-select-the-right-hardware-for-your-new-hadoop-cluster/</url><snippet>One of the first questions Cloudera customers raise when getting started with Apache Hadoop is how to select appropriate hardware for their new Hadoop clusters. Although Hadoop is designed to run on industry-standard hardware, recommending an ideal cluster configuration is not as easy as delivering a list of hardware specifications. Selecting hardware that provides the best balance of performance and economy for a given workload requires testing and validation. (For example, users with IO-intensive workloads will invest in more spindles per core.) In this blog post, you�ll learn some of the principles of workload evaluation and the critical role it plays in hardware selection. You�ll also learn the various factors that Hadoop administrators should take into account during this process. Marrying Storage with Compute Over the past decade, IT organizations have standardized on blades and SANs (Storage Area Networks) to satisfy their grid and processing-intensive workloads. While this model makes a lot of sense for a number of standard applications such as web servers, app servers, smaller structured databases, and data movement, the requirements for infrastructure have changed as the amount of data and number of users has grown. Web servers now have caching tiers, databases have gone massively parallel with local disk, and data movement jobs are pushing more data than they can handle locally. Most teams building a Hadoop cluster don�t yet know the eventual profile of their workload. Hardware vendors have created innovative systems to address these requirements including storage blades, SAS (Serial Attached SCSI) switches, external SATA arrays, and larger capacity rack units. However, Hadoop is based on a new approach to storing and processing complex data, with data movement minimized. Instead of relying on a SAN for massive storage and reliability then moving it to a collection of blades for processing, Hadoop handles large data volumes and reliability in the software tier. Hadoop distributes data across a cluster of balanced machines and uses replication to ensure data reliability and fault tolerance. Because data is distributed on machines with compute power, processing can be sent directly to the machines storing the data. Since each machine in a Hadoop cluster stores as well as processes data, those machines need to be configured to satisfy both data storage and processing requirements. Why Workloads Matter In nearly all cases, a MapReduce job will either encounter a bottleneck reading data from disk or from the network (known as an IO-bound job) or in processing data (CPU-bound). An example of an IO-bound job is sorting, which requires very little processing (simple comparisons) and a lot of reading and writing to disk. An example of a CPU-bound job is classification, where some input data is processed in very complex ways to determine ontology. Here are several more examples of IO-bound workloads: Indexing Grouping Data importing and exporting Data movement and transformation Here are several more examples of CPU-bound workloads: Clustering/Classification Complex text mining Natural-language processing Feature extraction Because Cloudera�s customers need to thoroughly understand their workloads in order to fully optimize Hadoop hardware, a classic chicken-and-egg problem ensues. Most teams looking to build a Hadoop cluster don�t yet know the eventual profile of their workload, and often the first jobs that an organization runs with Hadoop are far different than the jobs that Hadoop is ultimately used for as proficiency increases. Furthermore, some workloads might be bound in unforeseen ways. For example, some theoretical IO-bound workloads might actually be CPU-bound because of a user�s choice of compression, or different implementations of an algorithm might change how the MapReduce job is constrained. For these reasons, when the team is unfamiliar with the types of jobs it is going to run, as an initial approach it makes sense to invest in a balanced Hadoop cluster. The next step would be to benchmark MapReduce jobs running on the balanced cluster to analyze how they�re bound. To achieve that goal, it�s straightforward to measure live workloads and determine bottlenecks by putting thorough monitoring in place. We recommend installing Cloudera Manager on the Hadoop cluster to provide real-time statistics about CPU, disk, and network load. (Cloudera Manager is an included component of Cloudera Standard and Cloudera Enterprise — in the latter case with enterprise functionality, such as support for rolling upgrades, in place.) With Cloudera Manager installed, Hadoop administrators can then run their MapReduce jobs and check the Cloudera Manager dashboard to see how each machine is performing. � The first step is to know which hardware your operations team already manages. In addition to building out a cluster appropriate for the workload, we encourage customers to work with their hardware vendor to understand the economics of power and cooling. Since Hadoop runs on tens, hundreds, or thousands of nodes, an operations team can save a significant amount of money by investing in power-efficient hardware. Each hardware vendor will be able to provide tools and recommendations for how to monitor power and cooling. Selecting Hardware for Your CDH Cluster The first step in choosing a machine configuration is to understand the type of hardware your operations team already manages. Operations teams often have opinions or hard requirements about new machine purchases, and will prefer to work with hardware with which they�re already familiar. Hadoop is not the only system that benefits from efficiencies of scale. Again, as a general suggestion, if the cluster is new or you can�t accurately predict your ultimate workload, we advise that you use balanced hardware. There are four types of roles in a basic Hadoop cluster: NameNode (and Standby NameNode), JobTracker, TaskTracker, and DataNode. (A node is a machine performing a particular task.) Most machines in your cluster will perform two of these roles, functioning as both DataNode (for data storage) and TaskTracker (for data processing). Here are the recommended specifications for DataNode/TaskTrackers in a balanced Hadoop cluster: 12-24 1-4TB hard disks in a JBOD (Just a Bunch Of Disks) configuration 2 quad-/hex-/octo-core CPUs, running at least 2-2.5GHz 64-512GB of RAM Bonded Gigabit Ethernet or 10Gigabit Ethernet (the more storage density, the higher the network throughput needed) The NameNode role is responsible for coordinating data storage on the cluster, and the JobTracker for coordinating data processing. (The Standby NameNode should not be co-located on the NameNode machine for clusters and will run on hardware identical to that of the NameNode.) Cloudera recommends that customers purchase enterprise-class machines for running the NameNode and JobTracker, with redundant power and enterprise-grade disks in RAID 1 or 10 configurations. The NameNode will also require RAM directly proportional to the number of data blocks in the cluster. A good rule of thumb is to assume 1GB of NameNode memory for every 1 million blocks stored in the distributed file system. With 100 DataNodes in a cluster, 64GB of RAM on the NameNode provides plenty of room to grow the cluster. We also recommend having HA configured on both the NameNode and JobTracker, features that have been available in the CDH4 line for some time. Here are the recommended specifications for NameNode/JobTracker/Standby NameNode nodes. The drive count will fluctuate depending on the amount of redundancy: 4�6 1TB hard disks in a JBOD configuration (1 for the OS, 2 for the FS image [RAID 1], 1 for Apache ZooKeeper, and 1 for Journal node) 2 quad-/hex-/octo-core CPUs, running at least 2-2.5GHz 64-128GB of RAM Bonded Gigabit Ethernet or 10Gigabit Ethernet Remember, the Hadoop ecosystem is designed with a parallel environment in mind. If you expect your Hadoop cluster to grow beyond 20 machines, we recommend that the initial cluster be configured as if it were to span two racks, where each rack has a top-of-rack 10 GigE switch. As the cluster grows to multiple racks, you will want to add redundant core switches to connect the top-of-rack switches with 40GigE. Having two logical racks gives the operations team a better understanding of the network requirements for intra-rack and cross-rack communication. With a Hadoop cluster in place, the team can start identifying workloads and prepare to benchmark those workloads to identify hardware bottlenecks. After some time benchmarking and monitoring, the team will understand how additional machines should be configured. Heterogeneous Hadoop clusters are common, especially as they grow in size and number of use cases � so starting with a set of machines that are not �ideal� for your workload will not be a waste of time. Cloudera Manager offers templates that allow different hardware profiles to be managed in groups, making it simple to manage heterogeneous clusters. Below is a list of various hardware configurations for different workloads, including our original �balanced� recommendation: Light Processing Configuration (1U/machine): Two hex-core CPUs, 24-64GB memory, and 8 disk drives (1TB or 2TB) Balanced Compute Configuration (1U/machine): Two hex-core CPUs, 48-128GB memory, and 12 – 16 disk drives (1TB or 2TB) directly attached using the motherboard controller. These are often available as twins with two motherboards and 24 drives in a single 2U cabinet. Storage Heavy Configuration (2U/machine): Two hex-core CPUs, 48-96GB memory, and 16-24 disk drives (2TB – 4TB). This configuration will cause high network traffic in case of multiple node/rack failures. Compute Intensive Configuration (2U/machine): Two hex-core CPUs, 64-512GB memory, and 4-8 disk drives (1TB or 2TB) (Note that Cloudera expects to adopt 2×8, 2×10, and 2×12 core configurations as they arrive.) The following diagram shows how a machine should be configured according to workload: Other Considerations It is important to remember that the Hadoop ecosystem is designed with a parallel environment in mind. When purchasing processors, we do not recommended getting the highest GHz chips, which draw high watts (130+). This will cause two problems: higher consumption of power and greater heat expulsion. The mid-range models tend to offer the best bang for the buck in terms of GHz, price, and core count. When we encounter applications that produce large amounts of intermediate data — outputting data on the same order as the amount read in — we recommend two ports on a single Ethernet card or two channel-bonded Ethernet cards to provide 2 Gbps per machine. Bonded 2Gbps is tolerable for up to about 12TB of data per nodes. Once you move above 12TB, you will want to move to bonded 4Gbps(4x1Gbps). Alternatively, for customers that have already moved to 10 Gigabit Ethernet or Infiniband, these solutions can be used to address network-bound workloads.�Confirm that your operating system and BIOS are compatible if you�re considering switching to 10 Gigabit Ethernet. When computing memory requirements, remember that Java uses up to 10 percent of it for managing the virtual machine. We recommend configuring Hadoop to use strict heap size restrictions in order to avoid memory swapping to disk. Swapping greatly impacts MapReduce job performance and can be avoided by configuring machines with more RAM, as well as setting appropriate kernel settings on most Linux distributions. It is also important to optimize RAM for the memory channel width. For example, when using dual-channel memory, each machine should be configured with pairs of DIMMs. With triple-channel memory each machine should have triplets of DIMMs. Similarly, quad-channel DIMM should be in groups of four. Beyond MapReduce Hadoop is far bigger than HDFS and MapReduce; it’s an all-encompassing data platform. For that reason, CDH includes many different ecosystem products (and, in fact, is rarely used solely for MapReduce). Additional software components to consider when sizing your cluster include Apache HBase, Cloudera Impala, and Cloudera Search. They should all be run on the DataNode process to maintain data locality. Focusing on resource management will be your key to success. HBase is a reliable, column-oriented data store that provides consistent, low-latency, random read/write access. Cloudera Search solves the need for full text search on content stored in CDH to simplify access for new types of users, but also open the door for new types of data storage inside Hadoop. Cloudera Search is based on Apache Lucene/Solr Cloud and Apache Tika and extends valuable functionality and flexibility for search through its wider integration with CDH. The Apache-licensed Impala project brings scalable parallel database technology to Hadoop, enabling users to issue low-latency SQL queries to data stored in HDFS and HBase without requiring data movement or transformation. HBase users should be aware of heap-size limits due to garbage collector (GC) timeouts. Other JVM column stores also face this issue. Thus, we recommend a maximum of ~16GB heap per Region Server. HBase does not require too many other resources to run on top of Hadoop, but to maintain real-time SLAs you should use schedulers such as fair and capacity along with Linux Cgroups. Impala uses memory for most of its functionality, consuming up to 80 percent of available RAM resources under default configurations, so we recommend at least 96GB of RAM per node. Users that run Impala alongside MapReduce should consult our recommendations in �Configuring Impala and MapReduce for Multi-tenant Performance.� It is also possible to specify a per-process or per-query memory limit for Impala. Search is the most interesting component to size. The recommended sizing exercise is to purchase one node, install Solr and Lucene, and load your documents. Once the documents are indexed and searched in the desired manner, scalability comes into play. Keep loading documents until the indexing and query latency exceed necessary values to the project — this will give you a baseline for max documents per node based on available resources and a baseline count of nodes not including and desired replication factor. Conclusions Purchasing appropriate hardware for a Hadoop cluster requires benchmarking and careful planning to fully understand the workload. However, Hadoop clusters are commonly heterogeneous and Cloudera recommends deploying initial hardware with balanced specifications when getting started. It is important to remember when using multiple ecosystem components resource usage will vary and focusing on resource management will be your key to success. We encourage you to chime in about your experience configuring production Hadoop clusters in comments! Kevin O�Dell is a Systems Engineer at Cloudera.</snippet></document><document id="137"><title>Hadoop 2 is Now a Beta</title><url>http://blog.cloudera.com/blog/2013/08/hadoop-2-is-now-a-beta/</url><snippet>As announced last Sunday (Aug. 25) on the project mailing list, Apache Hadoop 2.1.0 is the first beta release for Hadoop 2. (See the Release Notes�for full list of new features and fixes.) Our congratulations to the Hadoop community for reaching this important milestone in the ongoing adoption of the core Hadoop platform! With the release of this new beta, and the follow-on GA release on the horizon, we expect to see more customers exploring Hadoop 2 for production use cases. In fact, the upcoming CDH5 beta will be based on the Hadoop 2 GA release, delivering features that we�ve thoroughly tested against enterprise requirements, including (but not limited to): YARN, for generalized resource management across MR and other applications YARN Client APIs, to make building YARN-based apps easier Improved NFS support in HDFS, for better integration with external storage systems HBase and HDFS snapshots, for improved recoverability We would also be remiss to ignore that the integration testing for Hadoop 2.1.0 beta was made possible by the Apache Bigtop project, which by virtue of its mission to ensure interoperability across the Hadoop ecosystem, provides the framework for this testing to continually occur. Thanks to that work, you can be assured of Hadoop 2.x interoperability with a wide range of ecosystem projects. In a future blog post, we will explain how and why compatibility between most applications written on 1.x and 2.x is assured. In the meantime, the entire community of users and developers deserves to bask in the glow of Hadoop 2.1.0 beta! Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="138"><title>How Cloudera Ensures HBase Client API Compatibility in CDH</title><url>http://blog.cloudera.com/blog/2013/08/how-cloudera-ensures-hbase-client-api-compatibility-in-cdh/</url><snippet>Apache HBase supports three primary client APIs that developers can use to bind applications with HBase: the Java API, the REST API, and the Thrift API. Therefore, as developers build apps against HBase, it’s very important for them to be aware of the compatibility guidelines with respect to CDH. This blog post will describe the efforts that go into protecting the experience of a developer using the Java API. Through its testing work, Cloudera allows developers to write code and sleep well at night, knowing that their code will remain compatible through supported upgrade paths. First, we�ll explore the compatibility guidelines themselves. From there, we will discuss some of the testing that ensures compatibility across CDH versions, as well as some of the interesting incompatibilities we’ve detected and fixed along the way. Note that API compatibility testing work goes on both upstream and internal to Cloudera. In this blog, we will focus mainly on the internal testing work, with a focus primarily on the Java API. Compatibility Policy and Versions Applications written against CDH should continue working without rewrite or recompile for any minor version of a given CDH major version. Between major versions, binary and RPC compatibility is best-effort, but not guaranteed. In order to allow product flexibility, deprecate APIs, and make major changes, it is often necessary to introduce breaking changes. We choose to do so only between major CDH versions and try to minimize the number and scope of those changes. The goal is to allow developers to make relatively minor modifications and recompile their code. What is a Major and Minor Version of CDH? If the CDH version is expressed as CDH X.Y.Z, then X is the major version and Y is the minor version. We also have point releases for each minor release, Z. For example, in CDH4.2.1, the major version is 4, the minor version is 2, and the point release is 1. A user who begins with 4.1.2, for example, should expect applications written against 4.1.2 to continue working when the cluster is upgraded to CDH 4.3.0. We call this �backward compatibility�. Also, if a user writes an application against CDH4.3.0, using only the features available in CDH4.1.2, that code should work with a cluster running CDH4.1.2. This is called �forward compatibility�. How Do CDH Versions Map to HBase Versions? For CDH4: CDH4.0.x and CDH4.1.y are based on HBase 0.92.1. CDH4.2.x is based on HBase 0.94.2. CDH4.3.x is based on HBase 0.94.6. For CDH5: The beta will be based on HBase 0.95.2. Note that CDH releases include bug fixes and sometimes even minor features back-ported from a later version of HBase. Forms of Incompatibility Using the hbase jar and the dependent libraries, it’s possible to write an HBase Java client. So what can go wrong when a cluster upgrade occurs? RPC Incompatibilities A Remote Procedure Call (RPC) incompatibility is a breakdown of understanding between the time a client has serialized a request and the time the client deserializes the response. For example, if the return type of a method changed, then the client cannot deserialize the response meaningfully and that call will fail. This will require a rewrite of that part of the application. Or, if a client request requires a specific set of arguments and the server expects a different set of arguments after an upgrade, that will cause an RPC incompatibility. For more information about Hadoop RPC communication between client and server, we suggest taking a look at Hadoop RPC mechanism, InterProcess Communication (IPC). More information about IPC is available here: http://wiki.apache.org/hadoop/ipc Binary Incompatibilities Client code has dependencies. If the dependencies change in an incompatible way, then when the client attempts to load the class and use the offending method, the result will be a runtime exception. For example, a client written using CDH4.2.0 bits will have hbase-0.94.2-cdh4.2.0-security.jar as a dependency. From that jar, a client application may use org.apache.hadoop.hbase.client.HBaseAdmin. If the constructor for HBaseAdmin changes in CDH4.3.0 to include an additional argument, and the client swaps in hbase-0.94.2-cdh4.3.0-security.jar because the cluster is upgraded to CDH4.3.0, then the client code cannot instantiate the HBaseAdmin object. From there, the client ceases to work until it is fixed. Sometimes breaking changes are more subtle. Suppose we change the return type of a method that was formerly ‘void’ to now return something. Although that change will not affect RPC compatibility, it will affect the binary compatibility. This is because the method’s signature has changed and the application cannot find the new version of that method in the new dependency jar. Example I: A specific example of this is HBASE-8273. This issue brought our attention to binary compatibility. In this example, the HColumnDescriptor setter methods originally returned void. However, they were later changed to use a builder pattern, in which the return type was a reference to the object itself. This change broke binary compatibility, but not RPC compatibility. Example II: During our testing of CDH4.2, our automation detected that two constructors in HTable were removed. Specifically: public HTable(final String tableName)
public HTable(final byte[] tableName)
   We made sure to add it back in to CDH 4.2, so developers who wrote code using that constructor against the CDH 4.1 hbase jar would not have to rewrite their code when upgrading to CDH4.2. If we had not, there would have been a binary incompatibility that would likely have manifested itself as a NoSuchMethodException exception. Other Compatibility Modes There are other implicit expectations baked into some requests. A good example is related to an incompatibility with a call to HBaseAdmin.getClusterStatus(). The problem was a version incompatibility of the return type (v1 to v2). HBASE-7072 has more information about that issue. This was fixed in CDH4.2 and later. RPC-Compatible vs. Binary-Compatible Clients Use cases are a fundamental issue. Where and how are the client applications going to be run? This is an important question when it comes down to dependencies. There are two places a client can be run: On a node of the cluster or on a machine not on the cluster. This is important because when an upgrade occurs, compatibility is stressed at least two different ways: RPC compatibility and binary compatibility. The two images below illustrate the distinction between the two configurations, as well as what form of compatibility is tested. Suppose initially that a cluster is running CDH4.0.1 and a developer creates two applications. The first, denoted on the left, runs as a standalone client on a remote machine. That means it bundles all its dependencies with it and uses those to make RPC calls to the cluster. The second application runs on the cluster itself. That means that it uses the same jars as would be specified in `hbase classpath` to run. These are the same dependencies that the services (e.g. regionserver) use to run. What happens after an upgrade of the cluster to CDH4.3.0? In the second image, note that for the RPC-compatible client, it is still the same 4.0.1 client and dependencies, but is now communicating with a server that is running CDH4.3.0. On the right, the binary-compatible client must now use a different set of dependencies to communicate with the CDH4.3.0 service. How does this relate to RPC or binary compatibility? For the RPC-compatible client running on a remote machine, the RPC mechanism between CDH4.0.1 and CDH4.3.0 must hold in order for the client to continue working. Hence, when we create such a scenario, we are testing RPC compatibility (shown in red). For the binary-compatible client, we know that CDH4.3.0 dependencies will work with the CDH4.3.0 services. What we don�t yet know, is whether the compiled client code can work with the CDH4.3.0 dependencies. Hence, this form of testing will remove uncertainty about the binary compatibility of the client and its upgraded dependencies. This is shown in blue. Before Cluster Upgrade to CDH4.3.0: After Cluster Upgrade to CDH4.3.0: Automation and Tools Cloudera Internal Test Framework for Java Compatibility Testing For all major versions, Cloudera releases a handful of minor versions. Since we guarantee compatibility for all versions of a major release, it is necessary to enumerate what those combinations are. The necessary combinations that should be tested for the Java API look like this: { server version } x { client version } x { use case }. In this case { use case } is either an RPC-compatible or binary-compatible client (see RPC-Compatible vs. Binary-Compatible Clients). A screenshot is shown below: &gt;The Testing Matrix: There are numerous combinations�but continuous integration only needs to run for cdh4Nightly. Constructing clusters quickly. Cloudera built a tool in-house that will stand up a single-node cluster from bits in a matter of minutes. Speed is important, as there are many combinations that need to be tested. This number grows at O(n^2) for total compatibility supported, and O(n)for the number of additional combinations per each new release. Constructing client code at runtime. After constructing the server, we will then construct a client running client version bits. We have a standard set of API calls that are run as part of the client. The goal is to cover a large percentage of the most common API calls. This includes calls for data mutation (e.g. Put) and retrieval (e.g. Get). It also includes a wide sampling of admin operations (e.g split). As we add features, we can add additional calls and limit the server versions that they run against. An example is snapshots, which were added in CDH4.2.0. Our snapshots testing will generate compatibility coverage, but not run against older server versions. Testing different use modes. To simulate the difference between RPC-compatible and binary-compatible use cases, it is necessary to adjust the jars that are being used to pose the request. First, the client is compiled using client version bits. Then it becomes a question of how the client is run. The remote client�s request will set the classpath to use jars of version client version. The binary-compatible client�s request will run with the jars that the server is currently using. The result is that this testing will reveal different compatibility faults, RPC and Binary. Both use cases are equally valid and both are tested. JDiff Public API Diff Tool We created a tool for diffing the public APIs that developers use to write clients. It is designed to diff the public APIs of any two arbitrary git branches, assuming that either branch is based on HBase 0.92.x or 0.94.x. A modification for 0.95.x is in the works. The script is called jdiffHBaseFromGitRepos.sh and is located in the dev-support folder. It is committed to trunk: https://github.com/apache/hbase/blob/trunk/dev-support/jdiffHBaseFromGitRepos.sh. Why is this useful? Anyone can run it and scan the report and see if their change introduced any incompatibilities. Ahead of releases, we examine this report to sanity-check our compatibility automation. Here are a couple screen shots: Report home: Diff from cdh4.0.1 to the latest cdh4. The left-hand side shows all additions, removals, and changes. This is organized by package, class, method, and constructor levels. Note that this will work for both CDH bits and Apache bits. Changes to a particular class: In this case, HBaseAdmin.java. Note that HBaseAdmin is a class that developers would use to write applications. Like most of the classes we are concerned about for compatibility, it is located in the org.apache.hadoop.hbase.client package. Impact of Secure HBase Security doubles our testing matrix, so Cloudera tests all three APIs on secure and unsecure clusters. Adding security to client code isn’t much beyond configuration changes. For setting up a secure server, the process is much more involved. Thankfully, the problem of automating secure setup has been solved nicely by Cloudera Manager. We leverage it heavily as part of our secure automation. In this case, we have long-running clusters against which we test different secure clients. Conclusion Compatibility is a very important guarantee for our customers and for the HBase community. We like to think that we have a solid understanding of the problem with the current testing infrastructure in place. However, there is more work to be done as we look for other forms of compatibility that developers value. Examples of areas where additional API compatibility testing can be added: Additional types of filters Bulk load Users and permissions We also examine the cdh-user@ mailing list and community forums carefully for potential disruption to the developer experience, and to learn from past incompatibilities. Aleksandr Shulman is a Software Engineer on the Platform team, working on HBase.</snippet></document><document id="139"><title>How-to: Achieve Higher Availability for Hue</title><url>http://blog.cloudera.com/blog/2013/08/how-to-achieve-higher-availability-for-hue/</url><snippet>Few projects within the Apache Hadoop umbrella have as much end-user visibility as Hue, the open source Web UI that makes Hadoop easier to use. Due to the great number of potential end users, it is useful to add a degree of fault tolerance to your deployment. This how-to describes how to achieve higher availability by placing several Hue instances behind a load balancer. Tutorial This tutorial demonstrates how to set up high availability by: Installing Hue 2.3 on two nodes in a three-node RHEL 5 cluster Managing all Hue instances via Cloudera Manager Load balancing using HA Proxy 1.4. (In fact, any load balancer with sticky sessions should work.) Before we begin, we suggest that you view this quick video demonstrating how to achieve HA in Hue: Installing Hue Hue should be installed on two of the three nodes. To have Cloudera Manager automatically install Hue, follow the �Parcel Install via Cloudera Manager� section. To install manually, follow the �Package Install� section. Parcel Install via Cloudera Manager For more information on Parcels, see Managing Parcels. From Cloudera Manager, click on Hosts in the menu. Then, go to the Parcels section. Find the latest CDH parcel, click Download. Once the parcel has finished downloading, click Distribute. Once the parcel has finished distributing, click Activate. Package Install Download the yum repository RPM. Install the yum repository with sudo yum --nogpgcheck localinstall cloudera-cdh-4-0.x86_64.rpm. For more information, see Installing CDH4. Install Hue on each node with sudo yum install hue. For more information on installing Hue, see CDH documentation. Managing Hue through Cloudera Manager Cloudera Manager provides management of the Hue servers on each node. Add two Hue services using the directions below. For more information on managing services, see the Cloudera Manager documentation. Go to Services -&gt; All Services in the menu. Click Actions -&gt; Add a Service. Select �Hue� and follow the steps on the screen. NOTE: For each Hue service we choose a unique host. Ensure that the �Jobsub Examples and Templates Directory� configuration points to different directories in HDFS for each Hue service. It can be changed by going to Services -&gt; . In the menu, go to Configuration -&gt; View and Edit. Then, click on Hue Server. �Jobsub Examples and Templates Directory� should be at the bottom of the page. Cloudera Manager handling two Hue services HA Proxy Installation/Configuration Download and unzip the binary distribution of HA Proxy 1.4 on the node that doesn�t have Hue installed (called serverc.cloudera.com in the example). Add the following HA Proxy configurationto /tmp/hahue.conf: global
��� daemon
��� nbproc 1
��� maxconn 100000
��� log 127.0.0.1 local6 debug

defaults
��� option http-server-close
��� mode http
��� timeout http-request 5s
��� timeout connect 5s
��� timeout server 10s
��� timeout client 10s

listen Hue 0.0.0.0:80
��� log global
��� mode http
��� stats enable
��� balance source
��� server hue1 servera.cloudera.com:8888 cookie ServerA check inter 2000 fall 3
��� server hue2 serverb.cloudera.com:8888 cookie ServerB check inter 2000 fall 3
   Start the HA Proxy with haproxy -f /tmp/hahue.conf The key configuration options are balance and server in the listen section. When the balance parameter is set to source, a client is guaranteed to communicate with the same server every time it makes a request. If the server with which the client is communicating goes down, the request will automatically be sent to another active server. This is necessary because Hue stores session information in process memory. The server parameters define which servers will be used for load balancing and takes the form: server &lt;name&gt; &lt;address&gt;[:port] [settings ...]
   In the configuration above, the server hue1 is available at servera.cloudera.com:8888 and hue2 is available at serverb.cloudera.com:8888. Both servers have health checks every two seconds and are declared down after three failed health checks. In this example, HAProxy is configured to bind to 0.0.0.0:80. Thus, Hue should now be available at http://serverc.cloudera.com. Conclusion Hue can be load-balanced easily as long as the server a client is directed to is constant (that is, there are “sticky” sessions). Load balancing can improve performance, but its primary goal is HA. (Note that for true high availability, Hue needs to be configured to use HA via MySQL, PostgreSQL, or Oracle Database.) Also, multiple Hue instances can be easily managed through Cloudera Manager. Have any suggestions? Feel free to tell us what you think through hue-user or via our new community discussion forum.  </snippet></document><document id="140"><title>What I Learned During My Summer Internship at Cloudera</title><url>http://blog.cloudera.com/blog/2013/08/what-i-learned-during-my-summer-internship-at-cloudera/</url><snippet>Catherine Ray, a Summer Intern at Cloudera this year, was kind enough to summarize her experiences for you below. Best of luck in your new field, Catherine! I’m currently 16 and a rising senior at George Mason University, majoring in Computational Physics. (The full title is Computational and Data Sciences with a concentration in Physics.). I had a wonderful time working on my project. In short, I worked on an Apache Hadoop-based downloads tracking system. In this system, raw downloads logs are ingested via Apache Flume into HDFS, then parsed with an MR Job into a Cloudera Impala-friendly format. I had the opportunity to collaborate with one of our teams in New York to pull the whole system together. To fully utilize the data contained in the logs, I created a Java library that finds the organizational information associated with a given IP address. I also helped to create dashboards that use queries against the collected data to analyze it and produce sales leads. As my internship came to an end, I was able to use a skill I developed through one of my many hobbies: making YouTube videos. Specifically, when faced with the task of creating my intern presentation, I ditched my PowerPoint and made a video in order to explain my project in a more engaging format. The video below describes the system I created this summer. Before I began my internship, I worried that I would encounter a specific obstacle in enjoying experiences that has repetitively appeared in my past. I worried that I wouldn�t be taken seriously due to my age. After meeting my fellow interns and conversing with my mentor, my fears were quickly assuaged. The Cloudera community was truly accommodating; I was treated as the other interns were treated: just like a full-time employee. My experience at Cloudera revealed a perspective previously hidden to me. (I also had amazing discussions with brilliant people over lunch and in the hallways on a regular basis.) Here, it is well known that one can find success at being both a scientist and engineer. The best data scientist has both curiosity and passion of a scientist faced with an unsolved problem, and the methodology and efficiency of an engineer given the task of implementing an effective solution. This realization has convinced me to pursue graduate studies in computer science, instead of narrowing my future studies to the applications of computer science in physics. I also learned coding tricks and new ways of thinking in programming languages I thought I knew well. I fell in love with regular expressions; I learned the practices of documenting code and creating readable source. Outside of computer science, I learned how to ride a RipStick (a skateboard variation), I learned the art of collaboration, and I experienced a strong sense of community. My mentor was happy to answer any questions I had in detail, and our code review sessions completely changed the way I think about object-oriented programming. I can’t thank my mentor (Aditya Acharya) enough for the time he devoted to answering all of my questions; I learned an incredible amount from him. The members of the teamswith which I worked were similarly kind, accommodating, and resourceful. My fellow interns were extremely friendly and helpful — competition did not taint our interactions. All in all, a very successful summer.</snippet></document><document id="141"><title>Visualization on Impala: Big, Real-Time, and Raw</title><url>http://blog.cloudera.com/blog/2013/08/visualization-on-impala-big-real-time-and-raw/</url><snippet>The guest post below is provided by Justin Langseth, Founder &amp; CEO of Zoomdata, Inc. Thanks, Justin! What if you could affordably manage billions of rows of raw Big Data and let typical business people analyze it at the speed of thought in beautiful, interactive visuals? What if you could do all the above without worrying about structuring that data in a data warehouse schema, moving it, and pre-defining reports and dashboards? With the approach I�ll describe below, you can. The traditional Apache Hadoop approach — in which you store all your data in HDFS and do batch processing through MapReduce — works well for data geeks and data scientists, who can write MapReduce jobs and wait hours for them to run before asking the next question. But many businesses have never even heard of Hadoop, don�t employ a data scientist, and want their data questions answered in a second or two — not in hours. We at Zoomdata, working with the Cloudera team, have figured out how to make Big Data simple, useful, and instantly accessible across an organization, with Cloudera Impala being a key element. Zoomdata is a next-generation user interface for data, and addresses streams of data as opposed to sets. Zoomdata performs continuous math across data streams in real-time to drive visualizations on touch, gestural, and legacy web interfaces. As new data points come in, it re-computes their values and turns them into visuals in milliseconds. Many businesses have never heard of Hadoop, don�t employ a data scientist, and want their questions answered in seconds. To handle historical data, Zoomdata re-streams the historical raw data through the same stream-processing engine, the same way you’d rewind a television show on your home DVR. The amount of the data involved can grow rapidly, so the ability to crunch billions of rows of raw data in a couple seconds is important �- which is where Impala comes in. With Impala on top of raw HDFS data, we can run flights of tiny queries, each to do a tiny fraction of the overall work. Zoomdata adds the ability to process the resulting stream of micro-result sets instead of processing the raw data. We call this approach �micro-aggregate delegation�; it enables users to see results immediately, allowing for instantaneous analysis of arbitrarily large amounts of raw data. The approach also allows for joining micro-aggregate streams from disparate Hadoop, NoSQL, and legacy sources together while they are in-flight, an approach we call the �Death Star Join� (more on that in a future blog post). The demo below shows how this works, by visualizing a dataset of 1 billion raw records per day nearly instantaneously, with no pre-aggregation, no indexing, no database, no star schema, no pre-built reports, and no data movement — just billions of rows of raw data in HDFS with Impala and Zoomdata on top. To do that the old way would have taken months to set up, days to load, and hours to run. And furthermore, doing it in real-time and historically, through any imaginable visualization, on any device, is now possible. Download Zoomdata and try it on your data for free at zoomdata.com/download.</snippet></document><document id="142"><title>How Improved Short-Circuit Local Reads Bring Better Performance and Security to Hadoop</title><url>http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/</url><snippet>One of the key principles behind Apache Hadoop is the idea that moving computation is cheaper than moving data — we prefer to move the computation to the data whenever possible, rather than the other way around. Because of this, the Hadoop Distributed File System (HDFS) typically handles many �local reads� reads where the reader is on the same node as the data: Initially, local reads in HDFS were handled the same way as remote reads: the client connected to the DataNode via a TCP socket and transferred the data via DataTransferProtocol. This approach was simple, but it had some downsides. For example, the DataNode had to keep a thread around and a TCP socket for each client that was reading a block. There was the overhead of the TCP protocol in the kernel, as well as the overhead of DataTransferProtocol itself. There was room to optimize. In this post, you�ll learn about an important new optimization for HDFS called secure short-circuit local reads, the benefits of its implementation, and how it can speed up your applications. Short-Circuit Local Reads with HDFS-2246 In HDFS-2246, Andrew Purtell, Suresh Srinivas, Jitendra Nath Pandey, and Benoy Antony added an optimization called �short-circuit local reads�. The key idea behind short-circuit local reads is this: because the client and the data are on the same node, there is no need for the DataNode to be in the data path. Rather, the client itself can simply read the data from the local disk. This performance optimization made it into CDH, Cloudera�s distribution of Hadoop and related projects, in CDH3u3. The implementation of short-circuit local reads found in HDFS-2246, although a good start, came with a number of configuration headaches. System administrators had to change the permissions on the DataNode�s data directories to allow the clients to open the relevant files.� They had to specifically whitelist the users who were able to use short-circuit local reads — no other users would be allowed. Typically, those users also had to be placed in a special UNIX group. The main problem with HDFS-2246 was that it opened up the DataNode�s data directories to the client. Unfortunately, those permission changes opened up a security hole: Users with the permissions necessary to read the DataNode�s files could simply browse through everything, not just things that they were supposed to have access to. This was a little bit like making the user a super-user!� This might be acceptable for a few users — such as the �hbase� user — but in general, it presented problems.� So although a few dedicated administrators enabled short-circuit local reads, it was not a common choice. HDFS-347: Making Short-Circuit Local Reads Secure The main problem with HDFS-2246 was that it opened up all of the DataNode�s data directories to the client. Instead, what we really want is to share only a few carefully chosen files. Luckily, UNIX has a mechanism for doing just that, called �file descriptor passing.� HDFS-347 uses this mechanism to implement secure short-circuit local reads. Instead of passing the directory name to the client, the DataNode opens the block file and metadata file and passes them directly to the client. Because the file descriptors are read-only, the client cannot modify the files it was passed. And because it has no access to the block directories itself, it cannot read anything to which it is not supposed to have access. Windows has a similar mechanism for passing file descriptors between processes.� Although Cloudera doesn�t support this yet in Hadoop, in the meantime, Windows users can use the legacy block reader by setting dfs.client.use.legacy.blockreader.local to true. Caching File Descriptors HDFS clients often read the same block file many times. (This is particularly true for HBase.) To speed up this case, the old short-circuit local read implementation, HDFS-2246, had a block path cache. This cache allowed the client to reopen a block file that it had already read recently without asking the DataNode for its path. Instead of a path cache, the new-style short-circuit implementation includes a file descriptor cache named FileInputStreamCache. This is better than a path cache, since it doesn�t require the client to re-open the file to re-read the block.�We found that this approach improved performance over the old short-circuit local read implementation. The size of the cache can be tuned with dfs.client.read.shortcircuit.streams.cache.size, whereas cache timeout is controlled by dfs.client.read.shortcircuit.streams.cache.expiry.ms. The cache can also be turned off by setting its size to 0. Most of the time, the defaults are a good choice. However, if you have an unusually large working set and a high file descriptor limit, you could try increasing it. HDFS-347 Configuration With the new-style short-circuit local reads introduced in HDFS-347, any HDFS user can make use of short-circuit reads, not just specifically configured ones. There is also no need to modify which UNIX group the users are in or change the ownership of the DataNode directories. However, because the Java standard library does not include facilities for file descriptor passing, HDFS-347 requires a JNI component in order to function. You will need to have libhadoop.so installed to use it. In testing, HDFS-347 was the fastest implementation in all cases. HDFS-347 also requires a UNIX domain socket path to be configured via dfs.domain.socket.path. This path must be secure to prevent unprivileged processes from performing a man-in-the-middle attack. Every path component of the socket path must be owned either by root or by the user who started the DataNode; world-writable or group-writable paths cannot be used. Luckily, if you install a Cloudera parcel, RPM, or deb, it will create a secure UNIX domain socket path for you in the default location. It will also install libhadoop.so in the right place. For more information about configuring short-circuit local reads, see the upstream documentation. Performance So, how fast is this new implementation? I used a program called hio_bench to get some performance statistics. The code for hio_bench is available at https://github.com/cmccabe/hio_test. These tests were run on an 8-core Intel Xeon 2.13 with 12 hard drives. I used CDH 4.3.1 with an underlying filesystem of ext4. Each number is the average of three runs. Error bars are provided. HDFS-347 is the fastest implementation in all cases, probably due to the FileInputStreamCache. In contrast, the HDFS-2246 implementation ends up re-opening the ext4 block file many times, and open is an expensive operation. The short-circuit implementations have a bigger relative advantage in the random read test than in the sequential read test. This is partly because readahead has not been implemented for short-circuit local reads yet. (See HDFS-4697 for a discussion.)� Conclusion Short-circuit local reads are a great example of an optimization enabled by Hadoop�s model of bringing the computation to the data. They�re also a good example of how, having tackled the challenges of scaling head-on, Cloudera is now tackling the challenges of getting more performance out of each node in the cluster. If you are using CDH 4.2 or later, give the new implementation a try! Colin McCabe is a Software Engineer on the Platform team, and a Hadoop Committer.</snippet></document><document id="143"><title>Spotlight: How National Institutes of Health Advances Genomic Research with Big Data</title><url>http://blog.cloudera.com/blog/2013/08/spotlight-how-national-institutes-of-health-advances-genomic-research-with-big-data/</url><snippet>This week, I�d like to shine a spotlight on innovative work the National Institutes of Health (NIH) is working on, leveraging Big Data, in the area of genomic research. Understanding DNA structure and functions is a very data-intensive, complex, and expensive undertaking. Apache Hadoop is making it more affordable and feasible to process, store, and analyze this data, and the NIH is embracing the technology for this reason. In fact, it has initiated a Big Data center of excellence — which it calls Big Data to Knowledge�(BD2K) — to accelerate innovations in bioinformatics using Big Data, which will ultimately help us better understand and control various diseases and disorders. Bob Gourley — a friend of Cloudera�s who wears many hats including publisher of CTOvision.com, CTO of Crucial Point LLC, and GigaOm analyst — recently interviewed Dr. Mark Guyer, the deputy director of the NIH�s National Human Genome Research Institute (NHGRI), about the BD2K effort. Some key highlights from the interview: The ability to generate genomic sequencing data has improved more than a million-fold since the beginning of the Human Genome Project. Goals of BD2K: To enhance the ability to analyze all types and volumes of data; To maximize the value of the growing volume and complexity of biomedical data; To advance the discipline of data science in the community by helping to develop and disseminate innovative analysis methods, tools and techniques. NIH supports widespread data sharing, including across government, as key to rapid progress. To read the full interview and for greater context, check out Bob Gourley�s blog. What other industries and lines of work would benefit from this kind of Big Data Center of Excellence? Add your comments and thoughts here! Karina Babcock is Cloudera�s Customer Programs &amp; Marketing Manager.</snippet></document><document id="144"><title>How-to: Install Cloudera Manager and Cloudera Search with Ansible</title><url>http://blog.cloudera.com/blog/2013/08/how-to-install-cloudera-manager-and-search-with-ansible/</url><snippet>The following guest post is re-published here courtesy of Gerd K�nig, a System Engineer with YMC AG. Thanks, Gerd! Cloudera Manager is a great tool to orchestrate your�CDH-based Apache Hadoop cluster. You can use it from cluster installation, deploying configurations, restarting daemons to monitoring each cluster component. Starting with version 4.6, the manager supports the integration of�Cloudera Search, which is currently in Beta state. In this post I�ll show you the required steps to set up a Hadoop cluster via Cloudera Manager and how to integrate Cloudera Search. Furthermore, I want to introduce a small level of automation using the tool�Ansible, to avoid performing the same manual steps again and again � thereby being able to replay them whenever you want, or to just use it for any upcoming cluster installations. I favour Ansible over Chef and Puppet, because it: requires no agents on the cluster nodes, is not focused on a special language, has a very low entry barrier; first missions will be completed within half an hour. Starting Point This tutorial is based on a very restricted Hadoop cluster, consisting of five newly created nodes based on Debian Squeeze. One of them is dedicated to the Cloudera Manager, the remaining four nodes will build the real cluster. The installation will be triggered from a client, e.g. your workstation, which requires Ansible to be�installed, obviously.� Ansible ensures that the prerequisites are fulfilled and the required packages are of the same version among all nodes. The corresponding playbook is�available via Github. Check out the repository containing the Ansible playbooks and change into this directory afterwards: cd ~
git clone git@github.com:ymc-geko/ansible-cdh-cluster.git
cd ansible-cdh-cluster
   Step-by-Step Instructions Install base packages, e.g. the oracle jdk and ntp, on all nodes. ansible-playbook -i config/hosts -k install-base-packages.yml
   Install Cloudera Manager. ansible-playbook -i config/hosts -k install-cloudera-manager.yml
   Login to the Cloudera Manager web interface and click through the wizard. Open URL: http://&lt;cloudera-manager-node:7180
   in your browser and login with the default credentials�admin/admin Wizard interactions (after each task click Continueto switch to next step). Select�free license�and click�Continue�twice. Specify your nodes and click�Search�(you can type name ranges, e.g. hadoop-pg-[2-5]). Ensure that all of your nodes are marked. Since we are going to integrate Cloudera Search, ensure that�SOLR�is activated, as well as installation method via Parcels. Provide user and password for connecting to the cluster nodes and installing the software. The provided user needs to have permission to install packages on each node. After clicking�Continue�the manager agents will be installed on each node and the result is displayed on this page. CDH, Solr, and Impala (if selected) are being installed on the cluster nodes. The host inspector scans the cluster and evaluates the correctness of the node state. Check the result of the host inspector thoroughly and eliminate any warnings. Choose the services that will be installed on the cluster. Enable all services (or your desired choice) and click�Inspect Role Assignments�afterwards. Ensure that each node is linked to the planned service correctly. By default the Cloudera Manager activates the role DataNode even on the master server, disable this role for the master node. Check the database connection to the embedded Postgresql database. Just click�Test connection, Cloudera Manager fills in the created credentials correctly. Review the Hadoop configuration. On this page you can verify/modify the properties that are �normally� (in the sense of a cluster installation without Cloudera Manager) included in the core-site.xml, hdfs-site.xml and mapred-site.xml config files. The Cloudera Manager now starts all cluster services. Integrate Cloudera Search into the Hue web interface. Add service Solr. At the time of writing, the Cloudera Search is currently in Beta state and the required steps to integrate it into the Hue web interface are split into tasks within the Manager web interface and tasks to perform on the command line of the Cloudera Manager node. Detailed instructions can be found�here, the shortened version is: Tab�Services�=&gt;�All services�=&gt; Dropdown�Actions�=&gt;�Add a service�=&gt; choose�Solr�service. Choose the Zookeeper quorum (there is currently just one). Choose the cluster nodes that shall serve the SOLR service. Accept the configuration modification. The SOLR service will be deployed in the cluster. Add service Flume: Perform the same steps as described for adding service SOLR, except choosing�Flume�instead of Solr as service to be added. Extend service Hue: Click�Services�=&gt;�Hue�=&gt;�Configuration�=&gt;�View and Edit�=&gt; search for the word �safety� Enter the text: [search]
## URL of the Solr Server
solr_url=http://:8983/solr
   into the text box of category�Hue Server (Default) / Advanced�and replace �� with the server�s name you�ve chosen inSstep 5 to serve the SOLR service. And finally, execute the last ansible playbook from your client�s command line to add the �Solr search� icon to the Hue web interface: ansible-playbook -i config/hosts -k integrate-cloudera-search.yml Summary Cloudera Manager combined with Cloudera Search is a great tool and the entry point to your Hadoop cluster for SystemEngineers/DevOps as well as for Data Analysts. Nevertheless, you should have a good understanding of the underlying concepts before you are going to set up a cluster, especially to verify the cluster configuration and role assignments. The guide in this post includes several manual clicks inside the Wizard to set up the cluster, but there�s also a REST API for Cloudera Manager available, which can be used to automate even those steps. I�ll write an article about using this API by introducing some more ansible playbooks soon, so stay tuned! If you have any questions or want to share your experience, just leave a reply or get in contact with�me.</snippet></document><document id="145"><title>Meet the Project Founder: Tom White</title><url>http://blog.cloudera.com/blog/2013/08/meet-the-project-founder-tom-white/</url><snippet>In this new installment of our �Meet the Project Founder� series, meet Tom White, founder of Apache Whirr, PMC Member for multiple other projects (Apache Hadoop, Apache Avro, Apache Bigtop, Apache Sqoop), and author of O’Reilly Media’s best-selling book, Hadoop: The Definitive Guide. What led you to your project idea(s)? Whirr grew out of some scripts I had written in 2006 for spinning up Hadoop clusters on Amazon EC2. At the time, I didn’t have access to a cluster to run Hadoop – even getting a handful of machines was a challenge – so when Amazon announced its EC2 service, I knew that it would be a great way to run Hadoop for individuals and organizations without plentiful hardware resources. While it was pretty easy to start up a collection of machines on EC2, installing Hadoop on those machines was tricky (e.g. configuring hostnames was, and remains, an art), so the case for having code do the work was compelling. Whirr was a step beyond those early scripts in that it provides a Java API for provisioning clusters on any cloud (EC2, Rackspace, etc) – and not just Hadoop either: it comes with a simple service provider API for those who want to write new services. A recent example of this is the service for launching Cloudera Manager via Whirr. Aside from doing the initial commit, what is your definition of the project founder�s role across the lifespan of the project? Benevolent dictator, referee, silent partner? Early on in the project’s life the founder helps set the direction of the project. The thing I found hardest to get right is understanding what belongs in the project and what doesn’t. Not everyone on the project agrees about this, and since the work done on open source projects is done by volunteers, you can’t just tell everyone what the scope is. You have to make your case, and get agreement from the people who are going to be doing some of the work. If they don’t agree, then they’ll go to a different project! The best way to ensure quality and quantity of contributions is to be very encouraging of new contributors. There have been many cases in Hadoop itself where features that were considered to be “core” have, over time, moved to new projects outside the core. I see this as a natural result of maturity as the project community grows to understand what the project’s core mission is, and learns to focus on that. Early on in a project, things are more fuzzy. For example, Hadoop used to have packaging code for RPMs and Debian packages – that function is now in a separate project (Bigtop). Eclipse integration has moved to Apache Hadoop Developer Tools, cloud scripts to Whirr, etc. All of these are positive developments since they allow a group that is interested in that particular area to focus on making that piece better. The founder has to learn when to step back, too. At the beginning the founder will typically answer every question that comes to the user mailing list. However, at some point, you have to let others help out. I remember another project founder telling me how he remembers the first day he deliberately didn’t immediately answer a question, so he could let others jump in. Someone else did answer the question, and on that day, the project’s community grew a little bit — just through that small act. What has surprised you the most about how your project has evolved/matured? Whirr is a small project in the Hadoop ecosystem, but I’m pleasantly surprised whenever I read about a project that uses Whirr in some way. I’m also proud to work with folks who participate in related open source cloud projects; for example, several of the jclouds contributors and committers are a part of the Whirr community. It was nice to see jclouds enter the Apache Incubator earlier this year. And Andrei Savu, who has been one of the most prolific Whirr contributors, started a new Apache Incubator project, Provisionr, for reliable cluster provisioning in the cloud. What is your philosophy, if you have one, for balancing quality versus quantity with respect to contributions? I think the best way of ensuring both quality and quantity of contributions is to be very encouraging of new contributors. You need to make their experience of contributing a patch as pleasant as possible. This means responding to their contributions in a timely fashion, and providing good quality feedback on their patches. This practice helps build a virtuous circle: contributors who have successfully contributed one patch are likely to come back and contribute more, because it is enjoyable and a low-friction process. Over time they will become committers, who will review other people’s patches, and continue the cycle. Do you have any other advice for potential project founders? I would reiterate the importance of building community if you want to build a successful project. I think Henri Yandell (former ASF Board Member) captured it well: “Projects begin by thinking they’re in the software engineering business; after a while, they realize they’re in the recruiting business.” Meet other project founders: Doug Cutting (Hadoop) Roman Shaposhnik (Bigtop) Alejandro Abdelnur (Oozie)</snippet></document><document id="146"><title>Five Ways to Get the Most Out of Cloudera’s Community Forums</title><url>http://blog.cloudera.com/blog/2013/08/five-ways-to-get-most-out-of-cloudera-community-forums/</url><snippet>It’s been a couple of weeks since Cloudera’s new Community Forums did a Hello World, and we’re seeing activity start to ramp up. If you haven’t dropped in yet, please do so! Today, I thought it would be helpful to highlight some features that will help you get the most out of this new service: 1. Get email updates and reply-by-email.�For the browser-averse, it is possible to “subscribe” to threads (AKA “topics”) as well as entire forums by selecting�Options &gt; Subscribe contextually. (You can also subscribe to topics in which you are involved by default via My Settings &gt; Subscriptions &amp; Notifications.) You will receive an email when an update occurs, and you may then reply by email instead of returning to the Web to do so, if you wish. (However, it is not possible to create a entirely new topic that way.) 2. Invite your friends. Involved in a topic that would interest a friend or colleague or otherwise benefit from their involvement? “Invite” them to contribute by email – again, by using the “Options” pick list. 3. Give back. If you receive an answer or solution that you appreciate, spread the love: Give the answerer a “kudo”, and “accept” the answer as a solution. The former rewards contributors for their efforts, and the latter rewards the rest of the community by saving someone else from repeating the same answer to the same question. 4. Get a birds-eye view. The quickest and easiest way to get an overview of activity is by clicking the View All link in the “Recent Topics” widget found on the homepage. You’ll see a simple reverse-chron list of all recent updates. (Unfortunately, you cannot subscribe to this list, but that would be a welcome enhancement.) 5. Bling up your profile. Although we happen to like the default set of avatar images, it is also possible to upload your own avatar via My Settings &gt; Avatars &gt; Upload an Avatar. (My personal choice? The immortal Joe Strummer.) I will bring you additional tips and hacks in the future as we observe the most popular features of the service. In the meantime, enjoy. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="147"><title>New E-Learning for Parcels</title><url>http://blog.cloudera.com/blog/2013/08/new-e-learning-for-parcels/</url><snippet>Cloudera’s new Parcels installation format�has been released, and I�m excited to highlight just how useful (and mind-blowingly cool) it is to system administrators and anyone responsible for maintaining a CDH cluster. If you haven’t read about or played with Parcels, they make components of the distribution significantly easier to manage, install, and upgrade. The new Parcel distribution format works with Cloudera Manager 4.5 and later. When you perform installations and upgrades using Parcels, you get access to new Cloudera Manager features such as: Rolling upgrades with little-to-no maintenance window Sudo-less installations, upgrades, and rollback of CDH or components Custom file system installations (for example, if you don’t have access to /etc.) Parcels also provide advantages over package installations since Cloudera Manager automates cleanup and customers can write their own Parcels to extend the benefits of the new features. The new Understanding Parcels e-learning module�both covers the fundamentals of Parcels� role in optimizing your Hadoop operations and takes you through a step-by-step demo of upgrading CDH and installing Impala, Search, and Hadoop LZO. For extra fun, download the new QuickStart VM and follow along with the Parcels demonstration. Once you see how much time and effort Parcels save in your component installations, you’ll never want to touch another Deb package ever again. John Souchak is a senior curriculum developer at Cloudera University.</snippet></document><document id="148"><title>Flexpod Select with Cloudera</title><url>http://blog.cloudera.com/blog/2013/08/flexpod-select-with-cloudera/</url><snippet>Earlier this week, our partners NetApp and Cisco announced the Flexpod Select Family with support for Cloudera�s Distribution including Apache Hadoop (CDH). We’re looking forward to the expansion of Flexpod Select to include Hadoop, as it provides additional options for customers to consume the benefits of the Cloudera Enterprise platform. FlexPod Select with Hadoop is an extension of the FlexPod initiative built on the Cisco Common Platform Architecture (CPA) for Big Data. Designed for deployments that need enterprise class external storage array features, this solution offers a comprehensive analytic stack for big data that includes compute, storage, connectivity, and CDH with a full range of services such as Cloudera Manager to manage heavy workloads. The solution is pre-validated for enterprise Hadoop deployments with breakthroughs around Hadoop stability, operations, and storage efficiency. By integrating all the hardware and software components and using highly reliable products, businesses can meet their tight SLAs around data performance while reducing the risk of deploying Hadoop. FlexPod Select with Hadoop is based on the NetApp Open Solution for Hadoop reference design running CDH. NetApp has deployed this solution in production to improve customer support by leveraging Cloudera Enterprise. For more details on the reference design, please download the NetApp Case Study. For the “Flexpod Select with CDH” validated design guide, please visit this URL. Sandeep Brahmarouthu is a Senior Manager on Cloudera’s Business Development team.</snippet></document><document id="149"><title>Demo: Using Hue to Access Hive Data Through Pig</title><url>http://blog.cloudera.com/blog/2013/08/demo-using-hue-to-access-hive-data-through-pig/</url><snippet>This installment of the Hue demo series is about accessing the Hive Metastore from Hue, as well as using HCatalog with Hue. (Hue, of course, is the open source Web UI that makes Apache Hadoop easier to use.)� What is HCatalog? HCatalog is a module in Apache Hive that enables non-Hive scripts to access Hive tables. You can then directly load tables with Apache Pig or MapReduce without having to worry about re-defining the input schemas, or caring about or duplicating the data’s location. Hue contains a Web application for accessing the Hive metastore called Metastore Browser, which lets you explore, create, or delete databases and tables using wizards. (You can see a demo of these wizards in a previous tutorial about how to analyze Yelp data.) However, Hue uses HiveServer2 for accessing the metastore instead of HCatalog. This is because HiveServer2 is the new secure and concurrent server for Hive and it includes a fast Hive Metastore API. HCatalog connectors are still useful for accessing Hive data through Pig, though. Here is a demo about accessing the Hive example tables from the Pig Editor: Tutorial To try this yourself, first, you need to install HCatalog via Cloudera Manager (or do it the manual way). If you are using a fully distributed cluster (e.g. not on a demo VM), make sure that the Hive Metastore is remote or you will see an error like the one below. Then, upload the three jars from /usr/lib/hcatalog/share/hcatalog/ and all the Hive ones from /usr/lib/hive/lib to the Oozie Pig sharelib in /user/oozie/share/lib/pig. This can be done in a few clicks while being logged in as �oozie� or �hdfs� in the File Browser. Keep in mind that all the jars will be included in all the future Pig scripts, which might be unnecessary. An alternative would be to upload these jars in your HDFS home directory and then include the path to the directory with the Hadoop property �oozie.libpath� in the Properties section of the Pig Editor. Then, confirm the Beeswax examples are installed (Step #2 in the Hue Quick Start Wizard), open the Pig Editor, and compute the average salary in the table — equivalent to this Hive query: -- Load table 'sample_07'
sample_07 = LOAD 'sample_07' USING org.apache.hcatalog.pig.HCatLoader();

-- Compute the average salary of the table
salaries = GROUP sample_07 ALL;
out = FOREACH salaries GENERATE AVG(sample_07.salary);
DUMP out;
   As HCatalog needs to access the metastore, you need to specify the hive-site.xml. Go to Properties &gt; Resources and add a �File� pointing to the hive-site.xml uploaded on HDFS. Then, submit the script by pressing CTRL + ENTER. The result (47963.62637362637)�will appear at the end of the log output. (Notice that you don�t need to redefine the schema as it is automatically picked up by the loader.) If you use the Oozie App, you can now freely use HCatalog in your Pig actions. Warning! If you get the error below, it means that your metastore is owned by the Hive user and is not remote. Cannot get a connection, pool error Could not create a validated object, cause: A read-only user or a user in a read-only database is not permitted to disable read-only mode on a connection.
2013-07-24 23:20:04,969 [main] INFO� DataNucleus.Persistence� - DataNucleus Persistence Factory initialisedfor datastore URL="jdbc:derby:;databaseName=/var/lib/hive/metastore/metastore_db;create=true"driver="org.apache.derby.jdbc.EmbeddedDriver" userName="APP"
   A workaround is to make sure that Beeswax is shut down and then change the permissions of the SQLite database: sudo rm /var/lib/hive/metastore/metastore_db/*lck
sudo chmod 777 -R /var/lib/hive/metastore/metastore_db
   Similar to HCatLoader, use HCatStorer to update the table, e.g.: STORE alias INTO 'sample_07' USING org.apache.hcatalog.pig.HCatStorer();
   Conclusion Here you have seen how Hue makes it easy to access Hive’s metastore and how it supports the HCatalog connectors for Pig. Hue 3.0 will simplify things even more by automatically copying the required jar files and making the table names auto-complete. As usual, we welcome any feedback � via our new Hue Community Forum or the user group!</snippet></document><document id="150"><title>Cloudera at Strata + Hadoop World 2013</title><url>http://blog.cloudera.com/blog/2013/08/cloudera-at-strata-hadoop-world-2013/</url><snippet>Strata Conference + Hadoop World 2013�is looming on the horizon and pacing to be the largest gathering of Big Data professionals on the globe. As co-hosts with O�Reilly, we have seen the conference thrive, grow, and are excited about the upcoming Oct. 28 � 30 event! Below you will find a listing of all the ways you can engage with Cloudera throughout the conference (all speakers are Cloudera employees unless otherwise indicated): Keynote Cloudera co-founder and Chief Strategy Officer Mike Olson will explain how Apache Hadoop and Cloudera are transforming the way organizations think about their data. Tutorial How to Build a Hadoop Data Application Mon., Oct. 28, 9am – Grand Ballroom West Tom White,�Eric Sammer,�Joey Echeverria In this tutorial, you�ll use the Cloudera Development Kit (CDK) to build a Java web app that logs application events to Hadoop, and then run ad hoc and scheduled queries against the collected data.� Breakout Sessions From Promise to a Platform: Next Steps in Bringing Workload Diversity to Hadoop Tues., Oct. 29, 11:50am – Grand Ballroom East Henry Robinson� In this session, Henry presents Cloudera�s vision and implementation for generalized resource management on Hadoop, suitable for all uses.� Parquet: An Open Columnar Storage for Hadoop Tues., Oct. 29, 1:45pm�- Gramercy Suite Julien Le Dem�(Twitter),�Nong Li� Parquet is a columnar file format for Hadoop that brings performance and storage benefits. Here, two of its architects discuss Parquet�s design and share performance numbers. What�s Next for Apache HBase: Multi-tenancy, Predictability, and Extensions. Tues., Oct. 29, 2:35pm�- Grand Ballroom East Jonathan Hsieh This talk will describe themes emerging from recent features slated for the upcoming post-0.96 release. These include improvements for multi-tenant deployments; a focus on predictable latencies; and the proliferation of new extensions for features traditionally from databases.� Securing the Apache Hadoop Ecosystem Tues., Oct. 29, 4:15pm – Grand Ballroom East Aaron Myers, Shreepadma Venugopalan This session covers how various parts of the Hadoop ecosystem can interact in a secure way to address enterprise security requirements. It will focus on the advanced authorization features enabled by the Apache Sentry (incubating) project.� Working with Geospatial Data Using Hadoop and HBase and How Monsanto Used It to Help Farmers Increase Their Yield Tues., Oct. 29, 5:05pm – Sutton Center/Sutton South Erich Hochmuth�(Monsanto),�Amandeep Khurana Monsanto is building new technology driven products for its customers that will leverage big data. This talk describes how Monsanto is building these scalable applications with geospatial data, using Hadoop and HBase as the backend systems.� Unifying Your Data Management Platform with Hadoop: Batch and Real-time Machine Data Ingest, Alerts, and Analytics Weds., Oct. 30, 11:50am�- Grand Ballroom East Jayant Shekhar� Today Hadoop serves as a unified platform for near-real-time (NRT) and batch workflows. In this session, dive into the details of using SolrCloud and Cloudera Impala together to serve search queries, by integrating Flume to stream events into Solr, Impala and HBase.� Practical Performance Analysis and Tuning for Cloudera Impala Weds., Oct. 30, 2:35pm – Murray Hill Suite Greg Rahn� Impala brings SQL to Hadoop, but it also brings SQL performance tuning to those using the platform. This session will cover Impala performance analysis topics to aid in answering the question, �Why is my query slow?�, as well as practical tips and techniques to get the best performance from Impala.� Trickery and Tooling for Distributed System Diagnosis and Debugging Weds., Oct. 30, 5pm – Murray Hill Suite Philip Zeyliger All is quiet on the log file front, but yet the system is down. What next? This talk covers the tricks of the trade for debugging distributed systems. Motivated by experience gained diagnosing Hadoop, we�ll dig into the JVM, Linux esoterica, and outlier visualization.� Meetups During the Week Evening meetups are planned for Impala, Cloudera Manager, Apache Hive, Apache Sqoop, Apache HBase, and Apache Flume, and Cloudera will help support the NYC Hadoop Users for its traditional pre-Hadoop World meeting. Stay tuned for more details about dates, times, and locations. Cloudera Exhibition Visit Booth 403 in the sponsor pavilion to learn more about how our Platform for Big Data empowers enterprises to Ask Bigger Questions of all their data! You can also see demos of Cloudera Manager and of Hue, the open source Web UI that makes Hadoop easier to use. Don’t forget your Cloudera Tshirt! We look forward to seeing you at Strata Conference + Hadoop World 2013! Register with the code CLOUDERA and receive 20% off admission.</snippet></document><document id="151"><title>This Month in the Ecosystem</title><url>http://blog.cloudera.com/blog/2013/08/this-month-in-the-ecosystem/</url><snippet>The ecosystem is evolving at a rapid pace – so rapidly, that important developments are often passing through the public attention zone too quickly. Thus, we think it might be helpful to bring you a digest (by no means complete!) of our favorite highlights on a regular basis. (This effort, by the way, has different goals than the fine Hadoop Weekly newsletter, which has a more expansive view – and which you should subscribe to immediately, as far as we�re concerned.) Find the first installment below. Although the time period reflected here is obviously more than a month long, we have some catching up to do before we can move to a truly monthly cadence. The HBase Community Has Its Day: HBaseCon 2013 On June 13, more than 700 Apache HBase enthusiasts converged in San Francisco to get deep-dives about HBase operations, internals, case studies, and ecosystem – including sessions from users like Pinterest, Groupon, Yahoo!, Box, Salesforce.com, and Twitter. Unquestionably, the memes of the day were SQL-over-HBase (hello, Cloudera Impala), Search-over-HBase (hello, Cloudera Search), and enterprise readiness (metrics, scalability, reliability). See session presentations and video Search Comes to Hadoop Cloudera Search, introduced as a beta release in late June, marries Apache Solr with Hadoop to bring a familiar, Google-like search experience to analysts who prefer to avoid Java and SQL — and to bring freedom from dedicated search clusters to the IT department. Yet another example of bringing the app/workload to the data, not the other way around. Explore Cloudera Search Hadoop Memes: Hadoop Summit San Jose Similar to HBaseCon, SQL-over-Hadoop was the prevailing theme (catch this panel). The imminent Hadoop 2.0 beta, including YARN, was another popular topic. (BTW, did you know that Hadoop 2.0 including MR2/YARN has been shipping as a deployment option in CDH4 for over a year? Indeed, it has.) “Data is the New Bacon” T-shirts proved to be the fashion statement of the show. See session presentations and videos Morphlines Graduates into the CDK Cloudera Morphlines is a fascinating new open source framework for building and integrating ETL apps that move/transform data between Hadoop, Solr, data warehouses, and so on. As of July 10, the Morphlines libraries are part of the Cloudera Development Kit (CDK). Learn more about Morphlines Concurrency and Authentication for Apache Hive: HiveServer2 For all its usefulness since its donation by Facebook to the ecosystem, Hive has proven to be lacking in several enterprise features — most notably, in support for concurrency and authentication. In response to those needs, Cloudera contributed HiveServer2 to Hive 0.11 (and ships it inside CDH), bringing concurrency and authentication to Hive. Technical overview of HiveServer2 UC Berkeley Creates “Masters of Big Data” We think this is great:� UC Berkeley�s School of Information announced that it will offer the country�s first fully online Master of Information and Data Science (MIDS) degree. The more people with the skills to consume Hadoop as end-users, the bigger the Hadoop ecosystem will grow. Learn more about the MIDS It�s All About the Use Case: Data Impact Awards Finally, users of CDH have their very own award. Any intriguing use case (running in production of course) is eligible for the awards, which will be announced at Strata + Hadoop World 2013. Explore the Data Impact Awards Sentry Fills Hadoop’s Enterprise Security Gap While Hadoop has strong security at the filesystem level, it lacks the granular support needed to adequately secure access to data by users and BI applications. Sentry, a new authorization module for Hadoop (now shipping with CDH and Impala and recently proposed for the Apache Incubator), aims to address that gap. Technical overview of Sentry OSCON Attendees Reveal Big Interest in Big Data Historically not focused on Big Data, OSCON 2013 proved itself to be a hidden reservoir of interest in that topic – if visitors to the Cloudera exhibit were any indication. The QuickStart VM and �Data is the New Bacon/Tofu� T-shirts were powerful attractive forces. Read more about Cloudera�s presence at OSCON 2013 It�s All About You: Community Forums for Cloudera Users One of my favorite developments. As a complement to tried-and-true mailing lists, users of Cloudera Standard and customers of Cloudera Enterprise can now ask questions, get answers, and build their online reputations via a new community forums environment.� Join the conversation at community.cloudera.com Columnar Storage for Hadoop: Parquet 1.0 Parquet, the open source columnar storage library project co-founded by Cloudera and Twitter, hit the 1.0 milestone. And Impala users worldwide rejoiced, because Impala performance will only get even better now. Learn more about Parquet 1.0� Making Data Integration Delicious: Apache Sqoop Cookbook Sqoop is an invaluable tool for integrating Hadoop clusters with traditional, relational-oriented infrastructure. This new book from Apache Sqoop Committers/PMC Members Kathleen Ting and Jarek Jarcec Cecho will help you use that tool with great effect. Peek inside the Apache Sqoop Cookbook The next installment of “This Month in the Ecosystem” will publish in early September. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="152"><title>How-to: Use Eclipse with MapReduce in Cloudera’s QuickStart VM</title><url>http://blog.cloudera.com/blog/2013/08/how-to-use-eclipse-with-mapreduce-in-clouderas-quickstart-vm/</url><snippet>One of the common questions I get from students and developers in my classes relates to IDEs and MapReduce: How do you create a MapReduce project in Eclipse and then debug it? To answer that question, I have created a screencast showing you how, using Cloudera’s QuickStart VM. The QuickStart VM helps developers get started writing MapReduce code without having to worry about software installs and configuration.�Everything is installed and ready to go.�You can download the image type that corresponds to your preferred virtualization platform. Eclipse is installed on the VM and there is a link on the desktop to start it. MapReduce and Eclipse You can run and debug MapReduce code in Eclipse just like any other Java program.�However, there are a few differences between running MapReduce in a distributed cluster and in an IDE like Eclipse. When you run MapReduce code in Eclipse, Hadoop runs in a special mode called LocalJobRunner, under which�all the Hadoop daemons are run in a single JVM (Java Virtual Machine) instead of several different JVMs.�Another difference is that all file paths default to local file paths, not HDFS ones. With those caveats in mind, you can start putting in your breakpoints and debug your MapReduce code like any other Java program. If you want to clone the same Git project as I do in the screencast, you can find it here.�From the terminal, type in: git clone https://github.com/eljefe6a/UnoExample.git The project will be cloned to the current directory as a subdirectory. Note that creating Eclipse projects manually is the easy way to get started.� If you are going to have Hadoop as part of an automated build process, you will want to do this in Maven.�In Maven, you can create Eclipse projects — this blog post tells you how.�If you want to compile Hadoop from source using Eclipse, this post shows you how. Conclusion Whether you to start writing some MapReduce code or debug existing code, the QuickStart VM will help you do it quickly and easily.�This screencast walks you through it and gets you coding on your favorite IDE. Further reading: How-to: Develop CDH Applications with Maven and Eclipse Jesse Anderson is an instructor with Cloudera University. (Jesse just released a series of screencasts about Hadoop MapReduce. It�s published again by the good people at Pragmatic Programmers. These screencasts are the best way for a beginner to learn about Hadoop — unless they�re sitting in his Cloudera University class!)</snippet></document><document id="153"><title>How-to: Deploy Hadoop Clusters Automatically with Dell Crowbar and Cloudera Manager</title><url>http://blog.cloudera.com/blog/2013/08/how-to-deploy-hadoop-clusters-automatically-with-dell-crowbar-and-cloudera-manager/</url><snippet>The following guest post, from Mike Pittaro of Dell’s Cloud Software Solutions team, describes his team�s use of the Dell Crowbar tool in conjunction with the Cloudera Manager API to automate cluster provisioning. Thanks, Mike! Deploying, managing, and operating Apache Hadoop clusters can be complex at all levels of the stack, from the hardware on up. To hide this complexity and reduce deployment time, since 2011, Dell has been using Dell Crowbar in conjunction with Cloudera Manager to deploy the Dell | Cloudera Solution for Apache Hadoop�for joint customers. Cloudera Manager does a great job of deploying and managing the Hadoop layers of a cluster, but it depends on an operating system to be in place first. Meanwhile, to complement those capabilities, Dell Crowbar�is a complete automated operations platform, designed to deploy layers of infrastructure on bare-metal servers and all the way up the stack. In the Dell | Cloudera Solution, we use Crowbar to provision the hardware, configure it, and install Red Hat Enterprise Linux and Cloudera Manager — then, Cloudera Manager takes over to guide the user to a functioning cluster. Furthermore, we use the Cloudera Manager API�to entirely automate the cluster setup process. In this post, I�ll provide more details about how we have successfully integrated the Cloudera Manager API with Dell Crowbar. Crowbar Overview Dell Crowbar, inspired by DevOps principles, is based on the concept of defining hardware and software configuration in code, just like applications. Crowbar uses a modular approach, where each component of the stack is deployed as an independent unit. The definition of each component is a Crowbar module called a �Barclamp�. Figure 1 shows the Barclamps included in a typical Hadoop installation — including hardware configurations and functions like DNS and NTP — all the way up to the Cloudera components. Crowbar Barclamps for Hadoop During the deployment process, the Crowbar interface is used to create a �proposal� based on a Barclamp. Within the proposal editor, the hardware nodes are assigned their intended roles, and then the proposal is saved and applied. A single node can have several roles assigned depending on its function, and Barclamps are aware of dependencies between roles. For example, in order to deploy CDH, the single Cloudera Manager proposal is applied and subsequently Crowbar takes care of all the other requirements. Cloudera Manager Barclamp Figure 2 shows the Cloudera Manager proposal within Crowbar. The Barclamp defines seven roles available for nodes within the cluster: Clouderamanager-cb-adminnode � the node running the Crowbar Clouderamanager-server � the node running the Cloudera manager server Clouderamanager-namenode � nodes running the name server, whether active/passive or quorum HA is being used. Clouderamanager-datanode – the cluster data nodes Clouderamanager-edgenode � an edge or gateway node for client tools Clouderamanager-ha-journaling node � a quorum-based journaling node for quorum HA Clouderamanager-ha-filernode � an NFS filer node, for active/passive HA using a shared NFS mount Cloudera Manager Barclamp proposal showing nodes and roles In the Crowbar interface, available hardware nodes are dragged to the appropriate roles in the proposal, and then the proposal is applied. At that point, Crowbar analyzes dependencies and makes any required changes to the nodes. If the hardware nodes are new, these changes might involve hardware configuration and a complete OS install. If the nodes already have an OS installed, the changes simply might involve installing some additional packages.� In the proposal editor, the roles defined within Crowbar closely correspond to the typical roles of nodes in a Hadoop cluster and can be mapped almost directly to their corresponding Hadoop services within Cloudera Manager. So, we decided to use this information to integrate with the Cloudera Manager API.� The integration is enabled by setting the deployment mode in the proposal to �auto mode�, as shown in Figure 3. In auto mode, applying the proposal will configure the hardware and OS as necessary, install Cloudera Manager on the edge node, install the agents on the remaining nodes, and then use the Cloudera Manager API to create the cluster based on the Crowbar roles. The install also sets up a local Yum repository for all the CDH packages, so the install can proceed without full Internet access. Auto-deployment mode in the Cloudera Manager Barclamp Behind the Scenes with the API Crowbar is implemented primarily in Ruby, and the Barclamps are wrappers around Opscode Chef, which also uses Ruby for its recipes and cookbooks. This means that any automatic deployment implementation in Crowbar is also written in Ruby. The Cloudera Manager API provides client libraries for Java and Python, but not Ruby, so the first step in implementing the integration was to create a Ruby client library for the API. The API is standards based, using REST and JSON as a data format, so this was a relatively straightforward process. The Ruby library parallels the Python implementation. (The library�is currently Apache licensed as part of Crowbar, but could be split out in the future if there�s community interest.) The actual cluster deployment logic is in the file cm-server.rb. It turns out to be relatively straightforward, and follows the flow described in �How-to: Automate Your Hadoop Cluster from Java�. The information about the nodes, their roles, and their configuration are already available in Crowbar, so the code primarily iterates through the Crowbar data structures, and makes the calls to create the cluster, the HDFS service, and the MapReduce Service. Since Cloudera Manager also uses a role-based approach, this mapping turns out to be very clean. There�s even some logic there to handle licensing. The Cloudera Manager API corresponds directly to Cloudera�s packaging, which includes Standard (free) and Enterprise (paid support, indemnification, and enterprise features, with a 60-day trial option) versions. If a Cloudera license is entered in the Crowbar proposal it is used; otherwise, the Enterprise trial license is activated. Our current integration uses the free APIs for HDFS and MapReduce configuration. So far, the automatic deployment capability has significantly reduced the time to deploy a cluster, especially if there are a large number of nodes. More important, it eliminates the error-prone manual process of re-entering the node and role information into the Cloudera Manager wizard. (This is a new feature, and we�re still collecting feedback on enhancements.) We currently set up the core HDFS and MapReduce services for the cluster. In the future, we will likely configure more services as part of the automatic deployment. Cloudera Manager role groupsprovide another interesting opportunity for further integration, since we have information about the actual hardware configuration in Crowbar, and can start grouping nodes together based on hardware-specific features. In the meantime, Dell and Cloudera continue to collaborate on other new features and integration in order to help customers more easily set up and manage Hadoop clusters. Mike Pittaro (@pmikeyp) is Principal Architect on Dell’s Cloud Software Solutions team. He has a background in high performance computing, data warehousing, and distributed systems, specializing in designing and developing big data solutions.</snippet></document><document id="154"><title>Impala Meetup in San Francisco on Aug. 20</title><url>http://blog.cloudera.com/blog/2013/07/impala-meetup-in-san-francisco-on-aug-20/</url><snippet>Cloudera Impala has made huge progress since its initial announcement – and there’s even more good news on the roadmap. To learn more, plan to attend an Impala meetup hosted by Cloudera in its San Francisco offices on the evening of Aug. 20: Tentative Agenda 6-6:30pm: Eating, Drinking &amp; Networking 6:30-7pm: Impala Roadmap/Discussion (Justin Erickson) 7-7:30pm: Using UDFs in Impala/Discussion (Nong Li) 7:30-8pm: Performance Tuning for Impala/Discussion (Marcel Kornacker) Plus, share your Impala experiences with the group and what developers can do to make them even better. Click the button below to RSVP!</snippet></document><document id="155"><title>Customer Spotlight: Cerner�s Ryan Brush Presents �Thinking in MapReduce� at StampedeCon</title><url>http://blog.cloudera.com/blog/2013/07/customer-spotlight-cerners-ryan-brush-presents-thinking-in-mapreduce-at-stampedecon/</url><snippet>For those of you attending this week�s StampedeCon event in St. Louis, I�d encourage you to check out the �Thinking in MapReduce� session presented by Cerner�s Ryan Brush. The session will cover the value that MapReduce and Apache Hadoop offer to the healthcare space, and provide tips on how to effectively use Hadoop ecosystem tools to solve healthcare problems. Big Data challenges within the healthcare space stem from the standard practice of storing data in many siloed systems. Hadoop is allowing pharmaceutical companies and healthcare providers to revolutionize their approach to business by making it easier and more cost efficient to bring together all of these fragmented systems for a single, more accurate view of health. The end result: smarter clinical care decisions, better understanding of health risks for individuals and populations, and proactive measures to improve health and reduce healthcare costs. Hadoop is quickly becoming a core component of Cerner�s data management environment, complementing existing relational systems. (Cerner has more than a petabyte across all Hadoop clusters today, and multiple petabytes on relational systems.) The unique value delivered by Hadoop lies in its ability to combine unstructured with structured data, and to then apply complex computations to that data at large scale. For example, Brush�s team is building search indexes for semi-structured medical docs that allow users to find information on heart attacks, incorporating both data that use colloquial language (e.g. �heart attack�; �heart disease�) or scientific terminology (e.g. �myocardial infarction�; �MI�; �acute MI�; �ST-elevation myocardial infarction�; �non-ST-elevation myocardial infarction�). This is no simple task, but with Hadoop and MapReduce, it is feasible. In Ryan�s session, which takes place on Weds., July 31, at 9:10am, he will walk through the basics of MapReduce, design best practices for logic and data models in Hadoop, and how Cerner is applying MapReduce design patterns to solve big healthcare problems. Don�t miss it! Karina Babcock is Cloudera�s Customer Programs &amp; Marketing Manager.</snippet></document><document id="156"><title>Announcing Parquet 1.0: Columnar Storage for Hadoop</title><url>http://blog.cloudera.com/blog/2013/07/announcing-parquet-1-0-columnar-storage-for-hadoop/</url><snippet>We’re very happy to re-publish the following post from Twitter analytics infrastructure engineering manager�Dmitriy Ryaboy�(@squarecog). In March we announced the Parquet project, the result of a collaboration between Twitter and Cloudera intended to create an open-source columnar storage format library for Apache Hadoop. Today, we�re happy to tell you about a significant Parquet milestone: a 1.0 release, which includes major features and improvements made since the initial announcement. But first, we�ll revisit why columnar storage is so important for the Hadoop ecosystem. What is Parquet and Columnar Storage? Parquet is an open-source columnar storage format for Hadoop. Its goal is to provide a state of the art columnar storage layer that can be taken advantage of by existing Hadoop frameworks, and can enable a new generation of Hadoop data processing architectures such as Impala, Drill, and parts of the Hive �Stinger� initiative. Parquet does not tie its users to any existing processing framework or serialization library. The idea behind columnar storage is simple: instead of storing millions of records row by row (employee name, employee age, employee address, employee salary�) store the records column by column (all the names, all the ages, all the addresses, all the salaries). This reorganization provides significant benefits for analytical processing: Since all the values in a given column have the same type, generic compression tends to work better and type-specific compression can be applied. Since column values are stored consecutively, a query engine can skip loading columns whose values it doesn�t need to answer a query, and use vectorized operators on the values it does load. These effects combine to make columnar storage a very attractive option for analytical processing. Implementing a columnar storage format that can be used by the many various Hadoop-based processing engines is tricky. Not all data people store in Hadoop is a simple table � complex nested structures abound. For example, one of Twitter�s common internal datasets has a schema nested seven levels deep, with over 80 leaf nodes. Slicing such objects into a columnar structure is non-trivial, and we chose to use the approach described by Google engineers in their paper Dremel: Interactive Analysis of Web-Scale Datasets. Another complexity derives from the fact that we want it to be relatively easy to plug new processing frameworks into Parquet. Despite these challenges, we are pleased with the results so far: our approach has resulted in integration with Hive, Pig, Cascading, Impala, and an in-progress implementation with Drill, and is currently in production at Twitter. What�s in the Parquet 1.0 Release Parquet 1.0 is available for download on GitHub and via Maven Central. It provides the following features: Apache Hadoop Map-Reduce Input and Output formats Apache Pig Loaders and Storers Apache Hive SerDes Cascading Shemes Impala support Self-tuning dictionary encoding Dynamic Bit-Packing / RLE encoding Ability to work directly with Avro records Ability to work directly with Thrift records Support for both Hadoop 1 and Hadoop 2 APIs Eighteen contributors from multiple organizations (Twitter, Cloudera, Criteo, UC Berkeley AMPLab, Stripe and others) contributed to this release. Improvements since Initial Parquet Release When we announced Parquet, we encouraged the greater Hadoop community to contribute to the design and implementation of the format. Parquet 1.0 features many contributions from the community as well as the initial core team of committers; we will highlight two of these improvements below. Dictionary encoding The ability to efficiently encode columns in which the number of unique values is fairly small (10s of thousands) can lead to a significant compression and processing speed boost. Nong Li and Marcel Kornacker of Cloudera teamed up with Julien Le Dem of Twitter to define a dictionary encoding specification, and implemented it both in Java and C++ (for Impala). Parquet�s dictionary encoding is automatic, so users do not have to specify it � Parquet will dynamically turn it on and off as applicable, given the data it is compressing. Hybrid bit packing and RLE encoding Columns of numerical values can often be efficiently stored using two approaches: bit packing and run-length encoding (RLE): Bit packing uses the fact that small integers do not need a full 32 or 64 bits to be represented, and packs multiple values into the space normally occupied by a single value. There are multiple ways to do this, but we use a modified version of Daniel Lemire�s JavaFastPFOR library (read more about it here). Run-length encoding turns �runs� of the same value, meaning multiple occurrences of the same value in a row, into just a pair of numbers: the value, and the number of times it is repeated. Our hybrid implementation of bit-packing and RLE monitors the data stream, and dynamically switches between the two types of encoding, depending on what gives us the best compression. This is extremely effective for certain kinds of integer data and combines particularly well with dictionary encoding. A Growing Community One of the major goals of Parquet is to provide a columnar storage format for Hadoop that can be used by many projects and companies, rather than being tied to a specific tool. We are heartened to find that so far, the bet on open-source collaboration and tool independence has paid off. This release includes quality contributions from 18 developers, who are affiliated with a number of different companies and institutions. Looking Forward We are far from done, of course; there are many improvements we would like to make and features we would like to add. To learn more about these items, you can see our roadmap on the README or look for the �pick me up!� label on GitHub. You can also join our mailing list at parquet-dev@googlegroups.com and tweet at us @ParquetFormat to join the discussion. �</snippet></document><document id="157"><title>Thanks for the Memories, #OSCON 2013</title><url>http://blog.cloudera.com/blog/2013/07/thanks-for-the-memories-oscon-2013/</url><snippet>OSCON 2013 is already receding in the rear-view mirror, but we had a great time. Cloudera’s sessions were very well attended — with Tom Wheeler taking the prize (well over 200 attendees for his “Introduction to Apache Hadoop” tutorial) — but best of all was the opportunity to meet and mingle with people in the broader open source community. If you visited us at Booth 420, we hope you will now download and install the QuickStart VM after seeing it in our demo, and that your questions were adequately answered (most popular question: “Can you tell me more about Cloudera Impala?”) In my biased opinion, the crowning achievement was our ability to not only distribute a couple hundred “Data is the New Bacon” Tshirts within a 36-hour period, but to clean ourselves out of the meat-free version shortly thereafter, as well: And the audience roared: I’m not sure if we can ever do better than that in the T-shirt department, but we will certainly try! Thanks to everyone who attended Cloudera’s sessions, watched a demo, or just stopped by the booth for a shirt — we appreciate it. Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="158"><title>It’s All About You: New Community Forums for Cloudera Customers and Users</title><url>http://blog.cloudera.com/blog/2013/07/its-all-about-you-new-community-forums-for-cloudera-customers-and-users/</url><snippet>This is a great day for technical end-users – developers, admins, analysts, and data scientists alike. Starting now, Cloudera complements its traditional mailing lists with a new, feature-rich community�forums�intended for users of Cloudera�s Platform for Big Data!� (Login using your existing credentials or click the link to register.) Although mailing lists have long been a standard for user interaction, and will undoubtedly continue to be, they have flaws. For example, they lack structure or taxonomy, which makes consumption difficult. Search functionality is often less than stellar and users are unable to build reputations that span an appreciable period of time. For these reasons, although they�re easy to create and manage, mailing lists inherently limit access to knowledge and hence limit adoption. The new service brings key additions to the conversation: functionality, search, structure and scalability. It is now considerably easier to ask questions, find answers (or questions to answer), follow and share threads, and create a visible and sustainable reputation in the community. And for Cloudera customers, there�s a bonus: your questions will be escalated as bonafide support cases under certain circumstances (see below). Please see the FAQs below for additional detail. Will these forums replace Cloudera�s existing mailing lists/groups, e.g., cdh-user? No. We believe it�s always best to provide choices and there will undoubtedly be people who prefer to use lists. However, we�re confident that forum features like reply-by-email will make mailing lists increasingly undesirable for most users, in comparison. Should I ask my questions on mailing lists/groups, or forums? See above. We encourage you to use the service that best meets your needs (or use both) — that said, we believe most users will eventually use forums more. Is registration required? No registration is expected or needed for browsing. However, if you choose to participate, we�ll need your name and email address. Who will answer my questions? As is currently the case with existing mailing lists, these forums will be closely monitored by Cloudera employees (from Support, Engineering, etc), many of whom are project committers. The ultimate goal, however, is to see users assist other users — which is a defining characteristic of a healthy community. What is the reasoning behind your taxonomy? We made a sincere effort to balance the requirements of simplicity and thoroughness. Of course, we�re always open to suggestions for improvements. Can I use these forums to promote products or services? No. These forums are intended for non-commercial purposes, and this principle will be strictly enforced. I�m a Cloudera customer. In what circumstances will my issue be escalated as a support case? We have both automated and business processes in place to assist subscription customers in getting their thread posts turned into support cases when necessary. In general, any paid customer�s thread that goes unsolved for two days will automatically become a support case. Furthermore, Support Engineers and moderators will use their good judgment based on the severity/urgency of the issue to intervene if necessary. Where is the user documentation? You can find all user doc (for getting started etc.) here. If you have questions or feedback, feel free to use comments below or post them to the �Community Suggestions� area at community.cloudera.com. Enjoy! Justin Kestelyn is Cloudera’s developer outreach director.</snippet></document><document id="159"><title>How-to: Manage Heterogeneous Hardware for Apache Hadoop using Cloudera Manager</title><url>http://blog.cloudera.com/blog/2013/07/managing-heterogeneous-hardware-for-apache-hadoop-using-cloudera-manager/</url><snippet>In a prior blog post, Omar explained two important concepts introduced in Cloudera Manager 4.5: Role Groups and Host Templates. In this post, I�ll demonstrate how to use role groups and host templates to easily expand an existing CDH cluster onto heterogeneous hardware. If you haven�t already looked at Omar�s post, I�d recommend doing so before reading this one, as I�ll assume you are familiar with role groups and host templates. Although these instructions/screenshots are premised on Cloudera Manager 4.5, they are valid for subsequent releases as well. Initial State and Goal This post describes enlarging a CDH4 cluster running HDFS and MapReduce from five nodes to 10. Initially, our cluster contains the hosts mikem-old-[1-5].ent.cloudera.com. Each host has a single physical drive storing HDFS data mounted at /data/1/. You can see the count of roles and services in the screenshot below: The goal is to add five new hosts to the above cluster: mikem-new-[1-5].ent.cloudera.com.�Moreover, each of these hosts should run a DataNode as well as a TaskTracker. The new hosts are identical to the old hosts except that they have two physical drives storing HDFS data mounted at /data/1/ and /data/2/. As such, the dfs.datanode.data.dir and mapred.local.dir parameters need to be set differently for DataNodes and TaskTrackers running on the new hosts. This difference will motivate the use of multiple Role Groups to enlarge the cluster. Step 1: Create an Empty Role Group for New DataNodes The DataNodes running on the old hosts have a single data directory at /data/1/dfs/dn. The DataNodes on the new hosts should have two data directories at /data/1/dfs/dn and /data/2/dfs/dn. To capture this difference, you can create a new DataNode role group for the DataNodes on the new hosts. From the HDFS service page, navigate to the Role Groups membership page for HDFS by selecting Role Groups from the Configuration drop-down menu. On the Role Groups membership page, click the Create new group… button to open up the Role Groups creation dialog. Call the new role group DataNode (2 Drives). Also, be sure to copy existing configuration values from the DataNode (Base) group. Before clicking Create, the dialog box will look something like this: After creating the group, select View and Edit from the Configuration dropdown, and select DataNode (2 Drives) from the left navigation bar. Set the DataNode Data Directory (dfs.datanode.data.dir) value to include both /data/1/dfs/dn and /data/2/dfs/dn. Step 2: Set Up an Empty Role Group For New TaskTrackers The TaskTrackers on the old hosts have a single MapReduce local directory at /data/1/mapred/local. The TaskTrackers on the new hosts should have two MapReduce local directories at /data/1/mapred/local and /data/2/mapred/local. As was done for HDFS above, navigate to the MapReduce service page, select the Role Groups option from the Configuration drop down, and create a new TaskTracker role group called TaskTracker (2 Drives). Make sure to copy the configuration values from TaskTracker (Base). Next, select the View and Edit option from the Configuration drop down, then select TaskTracker (2 Drives) from the left nav bar. Change the TaskTracker Local Data Directory List (mapred.local.dir) value to include both /data/1/mapred/local and /data/2/mapred/local. Step 3: Create a Host Template Containing the New Role Groups There are now two new role groups that reflect the configuration changes needed to support DataNode and TaskTracker roles on our new hosts. The next step is to use these role groups to create a host template to apply to the new hosts, mikem-new[1-5].ent.cloudera.com. Navigate to the Host Template Management page by clicking the top level Hosts tab, then the lower level Templates tab. Open the dialog for creating a new Host Template. For this example, call the new host template� Slave (2 Drives). Select the new role groups, DataNode (2 Drives) and TaskTracker (2 Drives).� After clicking the Create button, the Host Template screen should show the new host template. Step 4: Run Add Hosts Wizard and Apply the Host Template Now we are ready to actually add the new hosts to the cluster. From the Host Templates page, click the Status tab, and then the Add Hosts To Cluster button. Proceed through the Add Hosts Wizard as normal, adding hosts mikem-new-[1-5].ent.cloudera.com. After deploying packages or Parcels to the new hosts, we end up at a page that looks like this: This page allows you to apply a previously existing host template to the new hosts (as well as create one if necessary). Choose the Slave (2 Drives) template that we just created and click Continue. And that�s that! When the Add Hosts Wizard completes, it is done. Applying a host template automatically adds the necessary roles to the new hosts, then places those roles into the newly created role groups so that they are configured correctly. A look at the All Services page now shows that our cluster contains 10 DataNodes and 10 TaskTrackers. Tips If you forget to create a host template before running the Add Hosts Wizard, don�t worry!�As long as you have created the requisite role groups, the wizard provides an option to create a new host template. If you want to apply a host template to an existing host, you can do so by navigating to the Hosts page, selecting the hosts you want to apply a host template to, and executing �Apply Host Template� from the �Actions For Selected� menu. Applying a host template to existing host never deletes existing roles.� Instead, it adds new roles and moves existing roles to new groups if necessary. Further Reading: How-to: Easily Configure and Manage Clusters in Cloudera Manager 4.5 Managing Role Groups (Docs) Adding a Host to the Cluster (Docs) Mike Mellenthin is a Software Engineer on the Enterprise team. &gt; Have questions? Post them to the Community Forum for Cloudera Manager.</snippet></document><document id="160"><title>Calling All CDH Users: Submit Your Nomination for the Data Impact Awards</title><url>http://blog.cloudera.com/blog/2013/07/calling-all-cdh-users-submit-your-nomination-for-the-data-impact-awards/</url><snippet>Users of CDH, Cloudera’s Big Data platform, are solving big problems and building amazing solutions with Apache Hadoop. We at Cloudera are very proud of our customers� accomplishments, and it�s time to showcase them. This year we�re thrilled to present the first annual Data Impact Awards, an awards program designed to recognize Hadoop innovators for their achievements in five categories: Business Impact: Driving tangible results to an organization, company or industry by empowering new capabilities that generate top-line growth and/or increased competitive advantage, or improving operational efficiencies that result in quantifiable bottom-line savings. Social Impact: Putting Hadoop to work in a way that has a positive impact on the planet, society, or humankind at large. Community Contribution(s): Driving significant, steady innovations within the Hadoop community through active participation and contributions to ecosystem projects, online forums, and/or user groups. Pervasive User Adoption:Supporting the proliferation of Hadoop across user groups — which may include internal and/or external users — through concerted efforts, such as extensive training or certification. Integration with Existing IT: Successfully integrating Hadoop with existing relational data warehouse, data marts, and analytic systems for a truly symbiotic and interconnected big data ecosystem. We�re accepting nominations now through Sept. 30, 2013. Don�t be shy — submit your nomination today! Nominations are open to any individual representing any organization that runs CDH in a production environment. We will also accept entries from partner sponsors of nominated organizations, but a point of contact must be provided for both the sponsor and the nominee. We�ll announce the winners at Strata + Hadoop World in New York in October. Questions? Contact Karina@Cloudera.com. Good luck! Karina Babcock is Cloudera�s Customer Programs &amp; Marketing Manager.</snippet></document><document id="161"><title>With Sentry, Cloudera Fills Hadoop�s Enterprise Security Gap</title><url>http://blog.cloudera.com/blog/2013/07/with-sentry-cloudera-fills-hadoops-enterprise-security-gap/</url><snippet>Every day, more data, users, and applications are accessing ever-larger Apache Hadoop clusters. Although this is good news for data driven organizations overall, for security administrators and compliance officers, there are still lingering questions about how to enable end-users under existing Hadoop infrastructure without compromising security or compliance requirements. While Hadoop has strong security at the filesystem level, it lacks the granular support needed to adequately secure access to data by users and BI applications. Today, this problem forces organizations in industries for which security is paramount (such as financial services, healthcare, and government) to make a choice: either leave data unprotected or lock out users entirely. Most of the time, the preferred choice is the latter, severely inhibiting access to data in Hadoop. Today, Cloudera is excited to launch Sentry, a new open source project that addresses these concerns. Sentry is an authorization module for Hadoop that provides the granular, role-based authorization required to provide precise levels of access to the right users and applications. Its new support for role-based authorization, fine-grained authorization, and multi-tenant administration allows Hadoop operators to: Store more sensitive data in Hadoop, Give more end-users access to that data in Hadoop, Create new use cases for Hadoop, Enable multi-user applications, and Comply with regulations (e.g., SOX, PCI, HIPAA, EAL3) Sentry is now shipping as an add-on to CDH4.3 and will ship as a core component of CDH4.4 and Impala 1.1 and onward. Furthermore, we intend to nominate Sentry for the Apache Incubator to maximize its usefulness across the Hadoop ecosystem. In the remainder of this post, we�ll offer more detail about why Sentry is needed and provide a technical overview of its capabilities and architecture. Hadoop Security, Before and After For Hadoop operators in finance, government, healthcare, and other highly-regulated industries to enable access to sensitive data under proper compliance, each of the four functional requirements must be achieved: Perimeter Security: Guarding access to the cluster through network security, firewalls, and, ultimately, authentication to confirm user identities Data Security: Protecting the data in the cluster from unauthorized visibility through masking and encryption, both at rest and in transit Access Security: Defining what authenticated users and applications can do with the data in the cluster through filesystem ACLs and fine-grained authorization Visibility: Reporting on the origins of data and on data usage through centralized auditing and lineage capabilities Thanks to recent work in the Hadoop community (such as Cloudera�s contribution of HiveServer2 to Hive) as well as integration with solution providers, Requirements 1 and 2 are now addressed through Kerberos authentication, encryption, and masking. Cloudera Navigator�supports Requirement 4 via centralized auditing for files, records, and metadata. But Requirement 3, for access security, had been largely unaddressed, until Sentry. Access and Authorization without Sentry Without Sentry, there are two suboptimal choices for authorization — coarse-grained HDFS authorization and advisory authorization — that do not meet typical compliance and data security needs for these reasons: Coarse-grained HDFS authorization: The primary mechanism of secure access and authorization is limited by the granularity of the HDFS file model. File-level authorization is coarse grained in that there is no ability to control access to the data within the file: a user either has access to everything in a file, or nothing. Furthermore, the HDFS permission model does not enable multiple groups to have different levels of access on the same data set. Advisory authorization: Advisory authorization is a rarely-used mechanism in Hive, designed to let benevolent users self-regulate against accidently deleting or overwriting production data. The system is �self-service� in that users can grant themselves any permission they�d like, as well as circumvent it. Thus, it doesn�t stop a malicious user from gaining access to sensitive data once they�ve been authenticated. Access and Authorization with Sentry With the introduction of Sentry, Hadoop can now meet key RBAC (role-based access control) requirements for enterprise and government customers in these areas: Secure authorization: Sentry provides the ability to control and enforce access to data and/or privileges on data for authenticated users. Fine-grained access control: Sentry provides support for fine-grained access control to data and metadata in Hadoop. In its initial release for Hive and Impala, Sentry allows access control at the server, database, table, and view scopes at different privilege levels including select, insert, and all — allowing administrators to use views to restrict access to columns or rows. Administrators can also mask data within a file as required by leveraging Sentry and views with case statements or UDFs. Role-based administration: Sentry supports ease of administration through role-based authorization; you can easily grant multiple groups access to the same data at different privilege levels. For example, for a particular data set you may give your fraud detection team rights to view all columns, your analysts rights to view only non-sensitive or non-PII (personally identifiable information) columns, and your ingest processing pipeline rights to insert new data into HDFS. Multi-tenant administration: Sentry allows permissions on different data sets to be delegated to different administrators. In the case of Hive/Impala, Sentry allows administration of privileges at the level of a database/schema. Unified platform: Sentry provides a uniform platform for securing data; it uses existing Hadoop Kerberos security for authentication. Also, the same Sentry policy can be enforced while accessing data through either Hive or Impala. In the future, Sentry policy can also be extended to other components (more about that in the next section). Next, we�ll explain how the Sentry architecture delivers these capabilities. Sentry Architecture Sentry is a highly modular and extensible mechanism. Initially, it allows Impala and Hive to enforce fine-grained security policies, but that capability can be extended to other frameworks, as well. Sentry architecture: Initial bindings are for Hive and Impala, with built-in extensibility to other frameworks. Sentry comprises a core authorization provider and a binding layer. The core authorization provider contains a policy engine, which evaluates and validates security policies, and a policy provider, which is responsible for parsing the policy. The binding layer provides a pluggable interface that can be leveraged by a binding implementation to talk to the policy engine. (Note that the policy provider and the binding layer both provide pluggable interfaces.) At this time, we have implemented a file-based provider that can understand a specific policy file format. The policy file can reside either in the local filesystem or HDFS to get the benefits of replication and auditing. Although Cloudera has initially implemented support for Hive and Impala, it�s important to remember that the Sentry architecture is extensible: Any developer could implement a binding for a different component (such as Pig or Cloudera Search) or build a database provider that understands policies stored in a database-backed store. The component-specific binding in Sentry implements a privilege model for the specific component and understands internal data structures. For example, the Hive binding implements a Hive-specific privilege model that allows fine-grained access to row/columns in a table as well as metadata operations such as show tables. (Impala�s model is very similar to that of Hive.) Conclusion We believe that Sentry is a major step forward in Hadoop security, making Big Data increasingly accessible by even more industries, organizations, and end-users – and giving administrators the flexibility, multi-tenant administration, and unified platform they need to make that happen easily. Cloudera is especially proud of the fact that we can not only contribute these new capabilities to the Hadoop ecosystem, but ship and support them inside our Big Data platform, Cloudera Enterprise. Sentry is now available for download as an add-on to CDH4.3, and you can explore the source code here pending its Apache Incubator proposal status. Hive support is available through a base Cloudera Enterprise subscription and Impala support through an RTQ subscription. We eagerly await your suggestions and contributions! Shreepadma Venugopalan is a Software Engineer on the Platform team, working on Sentry among other projects. Brock Noland is a Software Engineer on the Platform team as well as a committer for Apache Hive, Apache Crunch, and Apache MRunit.</snippet></document><document id="162"><title>Customer Spotlight: Motorola Mobility�s Award-Winning Unified Data Repository</title><url>http://blog.cloudera.com/blog/2013/07/customer-spotlight-motorola-mobilitys-award-winning-unified-data-repository/</url><snippet>The Data Warehousing Institute (TDWI) runs an annual Best Practices Awards program to recognize organizations for their achievements in business intelligence and data warehousing. A few months ago, I was introduced to Motorola Mobility�s VP of cloud platforms and services, Balaji Thiagarajan. After learning about its interesting Apache Hadoop use case and the success it has delivered, Balaji and I worked together to nominate Motorola Mobility for the TDWI Best Practices Award for Emerging Technologies and Methods. And to my delight, it won! Chances are, you�ve heard of Motorola Mobility. It released the first commercial portable cell phone back in 1984, later dominated the mobile phone market with the super-thin RAZR, and today a large portion of the massive smartphone market runs on its Android operating system. In recent years, Motorola Mobility implemented a Hadoop system based on Cloudera Enterprise to help create a �unified data repository� that is positively impacting business KPIs. For instance, CSAT scores have risen based on call center insights and product returns (a metric of quality) are decreasing based on service center insights. With its unified data repository platform, Motorola is getting rid of data silos and gaining a single, global view of the business. With its new unified data repository, Motorola Mobility is better equipped to make data driven decisions that are making a big impact. Congrats, Balaji and team! Learn more: Read about the TDWI Best Practices Award program See the full list of 2013 TDWI Best Practices Award winners Learn about the TDWI World Conference taking place next month in San Diego, where Best Practices Award winners will be recognized in person Find out why other Cloudera customers in the telecommunications industry are using Hadoop Karina Babcock is Cloudera�s Customer Programs &amp; Marketing Manager.</snippet></document><document id="163"><title>Breaking News: Cloudera Impala is Fast. Really Fast.</title><url>http://blog.cloudera.com/blog/2013/07/breaking-news-cloudera-impala-is-fast-really-fast/</url><snippet>Editor’s note (added Feb. 2, 2014): You can review the latest (and exciting) Impala performance benchmark results by Cloudera here. In the presentation below, Scott Leberknight of Near Infinity has done such a good and thorough job of dissecting Cloudera Impala, we want to share it with you here. Notably, Scott has run unscientific but revealing benchmarks based on the current version (1.0.1) inside the QuickStart VM compared to Apache Hive 0.11. (Spoiler: Impala queries were up to 39x faster for interactive queries.)�See here�for a set of more scientific benchmarks based on concurrent interactive queries run by Cloudera recently (Impala up to 68x faster in that case). Conclusion: Hive continues to improve as a batch processing/MapReduce framework�with Cloudera’s help. But for interactive SQL for Hadoop, Impala is the solution. View for yourself below!</snippet></document><document id="164"><title>How HiveServer2 Brings Security and Concurrency to Apache Hive</title><url>http://blog.cloudera.com/blog/2013/07/how-hiveserver2-brings-security-and-concurrency-to-apache-hive/</url><snippet>Apache Hive�was one of the first projects to bring higher-level languages to Apache Hadoop. Specifically, Hive enables the legions of trained SQL users to use industry-standard SQL to process their Hadoop data. However, as you probably have gathered from all the recent community activity in the SQL-over-Hadoop area, Hive has a few limitations for users in the enterprise space. Until recently, two in particular � concurrency and security � were largely unaddressed. To address these gaps, for Hive release 0.11, Cloudera engineers built and contributed new infrastructure for meeting these needs. In this post, you�ll learn why it�s needed, and how it works. Customer Requirements As you probably know, relational databases almost universally have a server process to support clients connecting over IPC or network connections. The clients may be native command-line editors or applications/tools using a driver such as ODBC or JDBC. In Hive, a component called HiveServer serves this purpose. But over the past few years, as adoption of Hive increased, more and more customers reported two major requirements unaddressed by HiveServer: To run more users concurrently against Hive in traditional client/server architecture To authenticate users to prevent untrusted user access and to enforce authorization around permissions to their data assets Because Hive is so important for our customers, these requirements motivated us to implement a new server process for Hive 0.11. The goal was to create a framework that handles multiple concurrent clients, supports popular authentication mechanisms, and is easy to adopt for open client implementations like JDBC and ODBC. The result of that effort, HiveServer2 (HIVE-2935), finally brings concurrency, authentication, and a foundation for authorization to Hive. Next, we�ll provide some details about these new features. HiveServer2 Architecture HiveServer2 is now available in Hive 0.11 and all other releases of Hive in CDH 4.1 and later. It implements a new Thrift-based RPC interface that can handle concurrent clients. The current release supports Kerberos, LDAP, and custom pluggable authentication. The new RPC interface also has better options for JDBC and ODBC clients, especially for metadata access. Like the original HiveServer, HiveServer2 is a container for the Hive execution engine. For each client connection, it creates a new execution context that serves Hive SQL requests from the client. The new RPC interface enables the server to associate this Hive execution context with the thread serving the client�s request. Clients for HiveServer2 JDBC: Hive 0.11 includes a new JDBC driver that works with HiveServer2, enabling users to write JDBC applications against Hive. The application needs to use the JDBC driver class and specify the network address and port in the connection URL in order to connect to Hive. The following code snippet shows how to connect to HiveServer2 from JDBC: � Class.forName("org.apache.hive.jdbc.HiveDriver");
� Connection con = DriverManager.
���� getConnection("jdbc:hive2://localhost:10000/default",
��� "hive", "passwd");
   You can review a detailed example on the Hive wiki. Beeline CLI: Hive 0.11 also includes a new command-line interface (CLI) called Beeline that works with HiveServer2. Beeline is a JDBC application based on the SQLLine CLI that supports embedded and remote-client modes. The embedded mode is where the Hive runtime is part of the client process itself; there�s no server involved. (You can explore the detailed documentation for SQLLine, which is also applicable to Beeline, here.) Note that HiveServer2 doesn�t support the original Hive CLI client, as the Beeline CLI is a functional replacement designed for the HiveServer2 interface. ODBC: Although Hive 0.11 currently doesn�t include a ODBC driver that works with HiveServer2, Cloudera makes one available. Metastore Considerations The Hive metastore service runs in its own JVM process. Clients other than Hive, like Apache Pig, connect to this service via HCatalog for metadata access. HiveServer2 supports local as well as remote metastore modes � which is useful when you have more than one service (Pig, Cloudera Impala, and so on) that needs access to metadata. This is the recommended deployment mode with HiveServer2: Authentication Authentication support is another major feature of HiveServer2. In the original HiveServer, if you can access the host/port over the network, you can access the data � so it relies on support for multiple authentication options to restrict access. In contrast, HiveServer2 support Kerberos, pass-through LDAP, and pass-through plug-able custom authentication. All client types � JDBC, ODBC, as well as Beeline CLI — support these authentication modes. This enables the Hive deployment to easily integrate with existing authentication services. Gateway to Secure Hadoop Today, the Hadoop ecosystem only supports Kerberos for authentication. That means for accessing secure Hadoop, one needs to get a Kerberos ticket. However, enabling Kerberos on every client box can be a very challenging task and thus can restrict access to Hive and Hadoop. To address that issue, HiveServer2 can authenticate clients over non-Kerberos connections (eg. LDAP) and run queries against Kerberos-secured Hadoop data. This approach allows users to securely access Hive without complex security infrastructure or limitations. Foundation for Fine-grained Authorization As a stopgap until fine-grained authorization is available, HiveServer2 also supports access to Hadoop as itself or by impersonating the connected user. (This behavior is configurable.) In this so-called impersonation mode, MapReduce jobs are submitted as the user connecting to HiveServer2. If the underlying Hadoop cluster is secure, the service principle used by Hive needs Hadoop proxy privileges to impersonate the connecting users. This interim solution provides coarse-grained authorization based on ownership and permissions on files and directories in HDFS (as opposed to Hive tables and views), which unblocks some usage. HiveServer2�s strong authentication and revamped server-side architecture also provides the foundation for fine-grained authorization in Hive in the very near future. Stay tuned! (Update: read “With Sentry, Cloudera Closes Hadoop’s Enterprise Security Gap”) Conclusion In this post, you have received an overview of how Cloudera�s contribution of HiveServer2 brings concurrency, authentication, and a foundation for fine-grained authorization (more on this in a future post) to Hive. For further reading, you may want to explore the docs on Setting up HiveServer2 and HiveServer2 Clients. Prasad Mujumdar is a Software Engineer on the Platform team. &gt; Have questions? Post them to the�Community Forum for Schema Management and Usage.</snippet></document><document id="165"><title>Guide to Using Apache HBase Ports</title><url>http://blog.cloudera.com/blog/2013/07/guide-to-using-apache-hbase-ports/</url><snippet>For those people new to Apache HBase�(version 0.90 and later), the configuration of network ports used by the system can be a little overwhelming. In this blog post, you will learn all the TCP ports used by the different HBase processes and how and why they are used (all in one place) — to help administrators troubleshoot and set up firewall settings, and help new developers how to debug. A typical HBase cluster has one active master, one or several backup masters, and a list of region servers. The backup masters are standby masters waiting to be the next active one. Before they are active, they do not listen on any ports. (Learn more about how HBase scalability works here.) Each server in the cluster listens to a main port for requests from clients and/or other HBase servers. Each server also has an embedded Jetty web UI server. The following diagram shows the communication among different components. (Blue components belong to the HBase cluster, usually behind a firewall; grey components are external clients, usually outside the HBase cluster firewall; green component is a web browser, usually outside the firewall too.) Client applications talk to Apache ZooKeeper to find out the location of the master and the meta region server (the root region is removed in HBase version 0.96). Client applications talk to region servers to read from/write to/scan a table. Client applications talk to the master to get information about an existing table, dynamically create/remove a table, add/remove a column family. The master talks to region servers to open/close/move/split/flush/compact regions. The master puts data in ZooKeeper to store the active master and meta region server location, create log splitting tasks, track region servers� statuses. Region servers read data in ZooKeeper to do log splitting, track the master location and the cluster status. Region servers talk to the master to report region server start-ups, loads. Occasionally, region servers talk to meta region to check the status of a region, create new daughter regions in region splitting. REST clients talk to REST servers to access HBase. Thrift clients talk to Thrift servers to access HBase. Users access the master web UI from browsers. Users access region servers� web UI from browsers. Users access REST servers� web UI from browsers. Users access Thrift servers� web UI from browsers. Some HBase clusters may have a list of REST or Thrift servers. Both the REST server and the Thrift server are optional; they are needed only if you want to provide REST/Thrift access to your HBase cluster. To HBase, they are just other client applications. Just like other HBase servers, they also listen to a main port for client requests, and a web UI port. The following table shows the ports used by client applications to talk to an HBase cluster, users to check cluster information, and different HBase components to talk to each other. Component Configuration parameter Default value Used places ZooKeeper hbase.zookeeper.property.clientPort 2181 1,5,6 Master hbase.master.port 60000 3,7 Master hbase.master.info.port 60010 11 Region server hbase.regionserver.port 60020 2,4,8 Region server hbase.regionserver.info.port 60030 12 REST server hbase.rest.port** 8080 9 REST server hbase.rest.info.port* 8085 13 Thrift server hbase.regionserver.thrift.port** 9090 10 Thrift server hbase.thrift.info.port* 9095 14 * Introduced in HBase version 0.94.5. They can also be specified with command line option --infoport when starting up the corresponding server. ** They can also be specified with command line option -p when starting up the corresponding server. One port is not listed in the table — the HDFS namenode port � because here is not a separate parameter for it. It is configured as a part of �hbase.root� (for example, �hdfs://namenode.foobar.com:35802/hbase�) with the HDFS NameNode port configured to be 35802. Unless otherwise specified in the value of �hbase.root�, the default is 8020. Besides the main port, each server in the cluster (ZooKeeper excepted) also listens to a web UI port. A web UI is an embedded Jetty server in its corresponding server. The web UI provides human-readable information about the corresponding server — for example, the thread dump and local logs. The master web UI has links to all region server web UIs, which makes it the perfect entry point for checking the current status of an HBase cluster. The REST/Thrift servers are optional proxies to HBase. They talk to HBase the same way other HBase client applications do. However, they are usually deployed inside the HBase cluster, together with other HBase servers. Client applications are usually deployed out of the HBase cluster. REST/Thrift clients are deployed outside the cluster too. If the HBase cluster is behind a firewall, these corresponding ports should be open by default: To allow client application access: 2181 (hbase.zookeeper.property.clientPort) 60000 (hbase.master.port) 60020 (hbase.regionserver.port) To allow REST/Thrift client access: 8080 (hbase.rest.port) 9090 (hbase.regionserver.thrift.port) If web UI access from out of firewall is allowed, the corresponding web UI ports should be open too: 60010 (hbase.master.info.port) 60030 (hbase.regionserver.info.port) 8085 (hbase.rest.info.port) 9095 (hbase.thrift.info.port) Conclusion In this post, you got a summary of the ports used by HBase internal components, client applications, and by users/administrators, organized by use case.� However, because HBase runs on top of HDFS, it is also important to know HDFS ports. To run MapReduce with HBase, you need to know MapReduce ports too. For these Hadoop related ports, please refer to Hadoop Default Ports Quick Reference. Jimmy Xiang is a Software Engineer on the Platform team. &gt; Have questions? Post them to the�Community Forum for HBase.</snippet></document><document id="166"><title>Myrrix Joins Cloudera to Bring "Big Learning" to Hadoop</title><url>http://blog.cloudera.com/blog/2013/07/myrrix-joins-cloudera-to-bring-big-learning-to-hadoop/</url><snippet>What a short, strange trip it’s been. Just a year ago, I founded Myrrix in London�s Silicon Roundabout to commercialize large-scale machine learning based on Apache Hadoop and Apache Mahout. It’s been a busy scramble, building software and proudly watching early customers get real, big data-sized machine learning into production. And now another beginning: Myrrix has a new home in Cloudera. I’m excited to join as Director of Data Science in London, alongside Josh Wills. Some of the Myrrix technology�will be coming along to benefit CDH and its customers too. There was no question that Cloudera is the right place to continue building out the vision that started as Myrrix, because Josh, Jeff Hammerbacher and the rest of the data science team here have the same vision. It’s an unusually perfect match. Cloudera has made an increasingly complex big-data ecosystem increasingly accessible (Hadoop, real-time queries, search), and we’re going to make “Big Learning” on Hadoop easy and accessible too. What is Old is New Again Data-savvy companies of all sizes can now accomplish many viable machine learning projects. Why the fuss now about machine learning, a decades-old field? I started working on recommender systems relatively late, in 2005, as the open-source project Taste. In 2008, this was merged into the open source machine learning project Apache Mahout, and rebuilt on top of a nascent Hadoop project. Yet as a committer and part of the Mahout PMC, I have watched interest in machine learning suddenly reignite, and skyrocket, as interest in this new Hadoop thing did. It’s because these should go together well. Hadoop and cheap hardware have made big data analysis so much more feasible. With cheap disks and CPUs, and mature open-source databases and computation frameworks, startups and even individuals can afford to run terribly complex computations over terabytes. This is great for machine learning. Generally, learning works better with more data. If the price of collecting and processing data is falling, while the value of learning from it is increasing, then the number of situations where learning is profitable to deploy is exploding. Whereas before large-scale machine learning was something a few big specialized companies bothered with, now, data-savvy companies of all sizes can accomplish many viable machine learning projects. And, large companies can improve their existing learning by adding orders of magnitude more data into a system that might before be limited by scale. Making Big Learning Accessible Cheap infrastructure doesn’t help without accessible applications on top. And, machine learning gets surprisingly harder to implement at scale. Most research assumes a world in which all data fits on one machine. Adjusting these ideas to Hadoop’s data-parallel world takes some clever reinvention. This began most visibly in the Mahout project, where many algorithms have been parallelized for Hadoop. There is still so much to be done from these beginnings before learning on Hadoop is as accessible as it can be. After all, in the early days, Hadoop itself was a ball of source code that only adventurous specialists could effectively embrace. However, Cloudera has shown how to extend it, package it, support it and make it far more accessible to a much bigger audience. The same will happen for applications like Big Learning — that’s always been the Myrrix vision too, and now, we’re working together within Cloudera to start building this out for you, the bigger audience. That�s All For Now Exactly what form that will take is to be determined. There are no new products to announce at this point, as we’re busy in the lab figuring out how to incorporate the technology into CDH in just the right way. Finally, a public word of thanks to users and customers of Myrrix, who also share credit in evolving the vision of what large-scale learning should look like. As you’ve heard, while the software will eventually be discontinued in its current form, it is now freely available and remains fully supported in the medium term. This should be a new and interesting trip — watch this space.</snippet></document><document id="167"><title>The Book on Apache Sqoop is Here!</title><url>http://blog.cloudera.com/blog/2013/07/the-book-on-apache-sqoop-is-here/</url><snippet>Continuing the fine tradition of Clouderans contributing books to the Apache Hadoop ecosystem, Apache Sqoop Committers/PMC Members Kathleen Ting and Jarek Jarcec Cecho�have officially joined the book author community: their Apache Sqoop Cookbook is now�available�from O’Reilly Media (with a pelican the assigned cover beast).� The book arrives at an ideal time. Hadoop has quickly become the standard for processing and analyzing�Big Data, and in order to integrate a new Hadoop deployment into your�existing environment, you will very likely need to transfer data stored in legacy relational databases into your new cluster. Sqoop�is just the ticket; it optimizes data transfers�between Hadoop and RDBMSs via a command-line interface listing 60�parameters. This new cookbook focuses on applying these parameters to common use cases — one recipe at a time, Kate and Jarek guide you from basic commands that don’t require prior Sqoop knowledge all the way to very advanced use cases. These recipes are sufficiently detailed not only to enable you to deploy Sqoop in your environment, but also to understand its inner workings. If you’re interested in a sampler, O’Reilly has kindly made a sample chapter PDF available for download. As an added benefit for OSCON 2013 attendees in Portland next week, Kate will be signing copies of the cookbook at the O’Reilly pavilion on Thursday July 25 at 12:45pm. Don’t be late! &lt; Questions about Sqoop? Post them to the Community Forum for Data Ingestion.</snippet></document><document id="168"><title>How Does Cloudera Manager Work?</title><url>http://blog.cloudera.com/blog/2013/07/how-does-cloudera-manager-work/</url><snippet>At Cloudera, we believe that Cloudera Manager is the best way to install, configure, manage, and monitor your Apache Hadoop stack.�Of course, most users prefer not to take our word for it — they want to know how Cloudera Manager works under the covers, first.� In this post, I’ll explain some of its inner workings.� The Vocabulary of Cloudera Manager The image below illustrates the basic nouns and relationships of Cloudera Manager: A “deployment” is the whole shebang, and contains clusters.�Clusters are collections of coherent (in the sense that they run the same version of CDH) hosts.�Hosts are organized into racks. Services are instances of a specific system, and span many roles, each of which is assigned to a single host.�Role config groups are a way of configuring many roles at once, which is the common case. Configurations are attached to multiple contexts and may cascade, as appropriate.�For example, the path where the log files of a DataNode are stored is typically attached to a �Role Config Group,� but it may also be attached to a specific role as an override. A common point of confusion is the class-type/class-instance nature of �service� as well as �role.� Just like in many programming languages, �string� may indicate either the type (�java.lang.String�) or an instance of that type (�hey there�).� We sometimes say �role� to indicate the type (�RegionServer�) or an instance (�the RegionServer on machine17�).�When it�s ambiguous, we try to use �role type� and �role instance� to distinguish the two cases.�The same thing happens for service (�HBase� or �The Production HBase Service�). It’s also helpful to always remember this basic principle: Whereas traditionally,�multiple services run on one host, in�Hadoop — and other distributed systems — a single service runs on many hosts. Agent/Server Architecture Cloudera Manager runs a central server (�the Cloudera Manager Server,� which has also been called the “SCM Server” and the “CMF Server” in the past) which hosts the UI Web Server and the application logic for managing CDH.�Everything related to installing CDH, configuring services, and starting and stopping services is managed by the Cloudera Manager Server. The Cloudera Manager Agents are installed on every managed host.�They are responsible for starting and stopping Linux processes, unpacking configurations, triggering various installation paths, and monitoring the host. Heartbeating Heartbeats make up the primary communication channel in Cloudera Manager.� The agents sends heartbeats (by default) every 15 seconds to the server to find out what the agent should be doing.�(There�s an optimization that speeds up heartbeats when things are a-changin�, to reduce user latency.) The agent, when it�s heartbeating, is saying: �Here�s what I�m up to!� and the server, in its response, is saying, �Here�s what you should be doing.��Both the agent and the server end up doing some reconciliation: If a user has stopped a service via the UI, for example, the agent will stop the relevant processes; if a process failed to start, the server will mark the start command as having failed. Agent Process Supervision in Detail One of the agent�s main responsibilities is to start and stop processes.�When the agent sees a new process come in from the heartbeat, the agent creates a directory for it in /var/run/cloudera-scm-agent and unpacks the configuration. This is an important point; a Cloudera Manager process never travels alone.�A process is more than just the arguments to exec() — it also includes config files, directories that need to be created, and other information (like cgroups settings).�This way, there�s never any question about configuration files being out of date. Giving each process its own private execution and configuration environment allows us to control each process independently, which is crucial for some of the more esoteric configuration scenarios that show up. Here are the contents of an example uniquely-named (that�s what 879 is doing) process directory: $ tree -a /var/run/cloudera-scm-agent/process/879-hdfs-NAMENODE/
  /var/run/cloudera-scm-agent/process/879-hdfs-NAMENODE/
  ??? cloudera_manager_agent_fencer.py
  ??? cloudera_manager_agent_fencer_secret_key.txt
  ??? cloudera-monitor.properties
  ??? core-site.xml
  ??? dfs_hosts_allow.txt
  ??? dfs_hosts_exclude.txt
  ??? event-filter-rules.json
  ??? hadoop-metrics2.properties
  ??? hdfs.keytab
  ??? hdfs-site.xml
  ??? log4j.properties
  ??? logs
  ?�� ??? stderr.log
  ?�� ??? stdout.log
  ??? topology.map
  ??? topology.py
   To actually start and stop the process, we rely on an open source system called supervisord.�It takes care of redirecting log files, notifying us of process failure, setuid�ing to the right user, and so forth. Cloudera Manager places the �client configuration� for Hadoop in /etc/hadoop/conf.�(A similar approach is taken for /etc/hbase/conf and /etc/hive/conf.)�That is, by default, if you run a program that needs to talk to Hadoop, it will pick up the addresses of the NameNode and JobTracker, and other important configurations, from that directory.�Cloudera Manager makes the distinction between a �client� configuration and a �service� configuration.�Settings like the default HDFS replication factor or the heap size of your MapReduce tasks are (somewhat paradoxically) client configurations.�Therefore, when those settings are changed, you need to use the �Deploy Client Configuration� action to update these directories.� Processes that Cloudera Manager manages (i.e., the actual daemons like RegionServers and DataNodes and the like) don�t use /etc/hadoop/conf. Instead, they use their own private copy, as described above.This serves two purposes: first, it allows Cloudera Manager to carefully manage the lifecycle of those configurations; second, it makes it clear that the source of truth is the Cloudera Manager server. It would have been unintuitive for Cloudera Manager to override changes in /etc as part of a restart action. Often users have �edge� or �client� or �gateway� machines, which aren�t running any Hadoop daemons but are on the same network as the cluster. Users use these as a jumping-off point to launch jobs, access the filesystem, etc.� If you want to deploy client configurations on these machines, add a �gateway� role onto these machines: they will then receive the configurations when you use �Deploy Client Configuration.� So who starts the agent? What happens on restart? The agent is typically started by init.d at start-up. It, in turn, contacts the server and figures out what processes should be running.�The agent is monitored as part of Cloudera Manager�s host monitoring: if the agent stops heartbeating, the host will be marked as having bad health. Meanwhile, on the Server… The server maintains the entire state of the cluster.�Very roughly, you can divide this into �model� and �runtime� state, both of which are stored in the Cloudera Manager server�s database.� Model State is stuff like what�s supposed to run where, with what configs. That you have 17 hosts, each of which is supposed to run a DataNode, is model state.�We�ve modelled the Hadoop stack: its roles, configurations, and interdependencies. The user interacts with the model through the configuration screens (and operations like �Add Service�). Runtime State is what processes are running where, and what commands (e.g., rebalance HDFS or execute a Backup/Disaster Recovery schedule or rolling restart or plain ol� stop) are currently being executed. The runtime state includes the nitty-gritty, like the exact configuration files needed to run a process. In fact, when you press �Start� in Cloudera Manager, the server gathers up all the configuration for the relevant services and roles, validates them, and generates the config files, and stores them in the database. When you update a piece of configuration (say, Hue�s web port), you�ve updated the model.�However, if Hue is running while you do this, it�s still listening on the old port.�When this kind of mismatch happens, the role is marked as having an “outdated configuration.” To true up, you�ll need to restart the role (which will trigger the configuration re-generation and process restart). Many users ask us how backup should be done. A simple approach is to use the Cloudera Manager API to grab /api/cm/deployment API endpoint; this captures all the model information but not the runtime information. A second approach is to back up the entire Cloudera Manager server database (which is typically quite small).�There is almost nothing to back up on a per-host basis, since the agent�s configuration is typically simply the hostname of the server. �While we try to model all of the reasonable configurations, we found that, inevitably, there are some dark corners that require special handling.�To allow our users to workaround, for example, some bug (or, perhaps, to explore unsupported options), we have a �safety valve� which lets users plug in directly to the configuration files.�(The analogy, I�ll admit, is oriented toward the developers of Cloudera Manager: We�re �releasing pressure� if the model doesn�t hold up to some real-world oddity.) � Monitoring and Other Management Services Cloudera Manager itself manages some of its helper services. These include the Activity Monitor, the Service Monitor, the Host Monitor, the Event Server, the Reports Manager, and the Alert Publisher.�Cloudera Manager manages each separately (as opposed to rolling them all in as part of the Cloudera Manager Server) for scalability (e.g., on large deployments it�s useful to put the monitors on their own hosts) and isolation. For this behind-the-scenes blog post, we�ll only look at service monitoring in detail. Metric Collection To do its monitoring, Cloudera Manager collects metrics.�Metrics are simply numeric values, associated with a name (e.g., �cpu seconds�), an entity they apply to (�host17�), and a timestamp. Most of this metric collection is done by the agent. The agent talks to a supervised process, grabs the metrics, and forwards them to the service monitor.� In most cases, this is done once per minute.� A few special metrics are collected by the Service Monitor itself.�For example, the Service Monitor hosts an HDFS canary, which tries to write, read, and delete a file from HDFS at regular intervals, and measures not just whether it succeeded, but how long it took. Once metrics are received, they�re aggregated and stored. Using the Charts page in Cloudera Manager, users can query and explore the metrics being collected.�Some metrics (e.g., �total_cpu_seconds�) are counters, and the appropriate way to query them is to take their rate over time, which is why a lot of metrics queries look like �dt0(total_cpu_seconds)�.�(The �dt0� syntax is intended to remind you of derivatives.�The �0� indicates that since we�re looking at the rate of a monotonically increasing counter, we should never have negative rates.) Health Checks The service monitor continually evaluates �health checks� for every entity in the system.�A simple one asks whether there�s enough disk space in every NameNode data directory.�A more complicated health check may evaluate when the last checkpoint for HDFS was compared to a threshold or whether a DataNode is connected to a NameNode. Some of these health checks also aggregate other health checks: in a distributed system like HDFS, it�s normal to have a few DataNodes down (assuming you�ve got dozens of machines), so we allow for setting thresholds on what percentage of nodes should color the entire service down. Cloudera Manager encapsulates our experience with supporting clusters across our customers by distilling them into these �health checks.� When health checks go red, events are created, and alerts are fired off via e-mail or SNMP. One common question is whether monitoring can be separated from configuration. The answer is: No. One of our goals for monitoring is to enable it for our users without needing to do additional configuration and installing additional tools (e.g., Nagios).�By having a deep model of the configuration, we�re able to know which directories to monitor, which ports to talk to, and which credentials to use for those ports.�This tight coupling means that, when you install Cloudera Standard (the free version of the Cloudera platform), all the monitoring is immediately available. Any Questions? I hope that this post has provided a high-level overview of Cloudera Manager internals. If you�ve got any questions, please do ask us.�The best place is over on our mailing list, or, if you�re a Cloudera Enterprise customer, via support. If you�re not already using Cloudera Manager, and are interested in checking it out, see the Cloudera downloads page and the Cloudera Manager documentation page. Philip Zeyliger is a Software Engineer on the Enterprise team.</snippet></document><document id="169"><title>Introducing Morphlines: The Easy Way to Build and Integrate ETL Apps for Hadoop</title><url>http://blog.cloudera.com/blog/2013/07/morphlines-the-easy-way-to-build-and-integrate-etl-apps-for-apache-hadoop/</url><snippet>This post is the first in a series of blog posts about Cloudera Morphlines, a new command-based framework that simplifies data preparation for Apache Hadoop workloads. To check it out or help contribute, you can find the code here. Cloudera Morphlines is a new open source framework that reduces the time and effort necessary to integrate, build, and change Hadoop processing applications that extract, transform, and load data into Apache Solr, Apache HBase, HDFS, enterprise data warehouses, or analytic online dashboards. If you want to�integrate, build, or facilitate transformation pipelines�without programming and without substantial MapReduce skills, and get the job done with a minimum amount of fuss and support costs, this post gets you started. A “morphline” is a rich configuration file that makes it easy to define a transformation chain that consumes any kind of data from any kind of data source, processes the data, and loads the results into a Hadoop component. It replaces Java programming with simple configuration steps, and correspondingly reduces the cost and integration effort associated with developing, maintaining, or integrating custom ETL projects. Morphlines is a library, embeddable in any Java codebase. A morphline is an in-memory container of transformation commands. Commands are plugins to a morphline that perform tasks such as loading, parsing, transforming, or otherwise processing a single record. A record is an in-memory data structure of name-value pairs with optional blob attachments or POJO attachments. The framework is extensible and integrates existing functionality and third-party systems in a simple and straightforward manner. Morphlines replaces Java programming with simple configuration steps, reducing the cost and effort of doing custom ETL. The Morphlines library was developed as part of Cloudera Search. It powers a variety of ETL data flows from Apache Flume and MapReduce into Solr. Flume covers the real time case, whereas MapReduce covers the batch processing case. Since the launch of Cloudera Search, Morphlines development has graduated into the Kite SDK (formerly Cloudera Development Kit)�in order to make the technology accessible to a wider range of users, contributors, integrators, and products�beyond Search. The Kite SDK is a set of libraries, tools, examples, and documentation focused on making it easier to build systems on top of the Hadoop ecosystem (and hence a perfect home for Morphlines). The SDK is hosted on GitHub and encourages involvement by the community. Currently, Morphlines works with multiple indexing workloads, but could easily be embedded�into Crunch, HBase, Impala, Pig, Hive, or Sqoop.�Your feedback is not only welcome but crucial, so please let us know where you see more opportunities for integration going forward!� Later in this post, we’ll examine a simple example use case in which we collect syslog output and make it available for searching. Before we go into the details of the problem and solution, though, let’s take a look at the Morphlines data model and processing model. Processing Model Morphlines can be seen as an evolution of Unix pipelines where the data model is generalized to work with streams of generic records, including arbitrary binary payloads. A morphline is an efficient way to consume records (e.g. Flume events, HDFS files,� RDBMS tables, or Apache Avro objects), turn them into a stream of records, and pipe the stream of records through a set of easily configurable transformations on the way to a target application such as Solr, for example as outlined in the following figure: In this figure, a Flume Source receives syslog events and sends them to a Flume Morphline Sink, which converts each Flume event to a record and pipes it into a readLine command. The readLine command extracts the log line and pipes it into a grok command. The grok command uses regular expression pattern matching to extract some substrings of the line. It pipes the resulting structured record into the loadSolr command. Finally, the loadSolr command loads the record into Solr, typically a SolrCloud. In the process, raw data or semi-structured data is transformed into structured data according to application modelling requirements. The Morphlines framework ships with a set of frequently used high-level transformation and I/O commands that can be combined in application specific ways. The plugin system allows the adding of new transformations and I/O commands and integrates existing functionality and third-party systems in a straightforward manner. This integration enables rapid Hadoop ETL application prototyping, complex stream and event processing in real time, flexible log file analysis, integration of multiple heterogeneous input schemas and file formats, as well as reuse of ETL logic building blocks across Hadoop ETL applications. The Kite SDK ships an efficient runtime that compiles a morphline on the fly. The runtime executes all commands of a given morphline in the same thread. Piping a record from one command to another implies just a cheap Java method call. In particular, there are no queues, no handoffs among threads, no context switches, and no serialization between commands, which minimizes performance overhead. Data Model Morphlines manipulate continuous or arbitrarily large streams of records. A command transforms a record into zero or more records. The data model can be described as follows: A record is a set of named fields where each field has an ordered list of one or more values. A value can be any Java Object. That is, a record is essentially a hash table where each hash table entry contains a String key and a list of Java Objects as values. Note that a field can have multiple values and any two records need not use common field names. This flexible data model corresponds exactly to the characteristics of the Solr/Lucene data model. Not only structured data, but also binary data, can be passed into and processed by a morphline. By convention, a record can contain an optional field named _attachment_body, which can be a Java java.io.InputStream or Java byte[]. Optionally, such binary input data can be characterized in more detail by setting the fields named _attachment_mimetype (such as �application/pdf�) and _attachment_charset (such as �UTF-8�) and _attachment_name (such as �cars.pdf�), which assists in detecting and parsing the data type. (This is similar to the way email works.) This generic data model is useful to support a wide range of applications. For example, the Apache Flume Morphline Solr Sink embeds the morphline library and executes a morphline to convert flume events into morphline records and load them into Solr. This sink fills the body of the Flume event into the _attachment_body field of the morphline record, as well as copies the headers of the Flume event into record fields of the same name. As another example, the Mappers of the MapReduceIndexerTool fill the Java java.io.InputStream referring to the currently processed HDFS file into the _attachment_body field of the morphline record. The Mappers of the MapReduceIndexerTool also fill metadata about the HDFS file into record fields, such as the file’s name, path, size, last modified time, etc. This way a morphline can act on all data received from Flume and HDFS. Use Cases Commands can access all record fields. For example, commands can parse fields, add fields, remove fields, rename fields, find and replace values of existing fields, split a field into multiple fields, split a field into multiple values, or drop records. Often, regular expression-based pattern matching is used as part of the process of acting on fields. The output records of a command are passed to the next command in the chain. A command has a Boolean return code, indicating success or failure. For example, consider the case of a multi-line input record: A command could take this multi-line input record and divide the single record into multiple output records, one for each line. This output could then later be further divided using regular expression commands, splitting each single line record out into multiple fields in application specific ways. A command can extract, clean, transform, join, integrate, enrich and decorate records in many other ways. For example, a command might join records with external data sources such as relational databases, key-value stores, local files or IP Geo lookup tables. It might also perform tasks such as DNS resolution, expand shortened URLs, fetch linked metadata from social networks, perform sentiment analysis and annotate the record accordingly, continuously maintain statistics for analytics over sliding windows, or compute exact or approximate distinct values and quantiles. A command can also consume records and pass them to external systems. For example, a command might load records into Apache Solr or write them to a MapReduce Reducer, or load them into an Enterprise Data Warehouse or a Key Value store such as HBase, or pass them into an online dashboard, or write them to HDFS. Embedding into a Host System A morphline has no notion of persistence, durability, distributed computing, or node failover — it’s basically just a chain of in-memory transformations in the current thread. There is no need for a morphline to manage multiple processes, nodes, or threads because this is already addressed by host systems such as MapReduce, Flume, or Storm. However, a morphline does support passing notifications on the control plane to command subtrees. Such notifications include BEGIN_TRANSACTION, COMMIT_TRANSACTION, ROLLBACK_TRANSACTION, and SHUTDOWN. Syntax The morphline configuration file is implemented using the HOCON format (Human Optimized Config Object Notation) developed by typesafe.com. HOCON is basically JSON slightly adjusted for the configuration file use case. HOCON syntax is defined at the�HOCON github page. Currently Available Commands The Kite SDK includes several maven modules that contain morphline commands for flexible log file analysis, single-line records, multi-line records, CSV files, JSON, commonly used HDFS file formats Avro and Hadoop Sequence Files, regular expression based pattern matching and extraction, operations on record fields for assignment and comparison, operations on record fields with list and set semantics, if-then-else conditionals, string and timestamp conversions, scripting support for dynamic java code, a small rules engine, logging, metrics and counters, integration with Solr including SolrCloud, integration and access to the large set of file formats supported by the Apache Tika parser library, auto-detection of MIME types from binary data using Tika, and decompression and unpacking of arbitrarily nested container file formats, among others. These are described in detail in the Cloudera Morphlines Reference Guide. Syslog Use Case Now we’ll review a concrete example use case: We want to extract information from a syslog file and index it into Solr in order to make it available for Search queries. The corresponding program should be able to run standalone, or embedded inside a Flume Sink or embedded in a MapReduce job. A syslog file contains semi-structured lines of the following form: Feb� 4 10:46:14 syslog sshd[607]: listening on 0.0.0.0 port 22.
   The program should extract the following record from the log line, convert the timestamp, and load the record into Solr: priority : 164
timestamp : Feb� 4 10:46:14
hostname : syslog
program : sshd
pid : 607
msg : listening on 0.0.0.0 port 22.
message : Feb� 4 10:46:14 syslog sshd[607]: listening on 0.0.0.0 port 22.
   These rules can be expressed with the Morphline commands readLine, grok, convertTimestamp, sanitizeUnknownSolrFields, logInfo and loadSolr, by editing a morphline.conf file to read as follows: morphlines : [
� {
��� # Name used to identify a morphline. E.g. used if there are multiple
��� # morphlines in a morphline config file
��� id : morphline1

��� # Import all morphline commands in these java packages and their
��� # subpackages. Other commands that may be present on the classpath are
��� # not visible to this morphline.
��� importCommands : ["com.cloudera.**", "org.apache.solr.**"]

��� commands : [
����� {
������� # Parse input attachment and emit a record for each input line���������������
������� readLine {
��������� charset : UTF-8
������� }
����� }

����� {
������� grok {
��������� # Consume the output record of the previous command and pipe another
��������� # record downstream.
��������� #
��������� # A grok-dictionary is a config file that contains prefabricated
��������� # regular expressions that can be referred to by name. grok patterns
��������� # specify such a regex name, plus an optional output field name.
��������� # The syntax is %{REGEX_NAME:OUTPUT_FIELD_NAME}
��������� # The input line is expected in the "message" input field.
��������� dictionaryFiles : [src/test/resources/grok-dictionaries]
��������� expressions : {
����������� message : """%{SYSLOGTIMESTAMP:timestamp} %{SYSLOGHOST:hostname} %{DATA:program}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:msg}"""
��������� }
������� }
����� }

����� # Consume the output record of the previous command, convert
����� # the timestamp, and pipe another record downstream.
��� ��#
����� # convert timestamp field to native Solr timestamp format
����� # e.g. 2012-09-06T07:14:34Z to 2012-09-06T07:14:34.000Z
����� {
������� convertTimestamp {
��������� field : timestamp
��������� inputFormats : ["yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", ""MMM d HH:mm:ss"]
��������� inputTimezone : America/Los_Angeles
��������� outputFormat : "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"
��������� outputTimezone : UTC
������� }
����� }

����� # Consume the output record of the previous command, transform it
����� # and pipe the record downstream.
����� #
����� # This command deletes record fields that are unknown to Solr
����� # schema.xml. Recall that Solr throws an exception on any attempt to
����� # load a document that contains a field that isn't specified in
����� # schema.xml.
����� {
������� sanitizeUnknownSolrFields {
��������� # Location from which to fetch Solr schema
��������� solrLocator : {�����������
�� ���������collection : collection1������ # Name of solr collection
����������� zkHost : "127.0.0.1:2181/solr" # ZooKeeper ensemble
��������� }
������� }
����� }

����� # log the record at INFO level to SLF4J
����� { logInfo { format : "output record: {}", args : ["@{}"] } }

����� # load the record into a Solr server or MapReduce Reducer
����� {
������� loadSolr {
��������� solrLocator : {�����������
����������� collection : collection1������ # Name of solr collection
����������� zkHost : "127.0.0.1:2181/solr" # ZooKeeper ensemble
��������� }
������� }
����� }
��� ]
� }
]
   Example Driver Program This section provides a simple sample Java program that illustrates how to use the API to embed and execute a morphline in a host system (full source code). /** Usage: java ...   ...  */
public static void main(String[] args) {
� // compile morphline.conf file on the fly
� File configFile = new File(args[0]);
� MorphlineContext context = new MorphlineContext.Builder().build();
� Command morphline = new Compiler().compile(configFile, null, context, null);

� // process each input data file
� Notifications.notifyBeginTransaction(morphline);
� for (int i = 1; i &lt; args.length; i++) {
��� InputStream in = new FileInputStream(new File(args[i]));
��� Record record = new Record();
��� record.put(Fields.ATTACHMENT_BODY, in);
��� morphline.process(record);
��� in.close();
� }
� Notifications.notifyCommitTransaction(morphline);
}
   You can use this program to rapidly prototype ETL logic and try it with sample files. This code is, in essence, what production tools like MapReduceIndexerTool,�Apache Flume Morphline Solr Sink, and Apache Flume MorphlineInterceptor are running as part of their operation. To print diagnostic information such as the content of records as they pass through the morphline commands, consider enabling TRACE log level -- for example, by adding the following line to your log4j.properties file: log4j.logger.com.cloudera.cdk.morphline=TRACE
   Any Questions? If you�ve got any questions, please do ask us. The best place is over on the mailing list�or our new community forum. Alternatively, if you are trying out Cloudera Search, post your questions here. A detailed description of all morphline commands can be found in the Cloudera Morphlines Reference Guide. The Kite SDK lives on GitHub where users can freely browse, download, fork, and contribute back to the source. Community contributions are not only welcome but strongly encouraged. Since most Java developers use tools such as Maven (or tools that are compatible with Maven repositories), artifacts are also available from the Cloudera Maven Repository for easy project integration. Wolfgang Hoschek is a Software Engineer on the Platform team and the lead developer on Morphlines.  </snippet></document><document id="170"><title>Where to Find Cloudera Tech Talks Through September 2013</title><url>http://blog.cloudera.com/blog/2013/07/where-to-find-cloudera-tech-talks-through-september-2013/</url><snippet>Below please find our regularly scheduled quarterly update about where to find tech talks by Cloudera employees this year – this time, for July through September 2013. Note that this list will be continually curated during the period; complete logistical information may not be available yet. As always, we’re standing by to assist your meetup by providing speakers, sponsorships, and schwag! Date City Venue Speaker(s) July 11 Boston Boston HUG Solr Committer Mark Miller on Solr+Hadoop July 11 Santa Clara, Calif. Big Data Gurus Patrick Hunt on Solr+Hadoop July 11 Palo Alto, Calif. Cloudera Manager Meetup Phil Zeyliger on Cloudera Manager internals July 11 Kansas City, Mo. KC Big Data Matt Harris on Impala July 17 Mountain View, Calif. Bay Area Hadoop Meetups Patrick Hunt on Solr+Hadoop July 22 Chicago Chicago Big Data Hadoop and Lucene founder Doug Cutting on Solr+Hadoop July 22 Portland, Ore. OSCON 2013 Tom Wheeler on “Introduction to Apache Hadoop” July 24 Portland, Ore. OSCON 2013 Sqoop Committer Kate Ting on “Building an Impenetrable ZooKeeper” July 24 Portland, Ore. OSCON 2013 Jesse Anderson on “Doing Data Science On NFL Play by Play” July 24 Portland, Ore. OSCON 2013 Bigtop Committer Mark Grover on “Getting Hadoop, Hive and HBase up and running in less than 15 minutes” July 24 Portland, Ore. OSCON 2013 Hadoop Committer Colin McCabe on Locksmith July 25 San Francisco SF Data Engineering Wolfgang Hoschek on Morphlines July 25 Washington DC Hadoop-DC Joey Echeverria on Accumulo Aug. 14 San Francisco SF Hadoop Users TBD, but we’re hosting! Aug. 14 LA LA HBase Users Meetup HBase Committer/PMC Chair Michael Stack on HBase Aug. 29 London London Java Community Hadoop Committer Tom White on CDK Sept. 11 San Francisco Cloudera Sessions (SOLD OUT) Eric Sammer-led CDK lab Sept. 12 New York NYC Search, Discovery &amp; Analytics Meetup Solr Committer Mark Miller on Solr+Hadoop Sept. 12 Cambridge, UK Enterprise Search Cambridge UK Tom White on Solr+Hadoop Sept. 12 Los Angeles LA Hadoop Users Group Greg Chanan on Solr+Hadoop Sept. 16 Sunnyvale, Calif. Big Data Gurus Eric Sammer on CDK Sept. 17 Sunnyvale, Calif. SF Large-Scale Production Engineering Darren Lo on Hadoop Ops Sept. 18 Mountain View, Calif. Silicon Valley JUG Wolfgang Hoschek on Morphlines Sept. 19 El Dorado Hills, Calif. NorCal Big Data Apache Bigtop Committer Sean Mackrory on Bigtop &amp; QuickStart VM Sept. 24 Washington DC Hadoop-DC Doug Cutting on Apache Lucene</snippet></document><document id="171"><title>The Blur Project: Marrying Hadoop with Lucene</title><url>http://blog.cloudera.com/blog/2013/07/the-blur-project-marrying-hadoop-with-lucene/</url><snippet>Doug Cutting�s recent post about Cloudera Search included a hat-tip to Aaron McCurry, founder of the Blur project, for inspiring some of its design principles. We thought you would be interested in hearing more about Blur (which is mentored by Doug and Cloudera�s Patrick Hunt) from Aaron himself – thanks, Aaron, for the guest post below! Blur is an Apache Incubator project that provides distributed search functionality on top of Apache Hadoop, Apache Lucene, Apache ZooKeeper, and Apache Thrift. When I started building Blur three years ago, there wasn�t a search solution that had a solid integration with the Hadoop ecosystem. Our initial needs were to be able to index our data using MapReduce, store indexes in HDFS, and serve those indexes from clusters of commodity servers while remaining fault tolerant. Blur was built specifically for Hadoop — taking scalability, redundancy, and performance into consideration from the very start — while leveraging all the great features that already exist in the Hadoop stack. About three and a half years ago, I had an experience on a project that showed me just how powerful the fault tolerance characteristics of Hadoop are. This is what made me start to think about the core design behind Blur. It was around that time that my project began using Hadoop for data processing. We were having network stability issues that would randomly drop Hadoop nodes off the network. Over one weekend, we steadily lost network connections to 47 of the 90 data nodes in the cluster. When we came in on Monday morning, I noticed that some of the MapReduce jobs were a little sluggish but still working. When I checked HDFS, I saw that our capacity had dropped by about 50%. After running an fsck on the cluster I was expecting to find a catastrophic failure but to my amazement we still had healthy file system. There is interest in folding some of Blur�s code into Lucene for others to utilize. This experience left a lasting impression on me. It was at that point that I got the idea to somehow leverage the redundancy and fault tolerance of HDFS for the next version of a search system that I was just beginning to (re)write. At the time, I had already written a custom Lucene server that had been in production for a couple of years. Lucene performed really well and met all of our requirements for search. The issue that we faced was that it was running on a big-iron type of server that was not redundant and could not be easily expanded. After seeing the resilient characteristics of Hadoop first hand, I decided to look into marrying the already mature feature set of Lucene with the built-in redundancy and scalability of the Hadoop platform. From this experiment, Blur was created. Blur was initially released on Github as an Apache Licensed project and was then accepted into the Apache Incubator project in July 2012, with Patrick Hunt as its champion. Since then, Blur as a software project has matured and become much more stable. One of the major milestones over the past year has been the upgrade to Lucene 4, which has brought many new features and massive performance gains.� Recently there has been some interest in folding some of Blur�s code (HDFSDirectory and BlockCache) back into the Lucene project for others to utilize. This is an exciting development that legitimizes some of the approaches that we have taken to date. We are in conversations with some members of the Lucene community, such as Mark Miller, to figure out how we can best work together to benefit both the fledgling Blur project as well as the much larger and more well known/used Lucene project. Blur�s community is small but growing. Our project goals are to continue to grow our community and graduate from the Incubator project. Our technical goals are to continue to add features that perform well at scale while maintaining the fault tolerance that is required of any modern distributed system. We welcome your contributions at http://incubator.apache.org/blur/!</snippet></document><document id="172"><title>How-to: Use the Apache HBase REST Interface, Part 3</title><url>http://blog.cloudera.com/blog/2013/07/how-to-use-the-apache-hbase-rest-interface-part-3/</url><snippet>This how-to is the third in a series that explores the use of the Apache HBase REST interface.�Part 1�covered HBase REST fundamentals, some Python caveats, and table administration. Part 2�showed you how to insert multiple rows simultaneously using XML and JSON. Part 3 below will show how to get multiple rows using XML and JSON. Getting Rows with XML Using a GET verb, you can retrieve a single row or a group of rows based on their row keys.�(You can read more about the multiple value URL format here.)�Here we are going to use the simple wildcard character or asterisk (*) to get all rows that start with a specific string. In this example, we can load every line of Shakespeare’s comedies with “shakespeare-comedies-*”.�This also requires that our row key(s) be laid out by “AUTHOR-WORK-LINENUMBER”. Here is the code for getting and working with the XML output: request = requests.get(baseurl + "/" + tablename + "/shakespeare-comedies-*", headers={"Accept" : "text/xml"})

root = fromstring(request.text)

# Go through every row passed back
for row in root:
���� message = ''
���� linenumber = 0
���� username = ''
����
���� # Go through every cell in the row
���� for cell in row:
��������� columnname = base64.b64decode(cell.get('column'))

��������� if cell.text == None:
�������������� continue
����
��������� if columnname == cfname + ":" + messagecolumn:
�������������� message = base64.b64decode(cell.text)
��������� elif columnname == cfname + ":" + linenumbercolumn:
�������������� linenumber = decode(cell.text)
��������� elif columnname == cfname + ":" + usernamecolumn:
�������������� username = base64.b64decode(cell.text)

���� rowKey = base64.b64decode(row.get('key'))
   We start off the code with a get request.�This get will return all lines in Shakespeare’s comedies. These rows will come back as XML because of the change to the Accept header. Then we take the XML returned by the request and turn it into an XML DOM.�Each row from HBase is in a separate row element.�We’ll use a for loop to go through every row. Each cell in the row is a separate XML element.�We’ll use another for loop to go through all of these cells. (This block of code could be made simpler by using XPath to find the correct elements.) As each column is found, the value is saved out to a variable.�(The decode method is discussed in Part 1 this series.) All of the values coming back in XML are base64-encoded and need to be decoded before using them. Finally, the row key is retrieved and decoded. Once all of the data is found and decoded, you can start using it.�Your code would start after decoding the row.�Keep in mind that some of these variables don’t need to be decoded — I’m doing all of them here for the sake of completeness. Getting Rows with JSON Working with JSON is just like working with XML: Using a get�verb, you can retrieve a single row or a group of rows based on their row key.� Here is the code for getting and working with the JSON output: request = requests.get(baseurl + "/" + tablename + "/shakespeare-comedies-*", headers={"Accept" : "application/json"})

bleats = json.loads(request.text)

for row in bleats['Row']:
���� message = ''
���� lineNumber = 0
���� username = ''

���� for cell in row['Cell']:
��������� columnname = base64.b64decode(cell['column'])
��������� value = cell['$']
���������
��������� if value == None:
�������������� continue

��������� if columnname == cfname + ":" + messagecolumn:
�������������� message = base64.b64decode(value)
��������� elif columnname == cfname + ":" + linenumbercolumn:
�������������� lineNumber = decode(str(value))
��������� elif columnname == cfname + ":" + usernamecolumn:
�������������� username = base64.b64decode(value)

���� rowKey = base64.b64decode(row['key'])
   We start off the code with a get request that will return all lines in Shakespeare’s comedies.�These rows will come back as JSON because of the change to the Acceptheader. Then we take the JSON returned by the request and turn it into an JSON object.�Each row from HBase is in a separate index in the row array.�We’ll use a for loop to go through every row. Each cell in the row is a separate array index.�We’ll use another for loop to go through all of these cells.�As each column is found, the value is saved out to a variable. All of the values coming back in JSON are base64-encoded and need to be decoded before using them.�(Again, the decode method is discussed in�Part 1�this series.) Note that the values come back in the dollar sign ($) entry. Finally, the row key is retrieved and decoded. Once all of the data is found and decoded, you can start using it. Using curl As shown in the REST interface documentation, you can use curl to output XML or JSON directly to the console.�For example, you could do the same get as we just did using curl. The command is: curl -H "Accept: text/xml" http://localhost:8070/tablename/shakespeare-comedies-*
   That command would give you the XML output.� To get the JSON output, the command is: curl -H "Accept: application/json" http://localhost:8070/tablename/shakespeare-comedies-*
   With commands like these, you can quickly see what’s coming back or what the data looks like.�You can use curl to see the status code of a REST call with: [user@localhost HBaseREST]$ curl -I -H "Accept: text/xml" http://localhost:8070/messagestable/shakespeare-comedies-*
HTTP/1.1 200 OK
Content-Length: 0
Content-Type: text/xml
   Conclusion The HBase REST interface is a good way to use HBase if you don’t want to use Java.�It offers you a familiar REST interface that’s built in to many languages as well as a familiar data format.� Hopefully, the code samples and explanations in this series will save you a lot of Googling when embarking on your RESTful HBase project. Jesse Anderson is an instructor with Cloudera University.</snippet></document><document id="173"><title>One Engineer�s Experience with Parcel</title><url>http://blog.cloudera.com/blog/2013/07/one-engineers-experience-with-parcel/</url><snippet>We�re very pleased to bring you this guest post from Verisign engineer Benoit Perroud, which is based on his personal experiences with the new �Parcel� binary distribution format in Cloudera Manager 4.5. Among all the new features released with Cloudera Manager 4.5, Parcel is probably one of the most unnoticed � despite the fact it has the potential to become the administrator�s best friend. Parcel is a new package format to easily distribute CDH or other custom packages to all nodes in a cluster. A parcel is basically a monolithic gzip-compressed tarball file with some additional metadata, bundling one or more components. In this post we will dig into the Parcel format, explore how it is used in Cloudera Manager, and explore a concrete example that demonstrates how easy and powerful this new format is. (Note: the Parcel format is most likely subject to change and the description given here is a combination of reverse engineering and help from Philip Langdale of Cloudera. See this FAQ for more background.) Parcel�s Internals First, let�s dig into Parcel�s internals. As previously mentioned, a parcel is a gzip-compressed tarball file with some additional metadata. The parcel will be deployed in $PARCELS_ROOT directory, /opt/cloudera/parcels by default. From its folder, a parcel relies heavily on alternatives to create symlinks of the binaries and configuration files in Linux common folders like /usr/bin, /etc, and /usr/share �when the parcel is activated. All the metadata are stored in a mandatory folder called “meta.” It will comprise two main configuration files: parcel.json and permissions.json. Any additional directory structure can be added without restriction. Common examples include a lib folder containing jar and native libraries, etc folder containing configuration files, share folder containing documentation, and so on. Proposed parcel directory structure parcel.json The parcel.json file holds a large portion of the parcel�s configuration. It is obviously JSON formatted. The following nonexhaustive properties will be included in the file: Attribute name Attribute type Description name String Name of the parcel version String Version of the parcel setActiveSymlink boolean Tells if parcel activation also creates a symlink without the version in $PARCELS_ROOT. scripts Object This object references the scripts executed at activation and at runtime. Scripts listed here are relative to the meta folder. ��� defines String Environment scripts, usually export a couple of environment variables. ��� alternatives String Script to be used to set symlinks (alternatives) when the parcel is activated packages List of Objects List the packages installed with this parcel. Note here that the difference between packages and components seems to be for display only in Cloudera Manager. ��� name String Name of the package ��� version String Version of the package components List of Objects List of components installed with this parcel, explicitly listed in Cloudera Manager. �� �name String Name of the component ��� version String Version of the component users Object of Object Define the users that will be created by this parcel. The name of the attribute is the username to create. groups List of strings List of groups to create As a detailed example, the parcel.json file in the CDH-4.2.1 parcel looks like this: {
� "name":�������������� "CDH",
� "version":����������� "4.2.1-1.cdh4.2.1.p0.5",
� "extraVersionInfo": {
��� "fullVersion":������� "4.2.1-1.cdh4.2.1.p0.5-precise",
��� "baseVersion":������� "4.2.0",
��� "patchCount":�������� "p0"
� },
� "minPrevVersion":���� "",
� "maxPrevVersion":���� "",
� "setActiveSymlink":�� false,

� "scripts": {
��� "defines": "cdh_env.sh",
��� "alternatives": "cdh_alternatives.sh"
� },

� "packages": [
��� { "name":��� "bigtop-jsvc",
����� "version": "1.0.10-1.cdh4.2.1.p0.9~precise-cdh4.2.1"
��� }, ...
� ],

� "components": [
��� { "name":���� "flume-ng",
����� "version":� "1.3.0-cdh4.2.1",
����� "pkg_version":� "1.3.0+96"
��� }, ...
� ],

� "users": {
���� "flume": {
������ "longname"��� : "Flume",
������ "home"������� : "/var/run/flume",
������ " "������ : "/bin/false",
������ "extra_groups": [ ]
���� }, ...
� },

� "groups": [
���� "hadoop",
���� "sqoop"
� ]
}
   permissions.json The permissions.json file defines custom permissions to apply to given files. The files are relative to the parcel root folder: {
� "bin/myscript.sh ": {
��� "user":� "root",
��� "group": "mapred",
��� "permissions": "4754"
� }
}
   Parcel Repository A parcel is hosted on a HTTP web server, and the remote parcel repository URL is added into Cloudera Manager�s configuration. Adding a new remote parcel repository in Cloudera Manager�s configuration The repository must have a file, manifest.json, which will be read by Cloudera Manager. This file will contain a timestamp remembering the latest update and the parcels available in the repository. Finally, the parcel name needs to include the distro for which it is built: el5 for RHEL 5.x, el6 for RHEL 6.x, precise for Ubuntu 12.04 LTS Precise Pangolin, lucid for Ubuntu 10.04 LTS Lucid Lynx, and so on. {
� "lastUpdated": 13673911330000,
� "parcels": [
��� {
����� "parcelName": "elephantbird-3.0.9-el6.parcel",
����� "hash": "103d194a738202526aa5ba4354717b3341c68594",
����� "components": []
��� },
��� {
����� "parcelName": "elephantbird-3.0.9-precise.parcel",
����� "hash": "12cbe127fd660a490b82eefde92b0f04d86b1e98",
����� "components": []
��� },
��� {
����� "parcelName": "hadoop_lzo-0.4.15-el6.parcel",
����� "hash": "e2556fea20baf1068e32d755d9196eab04523325",
����� "components": []
��� },
��� {
����� "parcelName": "hadoop_lzo-0.4.15-precise.parcel",
����� "hash": "5362d28511f919e812d9377e60aa5588c24347f7",
����� "components": []
��� }
� ]
}
   The Parcel Life-cycle Once the remote parcel repository is added to Cloudera Manager, the administrator can download the new parcels to Cloudera Manager�s local parcel repository (/opt/cloudera/parcel-repo), from where it can be distributed to all nodes of the cluster. When the admin decides to distribute the parcel, every node starts downloading the file from the Cloudera Manager server (no need for nodes to have internet access). For large clusters, this will generate significant network traffic, but fortunately the number of concurrent uploads can be defined. Once the file transfer is done, the parcel is untar�ed on the nodes in $PARCELS_ROOT. No symlink will be created at that point. Symlinks are only created when the cluster is restarted with the parcel activated. And of course, parcels can be removed from all the nodes and deleted from the local repository. Creating Your Own Parcels One of the biggest pains we experienced with Hadoop at Verisign is LZO compression. Going back to 2009, LZO was the reasonable solution to compress data with decent decompression speed, and to turn on split-ability. Assuming you have LZO-compressed files in your cluster, you want to continue to give your data scientists the ability to process the files. Below is a proposed approach for using a parcel to install LZO to your cluster. Due to its license (GPL v2+), LZO can�t be embedded into Apache projects and thus needs to be shipped separately. LZO native libraries need to be broadcasted to every node of the cluster to be referenced into the java.native.library options of the TaskTracker in order to be loaded by mapper and reducer tasks. You could bundle the libraries with your job and hack some code to include the LZO native libraries in the java.native.library at runtime (see code snippet 2 below), but starting with such tricks is opening the door to non-recommended practices (and remember the broken windows theory). Moreover, if you�re extensively using Hadoop Streaming, you want com.hadoop.mapred.DeprecatedLzoTextInputFormat to be available as inputformat parameter: hadoop jar /opt/cloudera/parcels/CDH-4.2.1-1.cdh4.2.1.p0.5/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.2.1.jar -inputformat "com.hadoop.mapred.DeprecatedLzoTextInputFormat" -mapper "/bin/cat" -reducer "/usr/bin/wc -l" -input /tmp/random_numbers.txt.lzo -output "streaming/output1"
   LZO native and java libraries should be sent to every node of the cluster and the directories added respectively into the java.native.library and the classpath of the TaskTracker. Puppet is a good fit here but rather than digressing, let�s build a parcel enabling LZO compression to the cluster. Building Hadoop LZO The first step consists of building Hadoop LZO. Hadoop LZO is the Java library bundling the LZO native libraries and the JNA interface. It also contains the utilities to index the LZO files, a mandatory step in making it split-able. Hadoop LZO is not yet compatible with Hadoop 2.0, so a bit of hacking is required to make it work. Or you can simply clone a fork where the modification is already done. Below are the two main changes that have been done to make the code 2.0-compatible: org.apache.hadoop.io.compress.Decompressor interface has an additional function getRemaining() and org.apache.hadoop.io.compress.BlockDecompressorStream.getCompressedData returns an int instead of void. Code change to be 2.0-compatible Create the Parcel The parcel will contain the hadoop-lzo jar library, the native library, and the appropriate metadata. Parcel content The parcel.json file will have the following content: {
  "name":               "hadoop_lzo",
  "version":            "0.4.15",
  "extraVersionInfo": {
    "fullVersion":        "0.4.15-precise",
    "baseVersion":        "0.4.15",
    "patchCount":         "p1"
  },
  "minPrevVersion":     "",
  "maxPrevVersion":     "",
  "setActiveSymlink":   true,
  "scripts": {
    "defines": "lzo_env.sh"
  },
  "packages": [
    { "name":    "hadoop-lzo",
      "version": "0.14.5"
    }
  ],
  "components": [
    { "name":     "hadoop-lzo",
      "version":  "0.14.5",
      "pkg_version":  "0.14.5"
    }
  ],
  "users": {},
  "groups": []
}
   And the lzo_env.sh will be the following: #!/bin/bash
PARCEL="hadoop_lzo-0.4.15"
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$PARCELS_ROOT/$PARCEL/java /hadoop-lzo-0.4.15-1.cdh4.jar
export JAVA_LIBRARY_PATH=$JAVA_LIBRARY_PATH:$PARCELS_ROOT/$PARCEL/native
   Note that no alternatives script is shipped because there is no binary file inside. The complete contents of this parcel can be found here. Deploying the Parcel parcel is uploaded in a HTTP web server directory, you need to create the manifest.json. The timestamp is the epoch time in deci-milliseconds: echo $(date +%s)0000
   Cloudera�s official manifest file is always a great source of inspiration. Configuring Hadoop via Cloudera Manager In Cloudera Manager, the configuration steps should be minimal. The only required step is to add the appropriate io.compression.codec class to have the .lzo extension recognized. To do that, add the following value to �MapReduce Service Configuration Safety Valve for mapred-site.xml� property:   ��io.compression.codec.lzo.class
  ��com.hadoop.compression.lzo.LzoCodec
   That should do it. Unfortunately, due to a bug in Cloudera Manager version &lt; 4.6, environment scripts are not executed at components startup –� the HADOOP_CLASSPATH set in lzo_env.sh is in fact not set appropriately. The workaround is to add the content of the script in the property �MapReduce Service Environment Safety Valve�: HADOOP_CLASSPATH=/opt/cloudera/parcels/hadoop_lzo-0.4.15/java/hadoop-lzo-0.4.15-1.cdh4.jar
JAVA_LIBRARY_PATH=/opt/cloudera/parcels/hadoop_lzo-0.4.15/native
   Restart Your Service We�re done: Simply restart the MapReduce service, and LZO compression will be activated! Because often someone else has already done the work for you, you can also simply add a remote repository in Cloudera Manager pointing to Cloudera�s official GPL Extra repository at�http://archive.cloudera.com/gplextras/parcels/latest/ to have the official HADOOP_LZO parcel (including Impala�s drivers). Or, you can point to http://killerwhile.github.io/parcels-repo/repo where alongside of Hadoop_lzo package, Elephant-Bird (CDH4-compatible) is also available. Enjoy! Benoit Perroud is Software Engineer at Verisign Inc., developing and scaling the companywide offline data processing platform � based on Hadoop infrastructure. He is also an Apache Committer, NoSQL enthusiast, and frequent speaker at Swiss and other European tech conferences. Learn More About Parcels Want to see the power of Parcels in action? Watch our e-learning module on Understanding Parcels to learn the fundamentals of optimizing your Hadoop operations with Parcels. The video includes a step-by-step demo of upgrading CDH and installing Impala, Search, and Hadoop LZO.</snippet></document><document id="174"><title>Cloudera Search over Apache HBase: A Story of Collaboration</title><url>http://blog.cloudera.com/blog/2013/07/cloudera-search-over-apache-hbase-a-story-of-collaboration/</url><snippet>Thanks to Steven Noels, SVP of Products for NGDATA, for the guest post below. NGDATA builds and sells Lily, the next-generation Customer Intelligence Platform that helps enterprise marketing teams collect and store customer interaction data in order to profile, segment, and present better offers. We designed Lily from the ground up to run on Apache HBase and Apache Solr. Combining these technologies with our deep marketing segmentation expertise and unique machine learning techniques we’re able to deliver interactive data management, real-time statistical calculations, faceted search views of customers, offers, interactions and the permutations they each inspire. The team at NGDATA has been working since mid-2010 on HBase triggers (or update notifications, if you want), which we use in Lily to sync up Solr with HBase, to make HBase freely searchable, compute indexed views for data exploration and feed our online machine learning engine with customer behavior information. The foundation portion of our platform � the Lily Data Repository, based on the combination of HBase and Solr � is being used by large banks, media companies and pharmaceutical firms who value combing Apache Hadoop�s data storage and parallel data processing framework with ad-hoc search and discovery through Solr. Enter Cloudera. In 2011, Cloudera added support for HBase to CDH, in alignment and confirmation of our vision of HBase being an ideal platform for capturing and processing customer interaction data. And, now, Cloudera has added Search to CDH, adapting, and improving Solr to co-exist with the Hadoop data infrastructure. Cloudera users now have access to MapReduce for parallel data processing, Impala for ad-hoc SQL querying, and Solr Search for ad-hoc data discovery, all running on top of the same data stored in Hadoop and HBase. (Over time, we invested a great deal into making Solr work well with HBase, continuously improving and expanding our triggering and indexing mechanism, which we first released as part of Lily mid 2010.) We are pleased with the collaboration, innovation, and quality that Cloudera has produced by working with us. Of course, we are an applications company, not an infrastructure company. Last year, we initiated a conversation with Cloudera to collaborate on making HBase triggering and indexing part of a larger, more complete offer in CDH, the platform we had already selected as our base. Cloudera and NGDATA both believe that HBase will often service use cases of real-time data ingestion and data serving, with Search being an integral part of that. In line with the Apache spirit, we contributed and collaborated with some outstanding engineers at Cloudera on an improved triggering and indexing mechanism, based on the design DNA of our previous inventions. In this most recent edition, we introduced an order of magnitude performance improvement: a cleaner, more efficient, and fault-tolerant code path with no write performance penalty on HBase. In the interest of modularity, we decoupled the trigger and indexing component from Lily, making it into a stand-alone, collaborative open source project that is now underpinning both Cloudera Search HBase support as well as Lily. This made sense for us, not just because we believe in HBase and its community but because our customers in Banking, Media, Pharma and Telecom have unqualified expectations for both the scalability and resilience of Lily. Outsourcing some part of that responsibility towards the infrastructure tier is efficient for us. We are very pleased with the collaboration, innovation, and quality that Cloudera has produced by working with us and look forward to a continued relationship that combines joint development in a community oriented way with responsible stewardship of the infrastructure code base we build upon. Our HBase Triggering and Indexing software can be found on GitHub at: https://github.com/NGDATA/hbase-sep https://github.com/NGDATA/hbase-indexer Do you have any indexing or update side-effect needs for HBase? Tell us your thoughts on this solution.�</snippet></document><document id="175"><title>How-to: Use the Apache Oozie REST API</title><url>http://blog.cloudera.com/blog/2013/06/how-to-use-the-apache-oozie-rest-api/</url><snippet>Apache Oozie has a Java client and a Java API for submitting and monitoring jobs, but what if you want to use Oozie from another language or a non-Java system?�Oozie provides a Web Services API, which is an HTTP REST API.�That is, you can do anything with Oozie simply by making requests to the Oozie server over HTTP.�In fact, this is how the Oozie client and Oozie Java API themselves talk to the Oozie server.� In this how-to, I’ll explain how the REST API works. What is REST? REST (Representational State Transfer) is a stateless architectural style for a client and server to communicate over HTTP.�The client typically makes HTTP requests and the server sends back an HTTP response.�The Oozie server accepts GET, PUT, and POST requests depending on the command.�GET is typically used for commands that are querying the server for information and don�t have any side-effects (e.g. asking for a list of jobs).�PUT is typically used for commands that are changing an already existing job (e.g. suspending a job). And POST is used for submitting a job.� JSON (JavaScript Object Notation) is a standard typically used for sending structured data over a network (like XML, but is considered more human-readable).�Like many REST APIs, the data that the Oozie server sends back is in JSON format to make them easy to parse.�There are a few exceptions though: logs are returned as plain text, workflow/coordinator/bundle definitions are returned as XML, and job graphs are returned as png images. How to Make a REST Request Most languages provide a library or other method of making a REST request; anything that can make an HTTP connection should work.�For example, we can use Python: $ cat list_jobs.py
import urllib2
import json

req = urllib2.Request('http://localhost:11000/oozie/v1/jobs?jobtype=wf')
response = urllib2.urlopen(req)
output = response.read()
print json.dumps(json.loads(output), indent=4, separators=(',', ': '))
   The python code makes a GET request to http://localhost:11000/oozie/v1/jobs and passes the parameter �jobtype� with a value of �wf�, which is the REST API call for returning a list of Oozie workflow jobs. It then dumps the output to the console, as we can see below: $ python list_jobs.py
{
��� "offset": 1,
��� "total": 1,
��� "len": 50,
��� "workflows": [
������� {
����������� "status": "SUCCEEDED",
����������� "run": 0,
����������� "startTime": "Wed, 22 May 2013 21:28:54 GMT",
����������� "appName": "no-op-wf",
����������� "lastModTime": "Wed, 22 May 2013 21:28:54 GMT",
����������� "actions": [],
����������� "acl": null,
����������� "appPath": null,
����������� "externalId": null,
����������� "consoleUrl": "http://rkanter-MBP.local:11000/oozie?job=0000000-130522142644540-oozie-rkan-W",
����������� "conf": null,
��������� ��"parentId": null,
����������� "createdTime": "Wed, 22 May 2013 21:28:54 GMT",
����������� "toString": "Workflow id[0000000-130522142644540-oozie-rkan-W] status[SUCCEEDED]",
����������� "endTime": "Wed, 22 May 2013 21:28:54 GMT",
����������� "id": "0000000-130522142644540-oozie-rkan-W",
����������� "group": null,
����������� "user": "rkanter"
������� }
��� ]
}
   As you can see, the output is returned as a JSON blob.�We had one workflow named �no-op-wf� and there�s some other details about that job as well. In the rest of the examples, we�ll be using the command-line program curl instead of Python.�Using curl, we can specify the type of request with the -Xargument, though the default is a GET request. The previous query made with curl would look like this: $ curl http://localhost:11000/oozie/v1/jobs?jobtype=wf
{"total":1,"workflows":[{"appPath":null,"acl":null,"status":"SUCCEEDED","createdTime":"Wed, 22 May 2013 21:28:54 GMT","conf":null,"lastModTime":"Wed, 22 May 2013 21:28:54 GMT","run":0,"endTime":"Wed, 22 May 2013 21:28:54 GMT","externalId":null,"appName":"no-op-wf","id":"0000000-130522142644540-oozie-rkan-W","startTime":"Wed, 22 May 2013 21:28:54 GMT","parentId":null,"toString":"Workflow id[0000000-130522142644540-oozie-rkan-W] status[SUCCEEDED]","group":null,"consoleUrl":"http:\/\/rkanter-MBP.local:11000\/oozie?job=0000000-130522142644540-oozie-rkan-W","user":"rkanter","actions":[]}],"len":50,"offset":1}
   The output is the same but because curl doesn�t �pretty-print� it, the formatting is harder for humans to read. However, it should be fine for any consuming programs or scripts. To make a PUT request using curl, we would do this: $ curl -i -X PUT "http://localhost:11000/oozie/v1/job/0000000-130524111605784-oozie-rkan-W?action=kill"
HTTP/1.1 200 OK
Server: Apache-Coyote/1.1
Content-Length: 0
Date: Fri, 24 May 2013 18:22:34 GMT
   In this example, we told Oozie to kill the job with ID 0000000-130524111605784-oozie-rkan-W.�Note that we specified -X PUT to do a PUT request instead of a GET request.�We also specified the optional -i to make curl output the response headers to make it easier to see that the request succeeded.� And to make a POST request using curl, we would do this: $ cat config.xml

&lt;configuration&gt;
��� &lt;property&gt;
������� &lt;name&gt;user.name&lt;/name&gt;
������� &lt;value&gt;rkanter&lt;/value&gt;
��� &lt;/property&gt;
��� &lt;property&gt;
������� &lt;name&gt;oozie.wf.application.path&lt;/name&gt;
������� &lt;value&gt;${nameNode}/user/${user.name}/${examplesRoot}/apps/no-op&lt;/value&gt;
��� &lt;/property&gt;
��� &lt;property&gt;
������� &lt;name&gt;queueName&lt;/name&gt;
������� &lt;value&gt;default&lt;/value&gt;
��� &lt;/property&gt;
��� &lt;property&gt;
������� &lt;name&gt;nameNode&lt;/name&gt;
������� &lt;value&gt;hdfs://localhost:8020&lt;/value&gt;
��� &lt;/property&gt;
��� &lt;property&gt;
������� &lt;name&gt;jobTracker&lt;/name&gt;
������� &lt;value&gt;localhost:8021&lt;/value&gt;
��� &lt;/property&gt;
��� &lt;property&gt;
������� &lt;name&gt;examplesRoot&lt;/name&gt;
������� &lt;value&gt;examples&lt;/value&gt;
��� &lt;/property&gt;
&lt;/configuration&gt;

$ curl -X POST -H "Content-Type: application/xml" -d @config.xml "http://localhost:11000/oozie/v1/jobs?action=start"
{"id":"0000009-130524111605784-oozie-rkan-W"}
   In this example, we submitted and started the workflow located at ${nameNode}/user/${user.name}/${examplesRoot}/apps/no-op where the Oozie server will resolve ${nameNode}, ${user.name}, and ${examplesRoot} to hdfs://localhost:8020, rkanter, and examples respectively; these are defined in the above config.xml file as well.� Note that this time we specified -X POST to make a POST request.�The -d argument lets us pass the data, in this case the XML file describing the job we want to submit (equivalent to the job.properties we would use with the Oozie client to submit a job).� We could have also specified the XML directly on the command line to the -d argument instead of using @filename, but with the file we can keep the pretty printed XML and also we don�t have to worry about escaping characters that the shell would otherwise try to interpret.�We also had to specify -H �Content-Type: application/xml� to let the Oozie server know that we are sending XML instead of regular text.� Using REST with Security The REST API also works when Kerberos is enabled on our Oozie server.�We simply have to make sure that we�ve acquired Kerberos credentials (e.g. kinit) before trying to connect to the server: $ kinit -kt rkanter.keytab rkanter
$ curl --negotiate -u foo:bar http://localhost:11000/oozie/v1/admin/status
{"systemMode":"NORMAL"}
   Depending on the tool we�re using to connect, additional arguments might need to be specified.�For example, to use curl, as we can see above we have to specify the --negotiate and -u arguments.�The username and password we specify with -u doesn�t matter because we�re using Kerberos, so we can put whatever we want (e.g. foo:bar, or even just :).� If we omit the -u then we�ll get a 401 Unauthorized error; even though its value is not actually being used.� If our Oozie server is configured to use HTTPS (SSL), then we simply have to replace �http://� with �https://� and port �11000� with �11443�.� Some tools may automatically follow the redirect and we could continue using �http://� and �11000�; but some, like anything Java-based, will not.�Note that if our Oozie server is using a self-signed certificate, some tools will also complain that the connection cannot be trusted or a similar message.�Java doesn�t have an option to ignore this, so we�d have to provide the certificate, but some tools have this option.�For example, with curl, specifying -kignores this: $ curl -k https://localhost:11443/oozie/v1/admin/status
{"systemMode":"NORMAL"}
   One Final Helpful Hint Besides looking at the Oozie Web Services API documentation, it is also quite useful to enable debug mode on the Oozie client; doing so will cause the client to print out information about the exact REST request it�s making to the Oozie server and is a good way to experiment with how to use the API.� To enable debug mode, enter the following in your terminal session: export OOZIE_DEBUG=1
   For example, once debug mode is enabled, the output will include information like this: $ oozie job -config examples/apps/no-op/job.properties -run
POST http://localhost:11000/oozie/v1/jobs?action=start
&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;&lt;configuration&gt;
&lt;property&gt;&lt;name&gt;user.name&lt;/name&gt;&lt;value&gt;rkanter&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;name&gt;oozie.wf.application.path&lt;/name&gt;&lt;value&gt;${nameNode}/user/${user.name}/${examplesRoot}/apps/no-op&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;name&gt;queueName&lt;/name&gt;&lt;value&gt;default&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;name&gt;nameNode&lt;/name&gt;&lt;value&gt;hdfs://localhost:8020&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;name&gt;jobTracker&lt;/name&gt;&lt;value&gt;localhost:8021&lt;/value&gt;&lt;/property&gt;
&lt;property&gt;&lt;name&gt;examplesRoot&lt;/name&gt;&lt;value&gt;examples&lt;/value&gt;&lt;/property&gt;
&lt;/configuration&gt;
job: 0000000-130522142644540-oozie-rkan-W
   Notice that the Oozie client made a POST request to http://localhost:11000/oozie/v1/jobs?action=start with the contents of job.properties as the above XML as the payload; and this looks very similar to the POST request we made with curl earlier (and without the nice formatting). Conclusion This blog post only scratches the surface of what can be done using Oozie�s REST API.�Anything the Oozie client can do can be done with the REST API, which means that there�s a rich set of controls for interacting with Oozie that can be leveraged by any custom application, dashboard, script, or other workflow engine: We can turn just about anything into an Oozie client.�A great example of this is Hue, which uses the REST API to view, manage, and submit jobs to Oozie.� The full Web Services API documentation, which lists every command and has more examples, can be found here for CDH4: http://archive.cloudera.com/cdh4/cdh/4/oozie/WebServicesAPI.html. (Note: The documentation included with earlier versions of Oozie was missing some commands and was not always correct; this was addressed by OOZIE-1183 and included in CDH 4.3.) Robert Kanter is a Software Engineer on the Platform team and an Apache Oozie Committer/PMC Member.</snippet></document><document id="176"><title>Happy birthday, Cloudera!</title><url>http://blog.cloudera.com/blog/2013/06/happy-birthday-cloudera/</url><snippet>Five years ago today, on June 27, 2008, we filed the incorporation paperwork for Cloudera, Inc., a new company we created to bring the power of Google’s big data platform to the masses. Back then, nobody was talking about “big data” and the only people who knew about Apache Hadoop were wild-eyed engineers working in the consumer internet. Today, the software is right at the center of a major new market in technology. It’s used by hospitals, energy companies, retailers, banks and others. The past five years have been tremendous. My thanks and congratulations to Clouderans around the world, to our fantastic users, customers and partners, and to the open source community generally for all you’ve done to bring us this far. We’re off to a pretty good start!  </snippet></document><document id="177"><title>What a Great Year for Hue Users!</title><url>http://blog.cloudera.com/blog/2013/06/what-a-great-year-for-hue-users/</url><snippet>With the recent release of CDH 4.3, which contains Hue 2.3, I’d like to report on the fantastic progress of Hue in the past year. For those who are unfamiliar with it, Hue is a very popular, end-user focused, fully open source Web UI designed for interaction with Apache Hadoop and its ecosystem components. Founded by Cloudera employees, Hue has been around for quite some time, but only in the last 12 months has it evolved into the great ramp-up and interaction tool it is today. It’s fair to say that Hue is the most popular open source GUI for the Hadoop ecosystem among beginners — as well as a valuable tool for�seasoned Hadoop users (and users generally in an enterprise environment)�– and it is the only end-user tool that ships with Hadoop distributions today. In fact, Hue is even redistributed and marketed as part of other user-experience and ramp-up-on-Hadoop VMs in the market. We have reached where we are today � 1,000+ commits later – thanks to the talented Cloudera Hue team (special kudos needed to Romain, Enrico, and Abe) and our customers and users in the community. Therefore it is time to celebrate with a classy new logo and community website at gethue.com! Hue is the most popular open source GUI for Hadoop among beginners — an is a valuable tool for seasoned Hadoop users as well. But before you go there for a visit, allow me to reflect on the milestones for Hue�s success. The accelerated uptake really started with Hue 2.0, when the Hue team made significant efforts to reshape Hue in the modern Web 2.0 style with a much improved user experience, drag-and-drop functionality, and a well organized and integrated tabular user experience — in one browser window. That was followed by the addition of a very useful workflow designer and scheduling application � which, by the way, has been proposed to the Apache Oozie community as the new face of Oozie, as it turned out to be a much better user experience than the one shipping with Oozie today. In early 2013 (version 2.2) the team added an Impala application to enable real-time query interaction on top of CDH. We also added Hue-based guided tutorials to our Cloudera QuickStart VM to make getting started with Hadoop really easy (with no registration needed). In Hue 2.3, which ships in CDH 4.3, the Hue team has added well over 100 new features and bug fixes, including a new Apache Pig editor, a Table Browser app for managing�Apache Hive, and support for keeping configuration and data in�Oracle Database 11.2 (among other things). We�re far from done though; we will continue the journey toward providing the best user experience for Hadoop – and the best startup tool ever! We can only get there if you continue providing your feedback and contributions, so take this post not only as a celebration but also as a big thank-you for all your previous interactions with the Hue team! Additional resources: � Hue mailing list � Install Hue � QuickStart VM � Hue blog posts and demos Eva Andreasson is a senior product manager at Cloudera, responsible for Hue, Search, and other projects in CDH.</snippet></document><document id="178"><title>Apache Bigtop: The "Fedora of Hadoop" is Now Built on Hadoop 2.x</title><url>http://blog.cloudera.com/blog/2013/06/apache-bigtop-the-fedora-of-hadoop-built-on-hadoop2/</url><snippet>Just in time for Hadoop Summit 2013, the Apache Bigtop team is very pleased to announce the release of Bigtop 0.6.0: The very first release of a fully integrated Big Data management distribution built on the currently most advanced Hadoop 2.x, Hadoop 2.0.5-alpha. Bigtop, as many of you might already know, is a project aimed at creating a 100% open source and community-driven Big Data management distribution based on Apache Hadoop. (You can learn more about it by reading one of our previous blog posts on Apache Blogs.) Bigtop also plays an important role in CDH, which utilizes its packaging code from Bigtop — Cloudera takes pride in developing open source packaging code and contributing the same back to the community. The very astute readers of this blog will notice that given our quarterly release schedule, Bigtop 0.6.0 should have been called Bigtop 0.7.0. It is true that we skipped a quarter. Our excuse is that we spent all this extra time helping the Hadoop community stabilize the Hadoop 2.x code line and making it a robust kernel for all the applications that are now part of the Bigtop distribution. And speaking of applications, we haven�t forgotten to grow the Bigtop family: Bigtop 0.6.0 adds Apache HCatalog and Apache Giraph to the mix. The full list of Hadoop applications available as part of the Bigtop 0.6.0 release is: Apache Zookeeper 3.4.5 Apache Flume 1.3.1 Apache HBase 0.94.5 Apache Pig 0.11.1 Apache Hive 0.10.0 Apache Sqoop 2 (AKA 1.99.2) Apache Oozie 3.3.2 Apache Whirr 0.8.2 Apache Mahout 0.7 Apache Solr (SolrCloud) 4.2.1 Apache Crunch (incubating) 0.5.0 Apache HCatalog 0.5.0 Apache Giraph 1.0.0 LinkedIn DataFu 0.0.6 Cloudera Hue 2.3.0 The list of supported Linux platforms has expanded to include: CentOS/RHEL 5 and 6 Fedora 17 and 18 SuSE Linux Enterprise 11 OpenSUSE 12.2 Ubuntu LTS Lucid (10.04) and Precise (12.04) Ubuntu Quantal (12.10) We would like to invite everybody to give the Bigtop 0.6.0 binary distribution a try. All you have to do is to pick your favorite Linux distribution, follow our wiki instructions, and you will have your first pseudo-distributed cluster computing pi in no time. If you�re thinking about deploying Bigtop to a fully-distributed cluster, you might find our Puppet code to be useful � after all, we use it all the time ourselves to test Bigtop. There is brief documentation on how to run our Puppet recipes in a master-less puppet configuration, but a typical Puppet master setup should work as well. Finally, Bigtop would not have been possible without the tireless work of all the volunteer developers. This is an amazing community to be part of, and if you would like to join us, now is the time. In fact, we decided to take advantage of Hadoop Summit drawing a lot of Hadoop developers to the San Francisco Bay Area and have our first meeting of the Apache Bigtop Working Group on Thursday, June 27 at Elance’s offices in Mountain View. Come join us! It is a lot of fun to build the future of Big Data management together! Happy Big Data discoveries, Your faithful and tireless Bigtop development team. Roman Shaposhnik is a Software Engineer on the Infrastructure team and VP/PMC chair of the Bigtop project.</snippet></document><document id="179"><title>Customer Spotlight: Big Data Making a Big Impact in Healthcare and Life Sciences</title><url>http://blog.cloudera.com/blog/2013/06/customer-spotlight-big-data-making-a-big-impact-in-healthcare-and-life-sciences/</url><snippet>In this Customer Spotlight, I�d like to emphasize some undeniably positive use cases for Big Data, by�looking at some of the ways the healthcare and life sciences industries are innovating to benefit humankind. Here are just a few examples: Mount Sinai School of Medicine�has partnered with Cloudera�s own Jeff Hammerbacher to apply Big Data to better predict and understand disease processes and treatments. The Mount Sinai School of Medicine is a top medical school in the US, noted for innovation in biomedical research, clinical care delivery, and community services. With Cloudera�s Big Data technology and Jeff�s data science expertise, Mount Sinai is better equipped to develop solutions designed for high-performance, scalable data analysis and multi-scale measurements. For example, medical research and discovery areas in genotype, gene expression and organ health will benefit from these Big Data applications. �We are at the cutting edge of disease prevention and treatment, and the work that we will do together will reshape the landscape of our field,� said Dennis S. Charney, MD, The Mount Sinai Medical Center. Read the press release: Cloudera Chief Scientist Jeff Hammerbacher Teams with Mount Sinai School of Medicine to Solve Medical Challenges Using Big Data Watch Jeff Hammerbacher�s presentation at The Mount Sinai Medical Center�s SINAInnovation conference from November 2012 Treato�is applying large-scale digital and social media analysis to improve healthcare. Treato.com is a consumer-oriented website that aggregates patient experiences from the internet and organizes them into usable insights for patients, physicians, and other healthcare professionals. This is a big deal because crawling the entire web for medicines, symptoms, side effects, and other health-related user generated content is difficult not only due to the sheer size of the web, but Treato also needs to process colloquial language combined with medical terminology and translate that into a �single version of truth.� Treato has aggregated and analyzed more than 1.1 billion online posts about over 11,000 medications and over 13,000 conditions from thousands of English language websites. The system can currently process 150-200 million user posts per day. �It would have been impossible to accomplish Treato�s mission without a big data solution such as Hadoop,� explained Assaf Yardeni, head of R&amp;D for Treato. �Treato is based on the concept of taking all the patient experiences already written and still being written each day, and aggregating and organizing them to expose meaningful insights. With the continuously growing number of pages and increasing volumes of health-related data on the web, Hadoop was the logical choice.� Read the case study: Leveraging Hadoop to Improve Healthcare with Large-scale Social Media Analytics Read the blog: How Treato Analyzes Health-related Social Media Big Data with Hadoop and HBase RelayHealth, a subsidiary of McKesson, processes healthcare provider-to-payer interactions betwen 200,000 physicians, 2,000 hospitals, and 1,900 payers (health plans), according to a Forrester / Cloudera webinar. RelayHealth is in the business of expediting communications between these two groups to improve the patients� experience and to ensure that healthcare providers get paid for their services efficiently, and they�ve turned to Hadoop to process those communications even faster. This ultimately saves customers money through better cash flow. In an era when our nation is struggling to provide affordable healthcare to our citizens, RelayHealth is adding a lot of value. RelayHealth�s Hadoop environment supports millions of transactions generated, thousands of files received, and over 150GB log data collected every single day. “Healthcare is a very competitive, tight margin industry with increasing needs for compliance on data of massive scale,” said Marty Smith, Senior Director of Product Innovation, RelayHealth (McKesson). “RelayHealth processes millions of claims per day on Cloudera Enterprise, analyzing more than 1 million log files per day and integrating with multiple Oracle and IBM systems. As a result, we are able to assist our healthcare providers to get paid faster, improving their cost models and productivity.” Watch the webinar: Realizing the Promise of Big Data with Hadoop What are some other ways you�re seeing Big Data applied to benefit humankind? Share your thoughts, ideas, and examples here! Karina Babcock is Cloudera�s Customer Programs &amp; Marketing Manager.</snippet></document><document id="180"><title>Hadoop for Everyone: Inside Cloudera Search</title><url>http://blog.cloudera.com/blog/2013/06/hadoop-for-everyone-inside-cloudera-search/</url><snippet>CDH, Cloudera’s 100% open source distribution of Apache Hadoop and related projects, has successfully enabled Big Data processing for many years. The typical approach is to ingest a large set of a wide variety of data into HDFS or Apache HBase for cost-efficient storage and flexible, scalable processing. Over time, various tools to allow for easier access have emerged — so you can now interact with Hadoop through various programming methods and the very familiar structured query capabilities of SQL. However, many users with less interest in programmatic interaction have been shut out of the value that Hadoop creates from Big Data. And teams trying to achieve more innovative processing struggle with a time-efficient way to interact with, and explore, the data in Hadoop or HBase. Helping these users find the data they need without the need for Java, SQL, or scripting languages inspired integrating full-text search functionality, via Cloudera Search (currently in beta), with the powerful processing platform of CDH. The idea of using search on the same platform as other workloads is the key — you no longer have to move data around to satisfy your business needs, as data and indices are stored in the same scalable and cost-efficient platform. You can also not only find what you are looking for, but within the same infrastructure actually “do” things with your data. Cloudera Search brings simplicity and efficiency for large and growing data sets that need to enable mission-critical staff, as well as the average user, to find a needle in an unstructured haystack! As a workload natively integrated with CDH, Cloudera Search benefits from the same security model, access to the same data pool, and cost-efficient storage. In addition, it is added to the services monitored and managed by Cloudera Manager on the cluster, providing a unified production visibility and rich cluster management � a priceless tool for any cluster admin. In the rest of this post, I’ll describe some of Cloudera Search’s most important features. Apache Solr-based for Easy Integration�and Production Maturity Cloudera Search benefits from the same security model, data pool, and cost-efficient storage as CDH. Cloudera chose to adopt Apache Solr�into the Cloudera platform family and contribute innovative, valuable, use-case optimized integrations across the CDH platform. Solr is by far the most mature, active, and product-deployed open source search engine available. With Cloudera�s strong belief in the value of open source, it was a no-brainer to choose Solr after technical introspection of code, studying the active community, and realizing the Solr roadmap (especially with the then alpha-state SolrCloud project � later released in Solr 4). Solr provides rich APIs and is widely used and integrated, so it was a key goal in Cloudera Search to support standard Solr APIs. Any application relying on Solr APIs should seamlessly migrate, and any third-party integration relying on standard Solr APIs should work as-is. Robust and Scalable Index Storage in HDFS HDFS is the storage layer for Cloudera Search. It hosts both data and indices and provides out-of-the-box replication, so if a node goes down, you will be able to continue serving searches elsewhere in the cluster (not to mention the value of cost-efficient index storage in a highly scalable infrastructure). Scalable Batch Indexing Through MapReduce MapReduce is the foundation of the batch-oriented indexing workload provided out of the box with Cloudera Search. In the mapper phase, the data is extracted and prepared for indexing. In each reducing phase, data is indexed, the reducers embed Apache Lucene indexers, and those index pieces are merged. The indexing and merging continue to the point that the indexed data is merged into the right number of index shards, written and stored into HDFS. The MapReduce approach allows you to utilize all available resources assigned for MapReduce, and hence enables a very powerful scalable indexing workload. It also allows for flexibility in the sense that you can easily spin up an on-demand indexing job over a temporary data set, or one that you temporarily need to make searchable. So you don�t need to stick with expensive index storage; Hadoop for Everyone: Inside Cloudera Search. Immediate Serving of New Indexes via GOLIVE You can choose to apply the Cloudera-innovated GOLIVE feature and start serving new indices immediately into running Solr servers on the cluster � thanks to the platform integration. No downtime or warmup time for running search servers! Near Real-Time Indexing at Ingest via Flume The second option for indexing is via the scalable ingestion framework, Apache Flume. A Cloudera-contributed Flume sink allows for extraction and mapping of data on the Flume side, and immediate writing of indexable data to Solr servers running natively on HDFS. Flume also provides neat features of routing, filtering, and annotating data, as well as splitting channels so you can optionally choose to write data directly into HDFS as well as to a Solr server running on the cluster. The Flume integration allows for data to be indexed on its way into Hadoop. The Flume ingestion framework scales very well at production sites today, and the indexing at ingest is as scalable as Flume itself — depending on how much data you plan to push through your indexers running on the cluster at a time AND how many indexers you have running, of course. So it is use-case dependent, and Cloudera is more than happy to help you and your use case throughout that deployment and design process. Near Real-Time Indexing at Scale of HBase Records Another option for near real-time indexing is via the SEP and index process contributed to Cloudera Search by our partner, NGDATA. It camouflages a replication server, and taps into the replication events generated by a master Apache HBase region server. Via this seemless integration, it can consume all updates and events without disturbing normal HBase activity, and hence with no performance hit. The events are then optionally configured and prepared for indexing and then sent over to the Solr servers running on HDFS, which writes the indexed data as indexes into HDFS. Thus no coprocessors involved, and yet you get a very neat solution for doing secondary indices over your data in HBase. Simple and Re-usable Data Extraction Cloudera Morphlines is a very simple framework to do “mini-ETL” of your inbound data to prepare it for multiple indexing workloads (be it via Flume, MapReduce, or HBase). Similar to Unix pipelines, it consists of a library that allows for simple command-based modifications as you parse a data event. Morphlines provides dynamic abilities to change the characteristics of your data on its way to becoming searchable. Extended File Format Support The Morphlines framework lets you dynamically change the characteristics of your data on its way to becoming searchable. Cloudera Search the same standard file formats as Solr (or rather, that Apache Tika supports). We have also added support for Hadoop optimized file formats and compression formats such as Avro, Sequence files, and Snappy. CDH for Everyone Cloudera Search also comes with a simple UI provided as an application in Hue. Hue also provides easy applications to interact with other components in CDH (file browser, query applications, workflow scheduler, etc). Production Management and Monitoring through Cloudera Manager No solution would be production valid without proper monitoring and management capabilities. Cloudera Manager helps install, deploy, and monitor Search services on the cluster. Over time, there will be much opportunity to provide more advanced insights into search services and activities. Stay tuned! Hardware Specifications No specific hardware configurations are needed, but depending on your use case and real-time serving needs, you might want to consider SSD, larger RAM, and so on. That said, Solr works as-is on commodity hardware and has shown great performance on existing CDH clusters where it has been deployed in private beta. Security and Access Cloudera Search also provides options for user access, any of which you can consider a per-event (per-document) level access model. One option is to annotate events as they come in through Flume via Flume�s existing interceptor capability. Then, at query time, use fixed filters to control what documents � based on what annotations � get displayed to what user groups. Another option is to utilize index (or collection) level access. We are in the process of integrating the public beta version of Cloudera Search with Kerberos, and plan to support both authentication and token delegation (or similar solution) to enable authorized users accessing only the collections they have rights to read from. In the MapReduce case, it could be added to do annotations as part of the data extraction and mapping phase, or even as a MapReduce step that sends the data over to the batch indexing process. Conveniently scheduled via an Apache Oozie workflow perhaps? Standard Solr � Everything Solr That You Already Know With all this goodness, it is important to remember that Cloudera Search is standard Solr: All your Solr domain knowledge will be applicable to Cloudera Search, with the advantage to perhaps learn even more about Hadoop over time to enable other workloads over the very same data. Basically, you could buy any off-shelf book on Solr 4 and get up to speed on Cloudera Search at the same time. What Cloudera Search adds is the integration points and rich documentation. Next Steps And of course, if you know Cloudera, there is obviously much more to come as we continue our journey forward! Stay tuned, however in the meantime, to learn more: Watch the on-demand webinar Try out Cloudera Search Find out more about how others have used or plan to use Cloudera Search Eva Andreasson is a senior product manager at Cloudera, responsible for Hue, Search, and other projects in CDH.</snippet></document><document id="181"><title>Meet the Project Founder: Alejandro Abdelnur</title><url>http://blog.cloudera.com/blog/2013/06/meet-the-project-founder-alejandro-abdelnur/</url><snippet>In this installment of “Meet the Project Founder”, meet Apache Oozie PMC member (and ASF member) Alejandro Abdelnur, the Cloudera software engineer who founded what eventually became the Apache Oozie project in 2011. Alejandro is also on the PMC of Apache Hadoop. What led you to your project idea(s)? Back in 2008, while I was working at Yahoo! in Bangalore, we began to notice that other teams were taking a variety of manual, ad hoc approaches (whether using shell scripts, JobControl, Ant, and so on) to managing multiple Hadoop jobs. There was clearly an opportunity to build a single solution that everyone could use and that could be much more efficiently supported internally. My team built one of those ad hoc systems to process the ingestion of partner feeds. This system was called Pac-Man, and it abstracted the multiple steps required to build data pipelines via a server-based workflow engine. Leveraging what we learned while developing Pac-Man, and after talking with other teams, we designed and built Oozie, a general-purpose workflow system for Hadoop. After an extensive internal evaluation, Oozie became the default solution for workflow coordination internally. In May 2010, Yahoo! open-sourced Oozie’s source code. Some time later, I left Yahoo! and came to Cloudera. In 2011 Oozie became an Apache Incubator project, graduating into top-level project in 2012. Oozie has an almost perfect record on backward-compatibility, which is almost unheard of. Aside from doing the initial commit, what is your definition of the project founder�s role across the lifespan of the project — benevolent dictator, referee, silent partner? I see the project founder primarily as a “first shipper” – just another developer on the project. Secondarily, the founder is the caretaker of the original project vision and keeps everyone pointed in the right direction. You have to be careful though because there is always some emotional attachment there, and your instinct could be to be overly protective. Instead, you need to be open to things that add value to the project, even if you don’t agree with them 100 percent. That’s the whole point of being a community. What has surprised you the most about how your projects have evolved/matured? The stability, scalability, and reliability achieved by the community in such a short time have all surprised me. But the best thing of all has been backward compatibility – the Oozie developers have done an awesome job there. We have an almost perfect record on that point, which is not something you see from most projects. I think the flexibility of the implementation has also surprised a lot of people, even us. It lets us quickly integrate new technologies. Adding support for YARN and HCatalog in Oozie was a relatively simple task, for example. What is the major work yet to be done, from your perspective as a project founder? The developer community needs to get bigger and more diverse. Oozie also needs to be easier to use, and support more use cases. For example, Oozie does a good job of supporting synchronous processing (of data that arrives on a regular schedule and in expected amounts). It needs to get better at asynchronous processing, of data as it becomes available (or on demand, basically). We’re working on that. What is your philosophy, if you have one, for balancing quality versus quantity with respect to contributions? Quality always comes first. We always focus more on making sure that things work right, rather than just dumping half-baked features into it and destabilizing the project. My philosophy is to always take baby steps – take a lot of them, and make sure they’re tiny ones. Do you have any other advice for potential project founders? Stay patient and be open to new ideas. Like I said previously, you may not agree with them completely, but sometimes you have to sacrifice your own reservations for the good of the project and the community.</snippet></document><document id="182"><title>Congrats to Explorys, A Computerworld Honors Laureate for Big Data</title><url>http://blog.cloudera.com/blog/2013/06/explorys-a-computwerworld-honors-laureate/</url><snippet>The following guest post is courtesy of Doug Meil, Chief Architect at Explorys, Apache HBase Committer/PMC Member, and Champion of Big Data: On June 3, 2013, I represented Explorys at the Computerworld Honors Laureate Ceremony and Awards Gala ceremony held in Washington, D.C.�Explorys was named an Honors Program Laureate and nominated for a 2st Century Achievement Award in the “Collaboration” category (one of 11 categories) for the Explorys-Cloudera case study on “Big Data in Healthcare.” The theme this year was �A Search For New Heroes,� to recognize applications of information technology that promote positive social, economic, and educational change. The setting was spectacular: black tie at the Andrew W. Mellon Auditorium, right on the mall on Constitution Avenue.�Gravitas?�Check. (Truman signed the NATO treaty there.)�Medallions for the 267 laureates were passed out in efficient high-school graduation style, and were impressive both in look and weight (and lethal if swung overhead).�The highlight of the evening for me, at least, was the keynote from Vinton Cerf talking about the �Internet of Things� and the need for IPv6. We didn�t win the category’s award, but there were a lot of terrific projects.�One category winner was using biometric identification for identity management in India, another was fighting HIV in Africa, and the winner in the Safety and Security category was for a system to help combat piracy.�Not�software�piracy, but “piracy” piracy � with boats and guns. All and all, it was an honor to be there.�Looking forward to submitting something for next year!</snippet></document><document id="183"><title>Demo: The New Search App in Hue 2.4</title><url>http://blog.cloudera.com/blog/2013/06/demo-the-new-search-app-in-hue-2-4/</url><snippet>In version 2.4 of Hue, the open source Web UI that makes Apache Hadoop easier to use, a new app was added in addition to more than 150 fixes: Search! Using this app, which is based on Apache Solr, you can now search across Hadoop data just like you would do keyword searches with Google or Yahoo! In addition, a wizard lets you tweak the result snippets and tailors the search experience to your needs. The new Hue Search app uses the regular Solr API underneath the hood, yet adds a remarkable list of UI features that makes using search over data stored in Hadoop a breeze. It integrates with the other Hue apps like File Browser for looking at the index file in a few clicks. Here’s a video demoing queries and results customization. The demo is based on�Twitter Streaming data collected with Apache Flume and indexed in real time: The new Hue Search app adds an impressive list of features to the already enterprise-grade, industry-standard features that Solr provides. Here are the most significant ones: Based on standard Solr and Solr Cloud Optimized for Cloudera Search for searching Hadoop Result snippet editor with live preview Field, range, and date facets Sorting Highlighting Layout and function templates Custom CSS/Javascript placeholders Code from Snippet Editor Here are the final templates used in the Hue Search app demo for customizing the look-and-feel of the search results. With this HTML and CSS, we inserted the index fields we wanted to display, added Twitter profile images, icons, and links and changed the font and colors of the text. HTML from the Source tab: &lt;div class="row-fluid"&gt;
  �&lt;div class="row-fluid"&gt;
  �� &lt;div class="row-fluid"&gt;
  ���� &lt;div class="span1"&gt;
  ������ &lt;img src="http://twitter.com/api/users/profile_image/{{user_screen_name}}" class="avatar" /&gt;
  ������ &lt;/div&gt;
  ������ &lt;div class="span11"&gt;
  �������� &lt;a href="https://twitter.com/{{user_screen_name}}/status/{{id}}" class="btn openTweet"&gt;
  ���������� &lt;i class="icon-twitter"&gt;&lt;/i&gt;
  �������� &lt;/a&gt;
  �������� &lt;b&gt;{{user_name}}&lt;/b&gt;
  �������� &lt;br/&gt;
  �������� {{text}}
  �������� &lt;br/&gt;
  �������� &lt;div class="created"&gt;{{#fromnow}}{{created_at}}{{/fromnow}}&lt;/div&gt;
  ������ &lt;/div&gt;
  ���� &lt;/div&gt;
  ���� &lt;br/&gt;
  �&lt;/div&gt;�
  &lt;/div&gt;
   CSS from the advanced tab: &lt;style&gt;
  em {
  �font-weight: bold;
  �background-color: yellow;
  }

.avatar {
  margin: 10px;
  }

.created {
  margin-top: 10px;
  color: #CCC;
  }� 

.openTweet {
  �float: right;
  �margin: 10px;
  }
  &lt;/style&gt;
   More advanced features are on the way, like fine-grained security for indexes, multi-shard search, and even saving results. We welcome any feedback on hue-user and Solr-specific requests on search-user!</snippet></document><document id="184"><title>QuickStart VM: Now with Real-Time Big Data</title><url>http://blog.cloudera.com/blog/2013/06/quickstart-vm-now-with-real-time-big-data/</url><snippet>For years, Cloudera has provided virtual machines that give you a working Apache Hadoop environment out-of-the-box. It�s the quickest way to learn and experiment with Hadoop right from your desktop. We�re constantly updating and improving the QuickStart VM, and in the latest release there are two of Cloudera�s new products that give you easier and faster access to your data: Cloudera Search and Cloudera Impala. We�ve also added corresponding applications to Hue – an open source web-based interface for Hadoop, and the easiest way to interact with your data. Cloudera Search integrates Apache Solr with the rest of the platform, to let you do full-text search of the data stored in your cluster, just like you would with an online search-engine! Cloudera Impala, on the other hand, lets you execute SQL queries against that same data, on the same platform, and get results back fast enough to interactively explore and analyze. With both these workloads available on the cluster, it eliminates the pain of having to move large data sizes around. To help you get a sense of how these could work for you, we�ve set up a couple of examples in the Cloudera QuickStart VM. You can download the VM here for VirtualBox, VMware, and other hypervisors. Starting Services in Cloudera Manager When the QuickStart VM boots, it configures all the services you might expect on a Cloudera cluster. Obviously, this single-node �pseudo-distributed� simple setup does not represent the performance, scalability, and reliability of a fully-distributed cluster – but it does give you a taste of how easy it is to perform powerful work with your data. The core services are already running, but you�ll need to make sure Impala, and Solr, or any other additional service you�d like to try is started before you proceed. Enabling Cloudera Search From the welcome screen, select Cloudera Manager, or navigate to http://localhost:7180 in your browser. Log in with the username and password �admin�. When the Services page loads, look for the line with the solr1 service, and click Actions ? Start… ZooKeeper, HDFS, MapReduce, and Hue are started automatically and should already be running. Later in the tutorial, you may want to stop Solr and start Impala. If you go beyond the examples in this tutorial, you may need to allocate more memory for your VM. The default is 4GB, but starting more services in Cloudera Manager may require more, depending on your use case. Batch Indexing with MapReduce One way to make data searchable is to index it with a batch job. This is ideal for fixed data sets, like a collection of reports from last year. The script ~/datasets/batch-tweets.sh demonstrates how you would set up a �collection� in Solr and invoke MapReduce for this type of job. You can open the Terminal application from the taskbar at the top, and run this script to load and index some sample data: $ ~/datasets/batch-tweets.sh
   Note that this data does not consist of real tweets – it is just similarly structured data to demonstrate the process. (If you want to see some interesting, real data from Twitter, see the near-real-time example below!) Now that you have some data loaded, you can try out the interfaces for doing full-text search. Searching With Hue and Solr Click the Hue bookmark or navigate to http://localhost:8888/home in the browser, and open the Search app. (Look for this�icon.) You will be presented with a list of collections – select the batch_tweets collection that we just created and import it. Once it is imported, open that collection, click the Search It! link, and you will see all the data from the fake tweets we just indexed. As you type a query, the list is filtered accordingly. You can also click the Solr bookmark in Firefox or navigate to http://localhost:8983/solr to access the Solr admin web interface. This interface provides more advanced information about your data and the underlying search infrastructure. Hue also allows you to customize which fields in the data are shown, and how they are displayed. Check out this video that shows how to quickly make a professional-looking search page like the one below. Near-real-time Indexing with Flume You can configure Cloudera Search to stream live data from Twitter with Flume, index it with Solr as it comes in, and store it in HDFS for future searches. You�ll need to sign in to dev.twitter.com with your Twitter account, select My applications from the drop-down menu in the top-right corner, and Create a new application to represent your Search installation. (You don�t need a callback URL, and as you won�t be sharing this �app� with others, you may fill in the other fields according to your own preference.) Once the application is created, click Create my access token at the bottom of the page. To configure your cluster to connect to Twitter as this application, you�ll need the consumer key, consumer secret, access token, and access token secret from this page. (You may have to refresh to see the access token you created.) Keep this information confidential just as you would your Twitter username and password. You can learn more about Twitter�s public data streams and their policies here. The script ~/datasets/nrt-tweets.sh demonstrates how you would configure Flume for this type of job. By providing the Twitter credentials listed above, you can use this script to create a collection and start downloading data: $ ~/datasets/nrt-tweets.sh start [CONSUMER_KEY] [CONSUMER_SECRET] \
	[ACCESS_TOKEN] [ACCESS_TOKEN_SECRET]
   Tweets become searchable seconds later. In the Hue Search application, go to the Collection Manager, and import the new collection.�If you don�t see any tweets, look at the log file /var/log/flume-ng/flume.log for any errors that were returned by the Twitter API. If the credentials were wrong when running the setup script, you can edit them in /etc/flume-ng/conf/flume.conf and restart flume. If the VM was not able to set the correct system time during the boot process (`date --utc` must return the current UTC time for authentication), correct it with `ntpdate pool.ntp.org`. You should restart the services in Cloudera Manager after making this change. When you want to stop ingesting data, run: $ ~/datasets/nrt-tweets.sh stop
   or $ sudo service flume-ng-agent stop
   Note that nrt-tweets.sh runs Flume independently of the service in Cloudera Manager, so it will not interfere with any other Flume configuration you might set up in the QuickStart VM using Cloudera Manager.� Interactive Querying with Impala Another recent addition to our Big Data platform is Cloudera Impala. Impala allows you to execute SQL queries against your data in Hadoop or HBase, using the same tables and metastore you use with Apache Hive. Impala is designed for extremely low latency – it�s fast enough to interactively explore and analyze your data. Again, a single-node demo doesn�t truly demonstrate the speed and scalability of Impala, but it will let you execute some sample queries and see the performance relative to Hive. As we did with search, make sure the impala1 and hive1 services are started in Cloudera Manager. Hue provides some sample data sets for Hive and Impala (and other components too). You can install them from within Hue, just go to About (top-left icon) ? Step 2: Examples, and click the service you want to try out. The QuickStart VM comes with two additional data sets you can install for Hive and Impala. The first lists the median income for each zip code in the United States from the 2000 Census. The second is a much larger data set from the Transaction Processing Performance Council that is used to benchmark databases against realistic business workloads. There are scripts to install and configure each, but be sure to execute the refresh command to make Impala aware of the new data sets: $ ~/datasets/zipcode-setup.sh
$ ~/datasets/tpcds-setup.sh # requires Internet access
$ impala-shell -q �refresh�
   Once these data sets are installed you can explore the tables in the Metastore Manager application in Hue, and you can execute SQL queries through the Hive and Impala applications (or from the command-line using the hive and impala-shellutilities). Here are some example queries you can try. Compare them and see how much faster you can get results with Impala! select * from zipcode_incomes where zip='59101';
   select�
�� i_item_id,
�� s_state,
�� avg(ss_quantity) agg1,
�� avg(ss_list_price) agg2,
�� avg(ss_coupon_amt) agg3,
�� avg(ss_sales_price) agg4
FROM store_sales
JOIN date_dim on (store_sales.ss_sold_date_sk = date_dim.d_date_sk)
JOIN item on (store_sales.ss_item_sk = item.i_item_sk)
JOIN customer_demographics on (store_sales.ss_cdemo_sk = customer_demographics.cd_demo_sk)
JOIN store on (store_sales.ss_store_sk = store.s_store_sk)
where
�� cd_gender = 'M' and
�� cd_marital_status = 'S' and
�� cd_education_status = 'College' and
�� d_year = 2002 and
�� s_state in ('TN','SD', 'SD', 'SD', 'SD', 'SD')
group by
�� i_item_id,
�� s_state
order by
�� i_item_id,
�� s_state
limit 100;
   Next Steps… New products from Cloudera like Search and Impala are making it easy to extract valuable information from your data faster than ever before. To learn about how these tools can be used to help solve problems and answer bigger questions, please visit Cloudera�s website: Introducing Search Introducing Impala E-Learning Courses If you run into any problems or have questions, you can refer to our online documentation or contact us on the appropriate user groups, all of which are detailed here. Other links you may find useful: Cloudera Development Kit Hue – The UI for Apache Hadoop Sean Mackrory is a Software Engineer on the infrastructure team and an Apache Bigtop Committer.</snippet></document><document id="185"><title>Meetups at Hadoop Summit</title><url>http://blog.cloudera.com/blog/2013/06/meetups-at-hadoop-summit/</url><snippet>Hadoop Summit convenes next week, and even if you’re not attending, there are a host of meetup opportunities available to you during the week. Here are just a few, and you can find a full list here. Tues. June 25 Apache Accumulo Users Group�- 10am, San Jose Convention Center Apache Oozie Contributors – 12pm, San Jose Convention Center Bay Area Apache HBase Users – 1:30pm, San Jose Convention Center Apache�Hive Users – 1:30pm, San Jose Convention Center Apache Flume Users – 6pm, Hilton Hotel, Almaden Ballroom 1� Weds. June 26 Apache Sqoop Users – 6pm, Hilton Hotel, Winchester Room� If you ARE attending, remember also: “Securing the Hadoop Ecosystem” at 11:20am on Weds. June 26 “Panel – When Worlds Collide: SQL Meets Hadoop” at 2:05pm on Weds. June 26� “Trends in Supporting Production Apache HBase Clusters” at 2:55pm on Weds. June 26� “Parquet: Columnar storage for the People” at 4:05pm on Weds. June 26 “Deploying Apache Flume to enable low-latency analytics” at 4:55pm on Weds. June 26 …And don’t forget to visit the Cloudera booth for signed copies of Hadoop Operations (Eric Sammer) and HBase in Action (Amandeep Khurana co-author)!</snippet></document><document id="186"><title>Introduction to Apache HBase Snapshots, Part 2: Deeper Dive</title><url>http://blog.cloudera.com/blog/2013/06/introduction-to-apache-hbase-snapshots-part-2-deeper-dive/</url><snippet>In Part 1�of this series about Apache HBase snapshots, you learned how to use the new Snapshots feature and a bit of theory behind the implementation. Now, it�s time to dive into the technical details a bit more deeply. What is a Table? An HBase table comprises a set of metadata information and a set of key/value pairs: Table Info: A manifest file that describes the table �settings�, like column families, compression and encoding codecs, bloom filter types, and so on. Regions: The table �partitions� are called regions. Each region is responsible for handling a contiguous set of key/values, and they are defined by a start key and end key. WALs/MemStore: Before writing data on disk, puts are written to the Write Ahead Log (WAL) and then stored in-memory until memory pressure triggers a flush to disk. The WAL provides an easy way to recover puts not flushed to disk on failure. HFiles: At some point all the data is flushed to disk; an HFile is the HBase format that contains the stored key/values. HFiles are immutable but can be deleted on compaction or region deletion. (Note: To learn more about the HBase Write Path, take a look at the HBase Write Path blog post, and for more details on HFiles take a look at the HBase I/O – HFile blog post.) What is a Snapshot? A snapshot is a set of metadata information that allows the admin to get back to a previous state of the table it is taken on. A snapshot is not a copy of the table; the simplest way to think about it is as a set of operations to keep track of metadata (table info and regions) and the data (HFiles, memstore, WALs). No copies of the data are involved during the snapshot operation. Offline Snapshots: The simplest case for taking a snapshot is when a table is disabled. Disabling a table means that all the data is flushed on disk, and no writes or reads are accepted. In this case, taking a snapshot is just a matter of going through the table metadata and the HFiles on disk and keeping a reference to them. The master performs this operation, and the time required is determined mainly by the time required by the HDFS namenode to provide the list of files. Online Snapshots: In most situations, however, tables are enabled and each region server is handling put and get requests. In this case the master receives the snapshot request and asks each region server to take a snapshot of the regions for which it is responsible. The communication between the master and region servers is done via Apache ZooKeeper using a two-phase commit-like transaction.The Master creates a znode that means �prepare the snapshot�. Each region server will process the request and prepare the snapshot for the regions from the table for which it is responsible. Once they are done, they add a sub-node to the prepare-request znode with the meaning, �I�m done�. Once all the region servers have reported back their status, the master creates another znode that means �Commit snapshot�; each region server will finalize the snapshot and report the status as before joining the node. Once all the region servers have reported back, the master will finalize the snapshot and mark the operation as complete. In case of a region server reporting a failure, the master will create a new znode used to broadcast the abort message. Since the region server is continuously processing new requests, different use cases may require different consistency models. For example, someone may be interested in a sloppy snapshot without the new data in the MemStore, someone else may want a fully consistent snapshot that requires locking writes for a while, and so on. For this reason, the procedure to take a snapshot on the region server is pluggable. Currently, the only implementation present is �Flush Snapshot,� which performs a flush before taking a snapshot and guarantees only row-consistency. Other procedures with different consistency policies may be implemented in the future. In the online case, the time required to take a snapshot is bounded by the time required by the slowest region server to perform the snapshot operation and report success back to the master. This operation is usually on the order of a few seconds. Archiving As we have seen before, HFiles are immutable. This allows us to avoid copying the data during the snapshot or clone operations, but during compaction they are removed and replaced by a compacted version. In this case, if you have a snapshot or a cloned table that is referencing one of those files, instead of deleting them they are moved to an �archive� location. If you delete a snapshot and no one else is referencing the files referenced by the snapshot, those files will be deleted. Cloning and Restoring Tables Snapshots can be seen as a backup solution where they can be used to restore/recover a table after a user or application error, but the snapshot feature can allow much more than a simple backup-and-restore. After cloning a table from a snapshot, you can write a MapReduce job or a simple application to selectively merge the differences, or what you think is important, into production. Another use case is that you can test schema changes or updates to the data without having to wait hours for a table copy and without ending up with lots of data duplicated on disk. Clone a Table from a Snapshot When an administrator performs a clone operation, a new table with the table-schema present in the snapshot is created pre-split with the start/end keys in the snapshot regions info. Once the table metadata is created, instead of copying the data in, the same trick as with the snapshot is used. Since the HFiles are immutable, just a reference to the source file is created; this allows the operation to avoid data copies and allows the clone to be edited without impacting the source table or the snapshot. The clone operation is performed by the master. Restore a Table from a Snapshot The restore operation is similar to the clone operation; you can think about it as deleting the table and cloning it from the snapshot. The restore operation brings back the old data present in the snapshot removing any data from the table that is not also in the snapshot, and also the schema of the table is reverted to that of the snapshot. Under the hood, the restore is implemented by doing a diff between the table state and the snapshot, removing files that are not present in the snapshot and adding references to the ones in the snapshot but not present in the current state. Also the table descriptor is modified to reflect the table �schema� at the moment of the snapshot. The restore operation is performed by the master and the table must be disabled. Futures Currently, the snapshot implementation includes all basic required functionality. As we have seen, new snapshot consistency policies for the online snapshots can provide more flexibility, consistency, or performance improvements. Better file management can reduce the load on the HDFS Name Node and improve disk space management. Furthermore, metrics, Web UI (Hue), and more are on the to-do list. Conclusion HBase snapshots add new functionality like the �procedure coordination� used by the online snapshot, or the copy-on-write snapshot, restore, and clones. Snapshots provide a faster and better alternative to handmade �backup� and �cloning� solutions based on distcp or CopyTable. All the snapshot operations (snapshot, restore, clone) don�t involve data copies, resulting in quicker snapshots of the table and savings in disk space. For more information about how to enable and use snapshots, please refer to the HBase operational management doc. Matteo Bertozzi is a Software Engineer on the Platform team and an HBase Committer.</snippet></document><document id="187"><title>Welcome, Tom!</title><url>http://blog.cloudera.com/blog/2013/06/welcome-tom/</url><snippet>We announced a leadership change at Cloudera today. Tom Reilly, formerly CEO at Arcsight, is joining us in my old role � CEO � and I am assuming two new posts: Chief Strategy Officer and Chairman of the Board of Directors. When we started the company five years ago, almost no one had heard of Apache Hadoop. Big Data, to the extent the term was used at all, was strictly a consumer internet phenomenon. No other enterprise vendor believed the platform mattered. We did, of course, and we set out to make that true.� We�ve engaged closely with the open source community, worked hard to advance the state of the art in the platform and crafted a business strategy that allows us to grow quickly and to build a great company for the long term. We are all tremendously proud of the progress that Cloudera has made. The market is absolutely exploding around us. The entrance over the past couple of years of venture-backed and big-vendor products and companies is no surprise. The opportunity is absolutely enormous. Over the years, I have been most proud of the people we�ve hired. In every single department and at every level of the company, we aim to bring on the very best and to give them room to do great things. That strategy has always applied to our executive leadership team. Last year we added Alan Saldich from Riverbed to head up Marketing, Peter Cooper-Ellis from VMware to run Engineering and Jim Frankola out of IBM, Ariba and Yodlee, as CFO. Our goal � my goal � has been to hire world-class people at the right time so that we could take a disproportionate share of the enormous opportunity. We�re building a company that we expect to stay independent and to lead the market for many years to come. That will demand relentless focus on operational excellence and on the technology and solutions that our customers need. The combination is, at our present scale, two jobs, not one. Tom led Arcsight through its IPO to industry leadership in mission-critical, bet-the-business security. He knows the enterprise. He�s a been-there, done-that executive. He�s run very successful enterprise software companies at global scale. Tom�s appointment as CEO allows me to concentrate my time with key constituents: Our product and engineering teams, our partners, our sales force, our customer-facing technical teams and � most importantly � our customers themselves. He and I have spent considerable time together over the last several months, building trust in both directions and planning for a smooth transition. Cloudera is a better company, today and for the long term, with the two of us working together. Of course, it�s not just the two of us. We have the best and brightest talent in the business. We�ll continue to deepen the bench and to expand our reach and capacity across the entire market in the months and the years to come, just as we have since June of 2008.�</snippet></document><document id="188"><title>Make Hadoop Your Best Business Tool</title><url>http://blog.cloudera.com/blog/2013/06/make-hadoop-your-best-business-tool/</url><snippet>Data analysts and business intelligence specialists have been at the heart of new trends driving business growth over the past decade, including log file and social media analytics.�However, Big Data heretofore has been beyond the reach of analysts because traditional tools like relational databases don�t scale, and scalable systems like Apache Hadoop have historically required Java expertise.� Today, the rise of new ecosystem tools is rapidly broadening the community using Hadoop and Big Data. Projects like Cloudera Impala and Apache Hive and Apache Pig have for the first time made Big Data accessible to those with traditional analytics backgrounds. With the launch of Data Analyst Training, Cloudera is helping the world�s analysts prove there�s nothing traditional about data analytics and BI on Hadoop. The Democratization of Big Data Hadoop in production can disrupt the data center for the better, but on its own can�t necessarily transform the data into useful business intelligence. As enterprises advance in their Hadoop use cases, they�re placing more emphasis on the analyst role to deliver insights that target core business goals like market segmentation, profit maximization, and customer loyalty. As MapReduce code becomes less essential, even for highly complex Hadoop queries, BI and analysis on Hadoop inspire a whole new set of questions about Big Data�s potential value to the enterprise: Q: What if you could run exploratory queries on all your data before committing to specific reports? A: You could interactively refine or expand your queries and rapidly drive towards truly valuable insights (as opposed to only asking challenging questions in emergency situations or as time permits). Invariably, making the valuable routine leads to competitive advantage. Q: What if you could run analyses on a much larger portion of your total data? A: You could avoid sampling error and conduct analyses on the right data given a specific question, whether querying the newest unstructured data or old data that would otherwise be kept in archive. Q: What if you could analyze the data where they live rather than choosing between silos? A: You could eliminate the cost of moving data from one specialized environment to another, increase the timeliness of SLAs, and enable new analyses that were previously impossible due to the limited access to particular data sets. Q: What if you could run familiar business tools on top of petabyte-scale data in real time? A: You could more closely and regularly track the data that are meaningful to your organization and identify new profit-maximizing business opportunities, essentially monetizing data access. Skills to Pay the Bills Cloudera�s new Data Analyst training focuses on the technical skills analysts need to deliver value from Big Data in Hadoop using their existing expertise. It is the only course available in the market that supports the end-to-end fundamentals of managing and manipulating Big Data with Hadoop using SQL and familiar scripting languages, even without any prior knowledge of Hadoop or Java. Hands-on labs guide participants through real-world scenarios that assure mastery of Impala, Hive, and Pig.�Most importantly, the training equips participants to use the right tools for the job at hand, whether getting data into and out of systems, performing joins and transformations, systematizing complex queries for reporting, or interacting with massive data sets in real time. Attend the Webinar and Get Trained We�re hosting a webinar on Cloudera Data Analyst Training on Thursday, July 18, at 10am PT/1pm ET.� You�ll hear more about the course�s objectives, outline, prerequisites, and technical benefits, including a portion of the full training, plus Q&amp;A with the lead curriculum developer. Register now! You can also enroll in Cloudera Data Analyst Training by visiting Cloudera University.�Public classes start in July and are currently scheduled in Washington, D.C., Redwood City, Columbia, Herndon, Austin, Boston, Seattle, Atlanta, London, and New York. Private training for your team is also available at your location and according to your schedule, so contact us for more information.</snippet></document><document id="189"><title>Improvements in the Hadoop YARN Fair Scheduler</title><url>http://blog.cloudera.com/blog/2013/06/improvements-in-the-hadoop-yarn-fair-scheduler/</url><snippet>Starting in CDH 4.2, YARN/MapReduce 2 (MR2) includes an even more powerful Fair Scheduler.�In addition to doing nearly all that it could do in MapReduce 1 (MR1), the YARN Fair Scheduler can schedule non-MapReduce jobs, schedule based on fine-grained memory instead of slots, and support hierarchical queues. In this post, you’ll learn what the Fair Scheduler�s role is and how it fulfills it, what it means to be a YARN “scheduler,” and dive into its new features and how to get them running on your cluster. YARN/MR2 vs. MR1 YARN uses an updated terminology to reflect that it no longer just manages resources for MapReduce.�From YARN�s perspective, a MapReduce job is an application.�YARN schedules containers for map and reduce tasks to live in. What was referred to as pools in the MR1 Fair Scheduler has been updated to queue for consistency with the capacity scheduler.�An excellent and deeper explanation is available here. How Does it Work? How a Hadoop scheduler functions can often be confusing, so we�ll start with a short overview of what the Fair Scheduler does and how it works.� A Hadoop scheduler is responsible for deciding which tasks get to run where and when to run them.�The Fair Scheduler, originally developed at Facebook, seeks to promote fairness between schedulable entities by awarding free space to those that are the most underserved.�(Cloudera recommends the Fair Scheduler for its wide set of features and ease of use, and Cloudera Manager sets it as the default.�More than 95% of Cloudera�s customers use it.) More than 95% of all Cloudera customers use the Fair Scheduler in their deployments. In Hadoop, the scheduler is a pluggable piece of code that lives inside ResourceManager (the JobTracker, in MR1) the central execution managing service.�The ResourceManager is constantly receiving updates from the NodeManagers that sit on each node in the cluster, that say �What�s up, here are all the tasks I was running that just completed, do you have any work for me?� The ResourceManager passes these updates to the scheduler, and the scheduler then decides what new tasks, if any, to assign to that node. How does the scheduler decide? For the Fair Scheduler, it�s simple: every application belongs to a �queue�, and we give a container to the queue that has the fewest resources allocated to it right now. Within that queue, we offer it to the application that has the fewest resources allocated to it right now. The Fair Scheduler supports a number of features that modify this a little, like weights on queues, minimum shares, maximum shares, and FIFO policy within queues, but the basic idea remains the same. Beyond MapReduce In MR1, the Fair Scheduler was purely a MapReduce scheduler.�If you wanted to run multiple parallel computation frameworks on the same cluster, you would have to statically partition resources — or cross your fingers and hope that the resources given to a MapReduce job wouldn�t also be given to something else by that framework�s scheduler, causing OSes to thrash.�With YARN, the same scheduler can manage resources for different applications on the same cluster, which should allow for more multi-tenancy and a richer, more diverse Hadoop ecosystem. Scheduling Resources, Not Slots A big change in the YARN Fair Scheduler is how it defines a �resource�.�In MR1, the basic unit of scheduling was the �slot�, an abstraction of a space for a task on a machine in the cluster.�Because YARN expects to schedule jobs with heterogeneous task resource requests, it instead allows containers to request variable amounts of memory and schedules based on those.�Cluster resources no longer need to be partitioned into map and reduce slots, meaning that a large job can use all the resources in the cluster in its map phase and then do so again in its reduce phase.�This allows for better utilization of the cluster, better treatment of tasks with high resource requests, and more portability of jobs between clusters — a developer no longer needs to worry about a slot meaning different things on different clusters; rather, they can request concrete resources to satisfy their jobs� needs.�Additionally, work is being done (YARN-326) that will allow the Fair Scheduler to schedule based on CPU requirements and availability as well. An implementation detail of this change that prevents applications from starving under this new flexibility is the notion of reserved containers.�Imagine two jobs are running that each have enough tasks to saturate more than the entire cluster.�One job wants each of its mappers to get 1GB, and another job wants its mappers to get 2GB.�Suppose the first job starts and fills up the entire cluster.�Whenever one of its task finishes, it will leave open a 1GB slot.�Even though the second job deserves the space, a naive policy will give it to the first one because it�s the only job with tasks that fit. This could cause the second job to be starved indefinitely.� One big change in the YARN Fair Scheduler is how it defines a �resource�. To prevent this unfortunate situation, when space on a node is offered to an application, if the application cannot immediately use it, it reserves it, and no other application can be allocated a container on that node until the reservation is fulfilled.�Each node may have only one reserved container.�The total reserved memory amount is reported in the ResourceManager UI.�A high number means that it may take longer for new jobs to get space. Hierarchical Queues Perhaps the most significant addition to the Fair Scheduler in YARN is support for hierarchical queues.�Queues may now be nested inside other queues, each queue to split the resources allotted to it among subqueues.� Queues are most often used to delineate organizational boundaries, and this now allows a topology that can better reflect organizational hierarchies.�We can say that the Engineering and Marketing departments both get half the cluster and then each may split its half among sub-organizations or functions.�Another common use of queues is to divide jobs by their characteristics — one queue might house long running jobs with high resource requirements, while another carves out a fast lane for time-sensitive queries.�Now, we can put fast/slow lanes under every team, allowing us to concurrently account for inter-organizational fairness and intra-organizational performance requirements. From a technical standpoint, queue hierarchies define the procedure for assigning tasks to resources when they become available.�All queues descend from a queue named “root”.�When resources become available they are assigned to child queues of the root queue according to the typical fair scheduling policy. Then, these children distribute the resources assigned to them to their children with the same policy. When calculating current allocations for fairness, all the applications in all subqueues of a queue are considered.�Applications may only be scheduled on leaf queues. Configuring hierarchical queues is simple: Queues can be specified as children of other queues by placing them as sub-elements of their parents in the Fair Scheduler allocations file.� The following is an example allocations file (fair-scheduler.xml) that splits resources first between the high-level divisions, and second, between their teams: &lt;allocations&gt;

� &lt;queue name="Marketing"&gt;
��� &lt;minShare&gt;8192&lt;/minShare&gt;��� 

��� &lt;queue name="WebsiteLogsETL"&gt;
����� &lt;weight&gt;1.0&lt;/weight&gt;
��� &lt;/queue&gt;
���
��� &lt;queue name="CustomerDataAnalysis"&gt;
����� &lt;weight&gt;2.0&lt;/weight&gt;
����� &lt;queue name=�FastLane�&gt;
������� &lt;weight&gt;3.0&lt;/weight&gt;
������� &lt;maxShare&gt;4096&lt;/maxShare&gt;
����� &lt;/queue&gt;
����� &lt;queue name=�Regular�&gt;
����� &lt;/queue&gt;
��� &lt;/queue&gt;
���
� &lt;/queue&gt;

� &lt;queue name="Engineering"&gt;�
��� &lt;queue name="ImportantProduct"&gt;
����� &lt;weight&gt;2.0&lt;/weight&gt;
��� &lt;/queue&gt;

��� &lt;queue name="UnimportantProduct"&gt;
����� &lt;weight&gt;1.0&lt;/weight&gt;
��� &lt;/queue&gt;

� &lt;/queue&gt;

&lt;/allocations&gt;
   We�ve given Marketing a minimum share of 8192MB, meaning that tasks from Marketing jobs will get first priority until Marketing is using at least 8192MB.�We�ve given CustomerDataAnalysis team a fast lane that will get 3MB for every 1MB that its regular queue gets but can�t fit more than 4096MB.�A queue�s name is now prefixed with the names of its parent queues, so the CustomerDataAnalysis team�s fast lane�s full name would be root.Marketing.CustomerDataAnalysis.FastLane. For easier use, we can omit the root queue prefix when referring to a queue, so we could submit an application to it with� -Dmapreduce.queue.name=Marketing.CustomerDataAnalysis.FastLane. What�s Gone in the YARN Fair Scheduler? The YARN Fair Scheduler no longer supports moving applications between queues.�Applications also may no longer be given priorities within a leaf queue. Both of these features will likely be added back in the future. Coming Soon The improvements described here are just a start;�a number of additional features are planned for the Fair Scheduler.�With multi-resource scheduling, fairness will be determined with respect to CPU usage as well as memory usage, and each queue in the hierarchy will be able to have a custom scheduling policy.�Gang scheduling will allow YARN applications to request �gangs� of containers that will all be made available at the same time.�Guaranteed shares will allow queues to have resources sectioned off for only their use – unlike minimum shares, other queues will not be able to occupy them, even if the queue is not using them.� Look forward to another post describing these features when they arrive! Sandy Ryza is a Software Engineer on the Platform team and a contributor to Hadoop/MapReduce/YARN.</snippet></document><document id="190"><title>The HBaseCon 2013 Afterglow</title><url>http://blog.cloudera.com/blog/2013/06/the-hbasecon-2013-afterglow/</url><snippet>HBaseCon 2013 is in the books. Thanks to all our speakers, sponsors, and attendees! A great time was had by all. For those of you who missed the show, session video and presentation slides (as well as photos) will be available via hbasecon.com in a few weeks. (To be notified, follow @cloudera or @ClouderaEng.) Although it’s not quite as good as being there with the rest of the community, you’ll still be able to partake from the real-world experiences of Apache HBase users like Facebook, Box, Yahoo!, Salesforce.com, Pinterest, Twitter, Groupon, and more. While you’re waiting for that, allow me to bring you just this single photo to capture the HBaseCon experience: (And yes, we’re already thinking about bigger track rooms next year!) Finally, the HBaseCon team puts huge value on feedback from attendees – because the show would be desolate without the community. There are certainly things to improve on, but all in all, we feel that HBaseCon-ers walked away from the show with a positive impression: Thanks for the kudos! We’ll see you next time!</snippet></document><document id="191"><title>Customer Spotlight: It’s HBase Week!</title><url>http://blog.cloudera.com/blog/2013/06/customer-spotlight-its-hbase-week/</url><snippet>This is the week of Apache HBase, with HBaseCon 2013 taking place Thursday, followed by WibiData�s KijiCon on Friday. In the many conversations I�ve had with Cloudera customers�over the past 18 months, I�ve noticed a trend: Those that run HBase stand out. They tend to represent a group of very sophisticated Hadoop users that are accomplishing impressive things with Big Data. They deploy HBase because they require random, real-time read/write access to the data in Hadoop. Hadoop is a core component of their data management infrastructures, and these users rely on the latest and greatest components of the Hadoop stack to satisfy their mission-critical data needs. Today I�d like to shine a spotlight on one innovative company that is putting top engineering talent (and HBase) to work, helping to save the planet — literally. That company is Opower. Opower partners with 80+ utilities providers to offer an integratedcustomer engagement platform using a software-as-a-service (SaaS) model. Its goal: to help people save energy and reduce utilities bills by applying intensive, Big Data analytics to deliver informative dashboards, alerts, incentives, similar household comparisons, and other communications to customers across communication channels and via in-home devices. Opower combines utility data — such as that from smart meters — with weather information, geographic details, demographic data and more, over decades of history, so it can offer valuable insights. (Hint: this is where the value of Hadoop and HBase come in.) For example, Opower identifies similar households to each customer so it can tell you whether you�re using more or less energy than your peers. Interestingly enough, an Opower experiment revealed that this kind of social pressure to save energy has a much higher impact than incentives to save money, to be a good citizen, or even to save the planet. Opower is demonstrating tangible results, with more than 2.5 terawatts saved as a result of its efforts. Opower is demonstrating tangible results, measuring more than 2.5 terawatts saved as a result of its efforts — that is enough energy savings to power every household in Salt Lake City and St. Louis for more than a year! I actually spent the day today interviewing five different Opower employees so we can work together on a written and video case study — so stay tuned; there�s more where this came from. It was a fascinating day to say the least. In the meantime, if you�re interested in hearing how other companies — including Ancestry.com, Experian, Facebook, Pinterest, Salesforce.com, and Twitter — are applying HBase to solve Big Data problems, you should come to HBaseCon this Thursday in San Francisco, hosted by Cloudera. If you�re already using HBase and want to learn how to build personalized applications on top of the popular Hadoop data store, consider checking out KijiCon on Friday, hosted by WibiData. Opower and Cloudera are both sponsoring this HBase-centric event. It�s not too late to register for either HBaseCon or KijiCon. And for more information on Opower�s use of Hadoop and HBase, here are some resources to check out: Opower�s President &amp; Founder Alex Laskey at TED2013 Hadoop World 2012 presentation — Data Science with Hadoop at Opower On-demand webinar — Converting Your Smart Grid Data into Real Customer Value Karina Babcock is Cloudera�s Customer Programs &amp; Marketing Manager.</snippet></document><document id="192"><title>HBaseCon 2013: "Case Studies" Track Preview</title><url>http://blog.cloudera.com/blog/2013/06/hbasecon-2013-case-studies-track-preview/</url><snippet>HBaseCon 2013 is this Thursday (June 13 in San Francisco), and we can hardly wait! To complete the “preview” cycle, today we bring you a snapshot of the Case Studies track, which offers a cross-section of the many real-world use cases for Apache HBase. You will learn about how a range of companies across diverse industries use it at the heart of their IT infrastructure to run their business. Multi-tenant Apache HBase at Yahoo! Sumeet Singh &amp; Francis Liu, Yahoo! The introduction of multi-tenancy has lowered the barriers for all Hadoop users to use HBase. Here you will learn the traditional use cases for HBase at Yahoo!, and new use cases as a result in content management, advertising, log processing, analytics and reporting, recommendation graphs, and dimension data stores.� Near Real Time Indexing for eBay Search Swati Agarwal &amp; Raj Tanneru, eBay Here the presenters will talk about eBay�s new search indexing platform, and in particular, the HBase-based near real-time indexing capability. Apache HBase at Pinterest: Scaling our Feed Storage� Varun Sharma, Pinterest In this talk, learn about Pinterest’s experience architecting and scaling its Feed storage on HBase. Deal Personalization Engine with HBase @ Groupon Ameya Kantikar, Groupon At Groupon, HBase now powers most of the backend technology for real-time delivery of �deal� experience across all platforms, as well as powers batch clusters for consolidated user data. Being Smarter Than the Smart Meter� Jay Talreja, Oracle DataRaker, now part of the Oracle Utility Software Suite, was architected on HBase to scale to the largest smart meter deployments in the world. This session describes how HBase has provided the raw compute capacity to solve complex data analytics problems previously unavailable using traditional storage platforms. Apache Hadoop and Apache HBase for Real-Time Video Analytics� Suman Srinivasan, LongTail Video In this talk, the presenters discuss how LongTail Video uses Hadoop for real-time analytics by processing data in frequent batches, its experience with HBase for ingesting millions of aggregate data points and providing real-time results, a brief overview of its HBase schema, and using HBase Thrift and Python in production. ETL for Apache HBase Manoj Khanwalkar &amp; Govind Asawa, Experian Experian�s ETL framework can source data from various systems, transform it, and persist in HBase. The framework also provides the ability to populate star schema-like structures in HBase by stamping dimensions data to the fact table and the ability to populate multiple aggregate tables in HBase and /or RDBMS in real time or batch mode. Rebuilding for Scale on Apache HBase Robert Roland, Simple Measured This talk will cover why Simple Measured moved to HBase from MongoDB, how it integrated HBase with the least amount of downtime and impact to its customers, the financial costs of this migration, and where it’s going in the future. Evolving a First-Generation Apache HBase Deployment to Second Generation and Beyond Doug Meil, Explorys Explorys has been using HBase and Hadoop since HBase 0.20, and here will walk through lessons learned over years of usage from its first HBase implementation through a series of upgrades and changes, including impacts to schema design, data loading, data indexing, data access and analytics, and operational processes. Mixing Low Latency with Analytical Workloads for Customer Experience Management Neil Ferguson, Causata Causata recently migrated to HBase from its own custom-built data store, and here you will learn the challenges it overcame getting HBase to build customer profiles from many millions of unaggregated data points per second, per server, from many TBs of data. Realtime User Segmentation using Apache HBase: Architectural Case Study Murtaza Doctor &amp; Giang Nguyen, RichRelevance Behavioral targeting, specifically user segmentation and building personas, is critical for RichRelevance in generating triggers when a user is added to a segment or switches from a segment. In this presentation, you’ll learn not only how the events are captured, but also how they are stored in HBase in real time. Apache HBase, Apache Hadoop, DNA and YOU! Jeremy Pollack, Ancestry.com Find out how Ancestry DNA used Hadoop and HBase to implement a scalable cleanroom implementation of the GERMLINE algorithm, resulting in a 1700% performance improvement. If these previews have not convinced you that missing HBaseCon is unthinkable for HBase users/enthusiasts, nothing will! See you on Thursday!</snippet></document><document id="193"><title>What�s Next for HBase? Big Data Applications Using Frameworks Like Kiji</title><url>http://blog.cloudera.com/blog/2013/06/whats-next-for-hbase-big-data-applications-using-frameworks-like-kiji/</url><snippet>Michael Stack is the chair of the Apache HBase PMC and has been a committer and project “caretaker” since 2007. Stack is a Software Engineer at�Cloudera. Apache Hadoop and HBase have quickly become industry standards for storage and analysis of Big Data in the enterprise, yet as adoption spreads, new challenges and opportunities have emerged.�Today, there is a large gap — a chasm, a gorge — between the nice application model your Big Data Application builder designed and the raw, byte-based APIs provided by HBase and Hadoop. Many Big Data players have invested a lot of time and energy in bridging this gap. Cloudera, where I work, is developing the Cloudera Development Kit (CDK). Kiji, an open source framework for building Big Data Applications, is another such thriving option. A lot of thought has gone into its design.�More importantly, long experience building Big Data Applications on top of Hadoop and HBase has been baked into how it all works. Kiji provides a model and set of libraries that help you get up and running quickly. Kiji provides a model and a set of libraries that allow developers to get up and running quickly.�Intuitive Java APIs and Kiji�s rich data model allow developers to build business logic and machine learning algorithms without having to worry about bytes, serialization, schema evolution, and lower-level aspects of the system. The Kiji framework is modularized into separate components to support a wide range of usage and encourage clean separation of functionality. Kiji’s main components include KijiSchema, KijiMR, KijiHive, KijiExpress, KijiREST, and KijiScoring. KijiSchema, for example, helps team members collaborate on long-lived Big Data management projects, and does away with common incompatibility issues, and helps developers build more integrated systems across the board. All of these components are available in a single download called a BentoBox. Historically, Hadoop and HBase have been considered difficult platforms to develop for. At Cloudera, we have made Hadoop deployment seamless, allowing enterprises access to their Big Data. Going the next step, building applications that can make use of all this data, has usually been a lonely and trying endeavor with developers having to build it all themselves from scratch.�This is where projects like Kiji can help. I look forward to the day here building Big Data applications is “easy,” when developers do not have to concern themselves with serializations, schema evolution, nor ensuring that their design aligns with the underlying store because this is all handled for them in rich layers that sit atop raw Hadoop and HBase APIs.�Kiji is a welcome step toward such a future. If you are interested in learning more about Kiji, come to KijiCon�(on June 14, the day after HBaseCon), sponsored by Cloudera and Opower. Attend a half-day Kiji training workshop OR participate in a meetup-style open forum to discuss potential use cases, the corresponding Kiji features, and some hands-on Kiji hacking.�There will also be beer, food, and some killer Kiji stories. Register for KijiCon here: http://kijicon.eventbrite.com</snippet></document><document id="194"><title>Configuring Impala and MapReduce for Multi-tenant Performance</title><url>http://blog.cloudera.com/blog/2013/06/configuring-impala-and-mapreduce-for-multi-tenant-performance/</url><snippet>Cloudera Impala�has many exciting features, but one of the most impressive is the ability to analyze data in multiple formats, with no ETL needed, in HDFS and Apache HBase. Furthermore, you can use multiple frameworks, such as MapReduce and Impala, to analyze that same data. Consequently, Impala will often run side-by-side with MapReduce on the same physical hardware, with both supporting business-critical workloads. For such multi-tenant clusters, Impala and MapReduce both need to perform well despite potentially conflicting demands for cluster resources. In this post, we�ll share our experiences configuring Impala and MapReduce for optimal multi-tenant performance. Our goal is to help users understand how to tune their multi-tenant clusters to meet production service level objectives (SLOs), and to contribute to the community some test methods and performance models that can be helpful beyond Cloudera. Defining Realistic Test Scenarios Cloudera�s broad and diverse customer base makes it a top concern to do testing for real-world scenarios. Realistic tests based on common use cases offer meaningful guidance, whereas guidance based on contrived, unrealistic testing often fails to translate to real-life deployments. For Impala, our primary test workload directly replicates the queries, schema, and data selectivity of Impala customers. This approach allows us to test query and schema structure common to several classes of use cases. (We often collaborate with customers to obtain similar realistic test scenarios for all CDH components, as well.) Customers benefit by getting direct performance guidance for their specific use cases. Guidance based on contrived, unrealistic testing often fails to translate to real-life deployments. Because online analytical processing (OLAP) is one of many possible Impala use cases, we also run TPC-H, an established benchmark for OLAP, as a secondary workload for Impala. Although TPC-H provides a large, public repository of independently audited results, those results are based on a 1990s survey of OLAP users — so we run a subset of TPC-H queries that represent prominent use cases today. For MapReduce, we use a collection of benchmarks designed to use different data formats and stress different stages of the MapReduce compute pipeline. Some examples include the TeraSort Suite (includes TeraGen, TeraSort, and TeraValidate), data shuffling jobs, and jobs that process varying amounts of randomness in the data. These tests are intentionally different from other MapReduce tests that use open source tools like SWIM to directly replay customer workloads of thousands of MapReduce jobs. (Such workloads capture complexity and diversity and will be added in the near future.) In contrast, for our initial multi-tenant tests, we emphasize precisely controlled tests that create easily repeatable, well-defined loads.� The multi-tenant tests include the full combination of our Impala queries, run concurrently with our MapReduce benchmarks. We iterate through the combination — each time a different Impala query with a different MapReduce job. Multiply this by the range of data sizes, data formats, Impala query concurrency levels, and MapReduce job concurrency levels, and the result is a test matrix of hundreds of settings. This broad test matrix builds confidence that our findings are meaningful and that Impala can share cluster resources with MapReduce as expected. Modeling Multi-tenant Performance Expectations Multi-tenant resource management involves assigning different resources to workloads from different computation frameworks. It is intractable to model that for the full matrix of hundreds of tests. Instead, for simplicity, we constrict resource assignments to statements like this: �When cluster resources are under contention, Impala gets fraction x of all resources, and MapReduce gets the rest.� Our primary test workload directly replicates the queries, schema, and data selectivity of Impala customers. If Impala and MapReduce share resources well, having a fraction of all resources should slow down performance proportionally when the resources in question are under contention. In other words, Impala multi-tenant performance should drop to no less than fraction x of its stand-alone performance, and MapReduce multi-tenant performance should drop to no less than fraction 1-x of its stand-alone performance. Our goal is to find and validate a set of resource management controls such that the observed performance meets or exceeds these expectations. This is the most conceptually simple model and more elaborate ones are possible. For example, if a MapReduce job runs longer than an Impala query, multi-tenant slowdown would cover only the period that they are both running. The Impala query would see fraction x of stand-alone performance as in the previous model. The MapReduce job would see fraction 1-x of stand-alone performance for the initial part of the job, and stand-alone performance for the rest. Another extension of the model would be to cover non-uniform resource assignments. For example, most Impala queries have heavy memory demands and most MapReduce jobs have heavy disk IO demands. If we assign Impala 50% of CPU, 60% of memory, and 40% of disk, all the resource changes could simultaneously affect performance and we expect queries in general to slow by some range within 40-60%. This more complex style of resource allocations along multiple dimensions is supported in CDH but not evaluated here. Controlling Shared Resources The full set of multi-tenant resource management controls are documented in �Setting up a Multi-tenant Cluster for Impala and MapReduce� within the Cloudera Manager installation guide. They include controls to manage memory, CPU, and disk IO for each active compute framework. The documentation there contains numerical examples and step-by-step guides on how to configure each of the following resources. Memory. The controls include Impala Daemon Memory Limit — a direct setting for Impala memory consumption, and Maximum Number of Simultaneous Map Tasks and Maximum Number of Simultaneous Reduce Tasks– indirect memory controls for MapReduce. For memory management, our primary concern is to prevent memory thrashing, job/query failures, or one of the frameworks taking all the resources and preventing workloads in other frameworks from making progress. To give Impala a fraction x of all memory, we set Impala Daemon Memory Limit to (RAM size / accounting factor) * x. The �accounting factor� is a numerical constant to adjust for OS overhead in translating resident set size to actual RAM use. We�ve found that 1.5 is a relatively safe value, while 1.3 is a more realistic but also more risky value. For MapReduce, we cut to fraction 1-x the default value for Maximum Number of Simultaneous Map and Reduce Tasks. For example, suppose we give Impala 25% of cluster resources; i.e., x = 25%, we would cut Maximum Number of Simultaneous Map and Reduce Tasks to 75% their default values. If the default values are maximum of 16 simultaneous map tasks and 8 simultaneous reduce tasks, we would set the new values to maximum of 12 simultaneous map tasks and 6 simultaneous reduce tasks. This effectively gives MapReduce the remaining fraction 1-x of the cluster�s memory, as each task carries its own JVM container that holds the data and code needed for the map() and reduce() functions. With this change, any per-job tuning related to slot capacity should be adjusted also. (Note that slot is the smallest unit of adjustment in MR1. For CDH5 and YARN in the future, we would specify instead the quantity of RAM needed. YARN would hand out resources to ApplicationMasters, one of which happens to run MapReduce jobs.) CPU. We control CPU use through Linux Control Groups (Cgroups). Cgroups is a Linux kernel feature that users can configure via Cloudera Manager 4.5.x and onward. The more Cgroup CPU Sharesgiven to a role, the larger its share of the CPU when under contention. For example, a process with 4 CPU shares will be given roughly twice as much CPU time as a process with 2 CPU shares. We use these controls as “soft limits” –� i.e., the settings will have no effect until processes on the host (including both roles managed by Cloudera Manager and other system processes) are contending for all the CPUs. In a multi-tenant workload, all Impala and MapReduce processes – Impala daemons, DataNodes, and TaskTrackers – may be simultaneously consuming CPU. To assign fraction x of CPU to Impala and the rest to MapReduce, we set fraction x of CPU shares to the Impala daemon, and half of the remainder each to the DataNode and the TaskTracker. This is a conservative setting for MapReduce. It assumes that DataNode and task computational activity can simultaneously maximize CPU activity. If your MapReduce workload is far below cluster capacity, this is unlikely to be true. For example, when you have one MapReduce job, usually DataNode (job input/output) and TaskTracker (tasks) CPU activities will not overlap. It’s only when you have a very large workload that in aggregate, DataNode and TaskTracker activity will be simultaneous. Disk.�Disk IO is also controlled by Cgroups. The more the Cgroup I/O Weightgiven to a role, the higher priority will be given to I/O requests made by the role when I/O is under contention. The effect is nearly identical to that of the CPU controls. We also choose a conservative setting to assign disk resources: The Impala daemon gets fraction x of IO weights; half of the remainder goes to the DataNode, and the other half goes to the TaskTracker. For future tests, we will explore how to control Impala and MapReduce workload intensity to create sustained conflicts in resource demands. Multi-tenant Performance Results Our tests have these goals: Measure uncontrolled multi-tenant behavior Measure multi-tenant behavior using controls described so far Understand resource contention in both set of tests above Understand stand-alone behavior subject to resource management controls Our test cluster machines have 64GB memory, 12 cores of 2.0GHz Intel Xeon, 12x 2TB disks, and 10Gbps Ethernet. The baseline behavior is MapReduce running by itself, and Impala running by itself, on the cluster without any resource management controls. We express multi-tenant performance as a fraction of this uncontrolled, stand-alone performance. This is a good baseline because the uncontrolled, stand-alone setup is equivalent to dedicated MapReduce or Impala clusters. Uncontrolled Multi-tenant Behavior We measured multi-tenant performance without the resource management controls described previously. For our tests, MapReduce consumed the majority of cluster resources, which heavily affected Impala performance. Specifically, uncontrolled multi-tenant MapReduce dropped to a median of ~90% its uncontrolled stand-alone performance and uncontrolled multi-tenant Impala dropped to a median of ~50% of uncontrolled stand-alone performance. These numbers reflect a higher than expected uncontrolled multi-tenant performance. The suspected reason is that Impala and MapReduce resource demands are not always in conflict. For our future tests, we will explore how to control Impala and MapReduce workload intensity to create sustained conflicts in resource demands. Our resource management mechanisms improve on this uncontrolled behavior in several ways. First, the cluster resources are less likely to become overcommitted. Specifically, the controlled behavior setup is more �memory safe� such that memory-heavy workloads have a lower risk of causing swapping/thrashing. Second, the user gains the ability to dial-up or dial-down the resources consumed by each framework. For example, if the uncontrolled setup results in Impala performance that is below production SLOs, the user can turn on resource controls and shift resources from MapReduce to Impala so that both can meet desired SLOs. Multi-tenant Behavior under Resource Management The resource management controls allow us to assign fraction x of all cluster resources to, say, Impala, and the rest to MapReduce. We measured performance for 25-75, 40-60, 50-50, 60-40, and 75-25 resource splits between Impala and MapReduce. The test results demonstrate that we can indeed control multi-tenant performance by assigning more resources to a compute framework. The graphs below show Impala and MapReduce multi-tenant performance. Each data point on the graph shows the median slowdown for that setting while the error bars show the 25-75 percentile slowdown across the test matrix. They also show the expected performance according to the previous models. The graphs indicate that Impala and MapReduce multi-tenant performance both meet or exceed predicted performance according to the previous models. There are some Impala query and MapReduce job combinations where performance is barely affected in a multi-tenant environment (error bars close to fraction 1 of stand-alone performance). There are also some combinations where performance is affected beyond that predicted by earlier models (error bars below the �Modeled� line). Most combinations meet or exceed our expectations. As before, the suspected reason is that Impala and MapReduce resource demands are not always in conflict. Verifying Resource Contention We used Cloudera Manager to monitor physical resource utilization during both the uncontrolled and controlled multi-tenant tests. Memory is almost fully used all the time. CPU and disk loads are bursty and contention occurs only some of the time. This behavior likely explains our test results where Impala and MapReduce both exceed the performance model. Stand-alone Behavior with Resource Management Multi-tenant resource management should behave as soft limits. In other words, when there is only one framework active, it should behave as if it has the entire cluster to itself. The CPU and disk controls above are soft limits by design while the memory controls are hard limits. Hence, we felt it necessary to measure stand-alone performance subject to resource management controls. Note this is different from the previous baseline of stand-alone performance without resource management controls. Impala behaves exactly as if it had the entire cluster to itself. This is the intended behavior for the Impala daemon memory limit. Either the query behaves unaffected, or, upon hitting the memory limit, the query would be aborted. For MapReduce, the behavior is more complex. Cutting the available slots places an upper bound on the amount of parallelism. For modest cuts, there would still be enough parallelism to drive resources to full utilization, and the performance impact would be small. For severe cuts, the remaining slots would not be enough to drive resources to full utilization, and the performance impact would be large. What are �moderate� and �severe� cuts depend on the cluster hardware. For our test cluster, cutting the slots to 50% results in only 15% performance drop. However, cutting the slots to 25% gives stand-alone performance that is roughly the same as multi-tenant performance, meaning that the available slots, and not resource contention, becomes the performance constraint for MapReduce. Hence, depending on cluster hardware, there may be room to relax the settings for resource management controls as we described earlier. What�s Next? Big data multi-tenant performance represents a cutting edge engineering problem with immediate impact for many users. Our test methods, models, and results carry a high level of real-world relevance, mathematical rigor, and empirical repeatability. One immediate follow up effort is to expand our test scenarios to cover multi-query Impala workloads running concurrently with multi-job MapReduce workloads, each with a controlled level of workload intensity. Overall, we hope this post can help users tune their multi-tenant clusters to meet production SLOs, and help the community at large understand how to manage resources in a shared big data platform. Yanpei, Prashant, and Arun are members of the Performance Team at Cloudera.</snippet></document><document id="195"><title>Customer Spotlight: Embracing Big Data Innovations at The Cloudera Forum</title><url>http://blog.cloudera.com/blog/2013/06/customer-spotlight-embracing-big-data-innovations-at-the-cloudera-forum/</url><snippet>Earlier this week, we hosted The Cloudera Forum to reveal Cloudera�s �Unaccept the Status Quo� vision and to announce the public beta launch of Cloudera Search. The event featured a panel discussion between representatives from four companies that are embracing the latest big data innovations, moderated by our own CEO Mike Olson. Those are the companies I�d like to highlight in this week�s spotlight, for obvious reasons. The panelists were… (drumroll, please): Chris Poulin, managing partner at Patterns and Predictions. Patterns and Predictions is a predictive analytics company that is building a data fabric to identify risk factors for suicide among veterans as part of a government-funded Big Data observational study. The technology that Chris and his team are building has the potential to make a huge, positive impact on our society. Patterns and Predictions has participated in the Cloudera Search private beta program and plans to deploy Cloudera Impala next. The value that these technologies deliver to Patterns and Predictions: a more simplified Big Data environment where key functionality is embedded within the core Hadoop system. Julian Mann, co-founder and vice president of product management for Skybox Imaging. Like Patterns and Predictions, Skybox Imaging is a small company with a big mission: to change humanity�s understanding of earth. Skybox is building satellites that will be launched into space later this year to capture high-resolution images of the entire globe. It is also extracting information from those images and making that data available to organizations across all industries. Skybox Imaging is another early adopter of Cloudera Search — the product saved it from having to custom build something it knew would need to be incorporated into its product offering if it wants to make collected geopixel data consumable to a broad audience. David Green, director of data services at Trion Worlds. Trion Worlds is a large gaming company whose leading products include RIFT, End of Nations, and Defiance. Trion Worlds brought Hadoop into its existing IT infrastructure to provide a data abstraction layer that ties together the many tools used in-house. It has also been an early user of Cloudera Impala, which will improve the level of customer support it can deliver based on individualized, real-time user activity. Trion represents a strong use case that many organizations today can relate to: identifying the best way to integrate Hadoop with existing environments. Paul Sonderegger, big data strategist at Oracle — a strategic Cloudera partner, with past roles at Forrester Research and Endeca. Paul brought a broad and deep industry perspective to the conversation, and helped to identify the ways more traditional relational technologies such as Oracle data warehouses and Hadoop will continue complementing each other in the future. For the full recap of The Cloudera Forum, and the panel discussion specifically, visit http://cloudera.com/content/cloudera/en/campaign/unaccept-the-status-quo.html. The panel starts around the 58-minute mark. Karina Babcock is Cloudera�s Customer Programs &amp; Marketing Manager.</snippet></document><document id="196"><title>Meet the Engineer: Mark Miller</title><url>http://blog.cloudera.com/blog/2013/06/meet-the-engineer-mark-miller/</url><snippet>Given the recent news about Cloudera Search, we thought we’d devote this installment of “Meet the Engineer” to Apache Lucene/Apache Solr PMC member Mark Miller. What do you do at Cloudera (and in which Apache project(s) are you involved)? I’m a software engineer on the Search team. I’ve been involved in the Apache Lucene community since 2006 and Apache Solr since around 2009. I spend a lot of time adding features to Solr and fixing bugs, as well as working on improving Solr integration with the rest of the Hadoop ecosystem. I kind of think of myself as a “distributed search guy” at the moment. What do you like about your job? I really enjoy working on open source software, working at early stage companies, and building things. Cloudera has a great company culture, and is a great fit for those interests. It’s hard not to enjoy doing what you love doing. What is your favorite thing about Lucene/Solr? The community. Some of the most interesting and enjoyable people I know are part of the Lucene/Solr community. After that, I like the code. What is your advice for someone who is interested in participating in any open source project for the first time? Just start pitching in. Take a little time to watch how the community works, who the people with the most merit are, the range of acceptable parameters. Then emulate what you see. Learn the rules before you start breaking them. Pitch in on things that you can. Even if it’s small. Small regular contributions have a habit of becoming larger contributions. At what age did you become interested in programming, and why? I started programming at around age 8 when my Dad taught me a bit of BASIC. I fiddled with that off and on, but I did not really dive in until the Internet started to become more popular – around ’94-’95, when I was around 13 or 14. I’ve always liked to create things and programming represented a very large toolbox and cheap materials. I think it just kind of became my default “paintbrush”‘ due to its range and cost effectiveness.</snippet></document><document id="197"><title>CDH 4.3: Now Shipping with More Apache HBase Improvements</title><url>http://blog.cloudera.com/blog/2013/06/cdh-4-3-now-shipping-with-more-apache-hbase-improvements/</url><snippet>As you may know, Apache HBase has a vibrant community and gets a lot of contributions from developers worldwide. The collaborative development effort is so active, in fact, that a new point-release comes out about every six weeks (with the current stable branch being 0.94). At Cloudera, we’re committed to ensuring that CDH, our open source distribution of Apache Hadoop and related projects (including HBase), ships with the results of this steady progress. Thus, CDH 4.2 was rebased on 0.94.2, as compared to its predecessor CDH 4.1, which was based on 0.92.1. CDH 4.3 has moved one step further and is rebased on 0.94.6.1. Apart from the rebase, CDH 4.3 also has some important bug fixes backported from later versions of 0.94 and trunk. Following are some of the important features and improvements that now ship in CDH 4.2/CDH 4.3: New features: Snapshots/Metrics: As explained in “Introduction to Apache HBase Snapshots”, a user can take a snapshot of a table and restore its data/schema later. This sorely missed feature in HBase was added in CDH 4.2. CDH 4.3 has added some usability features such as snapshot metrics and in-progress task information of current commands to make it more user-friendly. (See for details: HBASE-7615, HBASE-7415.) HLog Compression/Replication compatibility: HBase Replication is now compatible with HLog compression. This means a cluster with WAL compression can replicate its data to another cluster with no WAL compression, and vice versa. (See for details: HBASE-5778.) Operability/performance improvements: NN HA Support: An improved RegionServer/Master side support for failing over to a standby NameNode is in CDH 4.3. It adds a retry logic around NameNode operations and thus avoids any RegionServer/Master abort. (See for details: HBASE-8211.) Lazy seek optimization: HBase now lazily reads StoreFiles while querying the data, optimizing on the number of disk seeks. This improves overall read throughput and is specifically useful for workloads that read the latest data (such as Increments). (See for details: HBASE-4465.) Atomic Put/Delete per row: In case of multiple types of operations on a row (Put on some columns and Delete on other), each of them takes a separate lock of its own. These combined mutations now take a single lock, improving the overall throughput. (See for details: HBASE-3584.) Conclusion As described above, CDH 4.3 introduces some important HBase features/bug fixes for better usability (and is compatible with its CDH 4.x predecessors). For more information, please refer to the HBase section in the CDH4 Installation Guide. Himanshu Vashishtha is a Software Engineer on the Platform team.</snippet></document><document id="198"><title>HBaseCon 2013: "Ecosystem" Track Preview</title><url>http://blog.cloudera.com/blog/2013/06/hbasecon-2013-ecosystem-track-preview/</url><snippet>Unbelievably,�HBaseCon 2013 is only one week away (June 13 in San Francisco)! Today we bring you a preview of the Ecosystems track, a grand tour (in 20-minute increments) of the fascinating current work being done across the community to extend or build on top of Apache HBase. Impala: Using SQL to Extract Value from Apache HBase Elliott Clark, Cloudera Cloudera Impala is an open source project that allows low latency and analytical queries over big data in Apache Hadoop; with Impala it is now possible to use SQL in conjunction with HBase. HBase SEP: Reliable Maintenance of Auxiliary Index Structures Steven Noels, NGDATA In this talk, we will present HBase SEP (Side-Effects Processor) and Indexer, two new open source projects that provide a reliable bridge between HBase and index systems but cater to the needs of anyone who wants to keep auxiliary data in lockstep sync with HBase updates. SQL Over HBase: A Case for Apache Hive� Enis S�ztutar &amp; Ashtutosh Chahaun, Hortonworks In this talk we will look at the current status of using Hive for querying your data stored in HBase. The talk will include a running example of a web table storing web crawl data in HBase, and Hive queries to that table for analysis. How (and Why) Phoenix Puts the SQL Back into NoSQL James Taylor, Salesforce.com Phoenix is an open source project from Salesforce.com that puts a SQL skin on top of HBase. This talk will focus on answering: 1) why put a SQL skin on top of HBase? and 2) how does Phoenix marry the SQL paradigm with NoSQL? Apache Drill: A Community-driven Initiative to Deliver ANSI SQL Capabilities for Apache HBase� Jacques Nadeau, MapR This session provides an overview of Apache Drill, which will deliver full ANSI SQL capability for HBase users. Honeycomb: MySQL Backed by Apache HBase� Dan Burkert, Near Infinity Honeycomb is an exciting new open source storage engine plugin for MySQL that enables MySQL to store and query tables directly in HBase. Using Coprocessors to Index Columns in an Elasticsearch Cluster Dibyendu Bhattacharya, HappiestMinds This presentation explores the design and challenges HappiestMinds faced while implementing a storage and search infrastructure for a large publisher where books/documents/artifacts related records are stored in Apache HBase. Full-Text Indexing for Apache HBase Maryann Xue, Intel Intel has extended HBase with a general full-text indexing framework based on Apache Lucene, which supports distributed search for any combination of words or phrases of interest in data stored within HBase. Streaming Data into Apache HBase using Apache Flume: Experience with High-Speed Writes� Hari Shreedharan, Cloudera In this talk, we discuss the lessons we learned while using the standard and async API, retrying puts and increments, and fine tuning batches to make sure we get optimum performance with minimal number of duplicates. Using Metrics to Monitor and Debug Apache HBase Elliott Clark, Cloudera In this session we will talk about the metrics exposed by HBase. We�ll cover what metrics are there, what they mean, and how to access them. High-Throughput, Transactional Stream Processing on Apache HBase Andreas Neumann &amp; Alex Baranau, Continuuity We have developed the Continuuity Data Fabric as a unified, transactional queuing and storage engine. This talk will discuss its implementation on top of HBase, evaluate performance, scalability and reliability, and share experiences, best practices, and lessons learned. Real-Time Model Scoring in Recommender Systems Jon Natkins &amp; Juliet Hougland, WibiData In this presentation, we�ll discuss how developers can use Apache HBase and Kiji to develop low-latency predictive models, using algorithms like clustering or collaborative filtering, and how to leverage those models in the context of a full application. Using Apache HBase for Large Matrices Gokhan Capan, Dilisim In this talk, we describe HBase-backed versions of Mahout matrices that allow us to access and manipulate matrix elements easily, perform common matrix operations, and input persistent matrices to existing machine learning algorithms. Project Valta: A Resource Management Layer over Apache HBase Lars George &amp; Andrew Wang, Cloudera Valta is an open-source project that acts as a layer between the user and the HBase API, employing client and server side mechanisms to guard precious resources. Interested yet? If not, next week, we’ll offer a sneak-peek of the Case Studies track.</snippet></document><document id="199"><title>Cloudera Manager 4.6: Now with Significantly More Free Features</title><url>http://blog.cloudera.com/blog/2013/06/cloudera-manager-46-free-features/</url><snippet>Yesterday we announced the availability of Cloudera Manager 4.6. As part of this release, the Free Edition of Cloudera Manager (now a part of Cloudera Standard) has been enhanced significantly to include many features formerly only available with a subscription license: Support for managing multiple CDH clusters Service and host monitoring with proactive health checks computed from hundreds of different key metrics Tunable thresholds that allow customizable alerts for your environment Activity monitoring: A single, real-time view of all cluster activities, including job details, job comparisons, and task distribution across the cluster Customizable charts with full access to thousands of metrics, and support for faceting and data transformation Intelligent log management to search all your cluster logs from with the ability to trigger events and alerts on certain messages Support for secure clusters, with Kerberos keytab management and configuration support (For more background, please refer to previous blog posts that articulate the thinking around the introduction of Cloudera Standard�and Cloudera Enterprise and the changes to Cloudera Manager Free Edition.) Some new features specifically in Cloudera Manager 4.6 include (all features available in both Cloudera Standard and Cloudera Enterprise unless otherwise noted): Support for JobTracker HA Support for installing and managing the Cloudera Search beta release Support for a new service (Sqoop 2) and new role (WebHCat) Enhanced monitoring for the Impala service and Impala Query monitoring API access to tsquery for accessing timeseries data Support for alerting on configuration changes Host-by-host rolling restart (Cloudera Enterprise only) API access to Operational Reports (Cloudera Enterprise only) API access to Cloudera Navigator is now supported (Cloudera Enterprise only) Cloudera Manager 4.6 also has lots of bug fixes and usability improvements; see the Release Notes for more information. Selected screenshots are below. Service Monitoring Heatmaps Log Search Customizable Charts While you may choose to start off with Cloudera Standard, you can also automatically get a trial license to test Cloudera Enterprise for 60 days. (Installation instructions for both options are here.) The trial includes access to advanced feature in Cloudera Manager, Cloudera Navigator, and Backup &amp; Disaster Recovery. After the 60-day trial period, the product will fall back to Cloudera Standard. You can see a full comparison between Cloudera Standard, Cloudera Enterprise, and Cloudera Enterprise Trial here. With the introduction of Cloudera Standard and Cloudera Enterprise, we are providing the easiest path to get started with Hadoop today. So why wait? Resources: Install Cloudera Standard or Enterprise Cloudera Manager 4.6 documentation and upgrade instructions Online training for Cloudera Manager Bala Venkatrao is a product director at Cloudera.</snippet></document><document id="200"><title>Updates to Cloudera Manager 4.6</title><url>http://blog.cloudera.com/blog/2013/06/cloudera-manager-46-updates/</url><snippet>The news this morning focused on the launch of Cloudera Search, an exciting new capability for our platform that was much anticipated by our customers and engineers.�Also released at the same time is a new release of Cloudera Manager (4.6). Cloudera Manager 4.6 includes a number of enhancements as well as improvements in quality and usability. (A follow-on blog post will do a deep dive on the new features and functions.)�Most notable in Cloudera Manager 4.6 is that the free version (included in Cloudera Standard) is greatly enhanced.�Cloudera Standard�now includes monitoring, health checks, events &amp; alerts, log search, kerberos automation, and multi-cluster support. There are a few motivations for this update: Improve adoption.�Too often we�d speak to users and prospects using Cloudera Manager Free who had built dependencies on a variety of different monitoring, alerting, and log management tools that they had to cobble together on their own. By rolling those kinds of features into the free version, we hope to simplify the lives of the early adopters. Don�t overcharge for yesterday�s innovation. When I talk to customers, it�s clear to me that part of the status quo they are looking to un-accept is the idea that they need to keep paying their vendors year after year for features that have not changed appreciably in over a decade.�For most customers, this is the definition of lock-in and we�re determined not to repeat that experience.�We�ve maintained a torrential pace of development of new enterprise capabilities with industry firsts like rolling upgrades, full disaster recovery, auditability, and more.�We�re confident in our ability to continue to innovate in areas that are a priority for enterprise customers. Continue to the lead the market.�Cloudera has always led the market in product innovation.�It is important to note that we�re equally committed to shipping the market-leading free product as we are to shipping the market-leading paid product.�Cloudera Manager 4.6 widens our lead for both free and paid versions. I�m excited to see the feedback on the user lists of these new capabilities. You can download the latest version of Cloudera Standard here and share your feedback via the mailing list. Charles Zedlewski is Cloudera’s VP, Products.</snippet></document><document id="201"><title>With New Product Packaging, Adopting the Platform for Big Data is Even Easier</title><url>http://blog.cloudera.com/blog/2013/06/adopting-cloudera-platform-even-easier/</url><snippet>Today is a big day: Cloudera is not only urging our customers to “Unaccept the Status Quo” (the continued and accelerating spending on data warehousing, expensive data storage, and associated software licenses), but we also announced that Cloudera Search has entered public beta. Now anyone who knows how to do a Google search can query data stored in Cloudera’s Platform for Big Data. In this post, however, I’d like to explain the new, simpler product naming/packaging structure that will make adopting and deploying Cloudera more straightforward. Introducing Cloudera Standard From now on, in addition to CDH, our 100% open source distribution of Apache Hadoop and related projects that is always available to whoever wants to try it, we will offer customers two options that also include Cloudera Manager, our management automation software: Cloudera Standard – No Charge. No Scaling Limits. Easy Deployment. (Formerly Cloudera Enterprise Free) Cloudera Standard was formerly known as Cloudera Enterprise Free. We think “Cloudera Standard” is simpler and easier to digest – and avoids the confusion that comes with having both options called “Cloudera Enterprise.” I hope you agree! Cloudera Standard remains completely free – it is simply the best choice for a company that wants to get started quickly but doesn’t yet need technical support or legal indemnification from Cloudera. It includes CDH and Cloudera Manager, without any limits to the number of nodes you can manage. With Cloudera Standard, some of the more enterprise-grade capabilities are not available, like: rolling upgrades &amp; restarts, SNMP support, LDAP integration, and a few other features that mission-critical production deployments may require. For those important capabilities, you might be interested in Cloudera Enterprise (see below). Since Cloudera Standard is free and includes CDH, you�ll get the power of all the open source projects around Hadoop that you want (including HDFS and MapReduce, Apache HBase, Apache Hive, Apache Pig, Apache Mahout, Apache ZooKeeper, Cloudera Impala, and many others), you can get going right away with a world-class, complete Hadoop deployment without any financial commitment or scaling limits whatsoever. Cloudera Enterprise (Subscription Based) – The Best Choice for Production Enterprise Hadoop Deployment (Formerly known as Cloudera Enterprise Core) When you’re ready to go into production you want not only the world’s best Hadoop distribution, but you will also want the best technical support available anywhere. Cloudera Enterprise is our flagship enterprise-class subscription and includes CDH, Cloudera Manager (with all functionality, including some only available with a Cloudera Enterprise subscription), and 8×5 or 24×7 technical support and legal indemnification. In addition, by choosing Cloudera Enterprise, you open the door to add other capabilities to your subscription as you wish – powerful tools like: Cloudera Enterprise RTD (Real Time Delivery) – Support for HBase Cloudera Enterprise RTQ (Real Time Query) – Support for Impala Cloudera Enterprise BDR (Backup and Disaster Recovery)�- Support for BDR Cloudera Navigator – Data management for your Cloudera Enterprise deployment And when Cloudera Search (beta) becomes generally available, you’ll be able to add: RTS (Real Time Search) – Support for Cloudera Search Cloudera Quickstart – The Fastest Way to Get Into Production We also continue to offer several Quickstart bundles that combine Cloudera Enterprise, training through Cloudera University, and a pre-defined set of professional services. This is a great option if your organization is new to Hadoop and you’re not sure exactly how to get into production. We hope you agree that the new packaging is a little simpler. Whichever option is right for you — CDH, Cloudera Standard, or Cloudera Enterprise — you’ll find that Cloudera offers the best path to a successful Hadoop deployment. Alan Saldich is Cloudera’s VP, Marketing.</snippet></document><document id="202"><title>Cloudera Search: The Newest Hadoop Framework for CDH Users and Developers</title><url>http://blog.cloudera.com/blog/2013/06/cloudera-search-the-newest-hadoop-framework-for-cdh-users-and-developers/</url><snippet>One of the unexpected pleasures of open source development is the way that technologies adapt and evolve for uses you never originally anticipated. Seven years ago, Apache Hadoop sprang from a project based on Apache Lucene, aiming to solve a search problem: how to scalably store and index the internet. Today, it�s my pleasure to announce Cloudera Search, which uses Lucene (among other things) to make search solve a Hadoop problem: how to let non-technical users interactively explore and analyze data in Hadoop. Cloudera Search is released to public beta, as of today. (See a demo here; get installation instructions�here.) Powered by Apache Solr 4.3, Cloudera Search allows hundreds of users to search petabytes of Hadoop data interactively. In the context of our platform, CDH (Cloudera�s Distribution including Apache Hadoop), Cloudera Search is another framework much like MapReduce and Cloudera Impala.�It�s another way for users to interact with Hadoop data and for developers to build Hadoop applications. Each framework in our platform is designed to cater to different families of applications and users: While different frameworks appeal to different users and applications, we�ve done no small amount of engineering to enable all of them to work on the same data in the same platform.� Cloudera Search leverages the same data as Impala and MapReduce.�It can index any data stored in HDFS, and it stores its own index in the same filesystem. This is a big step forward in simplicity and usability. Hadoop users will benefit from the ease of automatically indexing and free text searching the data in their clusters. Search users will benefit from the simplicity and affordability of leveraging the widely used HDFS as a basis for storage, data protection, high availability, and disaster recovery. Cloudera Search leverages the same security as the rest of the Hadoop stack. Data secured in HDFS will not be indexed or viewable by Search users who lack the proper credentials. Cloudera Search is arguably the most effective convergence of MapReduce, SQL, and Search we�ve seen to date. Just like HDFS and Apache HBase, Cloudera Search leverages Apache ZooKeeper to support index sharding and high availability. We�ve also built an exciting new integration between MapReduce and Search we call �push to go live.� With it, outputs of MapReduce jobs can be automatically merged into live Solr indices. Naturally, Cloudera Search can be deployed, configured, monitored, and automated via Cloudera Manager so users and customers get the benefit of a common management model. We�ve developed many more ways where Search integrates with the rest of CDH. Search can index streaming Apache Flume feeds. In the future, Search will also be able to index Apache Hive and HBase tables, and Search results will seamlessly feed Impala queries. In short, we�ve tried to take what was once a relatively complicated and involved freestanding system with its own hardware and operational model and turn it into a feature of a larger, more ubiquitous platform: CDH. We think this integrated approach represents a big step forward for users of Solr as well as Hadoop. Because we plan to incorporate Search into CDH, we intend to fully support our customers that run it in a production setting. Consequently, part of our development effort for Search has been to convince key committers and PMC members of the Lucene and Solr communities to join Cloudera so we can more easily support every component at a code level. I�m pleased that Cloudera continues to be a place where developers of important and new open source developments want to come and work. I also want to thank our open source collaborators.�Obviously, we�re building on the years of good work of the Lucene and Solr communities.�The work we�ve done for Cloudera Search has already resulted in dozens of new patches for the project.�In addition, I�d like to thank Aaron McCurry, whose work on Apache Blur (incubating) inspired the HDFS/Solr index integration.�Thanks also to the team at NGDATA, whose Solr/HBase integration work we will be incorporating into Cloudera Search in an upcoming release. I take some small additional pride in the fact that this is arguably the most effective convergence of MapReduce, SQL, and Search that we�ve seen in the data management industry to date. For years, databases attempted to provide search as a feature in their platforms but this approach was largely abandoned in favor of acquiring independent search products that require their own infrastructure, integration, and expertise. Hadoop�s flexibility has made it a much better supporting platform for search and consequently a much more general-purpose platform than relational databases. No wonder the center of gravity for data management has shifted toward Hadoop. Doug Cutting is Cloudera�s chief architect, a founder of the Apache Lucene and Apache Hadoop projects, and the current chair of the Apache Software Foundation. Third-party articles about Cloudera Search: - Wired: “Open Sourcers Build �Google Search for Big Data��” (6/4/2013) - The Register: “Cloudera brings Hadoop to the masses with Solr search” (6/4/2013) - ZDNet: “Search for Big Data: Cloudera and Lucene get hitched” (6/4/2013)� - GigaOm: “Cloudera adds search to Hadoop distro and says it�s just getting started” (6/4/2013) - CMSWire: “Cloudera Unveils Big Data Search, No Special Training Required” (6/4/2013)� - CRN: “Cloudera Adds Search Capabilities To Its Hadoop Big Data Platform” (6/4/2013)� - ReadWriteWeb: “Searching Hadoop Data Just Got A Lot Easier” (6/5/2013)�</snippet></document><document id="203"><title>How-to: Easily Configure and Manage Clusters in Cloudera Manager 4.5</title><url>http://blog.cloudera.com/blog/2013/06/how-to-easily-configure-and-manage-clusters-in-cloudera-manager-4-5/</url><snippet>Helping users manage hundreds of configurations for the growing family of Apache Hadoop services has always been one of Cloudera Manager�s main goals. Prior to version 4.5, it was possible to set configurations at the service (e.g. hdfs), role type (e.g. all datanodes), or individual role level (e.g. the datanode on machine17). An individual role would inherit the configurations set at the service and role-type levels. Configurations made at the role level would override those from the role-type level. While this approach offers flexibility when configuring clusters, it was tedious to configure subsets of roles in the same way. In Cloudera Manager 4.5, this issue is addressed with the introduction of role groups. For each role type, you can create role groups and assign configurations to them. The members of those groups then inherit those configurations. For example, in a cluster with heterogeneous hardware, a datanode role group can be created for each host type and the datanodes running on those hosts can be assigned to their corresponding role group. That makes it possible to tweak the configurations for all the datanodes running on the same hardware by modifying the configurations of one role group. In addition to making it easy to manage configurations of subsets of roles, role groups also make it possible to maintain different configurations for experimentation or managing shared clusters for different users and/or workloads. Viewing and Editing Role Group Configurations When your services are initially created in Cloudera Manager, role groups are automatically created for each role type based on variables such as the distribution of roles on hosts and hardware diversity in the cluster. Existing groups can be seen by going to a service page and clicking View &amp; Edit under the Configuration�tab. All the role groups under a service are listed on the left panel. Clicking on the group name or a configuration function under it will display the configurations for that group. When using the search box on the top left you can quickly find a particular configuration and compare or modify its value across different role groups on the same page. The category in the filtered configurations will be prefixed with the role group name. Managing Role Groups You can manage role groups by clicking Role Groups under the Configuration tab. This page lists all the role groups, organized by role type, under the service. Clicking on a role group will show all the roles that are currently assigned to it. Roles can be moved in batches by selecting them, clicking Move To Different Role Group…from the actions drop down menu and then specifying an existing group as the destination. Role groups can be created here by clicking the Create new group… button at the bottom left of the page and then specifying the group�s name and type. Optionally, an existing group can be selected so that its configurations will be copied to the new group, effectively cloning it. Cloning an existing group, especially one that was automatically created by Cloudera Manager, is generally the recommended way to create new groups as that will build on the automatic configuration that Cloudera Manager performed when the service was first created. Host Templates In typical environments, sets of hosts have the same hardware and the same set of services running on them. That�s where host templates�come into play. Host templates define a set of role groups (at most one of each type) in a cluster and provide two main functionalities: Adding new hosts to clusters easily — multiple hosts can have roles from different services created, configured, and started in a single operation. Altering the configuration of roles from different services on a set of hosts easily — which is useful for quickly switching the configuration of an entire cluster to accommodate different workloads or users. Create host templates by going to the Hosts page, clicking the Templates tab, and then clicking the Create button. Once a host template is created with the desired role groups, you can apply it to role groups by navigating to the Status tab. From there, multiple hosts can be selected and the Apply Host Template action used to either populate them with missing roles or alter the configuration of existing roles. Existing roles will never be deleted even if there�s no role group with a matching role type in the host template. You can also apply host templates via the Add New Hosts to Cluster wizard, which is used when expanding a cluster with new hosts. Conclusion As we�ve seen, the combination of role groups and host templates makes managing complex cluster configurations a breeze. The usefulness of these features extends to the Cloudera Manager API making scripts for managing configurations much simpler. A follow-up post will explain how to transition from the V2 API, where configurations were set at the role-type level, to the V3 API, where configurations are set the role-group level. Omar Alrubaiyan is a Software Engineer on the Enterprise team.</snippet></document><document id="204"><title>HBaseCon 2013: "Internals" Track Preview</title><url>http://blog.cloudera.com/blog/2013/05/hbasecon-2013-internals-track-preview/</url><snippet>As we march toward�HBaseCon 2013 (June 13 in San Francisco), it’s time to bring you a preview of the Internals track (see the Operations track preview here) — the track guaranteed to be of most interest to Apache HBase developers and other people tracking the progress of the code base. This track, hosted by Salesforce.com’s Lars Hofhansl (also an HBase PMC Member and HBaseCon keynote speaker),�focuses on the architecture, features, and development of HBase. You will learn about interesting features, best practices for using them in production/business-critical environments, and how development is done by the community. Apache HBase Table Snapshots Jonathan Hsieh (Cloudera), Matteo Bertozzi (Cloudera), and Jesse Yates (Salesforce.com) In this talk, the developers of this new feature will look at some of its history, describe use cases, dive into the internals, and provide a roadmap for future improvements. How to Get the MTTR Below 1 Minute and More Devaraj Das, Hortonworks &amp; Nicolas Liochon, Scaled Risk This talk will explain how to get to an MTTR for regions under one minute. It will first cover the possible failures and then explore how HBase detects the failures, recovers the data, and makes it available again. � A Developer�s Guide to Coprocessors John Weatherford, Telescope This talk will cover all the specifics needed for a Java developer to start creating coprocessors. It will start with a brief introduction to what a coprocessor is and why it is useful, describing the difference between observers and endpoints and showing examples of implementations.��� Apache HBase and HDFS: Understanding Filesystem Usage in HBase Enis S�ztutar, Hortonworks This talk will take an HDFS-centric look at the filesystem issues in HBase. It will dissect the interface between HBase and HDFS, with a focus on the filesystem services that HBase relies on, durability, crash recovery, and performance characteristics of HBase resulting from using HDFS.� Compaction Improvements in Apache HBase Sergey Shelukhin, Hortonworks Compactions are a critical aspect of HBase storage design, yet they are frequently a pain point in cluster management, affecting the availability and requiring manual tuning. This talk will provide brief overview of existing HBase compaction algorithm, the problems it encounters in specific data scenarios and in normal operation, as well as recent improvements.� 1500 JIRAs in 20 Minutes Ian Varley, Salesforce.com We all know there’s a thunderous pace of development on HBase. But what’s actually going into all of these JIRAs? In this “thunder talk,” we’ll cover the most interesting commits from the past year at a pace that will make your head spin.� � Apache HBase Replication Chris Trezzo, Twitter HBase Replication is a rich feature that enables users to asynchronously copy table data between HBase clusters. In this talk you will get a brief user tutorial of replication, look at a few example use cases, discuss the high-level core architecture, and take a detailed look at implementation.� Panel: Apache HBase Futures Attend this panel to learn about the new development efforts that are moving the HBase code base into the future � as well as ones on the community�s wish list. Nick Dimuduk, Hortonworks Jonathan Gray, Continuuity Lars Hofhansl, Salesforce.com Andrew Purtell, Intel Moderator: Todd Lipcon, Cloudera Interested yet? If not, next week, we’ll offer a sneak-peek of the Ecosystem track.</snippet></document><document id="205"><title>Apache ZooKeeper Made Simpler with Curator (Thanks, Netflix!)</title><url>http://blog.cloudera.com/blog/2013/05/zookeeper-made-simpler/</url><snippet>Our thanks to Jordan Zimmerman, software engineer at Netflix, for the guest post below about the recently announced Apache Curator (incubating) project.� Apache ZooKeeper (zookeeper.apache.org) is a client/server system for distributed coordination. On the client side, you use the client library (from Java, C/C++, etc.) to connect to the server. The client library exposes APIs that resemble a simple filesystem. You create/read/update/delete ZNodes via the API.� The ZooKeeper documentation describes high-level recipes that can be built using this API. However, someone new to ZooKeeper will quickly discover a steep learning curve where they have to: Manually deal with connection issues Handle �recoverable� errors as described in the ZooKeeper documentation Implement any needed �recipes� Learn and understand numerous undocumented ZooKeeper edge cases and best practices At Netflix, there was a lot of interest in ZooKeeper but not a lot of experience. A few trials had been done but none of them made it out of the testing sandbox. For that reason, Curator was initially conceived as a way to make ZooKeeper easier to use for non-experts. The original versions of what would become Curator consisted of a small library meant for internal use, but it quickly became clear that it would be useful to others outside Netflix.� Curator�s main benefits are: A simplified API Automatic ZooKeeper connection management with retries Complete, well-tested implementations of ZooKeeper recipes A framework that makes writing new ZooKeeper recipes much easier The creation of Curator also coincided with a desire to build a Netflix OSS presence, so, in 2011, Curator was released as an open source project on Github. It quickly became the de-facto way for Java programmers to use ZooKeeper, and as the Curator community grew, Netflix realized that Curator might be better suited to becoming an Apache project — thus, Curator is now in the Apache Incubator, with Cloudera’s Patrick Hunt serving as its champion.� Connection Management New users of ZooKeeper are surprised to learn that a significant amount of connection management must be done manually. For example, when the ZooKeeper client connects to the ensemble it must negotiate a new session and so on. This takes some time. If you use a ZooKeeper client API before the connection process is complete, ZooKeeper will throw an exception. These types of exceptions are referred to as �recoverable� errors. Curator automatically handles connection management for you, greatly simplifying client code. Instead of directly using the ZooKeeper APIs you use Curator APIs that internally check for connection completion and wrap each ZooKeeper API in a retry loop. Curator uses a retry mechanism to handle recoverable errors and automatically retry operations. The method of retry is customizable. Curator comes bundled with several implementations (ExponentialBackoffRetry, etc.) or you can write your own. Recipes The ZooKeeper documentation describes many possible uses for ZooKeeper calling each a �recipe�. While the distribution comes bundled with a few implementations of these recipes, most ZooKeeper users will need to manually implement one or more of the recipes. Implementing a ZooKeeper recipe is not trivial. Besides the connection handling issues mentioned earlier, there are numerous edge cases that are not well documented. For example, many recipes require that an ephemeral-sequential node be created. New users of ZooKeeper will not know that there is an edge case in ephemeral-sequential node creation that requires you to put a special �marker� in the node�s name so that you can search for the created node if an I/O failure occurs. This is but one of many edge cases that are not yet well documented. Enjoy Curator We hope that the Curator community will continue to grow and make using ZooKeeper more productive and more enjoyable. We invite you to participate and contribute via curator.incubator.apache.org.</snippet></document><document id="206"><title>CDH 4.3 is Released!</title><url>http://blog.cloudera.com/blog/2013/05/cdh-4-3-is-released/</url><snippet>I�m pleased to announce that CDH 4.3 is released and available for download. This is the third�quarterly update to our GA shipping CDH 4 line and the 17th�significant release of our 100% open source Apache Hadoop distribution. CDH 4.3 is primarily focused on maintenance. There are more than 400 bug fixes included in this release across the components of the CDH stack. This represents a great step forward in quality, security, and performance. There are also a few new features in this release. One new feature is the ability of HDFS to rebalance within a datanode. This is a great (configurable) way to help prevent drive failure and maintain performance without having to run more disruptive cluster-wide rebalances. Hue has also received a number of new features, including a Pig editor and support for using the HDFS trash bin. As always updates are intended to be non-disruptive. Moving to a new update is optional. Customers and users can skip updates (e.g. upgrade from CDH 4.1 to CDH 4.3 directly). Upgrading to a new update should not break compatibility with customer applications or certified third-party partner applications. We certify third-party software as compatible with �CDH 4.x or higher.� You can find more details about CDH 4.3 in the documentation here. If you have additional questions customers can file a support ticket and users can ask their question on CDH-users. As always thanks for your support of Apache Hadoop and of Cloudera. Charles Zedlewski is Cloudera’s VP, Products.�</snippet></document><document id="207"><title>Customer Spotlight: King.com�s Climb to the Social Gaming Throne</title><url>http://blog.cloudera.com/blog/2013/05/customer-spotlight-king-coms-climb-to-the-social-gaming-throne/</url><snippet>This week I�d like to highlight King.com, a European social gaming giant that recently claimed the throne�for having the most daily active users (more than 66 million). King.com has methodically and successfully expanded its reach beyond mainstream social gaming to dominate the mobile gaming market — it offers a streamlined experience that allows gamers to pick up their gaming session from wherever they left off, in any game and on any device. King.com�s top games include “Candy Crush Saga” and “Bubble Saga”. And — you guessed it — King.com runs on CDH. With a business model that offers all games for free, King.com relies advertising and in-game products like boosters and extra lives to generate revenue. In other words, it has to be smart in every communication with customers in order to create value for both the gamer and the advertiser. King.com uses Hadoop to process, store, and analyze massive volumes of log data generated from the games along with other data sources such as daily currency exchange rates from the European Central bank, multiple metadata feeds, and advertising servers� log files. So if you don�t have plans to celebrate the long Memorial Day weekend, how about playing a game or two? To learn more about King.com�s Hadoop-powered environment, check out the following resources: VentureBeat: “King overtakes Zynga as the largest social gaming company” TechRepublic: “How Hadoop is helping King.com ask the right questions” ComputerWeekly: “King.com gaming site unlocks big data with Hadoop” Cloudera press release: King.com gets ahead of the game with Cloudera Enterprise Karina Babcock is Cloudera�s Customer Programs &amp; Marketing Manager.</snippet></document><document id="208"><title>Demo: Apache Pig Editor in Hue 2.3</title><url>http://blog.cloudera.com/blog/2013/05/demo-apache-pig-editor-in-hue-2-3/</url><snippet>In the previous installment of the demo series about�Hue�� the open source Web UI that makes�Apache Hadoop�easier to use � you learned how to analyze data with Hue using Apache Hive via Hue�s Beeswax and Catalog applications. In this installment, we�ll focus on using the new editor for Apache Pig in�Hue 2.3. Complementing the editors for Hive and Cloudera Impala, the Pig editor provides a great starting point for exploration and real-time interaction with Hadoop. This new application lets you edit and run Pig scripts interactively in an editor tailored for a great user experience. Features include: UDFs and parameters (with default value) support Autocompletion of Pig keywords, aliases, and HDFS paths Syntax highlighting One-click script submission Progress, result, and logs display Interactive single-page application Here’s a short video demoing its capabilities and ease of use: The demo data is based on the previous Hive and Metastore demo�and its cleaned business file. Here is the Pig script used and explained in this demo. It is loading the Yelp business file that was converted in the previous demo and computing the top-25 most reviewed restaurants: business =
	LOAD '/user/hive/warehouse/business/yelp_academic_dataset_business_clean.json'
	AS (business_id: CHARARRAY, categories: CHARARRAY, city: CHARARRAY, full_address: CHARARRAY,
    	latitude: FLOAT, longitude: FLOAT, name: CHARARRAY, neighborhoods: CHARARRAY,
    	open: BOOLEAN, review_count: INT, stars: FLOAT, state: CHARARRAY, type: CHARARRAY);

business_group =
  GROUP business
  BY city;

business_by_city =
  FOREACH business_group
  GENERATE group, COUNT(business) AS ct;

top =
	ORDER business_by_city
	BY ct DESC;

top_25 = LIMIT top 25;

DUMP top_25;
   What’s Next? New features like support for Python UDFs and better integration with Apache Oozie and File Browser are on the way. As usual, we welcome all feedback!</snippet></document><document id="209"><title>HBaseCon 2013: "Operations" Track Preview</title><url>http://blog.cloudera.com/blog/2013/05/hbasecon-2013-operations-track-preview/</url><snippet>As you have probably learned by now, HBaseCon 2013 sessions are organized into four tracks: Operations, Internals, Ecosystem, and Case Studies. In combination, they offer a 360-degree view of Apache HBase that is invaluable for experts and aspiring experts alike. In the next few posts leading up to the conference (June 13 in San Francisco – register now�while there’s still room), we’ll offer sneak previews of what each track has to offer. First up is the Operations track, which will be hosted by Facebook’s Liyin Tang (HBase PMC Member and HBaseCon keynote speaker): The sessions in this track focus on the deployment, operation, and management of Apache HBase. You will learn about best practices for cluster deployment, monitoring, mitigating failures, security, and other aspects of running HBase for business-critical applications. Apache HBase Operations at Pinterest Jeremy Carroll, Pinterest This presentation explains how Pinterest operates HBase on Amazon EC2 with success. � Reliability: More 9�s for Apache HBase Amitanand Aiyer, Facebook HBase has been powering Facebook’s messaging system for over two years. This talk will discuss common failure scenarios that Facebook has encountered in this period and efforts to make HBase more reliable/available.� � OpenTSDB at Scale Jonathan Creasy &amp; Geoff Anderson, Box This presentation covers operating an OpenTSDB Cluster, including best practices for scaling, how to maximize throughput, and share metrics related to the scale of Box’s cluster.� � Apache HBase on Flash Matt Kennedy, Fusion-io Flash Memory technology deployed as server-side PCIe or SSDs is emerging as a critical tool for performance and efficiency in data centers at all scales. This presentation will discuss how Flash impacts HBase deployments in terms of configuration, DRAM requirements, and performance. � Scalable Network Designs for Apache HBase Benoit Sigoure, Arista Networks Based on production experience with HBase since 2009, as well as recent benchmark results, this talk will take you through network designs and optimizations for HBase, as well as possible ways to make the network work better for Apache Hadoop/HBase.� � Apache HBase, Meet Ops. Ops, Meet Apache HBase. Jean-Daniel Cryans &amp; Kevin O�dell, Cloudera This talk will help you with introducing HBase to your Ops team, mapping their current knowledge to the technology stack, and giving them a head start on supporting HBase in a production environment. � Panel: Apache HBase Operations Attend this panel to learn about best practices from the operators of world-class HBase deployments. Jeremy Carroll, Pinterest Dave Latham, Flurry Alex Levchuk, Facebook Rajiv Chittajallu, Yahoo! Moderator: Eric Sammer, Cloudera Interested yet? If not, next week, we’ll offer a sneak-peek of the Internals track.</snippet></document><document id="210"><title>FAQ: Understanding the Parcel Binary Distribution Format</title><url>http://blog.cloudera.com/blog/2013/05/faq-understanding-the-parcel-binary-distribution-format/</url><snippet>Have you ever wished you could upgrade to the latest CDH minor release with just a few mouse clicks, and even without taking any downtime on your cluster? Well, with Cloudera Manager 4.5 and its new “Parcel” feature, you can! That release introduced many new features and capabilities related to parcels, and in this FAQ-oriented post, you will learn about most of them. What are parcels? Parcel is an alternative binary distribution format supported for the first time in Cloudera Manager 4.5. There are a few notable differences between parcels and traditional CDH rpm/deb packages: CDH is provided as a single package. In contrast to having a separate package for each part of CDH, when using Cloudera Manager 4.5 and later, there is just a single parcel to install. Parcels can be installed side-by-side. Each parcel is self-contained and installed in a separate versioned directory. This means that multiple versions of a given parcel can be installed at the same time. You can then select one of these installed versions as the “active” one. (With traditional CDH packages, only one package can be installed at a time so there�s no distinction between what�s “installed” and what�s “active”.) Parcels can run from arbitrary locations. Parcels can be installed at any location in the filesystem. Parcels are gzipped tar files with metadata. From a strict implementation point of view, a parcel is simply a tarball containing the program files, along with some additional metadata that allows Cloudera Manager to understand what it is and how to use it. What are the benefits of parcels? As a consequence of the functional characteristics noted above, parcels offer a number of benefits: Simplified distribution: As a parcel is a single file, it�s much easier to move around than the dozens of packages that make up CDH. This is especially useful when managing a cluster that isn�t connected to the Internet. Internal consistency: By distributing CDH as a single parcel, we can help ensure that all CDH components are properly matched and that there isn�t a danger of different parts coming from different versions of CDH. Installation outside of /usr: In some IT environments, Hadoop admins do not have privileges to install system packages. In the past, these admins had to fall back to CDH tarballs, which deprived them of a lot of infrastructure that packages provide. With parcels, admins can install to /opt or anywhere else without having to step through all the additional manual steps of regular tarballs. Installation of CDH without sudo: Parcel installation is handled by the CM Agent already running as root so it’s possible to install CDH without needing sudo, which can be very helpful. Decoupling of distribution from activation: Thanks to side-by-side install capabilities delivered by parcels, it is now possible to stage a new version of CDH across the cluster in advance of switching over to it. This allows the longest running part of an upgrade to be done ahead of time without affecting cluster operations, consequently reducing upgrade downtime. Rolling upgrades: With the new version staged side-by-side, switching to a new minor version is simply a matter of changing which version of CDH is used when restarting each process. It then becomes practical to do upgrades with rolling restarts, where service roles are restarted in the right order to switch over to the new version with minimal service interruption. Note that major version upgrades (CDH3 -&gt; CDH4) require full service restarts due to the substantial changes between the versions. Easy downgrades: With the old version still available, moving back to it can be as simple as upgrading. (Note that some CDH components may require explicit additional steps due to things like schema upgrades.) What new capabilities in Cloudera Manager 4.5 are premised on parcels? Thanks to the introduction of parcels, a host of new capabilities are now delivered by Cloudera Manager: End-to-end deployment life-cycle management:�Starting with 4.5, Cloudera Manager can now fully manage all the steps involved in a CDH version upgrade. (In contrast, with traditional packages, Cloudera Manager can only help with initial installation.)   Life-cycle of a parcel Download: Parcels are published to Cloudera�s repository. Cloudera Manager will then download the parcel to the CM Server machine. Distribution: Once the Server has the parcel, Cloudera Manager can distribute the parcel out to all the hosts in the cluster. This process can be tuned in terms of how many hosts receive the parcel at the same time and the total aggregate bandwidth used for the process. Activation: Once a parcel is distributed, you can activate it. Once activated, it will be used for any processes that are subsequently started or restarted. Deactivation: Similarly, a parcel can be deactivated (and will automatically be deactivated if another one is activated). Removal: This is the reverse of distribution. A parcel that has been deactivated and is not serving any current processes is eligible for removal from the hosts in the cluster. Deletion: Finally, once removed from the cluster, the parcel can be deleted from the CM server, which completes the life-cycle of a parcel. The following screenshot shows: One active CDH and one active Impala parcel One CDH parcel being downloaded One CDH parcel being distributed One CDH parcel available for download   The Parcels page in Cloudera Manager End-to-end capabilities are optional: If there are specific reasons to use other tools for download and/or distribution, you can do so, and Cloudera Manager will work alongside your other tools. For example, you can handle distribution with something like Puppet. Or, if you want to download the parcel to CM Server manually (perhaps because your cluster has no Internet connectivity) and then have Cloudera Manager distribute the parcel to the cluster, you can do that too. Rolling upgrades: These are only possible with parcels, thanks to their side-by-side nature. Traditional packages would require shutting down the old process, upgrading the package, and then starting the new process. This can be hard to recover from in the event of errors and requires extensive integration with the package management system to function seamlessly. Distributing additional components: Parcels are not limited to CDH. Impala is available as a parcel too and we�ve just published an LZO parcel that provides the LZO plugins for both Hadoop and Impala. In a future blog post, we�ll discuss how you might build your own parcels to distribute other software. What parcels are currently available? CDH: 4.1.3 and newer Impala: 1.0 and newer (parcels are available for old betas, but only 1.0 is supported) LZO: Contains plugins for CDH 4.x and Impala 1.0 How do I configure parcels? All parcel-related configuration settings are collected in the Parcels section of the CM Server properties. Here are some of the key configuration settings: Local Parcel Repository Path: This is the location on the CM server where downloaded parcels will be stored. Parcel Update Frequency: This controls how often the CM server will check for the presence of new parcels in the repository(s). Remote Parcel Repository URLs: This is the list of parcel repositories that Cloudera Manager will check for parcels. By default, it includes Cloudera�s CDH and Impala repositories. If you need LZO support, you would want to add the LZO parcel repository to this list. Proxy Server Settings: There are a group of settings that allow an HTTP proxy to be configured, if one is required to access the Internet from the CM server. Automatic Download/Distribution: These settings allow you to configure the CM server to automate the download and distribution steps so that as soon as a new release is detected, it will be staged and ready for activation without any direct intervention.   Parcels configuration screen Conclusion/Next Steps As you can see, we believe that users get a lot of benefits from this new approach to binary distribution. To read more about parcels and how to use them in Cloudera Manager, see the parcels documentation available here. In future blog posts we�ll walk through the process of using parcels to upgrade from CDH3 to CDH4, and doing rolling upgrades to move between CDH4 releases with minimal downtime. Philip Langdale is a Software Engineer on the Enterprise team. &gt;�Ask questions and get answers about parcels in the community forum for Cloudera Manager.�� Learn More About Parcels Want to see the power of Parcels in action? Watch our e-learning module on Understanding Parcels to learn the fundamentals of optimizing your Hadoop operations with Parcels. The video includes a step-by-step demo of upgrading CDH and installing Impala, Search, and Hadoop LZO.</snippet></document><document id="211"><title>If It’s Tuesday, There Must Be a "Data Ride"</title><url>http://blog.cloudera.com/blog/2013/05/if-its-tuesday-there-must-be-a-data-ride/</url><snippet>Mark your calendars, all you data cyclists! I�m visiting Paris, London, and Edinburgh this June. When I travel I like to talk to locals. And, wherever I am, I like to bicycle. So, I thought I might combine these interests and host �data rides� in these three cities. In each city I�ll name a time and a meeting point, and then ride the local roads for an hour or two with whomever shows up. Afterward, we might need some libations at a local pub. I might even get Cloudera to throw in some schwag. Ride dates are as follows: Paris: Tuesday, June 4 London: Tuesday, June 11 Edinburgh: Tuesday, June 18 All rides will start at 5pm. I�ll pick a meeting point closer to the event and tweet it from @cutting. Also, please tweet me if you have ideas of where we should ride. Doug Cutting is Cloudera�s chief architect, a founder of the Apache Lucene and Apache Hadoop projects, and the current chair of the Apache Software Foundation.</snippet></document><document id="212"><title>Customer Spotlight: Gravity Creates Personalized Web Experience, 300-400% Higher Click-through</title><url>http://blog.cloudera.com/blog/2013/05/customer-spotlight-gravity-creates-personalized-web-experience-300-400-higher-click-through/</url><snippet>According to Jim Benedetto, Gravity�s co-founder and CTO, there have been two paradigm shifts that have transformed consumers� web experience to date: Search � Google figured out how to index all of the content on the internet so you (internet user) can find what you�re looking for. Social � Facebook, Twitter, LinkedIn and other social sites give your friends and other social connections a mechanism to push content you�re interested in to you, so you don�t have to search for it yourself. So what does Gravity do? Its goal is to drive the third paradigm shift: Personal — Creating a web experience that is totally optimized based on your individual interests, behaviors, and preferences. Or, as Jim puts it, �showing you today what you�re going to search for tomorrow.� Gravity collects and processes more than 10,000 data points every second. All of the data collected is loaded into HDFS, where two Apache Hadoop processes run. The first is a dynamic, real-time system that uses something called �eventual consistency,� meaning it correctly processes as many data points as it can�about both user activity across the web and content that is being published�in real time. 99.99% of that traffic is processed correctly. The second system runs every hour or two, catching the .01% of data points that were missed the first time around. Once the data is processed, it lands in Apache HBase where it is serialized and can be accessed via Apache Hive. With several Scala engineers in house, the Gravity team decided in 2011 to use the Scala programming language instead of Java. It doesn�t natively integrate with Hadoop or HBase, so the Gravity team wrote its own open source library called HPaste, which allows Scala engineers to take advantage of all the unique features of Scala on top of HBase. The results of this system? Higher click-through rates (CTR) � Gravity has measured CTR of people who engage with their personalized content versus standard segmented or generic content, and they�ve proven that personalized content delivers 300-400% higher CTR. Longer sessions � When personalized content is displayed on a web page, users stay on the page longer, which is a strong indication that they like the site more. More repeat visitors � If a web visitor sees personalized content their first time visiting a site, the number of times they return to that site afterward is more than 10X higher than when they engage with static content shown to all visitors. Gravity has proven this at scale across some of its largest customers. Want to learn more? Read the full case study. Watch Gravity�s Jim Benedetto explain its use case on video. Explore the HPaste project on GitHub. Learn more about Gravity. Karina Babcock is Cloudera�s Customer Programs &amp; Marketing Manager.</snippet></document><document id="213"><title>Meet the Project Founder: Roman Shaposhnik</title><url>http://blog.cloudera.com/blog/2013/05/meet-the-project-founder-roman-shaposhnik/</url><snippet>This installment of “Meet the Project Founder” features Apache Bigtop founder and PMC Chair/VP Roman Shaposhnik. What led you to your project idea(s)? Conceptually, Apache Bigtop can actually be traced as far back as me working at Sun Microsystems in 2007-2008. I was assisting the team responsible for coming up with a 100% community-driven, open source Solaris distribution that could also be used as a basis for an enterprise-grade commercial product offering (which eventually became OpenSolaris). I then joined Yahoo! Inc. as a manager of a small team of extremely talented engineers tasked with integration efforts around Yahoo’s internal cloud offering based on Hadoop. Our project was called HIT (Hadoop Integration Testing) and we were known as “HIT-men”. Aside from doing the initial commit, what is your definition of the project founder�s role across the lifespan of the project? Benevolent dictator, referee, silent partner? Honestly, my role model is Linus Torvalds. He’s somebody who’s deeply passionate about the state of the community yet he still finds enough time to be involved with most technical aspects of the Linux kernel on a daily basis. But at the end of the day, he’s just plain fun to be around. Of course, the governance framework of Apache Software Foundation is quite different than the governance model of the Linux kernel. I use that excuse when I can’t quite measure up to Linus where influence is concerned. What has surprised you the most about how your project has evolved/matured? I’m still amazed at how quickly the ‘Powered by Bigtop’ list is growing. The elevator pitch for Bigtop has always been: Bigtop is to Hadoop what Debian is to Linux. The most surprising development to me was how well that message resonates with the commercial vendors in the Big Data space. I’m still amazed at how quickly the “Powered by Bigtop” list is growing. What is the major work yet to be done, from your perspective as the project�s founder? Developers, developers, developers! We have to grow the community by leaps and bounds if we want to be remembered as the Debian of Hadoop. This translates into investing in outreach activities, but also (and maybe primarily) into creating enough value in the project itself so that external developers get hooked. Just steer clear of trying to boil the ocean by yourself — make the project interesting enough, and the developer community will come to the party. What is your philosophy, if you have one, for balancing quality versus quantity with respect to contributions? That’s a tough one. In the ideally balanced community everybody keeps an eye on all the proposed changes and casts +/-1 as needed. Hence there’s a self-throttling process that also provides a learning opportunity for the newcomers. Fundamentally though, developers don’t like to review patches; they like to write code. Making the review duty appealing is like making the broccoli-eating process appealing to your toddler. The bottom line is: You have to get creative (and personally brace yourself for way more reviews than direct code contributions). At the same time, accepting somebody’s patches is a great way of keeping that contributor around. Hence, personally, I try and err on the side of openness and community growth over polishing each patch ad infinitum. But my personal coding philosophy is: Commit early, commit often, and re-factor mercilessly. Any other advice for other potential project founders? Attachment is the root of all suffering; don’t get attached to your own code or ideas. The only thing that lasts is community. At ASF, we are reminded of the “community over code” mantra all the time, but it’s not just a phrase. It’s for real. P.S.: Oh, and here’s one more crucial bit of advice: Before naming your project, make sure to check that the vanity license place with its name is available in your state. Read other “Meet the Project Founders” installments: - Doug Cutting�(Apache Hadoop, Apache Avro, Apache Lucene)</snippet></document><document id="214"><title>How-to: Configure Eclipse for Hadoop Contributions</title><url>http://blog.cloudera.com/blog/2013/05/how-to-configure-eclipse-for-hadoop-contributions/</url><snippet>Contributing to Apache Hadoop or writing custom pluggable modules requires modifying Hadoop�s source code. While it is perfectly fine to use a text editor to modify Java source, modern IDEs simplify navigation�and�debugging of large Java projects like Hadoop significantly. Eclipse is a popular choice thanks to its broad user base and multitude of available plugins. This post covers configuring Eclipse to modify Hadoop�s source. (Developing applications against CDH using Eclipse is covered in�a different post.) Hadoop has changed a great deal since our previous post on configuring Eclipse for Hadoop development; here we�ll revisit configuring Eclipse for the latest �flavors� of Hadoop. Note that trunk and other release branches differ in their directory structure, feature set, and build tools they use. (The EclipseEnvironment Hadoop wiki page is a good starting point for development on trunk.) This post covers the following main flavors: The traditional implementation of MapReduce based on the JobTracker/TaskTracker architecture (MR1) running on top of HDFS. Apache Hadoop 1.x and CDH3 releases, among others, capture this setup. A highly-scalable MapReduce (MR2) running over YARN and an improved HDFS 2.0 (Federation, HA, Transaction IDs), captured by Apache Hadoop 2.x and CDH4 releases. Traditional MapReduce running on HDFS-2 — that is, the stability of MR1 running over critical improvements in HDFS-2. CDH4 MR1 ships this configuration. The below table captures the releases and the build tools they use along with the preferred version: Release Build Tool (preferred version) CDH3 (Hadoop 1.x) Ant (1.8.2) CDH4 (Hadoop 2.x) HDFS Maven (3.0.2) CDH4 (Hadoop 2.x) MR2 Maven (3.0.2) CDH4 MR1 Ant (1.8.2) Other Requirements: Oracle Java 1.6 or later Eclipse (Indigo/Juno) Setting Up Eclipse First, we need to set a couple of classpath variables so Eclipse can find the dependencies. Go to Window -&gt; Preferences. Go to Java -&gt; Build Path -&gt; Classpath Variables. Add a new entry with name ANT_PATH and path set to the ant home on your machine, typically /usr/share/ant. Add another new entry with name M2_REPO and path set to your maven repository, typically $HOME/.m2/repository (e.g. /home/user/.m2/repository). Hadoop requires tools.jar, which is under JDK_HOME/lib. Because it is possible Eclipse won�t pick this up: Go to Window-&gt;Preferences-&gt;Installed JREs. Select the right Java version from the list, and click “Edit”. In the pop-up, �Add External JARs�, navigate to “JDK_HOME/lib”, and add “tools.jar”. Hadoop uses a particular style of formatting. When contributing to the project, you are required to follow the style guidelines: Java formatting with all spaces and indentation as well as tabs set to 2 spaces. To do that: Go to Window -&gt; Preferences. Go to Java-&gt;Code Style -&gt; Formatter. Import this Formatter. It is a good practice to enable automatic formatting of the modified code when you save a file. To do that, go to Window-&gt;Preferences-&gt;Java-&gt;Editor-&gt;Save Actions and select �Perform the selected actions on save�, �Format source code�, �Format edited lines�. Also, de-select �Organize imports�. For Maven projects, the m2e plugin�can be very useful. To install the plugin, go to Help -&gt; Install New Software. Enter “http://download.eclipse.org/technology/m2e/releases”�into the �Work with� box and select� the m2e plugins and install them. ����������� � Configuration for Hadoop 1.x / CDH3 Fetch Hadoop using version control systems subversion or git and checkout branch-1 or the particular release branch. Otherwise, download a source tarball from the CDH3 releases or Hadoop releases. Generate Eclipse project information using Ant via command line: For Hadoop (1.x or branch-1), �ant eclipse� For CDH3 releases, �ant eclipse-files� Pull sources into Eclipse: Go to File -&gt; Import. Select General -&gt; Existing Projects into Workspace. For the root directory, navigate to the top directory of the above downloaded source. Configuration for Hadoop 2.x / CDH4 MR2 Apache Hadoop 2.x (branch-2/trunk based) and CDH4.x have the same directory structure and use Maven as the build tool. Again, fetch sources using svn/git and checkout appropriate branch or download release source tarballs (follow CDH Downloads). Using the m2e plugin we installed earlier: Navigate to the top level and run �mvn generate-sources generate-test-sources�. Import project into Eclipse: Go to File -&gt; Import. Select Maven -&gt; Existing Maven Projects. Navigate to the top directory of the downloaded source. The generated sources (e.g. *Proto.java files that are generated using protoc) might not be directly linked and can show up as errors. To fix them, select the project and configure the build path to include the java files under target/generated-sources and target/generated-test-sources. For inclusion pattern, select “**/*.java”. Without using the m2e plugin: Generate Eclipse project information using Maven: mvn clean &amp;&amp; mvn install -DskipTests &amp;&amp; mvn eclipse:eclipse. Note: mvn eclipse:eclipse generates a static .classpath file that Eclipse uses, this file isn’t automatically updated as the project/dependencies change. Pull sources into Eclipse: Go to File -&gt; Import. Select General -&gt; Existing Projects into Workspace. For the root directory, navigate to the top directory of the above downloaded source. Configuration for CDH4 MR1 CDH4 MR1 runs the stable version of MapReduce (MR1) on top of HDFS from Hadoop 2.x branches. So, we have to set up both HDFS and MapReduce separately. Follow Steps 1 and 2 of the previous section (Hadoop 2.x). Download MR1 source tarball from CDH4 Downloads and untar into a folder different than the one from Step 1. Within the MR1 folder, generate Eclipse project information using Ant via command line (ant eclipse-files). Configure .classpath using this perl script to make sure all classpath entries point to the local Maven repository: Copy the script to the top-level Hadoop directory. Run $ perl configure-classpath.pl Pull sources into Eclipse: Go to File -&gt; Import. Select General -&gt; Existing Projects into Workspace. For the root directory, navigate to the top directory of the above downloaded sources. Happy Hacking! Karthik Kambatla is a Software Engineer at Cloudera in the scheduling and resource management team and works primarily on MapReduce and YARN.</snippet></document><document id="215"><title>Fresh and Hot: HBaseCon 2013 Schedule Finalized!</title><url>http://blog.cloudera.com/blog/2013/05/fresh-and-hot-hbasecon-2013-schedule-finalized/</url><snippet>The schedule/agenda grid�for HBaseCon 2013 (rapidly approaching: June 13 in San Francisco) is a thing of beauty. If you lacked motivation to register up until this point, we think that this session line-up will convince you otherwise. We repeat: whether you’re an HBase committer or just getting started (or at any level in between), HBaseCon is simply an event that you can’t afford to miss – and with an entry fee of just $350, it’s also one you can easily afford. Register now while there’s still room! See also: - Top 5 Reasons to Attend HBaseCon 2013 - HBaseCon 2013 Speakers, Tracks, and Sessions Announced</snippet></document><document id="216"><title>How-to: Automate Your Hadoop Cluster from Java</title><url>http://blog.cloudera.com/blog/2013/05/how-to-automate-your-hadoop-cluster-from-java/</url><snippet>One of the complexities of Apache Hadoop is the need to deploy clusters of servers, potentially on a regular basis. At Cloudera, which at any time maintains hundreds of test and development clusters in different configurations, this process presents a lot of operational headaches if not done in an automated fashion. In this post, I�ll describe an approach to cluster automation that works for us, as well as many of our customers and partners. Taming Complexity At Cloudera engineering, we have a big support matrix: We work on many versions of CDH (multiple release trains, plus things like rolling upgrade testing), and CDH works across a wide variety of OS distros (RHEL 5 &amp; 6, Ubuntu Precise &amp; Lucid, Debian Squeeze, and SLES 11), and complex configuration combinations � highly available HDFS or simple HDFS, Kerberized or non-secure, using YARN or MR1 as the execution framework, etc. Clearly, we need an easy way to spin-up a new cluster that has the desired setup, which we can subsequently use for integration, testing, customer support, demos, and so on. This concept is not new; there are several other examples of Hadoop cluster automation solutions. For example, Yahoo! has its own infrastructure tools, and you can find publicly available Puppet recipes, with various degrees of completeness and maintenance. Furthermore, there are tools that work only with a particular virtualization environment. However, we needed a solution that is more powerful and easier to maintain. Cloudera’s automation system for Hadoop cluster deployment provisions VMs on-demand in our internal cloud. As cool as that capability sounds, it�s actually not the most interesting part of the solution. More important is that we can install and configure Hadoop according to precise specifications using a powerful yet simple abstraction — using Cloudera Manager�s open source REST API. Cloudera Manager API This is what our automation system does: Installs the Cloudera Manager (CM) packages on the cluster. Start CM server. Uses the API to add hosts, installs CDH, defines the cluster and its services. For configuration, we use the API to tune heap sizes, set up HDFS HA, turn on Kerberos security and generate keytabs, customize service directories and ports, and so on. Every configuration available in Cloudera Manager�is exposed in the API. The API also gives access to management features, such as gathering logs and monitoring information, starting and stopping services, polling cluster events, and creating a DR replication schedule. We use these features extensively in our automated tests. The end result is a system that has become an indispensable part of our engineering process. It makes the Hadoop setup easy to maintain. For example, the same API call retrieves logs from HDFS, HBase, or any other service, without the user worrying about the different log locations. The same API call stops any service, without the user worrying about any additional steps. (HBase needs to be gracefully shutdown.) And when Cloudera Manager adds support for more services (e.g. Impala), their setup flows are the same as the existing ones. Use Cases from Partners and Customers Many of our customers and partners have also adopted the Cloudera Manager API for cluster automation: Some OEM and hardware partners, delivering Hadoop-in-a-box appliances, use the API to set up CDH and Cloudera Manager�on bare metal in the factory. Some of our high-growth customers are constantly deploying new clusters, and have automated that with a combination of Puppet and the�Cloudera Manager�API. Puppet does the OS-level provisioning, and the software installation. After that, the Cloudera Manager�API sets up the Hadoop services and configures the cluster. Others have found it useful to integrate the API with their reporting and alerting infrastructure. An external script can poll the API for health and metrics information, as well as the stream of events and alerts, to feed into a custom dashboard. Code Samples A previous blog post gave an example of setting up a CDH4 cluster using the Python API client. Instead of repeating that, let me introduce you to the Java API client. (Although our internal automation tool uses the Python client today, we plan to move to Java to better work with our other Java-based tools like jclouds.) To use the Java client, add this dependency to your project�s pom.xml: �
&lt;project&gt;
� &lt;repositories&gt;
��� &lt;repository&gt;
����� &lt;id&gt;cdh.repo&lt;/id&gt;
����� &lt;url&gt;https://repository.cloudera.com/content/groups/cloudera-repos&lt;/url&gt;
����� &lt;name&gt;Cloudera Repository&lt;/name&gt;
��� &lt;/repository&gt;
��� �
� &lt;/repositories&gt;
� &lt;dependencies&gt;
��� &lt;dependency&gt;
����� &lt;groupId&gt;com.cloudera.api&lt;/groupId&gt;
����� &lt;artifactId&gt;cloudera-manager-api&lt;/artifactId&gt;
����� &lt;version&gt;4.5.2&lt;/version&gt;����� &lt;!-- Or the CM version you work with --&gt;
��� &lt;/dependency&gt;
��� �
� &lt;/dependencies&gt;
� ...
&lt;/project&gt;
   The Java client works like a proxy. It hides from the caller any details about REST, HTTP, and JSON. The entry point is a handle to the root of the API: RootResourceV3 apiRoot = new ClouderaManagerClientBuilder()
����������� .withHost("cm.cloudera.com")
����������� .withUsernamePassword("admin", "admin")
����������� .build()
����������� .getRootV3();
   From the root, you can traverse down to all other resources. (It�s called �v3� because the currently Cloudera Manager�API version is version 3. But the same builder also returns a v1 or v2 root.) Here is the tree view of some of the key resources and the interesting operations they support: ��� * RootResourceV3 ����� ��* ClustersResourceV3: hosts membership, start cluster ����������� * ServicesResourceV3: config, get metrics, HA, service commands ��������������� * RolesResource: add roles, get metrics, logs ��������������� * RoleConfigGroupsResource: config ���������� �* ParcelsResource: parcels management ������� * HostsResource: hosts management, get metrics ������� * UsersResource: users management Of course, these are all in the Javadoc, and the full API documentation. To give a short concrete example, here is the code to list and start a cluster:   // List of clusters
ApiClusterList clusters = apiRoot.getClustersResource()
                                 .readClusters(DataView.SUMMARY);
for (ApiCluster cluster : clusters) {
  LOG.info("{}: {}", cluster.getName(), cluster.getVersion());
}

// Start the first cluster
ApiCommand cmd = apiRoot.getClustersResource()
                        .startCommand(clusters.get(0).getName());
while (cmd.isActive()) {
   Thread.sleep(100);
   cmd = apiRoot.getCommandsResource().readCommand(cmd.getId());
}
LOG.info("Cluster start {}", cmd.getSuccess() ?
            "succeeded" : "failed " + cmd.getResultMessage());
   To see a full example of cluster deployment using the Java client, see whirr-cm. Specifically, jump straight to CmServerImpl#configure�to see the core of the action. You may find it interesting that the Java client is maintained with very little effort. Using Apache CXF, the client proxy comes free, quite magically. It figures out the right HTTP call to make by inspecting the JAX-RS annotations in the REST interface, which is the same interface used by the Cloudera Manager API server. Therefore, new API methods are available to the Java client automatically. What�s Your Plan? Overall, we are very happy with our automated deployment capability. I encourage you to try the Cloudera Manager API, and post your questions and feedback on the mailing list. bc Wong is a Software Engineer on the Enterprise team.</snippet></document><document id="217"><title>Tracking Hadoop Jobs from Your Mac: There’s an App for That</title><url>http://blog.cloudera.com/blog/2013/05/tracking-hadoop-jobs-from-your-mac-theres-an-app-for-that/</url><snippet>Our thanks to Etsy developer Brad Greenlee (@bgreenlee) for the post below. We think his Mac OS app for JobTracker is great! JobTracker.app�is a Mac menu bar app interface to the Hadoop JobTracker. It provides Growl/Notification Center notices of starting, completed, and failed jobs and gives easy access to the detail pages of those jobs. When I started writing Apache Hadoop jobs at�Etsy, I found myself wasting a lot of time checking the JobTracker page to see how my job was progressing. The first thing we did to try to solve this problem was to write a�Scalding�flow listener to announce completed and failed jobs to IRC, but that got a little noisy. So I wrote JobTracker.app. Installation and Usage You can download the binary from its�GitHub project page.�Just unzip it and drop it into your Applications folder. Running it will put a little pith helmet in your menu bar. Clicking that gets you this menu: You’ll first need to go to Preferences and enter your JobTracker URL: By default it will track all jobs. You probably don’t want this, so put your username and any other usernames you want to track in the “Usernames to track” field, comma-separated. Note that this has only been tested with the version of Hadoop that Etsy is running internally. Due to the somewhat horrifying way that the app gets the JobTracker data (by parsing the JobTracker HTML page, since there’s currently no API to JobTracker except via Java), it’s not unlikely that it could break on a different version of Hadoop/JobTracker. If you try it and it doesn’t work for you,�file an issue�on GitHub and I�ll work with you on fixing it. Future Development Next on my list of features is allowing for�tracking multiple clusters at once. If you have any requests, please let me know.</snippet></document><document id="218"><title>Top 5 Reasons to Attend HBaseCon 2013</title><url>http://blog.cloudera.com/blog/2013/05/top-5-reasons-to-attend-hbasecon-2013/</url><snippet>HBaseCon 2013 is approaching fast – June 13 in San Francisco. If you’re on the fence about attending – or perhaps your manager is on the fence about approving your participation – here are a few things that you/they need to know (in no particular order): HBaseCon is the annual rallying point for the HBase community. If you’ve ever had a desire to learn how to get involved in the community as a contributor, or just want to ask a committer or PMC member why things are done (or not done) a certain way, this is your opportunity – because this is where those people are. Participating in a mailing list thread is never quite the same once you’ve met the people behind it.� � HBaseCon is a one-stop shop for learning about the HBase roadmap, as well as other projects across the ecosystem.�Current HBase users should be particularly interested in learning about which JIRAs will have the most impact on the user experience – and once again, most of the committers working on those JIRAs will either be leading sessions or otherwise present. Plus, you can learn about how new complementary projects like Impala, Kiji, Phoenix, and Honeycomb are transforming the use cases for HBase and helping to expand its footprint across the enterprise. � HBaseCon is a feast of real-world experiences and use cases.�Sure, maybe you’ve read about the HBase-backed applications used by companies like Facebook, Salesforce.com, eBay, Pinterest, and Yahoo!. But wouldn’t it be helpful to hear technical details and best practices directly from the people who built and run them? I’ll bet it would. And you really can’t do that anywhere else — in the whole world. (Plus, you can take advantage of formal training right before the conference, at a discount.) � HBaseCon is a pageant of engineer rock-stars.�If your company is an HBase user and hungry for talent, there’s no better place to find it: HBaseCon is literally the world’s biggest gathering of HBase experts under one roof. � HBaseCon is a heck of a blast. Come for the deep-dives and advice, stay for the after-event party. The libations will be extensive! If you have any interest in HBase whatsoever, whether as a user or prospective user, missing HBaseCon is almost unthinkable.� Register early, because space is limited and filling up fast. Don’t get left out!</snippet></document><document id="219"><title>Metrics2: The New Hotness for Apache HBase Metrics</title><url>http://blog.cloudera.com/blog/2013/05/metrics2-the-new-hotness-for-apache-hbase-metrics/</url><snippet>The post below was originally published at blogs.apache.org/hbase. We re-publish it here for your convenience. Apache HBase is a distributed big data store modeled after Google�s Bigtable paper. As with all distributed systems, knowing what�s happening at a given time can help �spot problems before they arise, debug on-going issues, evaluate new usage patterns, and provide insight into capacity planning. Since October 2008, version 0.19.0 (HBASE-625), HBase has been using Apache Hadoop�s metrics system to export metrics to JMX, Ganglia, and other metrics sinks. As the code base grew, more and more metrics were added by different developers. New features got metrics. When users needed more data on issues, they added more metrics. These new metrics were not always consistently named, and some were not well documented. As HBase�s metrics system grew organically, Hadoop developers were making a new version of the Metrics system called Metrics2. In HADOOP-6728 and subsequent JIRAs, a new version of the metrics system was created. This new subsystem has a new name space, different sinks, different sources, more features, and is more complete than the old metrics. When the Metrics2 system was completed, the old system (aka Metrics1) was deprecated. With all of these things in mind, it was time to update HBase�s metrics system so HBASE-4050 was started. I also wanted to clean up the implementation cruft that had accumulated. Definitions The implementation details are pretty dense on terminology so lets make sure everything is defined: Metric: A measurement of a property in the system. Snapshot: A set of metrics at a given point in time. Metrics1: The old Apache Hadoop metrics system. Metrics2: The new overhauled Apache Hadoop Metrics system. Source: A class that exposes metrics to the Hadoop metrics system. Sink: A class that receives metrics snapshots from the Hadoop metrics system. JMX: Java Management Extension. A system built into java that facilitates the management of java processes over a network; it includes the ability to expose metrics. Dynamic Metrics: Metrics that come and go. These metrics are not all known at compile time; instead they are discovered at runtime. Implementation The Hadoop Metrics2 system implementations in branch-1 and branch-2 have diverged pretty drastically. This means that a single implementation of the code to move metrics from HBase to metrics2 sinks would not be performant or easy. As a result I created different hadoop compatibility shims and a system to load a version at runtime. This led to using ServiceLoader to create an instance of any class that touched parts of Hadoop that had changed between branch-1 and branch-2. Here is an example of how a region server could request a Hadoop 2 version of the shim for exposing metrics about the HRegionServer. (Hadoop 1�s compatibility jar is shown in dotted lines to indicate that it could be swapped in if Hadoop 1 was being used.) This system allows HBase to support both Hadoop 1.x and Hadoop 2.x implementations without using reflection or other tricks to get around differences in API, usage, and naming. Now that HBase can use either the Hadoop 1 or Hadoop 2 versions of the metrics 2 systems, I set about cleaning up what metrics HBase exposes, how those metrics are exposed, naming, and performance of gathering the data. Metrics2 uses either annotations or sources to expose metrics. Since HBase can�t require any part of the metrics2 system in the core classes I exposed all metrics from HBase by creating sources. For metrics that are known ahead of time I created wrappers around classes in the core of HBase that the metrics2 shims could interrogate for values. Here is an example on how HRegionServer�s metrics(the non-dynamic metrics) are exposed. The above pattern can be repeated to expose a great deal of the metrics that HBase has. However metrics about specific regions are still very interesting but can�t be exposed following the above pattern. So a new solution that would allow metrics about regions to be exposed by whichever HRegionServer is hosting that region was needed. To complicate things further Hadoop�s metrics2 system needs one MetricsSource to be responsible for all metrics that are going to be exposed through a JMX mbean. In order for metrics about regions to be well laid out, HBase needs a way to aggregate metrics from multiple regions into one source. This source will then be responsible for knowing what regions are assigned to the regionserver. These requirements led me to have one aggregation source that contains sudo-sources for each region. These sudo-sources each contain a wrapper around the region. This leads to something that looks like this. Benefits That�s a lot of work to re-do a previously working metrics system, so what was gained by all this work? The entire system is much easier to test in unit and systems tests. The whole system has been made more regular; that is everything follows the same patterns and naming conventions. Finally everything has been rewritten to be faster. Since the previous metrics have all been added on as needed they were not all named well. Some metrics were named following the pattern: �metricNameCount� others were named following �numMetricName� while still others were named like �metricName_Count�. This made parsing hard and gave a generally chaotic feel. After the overhaul metrics that are a counter start with the camel cased metric name followed by the suffix �Count.� The mbeans were poorly laid out. Some metrics we spread out between two mbeans. Metrics about a region were under an mbean named Dynamic, not the most descriptive name. Now mbeans are much better organized and have better descriptions. Tests have found that single threaded scans run as much as 9% faster after HBase�s old metrics system has been replaced. The previous system used lots of ConcurrentHashMap�s to store dynamic metrics. All accesses to mutate these metrics required a lookup into these large hash maps. The new system minimizes the use of maps. Instead every region or server exports metrics to one pseudo source. The only changes to hashmaps in the metrics system occurs on region close or open. Conclusion Overall the whole system is just better. The process was long and laborious, but worth it to make sure that HBase�s metrics system is in a good state. HBase 0.95, and later 0.96, will have the new metrics system. �There�s still more work to be completed but great strides have been made. Elliott Clark is a Software Engineer at Cloudera and an HBase committer.</snippet></document><document id="220"><title>Cloudera Partners and Impala: Alteryx</title><url>http://blog.cloudera.com/blog/2013/05/cloudera-partners-and-impala-alteryx/</url><snippet>Our thanks to Brian Dirking,�Director of Product Marketing for Alteryx, for the guest post below: At Alteryx we are excited about the release of Cloudera Impala. The impact on Big Data Analytics is that the ability to perform real-time queries on Apache Hadoop will provide faster access and results. This is applicable to our customers, the business users who are running analytics to get access to data, perform analytics, and then follow up with new questions. Insight doesn�t happen all at once. The ability to query and refine quickly is ultimately what will lead business users to insight. As business users need faster access to data, Alteryx provides a user friendly way to access new solutions like Impala. With Impala support in Alteryx Strategic Analytics, business users can get faster access, and can refine data queries and the corresponding analytics to get the answers they need. They can combine these results with other datasets to provide the context necessary to make the right decision, and they can do it without having to go through months of training to master programming and query languages. A great example of where Impala can have a big impact is in churn analytics. When customers leave a company or service, there are usually a few interactions prior to leaving that are the cause. In the telecom world, these interactions can be dropped calls, support queries, and rate adjustments. The interactions that lead to churn can happen over the course of just a few hours. To be able to log those events, and then have them show up in an analytics query quickly so customers can be saved, can have a huge impact on an organization. Impala enables Alteryx to iteratively analyze the fast moving data involved in churn analysis and prevention. Impala enables Alteryx to iteratively analyze the fast moving data involved in churn analysis and prevention. At Alteryx, we recognize that organizations not only need to scale technology to address Big Data, they need to scale human capabilities. That is why Alteryx Strategic Analytics provides an easy to use drag-and-drop interface for business users. Subject matter experts and business analysts can quickly build analytics workflows that gather, cleanse, and blend datasets; enrich them with third party data; and then run sophisticated statistical, predictive, or geo-spatial analytics. By giving business users the access to query, analyze, and refine quickly, the analysis takes place at the business user level, where the business impact is understood. Then by having business users run the analytics, it enables the organization to scale. With Impala support, Alteryx enables business users to benefit from huge innovations in the Big Data market. As the market matures and more ways of accessing data become available, Alteryx provides an easy interface that enables users to benefit from the power of these innovations, while shielding them from the complexity. This makes users more productive, and able to focus on getting the answers they need to make better decisions faster. For more information about the Alteryx integration with Cloudera, visit www.alteryx.com/cloudera.</snippet></document><document id="221"><title>Extending the Data Warehouse with Hadoop</title><url>http://blog.cloudera.com/blog/2013/05/extending-the-data-warehouse-with-hadoop/</url><snippet>“Are data warehouses becoming victims of their own success?”, Tony Baer asks in�a�recent blog post: “While SQL platforms have steadily increased scale and performance (it�s easy to forget that 30 years ago, conventional wisdom was that they would never scale to support enterprise OLTP systems), the legwork of operating data warehouses is becoming a source of bottlenecks. Data warehouses and transactional systems have traditionally been kept apart because their workloads significantly differed; they were typically kept at arm�s length with separate staging servers in the middle tier, where ETL operations were performed. Yet, surging data volumes are breaking this pattern. With growing data volumes has come an emerging pattern where data and processing are brought together on the same platform. The �ELT� pattern was thus born based on the notion that collocating transformation operations inside the data warehouse would be more efficient as it would reduce data movements. The downside of ELT, however, is that data transformation compute cycles compete for finite resource with analytics.” And this competition for resources is only getting worse as data volumes grow and more users demand access to business information. Data warehouses become saturated, critical workloads back up, SLAs are missed, BI queries take longer, and the high-end analytic databases are effectively unable to take on new high-value analytic workloads, being consumed with batch processing. The result? A constrained user experience, little room for new projects, and an expensive expansion upgrade path. Until recently there hasn�t been a cost-effective solution to these problems. But today, a wide range of customers are using open source Apache Hadoop to�rationalize and complement their existing data warehouses�to reduce costs, improve performance, and enable new insights. Yet as Tony points out, Hadoop’s value has extended beyond just affordable, scalable storage and processing: “[Hadoop] presents a lower cost target for shifting transform compute cycles. More importantly, it adds new options for analytic processing. With SQL and Hadoop converging, there are new paths for SQL developers to access data in Hadoop without having to learn MapReduce. These capabilities will not eliminate SQL querying to your existing data warehouse, as such platforms are well-suited for routine queries (with many of them carrying their own embedded specialized functions). But they supplement them by providing the opportunity to conduct exploratory querying that rounds out the picture and provides the opportunity to test drive new analytics before populating them to the primary data warehouse.” Cloudera believes that the future of Hadoop is as a�Platform for Big Data�that will complement, not replace, existing data management systems, enabling new ways of interacting with large and diverse data sets.�Last week, for example, Cloudera announced the�general availability of Cloudera Impala, the industry�s first and only open source interactive SQL framework for the Hadoop platform. Through innovations like Impala, Hadoop presents exciting new opportunities for the enterprise. Want to hear more?�Join us for a webinar on May 9 with Tony Baer, Principal Analyst at Ovum, and get insights from the recently published whitepaper, Hadoop: Extending Your Data Warehouse. Upon registration you will get access to the whitepaper. (5/12/2013 update: This webinar has lapsed, but you can watch the replay here.)</snippet></document><document id="222"><title>Cloudera Development Kit (CDK): Hadoop Application Development Made Easier</title><url>http://blog.cloudera.com/blog/2013/05/cloudera-development-kit-cdk/</url><snippet>Editor’s Note (Dec. 11, 2013): As of Dec. 2013, the Cloudera Development Kit is now known as the Kite SDK. Links below are updated accordingly. At Cloudera, we have the privilege of helping thousands of developers learn Apache Hadoop, as well as build and deploy systems and applications on top of Hadoop. While we (and many of you) believe that platform is fast becoming a staple system in the data center, we’re also acutely aware of its complexities. In fact, this is the entire motivation behind Cloudera Manager: to make the Hadoop platform easy for operations staff to deploy and manage. So, we�ve made Hadoop much easier to �consume� for admins and other operators — but what about for developers, whether working for ISVs, SIs, or users? Until now, they�ve largely been on their own. That�s why we’re really excited to announce the Cloudera Developer Kit (CDK), a new open source project designed to help developers get up and running to build applications on CDH, Cloudera�s open source distribution including Hadoop, faster and easier than before. The CDK is a collection of libraries, tools, examples, and documentation engineered to simplify the most common tasks when working with the platform. Just like CDH, the CDK is 100% free, open source, and licensed under the same permissive Apache License v2, so you can use the code any way you choose in your existing commercial code base or open source project. The CDK lives on GitHub where users can freely browse, download, fork, and contribute back to the source. Community contributions are not only welcome but strongly encouraged. Since most Java developers use tools such as Maven (or tools that are compatible with Maven repositories), artifacts are also available from the Cloudera Maven Repository�for easy project integration. The CDK is a collection of libraries, tools, examples, and docs engineered to simplify common tasks. What’s In There Today Our goal is to release a number of CDK modules over time. The first module that can be found in the current release is the CDK Data module; a set of APIs to drastically simplify working with datasets in Hadoop filesystems such as HDFS and the local filesystem. The Data module handles automatic serialization and deserialization of Java POJOs as well as Avro Records, automatic compression, file and directory layout and management, automatic partitioning based on configurable functions, and a metadata provider plugin interface to integrate with centralized metadata management systems (including HCatalog). All Data APIs are fully documented with javadoc. A reference guide is available to walk you through the important parts of the module, as well. Additionally, a set of examples is provided to help you see the APIs in action immediately. The current version of the CDK is 0.2.0, with maintenance releases rolling out monthly, so you should expect rapid evolution as we build toward a 1.0.0 release. What you see today is just the tip of the iceberg — a framework and long-term initiative for bringing more codified best practices, docs, examples, and APIs to developers. To get a jump-start, take a look at the CDK Data module javadoc. What You Can Expect Features and functionality driven by the collective experience and requirements of Cloudera’s users and partners, as well as its own solution architects A fast path to get up and running for the most common use cases Frequent releases with new features, bug fixes, and your contributions Docs, examples, and guides for all modules Well-defined API compatibility guarantees for public APIs All open source, all Apache License v2, all the time Quick FAQ Can I contribute to the CDK? Yes, please! As explained above, we welcome and encourage contributions to the CDK, and look forward to your pull requests. On the other side of the coin, feel free to fork and modify the CDK for your own purposes, if that�s your desire. Where do I go for CDK discussion, questions, and help? For now, we’re going to direct all CDK discussion to the cdk-dev@cloudera.org�discussion group. If you�re not a member, please join the group at https://groups.google.com/a/cloudera.org/d/forum/cdk-dev. Where do I file bugs and feature requests? There’s a dedicated public JIRA�project for the CDK. Where can I see the road map? The best place to look is the road map view in the CDK JIRA project. All work we do will have public JIRAs so you can see what’s coming and participate. Happy hacking! Eric Sammer is an Engineering Manager at Cloudera and a CDK project co-lead. He is also a Committer/PMC Member on the Apache Flume and Apache MRUnit projects and the author of the O�Reilly book, Hadoop Operations. Tom White is a Software Engineer at Cloudera and a CDK project co-lead. He is also a Committer/PMC Member on the Apache Avro, Apache Hadoop, and Apache Whirr projects and the author of the O�Reilly book, Hadoop: The Definitive Guide. To learn more about the CDK, register for this webinar with Eric Sammer airing on May 21, 2013.</snippet></document><document id="223"><title>Cloudera Impala and Partners: Tableau</title><url>http://blog.cloudera.com/blog/2013/05/cloudera-impala-and-partners-tableau/</url><snippet>Our thanks to Ted Wasserman, product manager for Tableau, for the guest post below: Many of our customers are turning to Apache Hadoop as they grapple with their big data challenges. Hadoop offers many benefits such as its scalability, economics, and versatility. Even so, adoption-to-date has largely centered around applications with “batch”-oriented workloads because of the latency imposed by the MapReduce framework. To increase Hadoop�s usefulness and adoption in the business intelligence space where users need fast, interactive response times when they ask a question, a new approach was needed. Cloudera�Impala technology moves the ball forward for doing ad hoc visual analytics on Hadoop. In particular, we like Impala for several reasons: Performance. Analysts can now have an interactive conversation with their Hadoop data and stay focused in the cycle of visual analysis. They no longer have to wait minutes or hours for most queries to return results. Reusability. Impala leverages a lot of the Hadoop infrastructure already in place today. By using the existing Apache Hive meta store and Hive SQL language, the development effort required to support Impala is reduced, and a broad ecosystem of customer applications and partners that already have integrations with Hive can more easily support Impala. Innovation. Cloudera is a thought leader in the Hadoop space, and is helping move Hadoop forward with innovative technologies and new approaches like Impala. We value our close working relationship with the Cloudera development and product teams. They have the same dedication to customer success as we do, which results in a market-leading combined solution. Tableau has developed a native connector to Impala, which makes it easy for any business user to quickly connect to their Hadoop cluster and begin doing analytics using Tableau�s visual drag-and-drop interface. Simply install the Cloudera ODBC driver and Tableau, connect to the cluster, and away you go! No Java or MapReduce programming required. Tableau takes care of generating the optimal queries needed to get the data out of Hadoop for the analysis and render it on the screen in the form of beautiful and rich data visulizations. We�ve put together a short video demonstrating Tableau�s native Impala connector. If you like what you see, try it on your own data. Download a free trial copy of Tableau and experience first hand how easy it is to get started and quickly build value around your Hadoop data.</snippet></document><document id="224"><title>Customer Spotlight: Sneak Peek into Skybox Imaging�s Cloudera-powered Satellite System</title><url>http://blog.cloudera.com/blog/2013/05/customer-spotlight-sneak-peek-into-skybox-imagings-cloudera-powered-satellite-system/</url><snippet>This week, the Cloudera Sessions head to Washington, DC, and Columbus, Ohio, where attendees will hear from AOL, Explorys, and Skybox Imaging about the ways Apache Hadoop can be used to optimize digital content, to improve the delivery of healthcare, and to generate high-resolution images of the entire globe that provide value to retailers, farmers, government organizations and more. I�d like to take this opportunity to shine a spotlight on Skybox Imaging, an innovative company that is putting Hadoop to work to help us see the world more clearly, literally. Skybox�s vice president of ground software, Ollie Guinan, recently posted a guest blog to Cloudera.com to give readers a glimpse into their Hadoop use case, which I�d like to promote again here. I would encourage anyone in the DC area to meet Ollie (who is also a Champion of Big Data) in person at the Cloudera Sessions event in DC this Tuesday to learn more about Skybox and its fascinating use case. Want to learn more? Read Oliver Guinan�s blog Attend the Cloudera Sessions in DC this week to see Ollie speak View Ollie�s Hadoop World 2011 presentation Karina Babcock is Cloudera�s Customer Programs &amp; Marketing Manager.</snippet></document><document id="225"><title>Cloudera Partners and Impala: Talend</title><url>http://blog.cloudera.com/blog/2013/05/cloudera-partners-and-impala-talend/</url><snippet>Our thanks to Yves de Montcheuil,�Vice President of Marketing for Talend, for the guest post below: According to�Wikipedia, the impala is a medium-sized African antelope; its name comes from the Zulu language meaning “gazelle”. Like elephants, it is found in savannas, and this may be the link with Hadoop.�Impala�is also the name of�Cloudera�s SQL-on-Apache Hadoop project, launched in beta at Strata last October and just released in version 1.0. SQL-on-Hadoop � wait a minute� isn�t it what Apache Hive is for?�Well, yes and no. HiveQL certainly brings a set of SQL-like commands to Hadoop data. The big issue with Hive: it�s very slow. More precisely, it�s not interactive. Queries take a long time to be �parsed� and distributed across the cluster. Response times can reach the minute, which is highly impractical for interactive use. It works fine for batch use (response times actually don�t vary much based on the dataset size), but when users want to mine Hadoop data, perform interactive queries or drill-downs, profile data, etc. � they end up spending lots of time glaring at their screen (or fetching more coffee than they should). As use cases for Hadoop evolve past batch requirements,�often for �mundane� tasks�such ETL offload or online archiving, and enterprises discover the value of real-time data exploration, mining and analytics on big data, interactive performance becomes a must. Since Impala is native to Hadoop, it provides access to the same data sets that have already been loaded. A great example of data exploration is data profiling. Since v5.2, Talend has been providing native data profiling on Hadoop.�Based on Hive, profiling is performed �in place�, which means that data does not need to be extracted from Hadoop before being profiled. The issue here is the time it takes to instantiate the profiling job on MapReduce � not very practical for interactive profiling even though it works well for batch. Impala will provide the potential to speed up this process and make it more efficient. And since Impala is native to Hadoop, it provides access to the same data sets that have already been loaded � no need to replicate/duplicate the data. Talend and Cloudera have been partners for a long time. We started to support Hadoop in its infancy, well before all the hype started � and that puts us in a unique situation to leverage new and upcoming technologies. Developments such as Impala are clearly providing value and we look forward to more exciting news from the broad Hadoop ecosystem! Yves PS: Am I jumping to conclusions as to why the Impala name was picked, with the reference to antelopes and elephants? Comments from engineers/product managers working on the project are welcome!</snippet></document><document id="226"><title>Cloudera Partners and Impala: MicroStrategy</title><url>http://blog.cloudera.com/blog/2013/05/cloudera-partners-and-impala-microstrategy/</url><snippet>Our thanks to Kevin Spurway,�Senior Vice President of Marketing for MicroStrategy Inc., for the guest post below: Squeezing insight from Big Data isn�t easy. It�s a delicate balance between scalability, performance, and cost effectiveness across an entire architecture, spanning everything from data storage to mobile app consumption. That�s why MicroStrategy and Cloudera have been working closely together from a technology standpoint. And, that�s why we�re proud to stand as a launch partner, certifying the integration between Cloudera�s new Impala project and our core MicroStrategy enterprise analytics platform. Impala is a giant step toward an era of highly cost-effective interactive analytics for Hadoop-based Big Data. We�ve been collaborating with Cloudera on Impala since its early stages, actively testing functionality, recommending enhancements, reviewing roadmaps, and sharing performance results. We�re especially enthusiastic because we see the launch of Impala as a giant step toward an era of highly cost-effective interactive analytics for Apache Hadoop-based Big Data, at speeds previously not possible.� We�ve been doing our part to optimize our architecture to take advantage of the innovation in Impala. Working with Cloudera, we�ve built specialized Very Large Database (VLDB) Drivers specifically for Cloudera Impala and CDH that generate optimized queries with incredible response times. Between Impala and our new optimized connectors, we�re seeing a significant query performance improvement over previous unaccelerated Hadoop distributions. These performance gains take Hadoop to the next level�in terms of analytical price/performance. We think it�s crossing an inflection point that makes cost-effective analytics on massive data more affordable for everyone. It�s an exciting time. Now, we can enable any business user to grab data on-the-fly from Hadoop, bring it into our in-memory data structure for hyperdrive performance, and run interactive visual analytics. That�s revolutionary. But it�s only the beginning. We can envision use cases where we�re using MicroStrategy to combine structured and unstructured data from Hadoop with data from enterprise data warehouses, in SaaS-based applications, or other data sources. Then build powerful analytic apps, and deploy them broadly to directly impact business processes with relevant data and better decision making. Buckle your seatbelts. The operationalization of Big Data analytics is about to go mainstream.</snippet></document><document id="227"><title>Customer Spotlight: Six3 Systems� Wayne Wheeles Drives Cyber Security Innovation using Impala</title><url>http://blog.cloudera.com/blog/2013/05/customer-spotlight-six3-systems-wayne-wheeles-drives-cyber-security-innovation-using-impala/</url><snippet>This week represents quite a milestone for Cloudera and, at least we�d like to believe, the Hadoop ecosystem at large: the general availability release of Cloudera Impala. Since we launched the Impala beta program last fall, I�ve been fortunate enough to work with many of the 40+ early adopters who�ve been testing this near-real-time SQL-on-Hadoop engine in an effort to learn about their use cases and keep tabs on early experiences with the tool. Customers running Impala today span a variety of industries, from large biotech company to online travel provider to digital advertiser to major financial institution, and each one has a unique use case for Impala. Stay tuned to learn more about their various use cases. This week, I�d like to highlight Six3 Systems� Wayne Wheeles (also a Champion of Big Data), who has been working with Impala to improve cyber security solutions, in particular the open source SherpaSurfing product. Here�s a blog that Wayne Wheeles posted shortly after testing the Impala beta release, revealing his performance results and perceived value. Expect a follow-up blog from Wayne in the near future! Want to learn more? Download SherpaSurfing from GitHub View Wayne�s Hadoop World 2011 presentation “SherpaSurfing: Open Source Cyber Security Solution” We�d love to hear from readers about how you�re thinking of deploying Impala in your Hadoop environment. Please feel free to share your comments here. Karina Babcock is Cloudera’s Customer Programs &amp; Marketing Manager.</snippet></document><document id="228"><title>How the SAS and Cloudera Platforms Work Together</title><url>http://blog.cloudera.com/blog/2013/05/how-the-sas-and-cloudera-platforms-work-together/</url><snippet>On Monday April 29, Cloudera announced a strategic alliance with SAS. As the industry leader in business analytics software, SAS brings a formidable toolset to bear on the problem of extracting business value from large volumes of data. Over the past few months, Cloudera has been hard at work along with the SAS team to integrate a number of SAS products with Apache Hadoop, delivering the ability for our customers to use these tools in their interaction with data on the Cloudera platform. In this post, we will delve into the major mechanisms that are available for connecting SAS to CDH, Cloudera�s 100% open-source distribution including Hadoop. SAS/ACCESS to Hadoop SAS/ACCESS provides the ability to access data sets stored in Hadoop in SAS natively. With SAS/Access to Hadoop: LIBNAME statements can be used to make Hive tables look like SAS data sets on top of which SAS Procedures and SAS DATA steps can interact. PROC SQL commands provide the ability to execute direct Hive SQL commands on Hadoop. PROC HADOOP provides the ability to directly submit MapReduce, Apache Pig, and HDFS commands from the SAS execution environment to your CDH cluster. The SAS/ACCESS interface is available from the SAS 9.3M2 release and supports CDH 3U2 as well as CDH 4.01 and higher. SAS/ACCESS enables users familiar with the SAS interface to operate seamlessly on data stored in Hadoop while bringing the power of SAS to these data sets. SAS High Performance Analytics (HPA) SAS HPA brings the ability to create analytical models using entire data sets, without down-sampling, while quickly iterating over multiple models to find the right solution for the problem. SAS HPA is designed to provide blazing-fast response while iterating on models that have been implemented from the ground up to be parallelizable. Built on a distributed, in-memory architecture that scales with cluster size, SAS HPA is a perfect fit for the Cloudera system architecture: SAS agents are deployed on each node of the CDH cluster. SAS agents, as required by users to perform analytics, load data sets into memory from the HDFS filesystem. Once data is loaded, the agents communicate with each other to execute analytical queries on the data and return results straight from memory. As new nodes come online, SAS agents can take advantage of additional resources available in the cluster. SAS HPA can operate in parallel with MapReduce and Cloudera Impala to provide another powerful computational framework that can operate on data stored in CDH. SAS HPA includes support for high-performance statistical methods, data-mining operations, econometric models, text mining, optimization techniques, and many more types of models. A full list of supported features is available here. SAS HPA is supported on CDH 4.01 and higher. SAS Visual Analytics SAS Visual Analytics provides rich data visualization capabilities and sophisticated analytic techniques to end consumers directly � whether a business user with limited technical skills or a data scientist. Flexible and sophisticated reporting, forecasting, and charting on all your data can be generated in seconds using in-memory computation capabilities of SAS Visual Analytics. SAS Visual Analytics is built on the SAS LASR Analytics Server, a high-performance in-memory engine that can leverage the capabilities of a CDH cluster: SAS LASR daemons run on each node of your Hadoop cluster. Based on administrative policies, data is loaded into LASR daemons from the HDFS filesystem. As users log in to perform new analyses, the analytics engine distributes computation across the nodes, which generate results and return the results to the visualization layer for presentation. As new nodes come online, SAS LASR daemons can take advantage of the resources available on these nodes as well. SAS Visual Analytics is supported on CDH 4.01 and higher. SAS Data Management Advanced SAS Data Management Advanced includes support for CDH as a data source or data target for Data Integration Studio. Besides standard transforms for data on Hadoop, the Studio also provides ability to integrate Apache Hive, Pig, MapReduce, and HDFS commands as part of a data flow. SAS DataFlux tools can be used on data in HDFS since CDH is treated as another data source by SAS. SAS Metadata Server can be used to record metadata based on data in CDH. Lineage tools are also available within SAS Data Management Advanced so that all SAS processing that is done on Cloudera platforms can also be tracked. The combination of the capabilities present in SAS Data Management advanced makes CDH easy to integrate as a data source or sink in a SAS management environment with minimal disruption, and makes the capabilities of the SAS Data Management suite available to data on Hadoop as well. Overview of SAS Analytics on Cloudera SAS and Cloudera Looking Forward In summary, SAS provides a rich and familiar set of data analytics tools that are compelling to users who wish to take advantage of the storage and computational capabilities unlocked by a Hadoop cluster. New products such as SAS HPA and SAS Visual Analytics provide unparalleled performance built on the highly scalable architecture of CDH, providing quicker analysis and insight into your data for solving crucial business problems. While we�ve made great progress in enabling support for SAS on our platform, Cloudera and SAS continue to work closely together to develop richer integrations that combine the power of Hadoop with the capabilities of the SAS product suite. Stay tuned for more updates in the coming months! Jairam Ranganathan is a product director at Cloudera.</snippet></document><document id="229"><title>Cloudera Impala 1.0: It’s Here, It’s Real, It�s Already the Standard for SQL on Hadoop</title><url>http://blog.cloudera.com/blog/2013/05/cloudera-impala-1-0-its-here-its-real-its-already-the-standard-for-sql-on-hadoop/</url><snippet>In October 2012, we introduced the Impala project, at that time the first known effort to bring a modern, open source, distributed SQL query engine to Apache Hadoop. Our release of source code�and a beta implementation were met with widespread acclaim — and later inspired similar efforts across the industry that now measure themselves against the Impala standard. Today, we are proud to announce the first production drop of Impala (download here), which reflects feedback from across the user community based on multiple types of real-world workloads. Just as a refresher, the main design principle behind Impala is complete integration with the Hadoop platform (jointly utilizing a single pool of storage, metadata model, security framework, and set of system resources). This integration allows Impala users to take advantage of the time-tested cost, flexibility, and scale advantages of Hadoop for interactive SQL queries, and makes SQL a first-class Hadoop citizen alongside MapReduce and other frameworks. The net result is that all your data becomes available for interactive analysis simultaneously with all other types of processing, with no ETL delays needed. Although the features and performance results described below are impressive, it�s important to note that they represent only a down payment toward the full promise of Impala. There is much more to come — and soon. Features in Impala 1.0 First, a brief summary of features (see release notes for full detail). In combination with the design principles described above, they deliver on all requirements for a SQL-on-Hadoop platform: local processing to avoid networking bottlenecks, interactive response time, a single pool of native data, and the freedom to do different kinds of processing on the same data at the same time: Support for a subset of ANSI-92 SQL (compatible with Hive SQL), including CREATE, ALTER, SELECT, INSERT, JOIN, and subqueries Support for partitioned joins, fully distributed aggregations, and fully distributed top-n queries Support for a variety of data formats: Hadoop native (Apache Avro, SequenceFile, RCFile with Snappy, GZIP, BZIP, or uncompressed); text (uncompressed or LZO-compressed); and Parquet (Snappy or uncompressed), the new state-of-the-art columnar storage format Support for all CDH4 64-bit packages: RHEL 6.2/5.7, Ubuntu, Debian, SLES Connectivity via JDBC, ODBC, Hue GUI, or command-line shell Kerberos authentication and MR/Impala resource isolation Current State of Performance Much effort has gone into improving performance over the beta release. But before we offer an overview of benchmark numbers, we want to explain how performance testing was done to ensure a realistic preview of real-world use cases. Because doing BI and analytics often involves running a set of different queries to generate a report, our primary emphasis for measuring performance is to use a diverse set of real-world customer queries against files in their native Hadoop file formats — not cherry-picked queries against pre-loaded specialized file formats, as we’ve seen elsewhere. Furthermore, to measure the full spectrum of performance across the platform, we quantified stand-alone performance as well as the multi-tenant performance of Impala queries and other processing jobs working concurrently. Finally, we ultimately relied on beta customers and the broader community to validate the results even more widely with their queries in their own environments. We believe that any approach other than that above will not provide meaningful results. (In fact, they will be quite misleading.) Some other important facts about the testing process: Where Impala and Hive/MapReduce are compared for single-user results, both sets of queries operated against precisely the same Snappy-compressed SequenceFile in HDFS. The fact table contains 5 years of data totaling over 1TB. Queries were diverse in date range (1 month to 5 years) and amount of latency (and categorized into Interactive Exploration, Reports, and Deep Analytics buckets). Queries involved a variety of fairly standard joins (from one to seven in number) and aggregations as well as complex multi-level aggregations and inline views. This query set is one among several from customers that we run regularly against various native file formats. Here are the results (in seconds) for single-user query response on a 20-node cluster, bucketed by type and expressed as a geometric mean across those buckets (geometric mean being our preferred approach over arithmetic mean because response times vary by query): Impala 1.0 versus Hive: Query response time (geometric mean, by category) And the results above viewed through a �Times Faster Than Hive� lens (expressed as ranges): Now, let’s look at how Impala achieves better-than-linear scale as we add more concurrent clients:� Impala 1.0: Multi-tenant query response times (geometric mean, by category) Note from the above that even with a 24x increase in the number of concurrent clients running, performance is still faster than single-user results for Hive! (Note: Concurrency is an important subject, so we will provide in-depth benchmarking results for this area in a future post.) The above proves out that Impala, unlike Hive, is suitable for modern BI-scale environments (in which many users are running different query types concurrently) — and with Impala, as you increase the number of nodes, you see performance similarly increase. Although we�re proud of these results, we also consider them to be only scratching the surface of what Impala will do when all the benefits of Parquet (currently available in preview form) and full multi-threaded execution are brought to bear over the next two releases. The Road Ahead for Impala As you can deduce from the above, Impala 1.0 offers significant performance improvements over MapReduce/Hive for a wide range of BI/analytic queries, making BI over Hadoop feasible. And, thanks to its complete integration with Hadoop, Impala also offers that platform�s familiar flexibility and TCO advantages over remote-query approaches and siloed DBMS/Hadoop hybrids – making costly redundant infrastructure unnecessary. As we reach new milestones along the roadmap, Impala will move toward achieving the ultimate goal: allowing users to store all their data in the same flexible, open, and native Hadoop file formats and simultaneously run all their batch MapReduce, machine learning, interactive SQL/BI, math (e.g., SAS), and other jobs on the same data. We look forward to your continuing feedback as Impala travels in that direction! Additional resources: ��Impala FAQ � Impala 1.0 source code � Impala 1.0 downloads (binaries and inside the QuickStart VM) � Installing Impala using Cloudera Manager Free Edition � Impala 1.0 documentation � Public JIRA � Impala mailing list � Cloudera RTQ Support Subscription Marcel Kornacker is the architect of Impala. Prior to joining Cloudera, he was the lead developer for the query engine of Google�s F1 project. Justin Erickson is the product manager for Impala.</snippet></document><document id="230"><title>The Platform for Big Data is Here</title><url>http://blog.cloudera.com/blog/2013/04/platform-for-big-data-is-here/</url><snippet>It has been an exciting couple of days for new product announcements at Cloudera — exciting especially for me as the edges of the new platform for big data we have been talking about since Strata + Hadoop World 2012 come into focus. Yesterday, Cloudera announced a strategic alliance with SAS. SAS is the industry leader in business analytics software, especially predictive analytics. Ninety percent of the Fortune 100 run SAS today. We have been working with SAS to make a number of its products work well with Cloudera including SAS Access, SAS Visual Analytics, and SAS High Performance Analytics (HPA). SAS HPA is an excellent case example of the future direction of Apache Hadoop as a data management platform: Hadoop is a big opportunity for the data science user: no downsampling, unlimited model features, and freedom from the inflexibility of third normal form. MapReduce-based data science has been useful to a point but is�limited. Most data science users are familiar with SAS, not MapReduce, and many popular machine learning�algorithms�simply cannot be implemented in MapReduce. SAS HPA runs natively on CDH, Cloudera’s Distribution Including Apache Hadoop. It leverages the same data on the same cluster that the MapReduce and SQL users use. It adheres to the same security model. This is a win for the SAS users who might have previously felt alienated from this new Hadoop world, and it’s a win for Hadoop users who can get more value out of the repository of data growing in their clusters. Today Cloudera is pleased to announce the general availability of Cloudera Impala 1.0: the industry’s first and only open source interactive SQL framework for the Hadoop platform. Since we announced the public beta in October 2012, Impala has made impressive strides. The product has advanced in functionality, quality, and performance. The third-party developer community has made�significant�new contributions. The user community has grown at a torrential pace. We’ve also received gratifying positive feedback from the analyst community. GigaOm Pro recently determined that Impala was the industry’s leading SQL-on-Hadoop offering. Impala is another excellent proof point of the future of the platform for big data: Hadoop is a big opportunity for the SQL user: explore structured data at full fidelity�granularity, take advantage of Hadoop’s flexible schema to easily experiment with new data sets. MapReduce-based SQL has been useful to a point but is limited. Most SQL users have been weaned on business intelligence tools that expect interactive SQL, not batch SQL, from the underlying engine. Also, things that were simple in a database like “cancel query” are not available in a MapReduce�paradigm. Impala runs natively on CDH. It leverages the same data on the same cluster that the MapReduce and SAS users use. It adheres to the same�security�model. It works within the same management framework. It uses the same schema and metadata catalog so objects don’t need to be ETL’ed into Impala for use. This is a win for SQL users. Many of the most popular business intelligence tools have been tested to run on Impala, and we’ve been gratified to get great feedback on quality of experience from our BI partners. It’s also a win for the Hadoop users who can get more value out of the same repository of data. Over the course of the next week we’ll be adding blog posts that flesh out the�technical�details of these two frameworks and how they can be used. Some of our partners will be doing the same. I want to emphasize the�significance�of these developments for customers, users, and partners everywhere. Today we have a scalable, flexible, 100% open source data management platform that lets users bring batch processing, interactive SQL, and math applications to a common repository of data running on industry-standard hardware. These frameworks are truly integrated parts of a larger data management platform with no costly specialized hardware or elaborate integration and data replication frameworks. Hadoop’s versatility has become more important than its scalability and low cost as the principal reason for its growing popularity. The diversity of workloads and applications now available on Hadoop are broader than those of the legacy data management technologies that most organizations run today. Organizations will continue to use various other data management technologies besides Hadoop to take advantage of their unique strengths. Cloudera will continue to maintain�excellent�integration�to all of them. Still, for leading-edge organizations, we see that Hadoop is increasingly becoming their central strategic platform. This is what we mean by The Platform for Big Data. To learn more, read the Cloudera white paper, “The Platform for Big Data”. Charles Zedlewski is Cloudera’s VP, Products.</snippet></document><document id="231"><title>What’s New in Hue 2.3</title><url>http://blog.cloudera.com/blog/2013/04/whats-new-in-hue-2-3/</url><snippet>We’re very happy to announce the 2.3 release of Hue, the open source Web UI�that makes Apache Hadoop easier to use. Hue 2.3 comes only two months after 2.2 but contains more than 100 improvements and fixes.�In particular, two new apps were added (including an Apache Pig editor) and the query editors are now easier to use. Here’s a video demoing the major changes: Here’s the new features list: Pig Editor: new application for editing and running Apache Pig scripts with UDFs and parameters Table Browser: new application for managing Apache Hive databases, viewing table schemas and sample of content Apache Oozie Bundles are now supported SQL highlighting and auto-completion for Hive/Impala apps Multi-query and highlight/run a portion of a query Job Designer was totally restyled and now supports all Oozie actions Oracle databases (11.2 and later) are now supported We would like to thank everybody who worked on this release. New features and feedback are continuously being integrated!  </snippet></document><document id="232"><title>How Scaling Really Works in Apache HBase</title><url>http://blog.cloudera.com/blog/2013/04/how-scaling-really-works-in-apache-hbase/</url><snippet>This post was originally published via blogs.apache.org, we republish it here in a slightly modified form for your convenience: At first glance, the Apache HBase architecture appears to follow a master/slave model where the master receives all the requests but the real work is done by the slaves. This is not actually the case, and in this article I will describe what tasks are in fact handled by the master and the slaves. Regions and Region Servers HBase is the Hadoop storage manager that provides low-latency random reads and writes on top of HDFS, and it can handle petabytes of data. One of the interesting capabilities in HBase is auto-sharding, which simply means that tables are dynamically distributed by the system when they become too large. The basic unit of horizontal scalability in HBase is called a Region. Regions are a subset of the table�s data and they are essentially a contiguous, sorted range of rows that are stored together. Initially, there is only one region for a table.�As shown below, when regions become too large after adding more rows, the region is split into two at the middle key, creating two roughly equal halves. In HBase the slaves are called Region Servers. Each Region Server is responsible to serve a set of regions, and one Region (i.e. range of rows) can be served only by one Region Server. The HBase architecture has two main services: HMaster that is responsible to coordinate the cluster and execute administrative operations, and the HRegionServer responsible for handling a subset of the table�s data. HMaster, Region Assignment, and Balancing As previously mentioned, the HBase Master coordinates the HBase Cluster and is responsible for administrative operations. A Region Server can serve one or more Regions. Each Region is assigned to a Region Server on startup and the master can decide to move a Region from one Region Server to another as the result of a load balance operation. The Master also handles Region Server failures by assigning the region to another Region Server. The mapping of Regions and Region Servers is kept in a system table called META. By reading META, you can identify which region is responsible for your key. This means that for read and write operations, the master is not involved at all and clients can go directly to the Region Server responsible to serve the requested data. Locating a Row-Key: Which Region Server is Responsible? To put or get a row clients don�t have to contact the master, clients can directly contact the Region Server that handles the specified row, or in case of a client scan, can directly contact the set of Region Servers responsible for handling the set of keys: To identify the Region Server, the client does a query on the META table. META is a system table used to keep track of regions. It contains the server name and a region identifier comprising a table name and the start row-key. By looking at the start-key and the next region start-key clients are able to identify the range of rows contained in a a particular region. The client keeps a cache for the region locations. This avoids clients to hit the META table every time an operation on the same region is issued. In case of a region split or move to another Region Server (due to balancing, or assignment policies), the client will receive an exception as response and the cache will be refreshed by fetching the updated information from the META table: Since META is a table like the others, the client has to identify on which server META is located. The META locations are stored in a ZooKeeper node on assignment by the Master, and the client reads directly the node to get the address of the Region Server that contains META. HBase’s original design was based on BigTable, with another table called -ROOT- containing the META locations and Apache ZooKeeper pointing to it. HBase 0.96 removed that arrangement in favor of ZooKeeper only, since META cannot be split and therefore consists of a single region. Client API: Master and Regions Responsibilities The HBase Java client API has two main interfaces: HBaseAdmin allows interaction with the �table schema” by creating/deleting/modifying tables, and it allows interaction with the cluster by assigning/unassigning regions, merging regions together, calling for a flush, and so on. This interface communicates with the Master. HTable allows the client to manipulate the data of a specified table by using get, put, delete, and all the other data operations. This interface communicates directly with the Region Servers responsible for handling the requested set of keys. Those two interfaces have separate responsibilities: HBaseAdmin is only used to execute admin operations and communicate with the Master while the HTable is used to manipulate data and communicate with the Regions. Conclusion As we�ve seen here, having a Master/Slave architecture does not mean that each operation goes through the master. To read and write data, the HBase client, in fact, goes directly to the specific Region Server responsible for handling the row keys for all the data operations (HTable). The Master is used by the client only for table creation, modification, and deletion operations (HBaseAdmin). Although the a concept of a Master exists, the HBase client does not depend on it for data operations and the cluster can keep serving data even if the master goes down. Matteo Bertozzi is Software Engineer on the Platform team and an HBase Committer. If you’re interested in HBase, be sure to register for HBaseCon 2013 (June 13, San Francisco) – THE community event for HBase contributors, developers, admins, and users. Space is limited!</snippet></document><document id="233"><title>Meet the Project Founder: Doug Cutting (First in a Series)</title><url>http://blog.cloudera.com/blog/2013/04/meet-the-project-founder-doug-cutting-first-in-a-series/</url><snippet>At Cloudera, there is a long and proud tradition of employees creating new open source projects intended to help fill gaps in platform functionality (in addition to hiring new employees who have done so in the past). In fact, more than a dozen ecosystem projects — including Apache Hadoop itself — were founded by Clouderans, more than can be attributed to employees of any other single company. Cloudera was also the first vendor to ship most of those projects as enterprise-ready bits inside its platform. We thought you might be interested in meeting some of them over the next few months, in a new �Meet the Project Founder� series. It’s only appropriate that we begin with Doug Cutting himself � Cloudera�s chief architect and the quadruple-threat founder of Apache Lucene, Apache Nutch, Apache Hadoop, and Apache Avro. What led you to your project idea(s)? I wrote Lucene initially in 1997 when I had an idea for a different way to implement a text indexing and search library. (I�d written three of them previously.) My day job didn�t need it, so I wrote it on my own time and open-sourced it a few years later. Founders are rarely the future of open source projects; recruiting new contributors is their lifeblood. I started Nutch at the instigation of Overture in 2002, which thought that an open source web search engine would be good for the world. It funded me part-time to start that project and gave me little other direction, which was awesome! In 2006, I formed Hadoop by pulling the MapReduce and distributed filesystem code out of Nutch at the request of Yahoo!, which wanted to enhance the distributed computing framework, but already had its own web crawler and search systems. In 2009, I created the Avro data serialization framework at the suggestion of Raymie Stata, then CTO of Yahoo!, to provide the �glue� that could connect efforts across different parts of Yahoo!. Today, of course, it’s a component in Cloudera’s Distribution for Apache Hadoop (CDH). Aside from doing the initial commit, what is your definition of the project founder�s role across the lifespan of the project — benevolent dictator, referee, silent partner? A founder should be like an old man hanging around � but hopefully more wise than cranky.� Since founders have been there from the start, they understand the motivations underlying the code. When someone proposes a change, a founder can often better see potential avenues for the project that are opened or closed by that change. But founders are rarely the future of open source projects; rather, recruiting new contributors is the lifeblood of those projects.�Contributions by founders typically decrease over time, so a founder needs to gently remind the new kids about the project�s past and help them make the right choices for its future. What has surprised you the most about how your projects have evolved/matured? My biggest surprise about open source software generally is just how many folks use it. When you create proprietary software, you have to work hard to get each customer. But with open source, folks just start using it. More than 90% of its users are probably people you will never hear from and who never get involved in the project. Some contributors resent such users since they�re not giving back. But if you demand something in return, then you shouldn�t be contributing to open source — it doesn�t make sense to be selfish about something you�re giving away. Furthermore, �silent� users are also a mark of a project�s success. If folks are able to download the software and use it without reporting bugs or submitting patches, that means the code works and the documentation is sufficient. That said, you need some people to get involved to create a community that develops the code. That�s usually not too hard if your software is useful. What is the major work yet to be done, from your perspective as a project founder? The direction an Apache project takes is determined by those who contribute. As one developer, my ability to determine the future of these projects is thus quite limited. Patches demonstrate a contributor’s ability as well as their self-knowledge and judgment. That said, there are areas I hope projects will grow. For Hadoop, I hope it will gain fine-grained scheduling, so that batch and interactive loads can more efficiently share resources. For Avro, I hope it will better integrate with high-level tools, so that folks can peek at Avro data files as naturally as they can text files. What is your philosophy, if you have one, for balancing quality versus quantity with respect to contributions? You need enough sustained contribution from someone to get a sense of how they work. If I�ve seen around five patches get committed in a few months without a lot of fuss, I usually feel someone is ready to become a committer. With patches, folks demonstrate not only their ability but also their self-knowledge and judgment.�If someone new to the community proposes big, fundamental changes that are not well thought out, that shows poor judgment. If they try to do something that�s beyond their level of competence, it shows poor self-knowledge. On the other hand, if they provide well-considered changes in areas they clearly understand, they�ve proven to be someone who will probably be collaborative. When they�ve repeated that process a few times, the pattern is clear. Some people confuse patch quality with patch depth. If someone makes five high-quality improvements to a trivial part of the system, they deserve to be a committer every bit as much as someone who makes five high-quality contributions to its kernel. What matters is that they know their limitations and are able to peacefully collaborate. Do you have any other advice for potential project founders? Write something that solves a problem well enough for folks to start using it. It doesn�t need to be fully optimized but it needs to be fast enough to be useful. It doesn�t need to integrate with every other system in the world, but it does need to integrate enough so that some folks can try it out. Make it easy for people to get started. APIs should be simple and intuitively named.�Documentation and examples should be sufficient so that one can get started in minutes. You also need to recruit new contributors and users. Users who get helpful responses and contributors who get constructive feedback will hang around and get more involved in the project. If you act like you don�t want their input, then you won�t get their help. Read other �Meet the Project Founders� installments: - �Roman Shaposhnik (Apache Bigtop)</snippet></document><document id="234"><title>Algorithms Every Data Scientist Should Know: Reservoir Sampling</title><url>http://blog.cloudera.com/blog/2013/04/hadoop-stratified-randosampling-algorithm/</url><snippet>Data scientists, that peculiar mix of software engineer and statistician, are notoriously difficult to�interview. One approach that I’ve used over the years is to pose a problem that requires some mixture of algorithm design and probability theory in order to come up with an answer. Here’s an example of this type of question that has been popular in Silicon Valley for a number of years:� Say you have a stream of items of large and unknown length that we can only iterate over once. Create an algorithm that randomly chooses an item from�this stream such that each item is equally likely to be selected. The first thing to do when you find yourself confronted with such a question is to stay calm. The data scientist�who is interviewing you isn’t trying to trick you by asking you to do something that is impossible. In fact,�this data scientist is desperate to hire you. She is buried under a pile of analysis requests, her ETL pipeline�is broken, and her machine learning model is failing to converge. Her only hope is to hire smart people such�as yourself to come in and help. She wants you to succeed. Remember: Stay Calm. The second thing to do is to think deeply about the question. Assume that you are talking to a good person�who has read Daniel Tunkelang’s excellent advice about interviewing data scientists. This means that�this interview question probably originated in a real problem that this data scientist has encountered in her�work. Therefore, a simple answer like, “I would put all of the items in a list and then select one at random�once the stream ended,” would be a bad thing for you to say, because it would mean that you didn’t think deeply about what would�happen if there were more items in the stream than would fit in memory (or even on disk!) on a single computer. The third thing to do is to create a simple example problem that allows you to work through what should happen�for several concrete instances of the problem. The vast majority of humans do a much better job of solving problems when they work with concrete examples instead of abstractions, so making the problem concrete can go a long way toward helping you find a solution. A Primer on Reservoir Sampling For this problem, the simplest concrete example would be a stream that only contained a single item. In this case, our algorithm�should return this single element with probability 1. Now let’s try a slightly harder problem, a stream with exactly�two elements. We know that we have to hold on to the first element we see from this stream, because we don’t know if�we’re in the case that the stream only has one element. When the second element comes along, we know that we want�to return one of the two elements, each with probability 1/2. So let’s generate a random number R between 0 and 1, and�return the first element if R is less than 0.5 and return the second element if R is greater than 0.5. Now let’s try to generalize this approach to a stream with three elements. After we’ve seen the second element in the stream, we’re now holding on�to either the first element or the second element, each with probability 1/2. When the third element arrives, what should�we do? Well, if we know that there are only three elements in the stream, we need to return this third element with�probability 1/3, which means that we’ll return the other element we’re holding with probability 1 – 1/3 = 2/3. That�means that the probability of returning each element in the stream is as follows: First Element: (1/2) * (2/3) = 1/3 Second Element: (1/2) * (2/3) = 1/3 Third Element: 1/3 By considering the stream of three elements, we see how to generalize this algorithm to any N: at every step N, keep�the next element in the stream with probability 1/N. This means that we have an (N-1)/N probability of keeping the element we�are currently holding on to, which means that we keep it with probability (1/(N-1)) * (N-1)/N = 1/N. This general technique is called reservoir sampling, and it is useful in a number of applications that require us�to analyze very large data sets. You can find an excellent overview of a set of algorithms for performing reservoir�sampling in this blog post by Greg Grothaus. I’d like to focus on two of those algorithms in particular, and talk about�how they are used in Cloudera ML, our open-source collection of data preparation and machine learning algorithms for Hadoop. Applied Reservoir Sampling in Cloudera ML The first of the algorithms Greg describes is a distributed reservoir sampling algorithm. You’ll note that for the algorithm we described above�to work, all of the elements in the stream must be read sequentially. To create a distributed reservoir sample of size K, we use�a MapReduce analogue of the ORDER BY RAND() trick/anti-pattern�from SQL: for each element in the set, we generate a random number R between�0 and 1, and keep the K elements that have the largest values of R. This trick is especially useful when we want to create�stratified samples from a large dataset. Each stratum is a specific combination of categorical variables that is important�for an analysis, such as gender, age, or geographical location. If there is significant skew in our input data set, it’s possible�that a naive random sampling of observations will underrepresent certain strata in the dataset. Cloudera ML has a sample command that�can be used to create stratified samples for text files and Hive tables (via the HCatalog interface to the Hive Metastore) such that N records will be selected for every combination of the categorical variables that define the strata. The second algorithm is even more interesting: a weighted distributed reservoir sample, where every item in the set has an associated weight, and we want to sample such that the probability that an item is selected is proportional to its weight. It wasn’t even clear whether or not this was even possible until Pavlos Efraimidis�and Paul Spirakis�figured out a way to do it and published it in the 2005 paper “Weighted Random Sampling with a Reservoir.” The solution is as simple as it is elegant, and it is based on the same idea as the distributed reservoir sampling algorithm described above. For each item in the stream, we compute a score as follows: first, generate a random number R between 0 and 1, and then take the nth root of R, where n is the weight of the current item. Return the K items with the highest score as the sample. Items with higher weights will tend to have scores that are closer to 1, and are thus more likely to be picked than items with smaller weights. In Cloudera ML, we use the weighted reservoir sampling algorithm in order to cut down on the number of passes over the input data that the scalable k-means++ algorithm needs to perform. The ksketch command runs the k-means++ initialization procedure, performing a small number of iterations over the input data set to select points that form a representative sample (or sketch) of the overall data set. For each iteration, the probability that a given point should be added to the sketch is proportional to its distance from the closest point in the current sketch. By using the weighted reservoir sampling algorithm, we can select the points to add to the next sketch in a single pass over the input data, instead of one pass to compute the overall cost of the clustering and a second pass to select the points based on those cost calculations. These Books Behind Me Don’t Just Make The Office Look Good Interesting algorithms aren’t just for the engineers building distributed file systems and search engines, they can also come in handy when you’re working on large-scale data analysis and statistical modeling problems. I’ll try to write some additional posts on algorithms that are interesting as well as useful for data scientists to learn, but in the meantime, it never hurts to brush up on your Knuth.  </snippet></document><document id="235"><title>Customer Spotlight: Nokia�s Big Data Ecosystem Connects Cloudera, Teradata, Oracle, and Others</title><url>http://blog.cloudera.com/blog/2013/04/customer-spotlight-nokias-big-data-ecosystem-connects-cloudera-teradata-oracle-and-others/</url><snippet>As Cloudera�s keeper of customer stories, it�s dawned on me that others might benefit from the information I�ve spent the past year collecting: the many use cases and deployment patterns for Hadoop amongst our customer base. This week I�d like to highlight Nokia, a global company that we�re all familiar with as a large mobile phone provider, and whose Senior Director of Analytics – Amy O�Connor – will be speaking at tomorrow�s Cloudera Sessions event in Boston. Fun fact: Nokia has been in business for more than 150 years, starting with the production of paper in the 1800s. When I first met Amy O�Connor in early 2012, she explained to me that Nokia has always been in the business of transforming resources into useful products — from paper and rubber over a century ago, to the electronics and mobile devices we�re familiar with today. One of the hottest resources right now is data, so it makes sense that Nokia has implemented a robust big data platform to milk that data for all it�s worth. With their 2007 acquisition of geospatial information systems (GIS) pioneer NAVTEQ, Nokia found itself sitting on a mountain of valuable information, and it set out on a mission: to help people navigate the physical world using digital data. To do this, Nokia needed a technology infrastructure that would support the collection, storage and analysis of virtually unlimited data types and volumes. This is what led Nokia to Apache Hadoop and Cloudera. Nokia has implemented a robust big data platform to milk that data for all it�s worth. Nokia�s big data ecosystem consists of a centralized, petabyte-scale Hadoop cluster that is interconnected with a 100-TB Teradata enterprise data warehouse (EDW), numerous Oracle and MySQL data marts, and visualization technologies that allow Nokia�s 60,000+ users around the world tap into the massive data store. Multi-structured data is constantly being streamed into Hadoop from the relational systems, and hundreds of thousands of Scribe processes run every day to move data from, for example, servers in Singapore to a Hadoop cluster in the UK. Nokia is also a big user of Apache Sqoop and Apache HBase. This ecosystem is helping Nokia achieve its goals by ingesting terabytes of data — from phones in use, services, log files, and other sources — and processing that data for market insights and other analytics. Specifically, Nokia is using the data to help people navigate the physical world by building 3D digital maps that incorporate traffic models that understand speed categories, recent speeds on roads, historical traffic patterns, elevation, ongoing events, video streams of the world, and more. Want to learn more? Attend Cloudera Sessions in Boston tomorrow, where Amy will join a customer fireside chat hosted by Cloudera�s Amr Awadallah, along with Chris Poulin of Patterns and Predictions. Read the Nokia case study. Watch the Nokia video case study. Check out Amy O�Connor�s blog.</snippet></document><document id="236"><title>HBaseCon 2013 Speakers, Tracks, and Sessions Announced</title><url>http://blog.cloudera.com/blog/2013/04/hbasecon-2013-speakers-tracks-and-sessions-announced/</url><snippet>Thanks to a dazzling array of excellent proposals from across the Apache HBase community, the HBaseCon 2013 Program Committee has cooked up a great list of sessions.� HBaseCon (hosted by Cloudera), now in its second year, is THE community event for Apache HBase contributors, developers, admins, and users. There is no better place to dive head-first into use cases, best practices, internals, and futures as well as to meet the rest of the community.� Organized into four tracks – Operations, Internals, Ecosystem, and Case Studies – the accepted sessions will be led by employees from companies spanning the use-case spectrum. Represented companies include: Ancestry.com Box eBay Facebook Groupon� Pinterest Salesforce.com Twitter Yahoo! …and more!� We offer our heartiest congratulations to all accepted speakers, and hope that everyone else who submitted a proposal will do the same next year. Let me also thank our sponsors thus far for supporting HBaseCon 2013 – without them, there would be no conference! Register�now while you have a chance – Early Bird registrations ends tomorrow (April 23) at midnight PT! This show is going to sell out, peeps!�</snippet></document><document id="237"><title>Demo: Analyzing Data with Hue and Hive</title><url>http://blog.cloudera.com/blog/2013/04/demo-analyzing-data-with-hue-and-hive/</url><snippet>In the first installment of the demo series about Hue — the open source Web UI that makes Apache Hadoop easier to use — you learned how file operations are simplified via the File Browser application. In this installment, we�ll focus on analyzing data with Hue, using Apache Hive via Hue’s Beeswax and Catalog applications (based on Hue 2.3 and later). The Yelp Dataset Challenge provides a good use case. This post explains, through a video and tutorial, how you can get started doing some analysis and exploration of Yelp data with Hue. The goal is to find the coolest restaurants in Phoenix! Dataset Challenge with Hue The demo below demonstrates how the “business” and “review” datasets are cleaned and then converted to a Hive table before being queried with SQL. � Now, let�s step through a tutorial based on this demo. The queries and scripts are available on GitHub. Getting Started &amp; Normalization First, get the dataset from the Yelp Challenge webpage. Then, clean the data using this script. Retrieve the data and extract it. tar -xvf yelp_phoenix_academic_dataset.tar

cd yelp_phoenix_academic_dataset
wget https://raw.github.com/romainr/yelp-data-analysis/master/convert.py

yelp_phoenix_academic_dataset$ ls
convert.py notes.txt READ_FIRST-Phoenix_Academic_Dataset_Agreement-3-11-13.pdf yelp_academic_dataset_business.json yelp_academic_dataset_checkin.json yelp_academic_dataset_review.json yelp_academic_dataset_user.json
   Convert it to TSV. chmod +x convert.py
./convert.py
   The following column headers will be printed by the above script. ["city", "review_count", "name", "neighborhoods", "type", "business_id", "full_address", "state", "longitude", "stars", "latitude", "open", "categories"]
["funny", "useful", "cool", "user_id", "review_id", "text", "business_id", "stars", "date", "type"]
   Create the Tables Next, create the Hive tables with the “Create a new table from a file” screen in the Catalog app or Beeswax “Tables” tab. Creating a new table Upload the data files yelp_academic_dataset_business_clean.json and yelp_academic_dataset_review_clean.json. Hue will then guess the tab separator and then lets you name each column of the tables. (Tip: in Hue 2.3, you can paste the column names in directly.) Naming columns You can then see the table and browse it. Browsing the table Queries Open up Hue’s Hive editor (Beeswax) and run one of these queries: Top 25: business with most of the reviews SELECT name, review_count
FROM business
ORDER BY review_count DESC
LIMIT 25
   Top 25: coolest restaurants SELECT r.business_id, name, SUM(cool) AS coolness
FROM review r JOIN business b
ON (r.business_id = b.business_id)
WHERE categories LIKE '%Restaurants%'
GROUP BY r.business_id, name
ORDER BY coolness DESC
LIMIT 25
   Query editor with SQL syntax highlighting and auto-complete � Watch the query runs � See the results with an infinite scroll   Now let your imagination run wild and execute some of your own queries! Note: This demo is about doing some quick data analytics and exploration. Running more machine learning oriented jobs like the Yelp Examples would deserve a separate blog post on how to run MrJob. Hue users would need to create an Apache Oozie workflow with a Shell action (see below). Notice that a �mapred� user would need to be created first in the User Admin. Running MrJob Wordcount example in the Oozie app with a Shell action What�s Next As you can see, getting started with data analysis is simple with the interactive Hive query editor and Table browser in Hue. Moreover, all the SELECT queries can also be performed in Hue�s Cloudera Impala application for a real-time experience. Obviously, you would need more data than the sample for doing a fair comparison but the improved interactivity is noticeable. In upcoming episodes, you�ll see how to use Apache Pig for doing a similar data analysis, and how Oozie can glue everything together in schedulable workflows. Thank you for watching and hurry up, only one month before the end of the Yelp contest! Romain Rigaux is a Software Engineer working on the Platform team.</snippet></document><document id="238"><title>Cloudera Academic Partnership Program: Creating Hadoop Lovers in Universities Worldwide</title><url>http://blog.cloudera.com/blog/2013/04/cloudera-academic-partnership-program-creating-hadoop-lovers-in-universities-worldwide/</url><snippet>Today Cloudera announced a new Cloudera Academic Partnership program, in which participating universities worldwide get access to curriculum, training, certification, and software.� As noted in the press release, the global demand for people with Apache Hadoop and data science skills is dwarfing all supply. We consider it an important mission to help accredited universities meet that demand, by equipping them with the content and training they need to educate students in the Hadoop arts. Furthermore, we are cognizant of the fact that many academic research labs are in need of tools to help deploy, manage, and extend Hadoop clusters. For that reason,�CAP members get free access to Cloudera Manager Enterprise Edition for 12 months to support data-intensive testing, development, and research. It’s a giant Win/Win! You can learn more about CAP by attending our free, live�webinar�on Thursday, May 16 at 10 AM PT / 1 PM ET. �Cloudera’s Director of Education Programs�will�will present an overview of the program’s objectives, partner benefits, and how your college or university can become a member, followed by a Q&amp;A. Register today!</snippet></document><document id="239"><title>Learn How To Hadoop from Tom White in Dr. Dobb’s</title><url>http://blog.cloudera.com/blog/2013/04/learn-how-to-hadoop-from-tom-white-in-dr-dobbs/</url><snippet>It’s always a great thing for everybody when the experts are willing and eager to share. So, it’s with special pleasure that I can point you toward a new three-part series by Cloudera’s own Tom White (@tom_e_white) to be published in Dr Dobb’s, which has long been one of the publications of record in the mainstream developer world – from which many original programmers learned basics like BASIC. Now, Dobb’s turns its attention to Apache Hadoop, which says a lot about Hadoop’s continuing adoption. Tom, of course, is the author of the O’Reilly best-seller�Hadoop: The Definitive Guide, and few people have a better record of being both knowledgeable and helpful for those who want to learn “how to Hadoop”. In Part 1 (published April 16), Tom offers a lay-of-the-land, explaining�what Hadoop is and how and why to use it. Part 2 (publishes April 23) will guide readers through writing and running their first MapReduce job. And Part 3 (publishes April 30) includes an introduction to MapReduce frameworks: Apache Pig, Apache Hive, and Apache Crunch.�You’ll find simple, hands-on examples throughout. Whether �you’re a Hadoop�aficionado�or just dipping your toes in the water (jump in, the water’s fine!), this series is a great resource for you.�</snippet></document><document id="240"><title>How Persado Supports Persuasion Marketing Technology with Data Analyst Training</title><url>http://blog.cloudera.com/blog/2013/04/how-persado-supports-persuasion-marketing-technology-with-hive-and-pig-training/</url><snippet>This guest post comes from Alex Giamas, Senior Software Engineer on the data warehouse team at Persado, an ultra-hot persuasion marketing technology company with operations in Athens, Greece. A World-Class EDW Requires a World-Class Hadoop Team Persado is the global leader in persuasion marketing technology, a new category in digital marketing. Our revolutionary technology maps the genome of marketing language and generates the messages that work best for any customer and any product at any time. To assure the highest quality experience for both our clients and end-users, our engineering team collaborates with Ph.D. statisticians and data analysts to develop new ways to segment audiences, discover content, and deliver the most relevant and effective marketing messages in real time. Given the challenge of creating a market based on ongoing data collection and massive query ability, the data warehouse organization ultimately plays the most important role in the persuasion marketing value chain, assuring a steady and unobstructed multidirectional flow of information.�My team continuously ensures Persado�s infrastructure is aligned to the needs of our data scientists, including regularly generating KPI reports, managing data from heterogeneous sources, preparing customized analyses, and even implementing specific statistical algorithms in Java based on reference implementations of R. As a senior engineer in the data operations organization in Athens, Greece, and the first to sit on both Persado�s data warehouse team and the data reporting team at Upstream Systems (which incubated and spun off Persado in 2012), I was responsible for the technical recommendation to step away from RDBMS and take strides towards the magical realm of NoSQL several years ago.�This decision became a primary enabler for Persado�s transition from being a useful attribute of Upstream�s platform to becoming a full-on software company, delivering real value to clients. Since 2010, my team has designed and implemented a variety of NoSQL systems. Although our initial experiments were frustrating at times, we eventually succeeded in creating a world-class Online Transactional Processing (OLTP) system based on MongoDB to handle ad interactions with customers. The database�s internal MapReduce mechanism was able to generate the required reports, the aggregation framework introduced new features to our reporting platform, and we applied machine learning algorithms to help process the data. As our analytics and report needs became more sophisticated, we eventually needed to decouple OLAP into a technology stack of its own. We had too few experienced Big Data engineers on staff to grow our capabilities. We quickly identified Apache Hadoop as the perfect solution to help us pick up, aggregate, and process the data from heterogeneous�sources like MongoDB, MySQL config servers, and Apache logs that were being populated in documents within AWS S3 buckets and consumed by Apache Kafka and Apache ZooKeeper. However, like many organizations that mature into sophisticated systems and analytics, we faced a fundamental problem: we had too few experienced Big Data engineers on staff to grow our capabilities and scale out our systems. Given the strategic priority of developing the best data warehouse platform to fulfill our customers� needs, we decided that Hadoop training was the most immediately actionable solution and would help us choose the right vendor to support our long-term Big Data objectives. We evaluated the three most well-known Hadoop companies, including two supporting an open-source platform and one selling a proprietary distribution. We ultimately chose Cloudera because of the experience of its instructors, its vast partner ecosystem, its role as the innovator driving Hadoop advancement, its fundamental commitment to open source, and its reputation as an amazing company with which to work. In the end, there was evidence in the market that Cloudera would be able to support our use case and growth from the first step of implementation, while the claims of the other two companies could not be as readily and independently validated. Fear Not the Pig We worked with Cloudera University�s expert curriculum team to tailor a private weeklong training that would meet our immediate and long-term needs. We started benefiting from our decision to work with Cloudera almost right away since no other company offers a full Data Analyst training targeted at both developers and analysts, which was one of our biggest priorities. The intensive workshop also included the full Cloudera Developer Training for Apache Hadoop with the option of testing for the sought-after CCDH certification�following the class. Working with Cloudera helped us save time, money, and productivity. Having an instructor onsite allowed the team to ask questions based on our actual experiences and explain our architecture and goals to validate that we were moving in the right direction. Working with Cloudera, who has the only (and best) full-time Hadoop trainers in mainland Europe, helped us save time, money, and productivity that would have otherwise been lost to travel, jet lag, and the stresses of being away from family and colleagues. Our takeaways were significant in both the general technical and software engineering domains. The team learned to �embrace the Pig,� and we are now able to combine Hive and Pig jobs as appropriate to our use case. Throughout the excellent labs, we saw the importance of custom partitioning and how it can affect our MapReduce jobs� performance. ���������������� One of the greatest values of the live training was learning pointers for handling common issues and useful tricks for the more complex challenges. For example, we had run into an imbalanced user attribute that was resulting in few reducers taking the bulk of the load. A custom partitioner remedied this, allowing for an even distribution of load with improved speeds and reduced times for the execution of the MapReduce job flow. By learning more about the HDFS internals, we realized the need to balance writing to and reading from files further down our data pipeline. Our Kafka system was previously getting messages as JSON documents and dumping them onto S3. This can work, but it imposes severe overhead in terms of searching through a huge number of files for processing. Also, if we were to use HDFS in local EC2 nodes, this could create an issue with the NameNode, and we would have to resort to federating our HDFS namespace or scaling our NameNode capacity. As engineers, we are naturally inclined to research new technologies, work on our own pet projects, and join interesting communities every day. However, Hadoop has proven to be an exceptional case in that true expertise, particularly tied to a specific use case, is very difficult to achieve, even with intense study. Cloudera training expedited our way through the learning curve, helped us answer our specific questions, and offered best practices derived from the engineers who built the platform. Cloudera also offers the unique added value of incorporating insights into its courses from the use cases that are driving the Hadoop market. Moreover, everyone on my team is confident that they learned Hadoop on the world�s most relevant and up-to-date open-source distribution in CDH4. Trained to Persuade At Persado, we collect data from a wide variety of sources, convert it to a base reference, and finally perform aggregations to derive meaningful reports for our internal teams and clients. An array of libraries, from R to Mahout to Java, enables a wide range of functions. Processes such as clustering, classification, and recommendations feed our objective to constantly identify the best message to serve each specific audience. Needless to say, without the right solution, we could have a Big Data problem on our hands. We were able to quickly implement Hadoop as a key component of our data warehouse. After the Cloudera training, it was evident to all participants that the strategy to move towards Hadoop was the right solution for achieving the company�s vision and goals. Cloudera is helping us discover useful insights from our data and allowing our employees around the world to better analyze both ad hoc queries and precalculated aggregates. We were able to quickly implement Hadoop as a key component of our data warehouse, which removed the burden we anticipated for our other systems and coordinated diverse projects for deeper, more relevant queries and greater speed to insight. Cloudera�s tailored private training was a perfect fit for our objectives and was the cost-effective option for our needs. With the right training to get the team up to speed and working towards our Hadoop strategy, we are now well along on our journey to deliver unmatched value to our customers from the most sophisticated persuasion marketing platform in the industry. Cloudera has not only prepared us for success today, but has also trained us to face and prevail over our Big Data challenges in the future.</snippet></document><document id="241"><title>It’s Only Rock and Roll</title><url>http://blog.cloudera.com/blog/2013/04/its-only-rock-and-roll/</url><snippet>It�s only Rock and Roll, but I like it! ���������� – Mick Jagger Copyright is having a tough time in the digital age. New copies of music, movies and software can be created at near zero cost. Some wonder whether it still makes sense to ever charge for content. Over the past century large industries have developed that sell content. These industries resist change. We consumers love our content, but don�t love paying for it. But would all the content we love still exist without payment for copyright? One solution might be to replace sales of copyrighted material with services that provide access to the content. We could buy tickets to concerts, a service provided by musicians. We could enjoy streaming music services via subscriptions or supported by advertising. Similarly, we could access software as a service in the cloud. Some companies, like Google, create proprietary software yet don�t make money off it by selling its copyright, but only through services. That�s a great model, but is it the only way forward? Not yet, it seems. In both music and software, sometimes folks still want to own a copy. They want their favorite tracks on their mp3 players without ongoing fees. Likewise, people might reasonably choose to buy software that they can run on their own hardware. Businesses may want a reliable vendor, but also don’t want to be locked-in to that vendor�s software. To satisfy this demand, a hybrid product-and-services model is most common in music. Few well-known musicians survive on concert revenues alone. Most sell songs as well. Those musicians who survive on performance income tend to be folks who play at weddings, not those you listen to at home. I�m in the open source software business. I work with other developers to create software that folks can freely copy. The software itself is not sold, but it�s used by many, and we somehow need to be compensated, so we can feed our families and still do this work. For many years I was an independent contractor, selling my services, extending open source software. Like a wedding musician, I didn�t have long-term relationships with my clients. As businesses come to depend on open source software, many would like to have a relationship with a vendor. A vendor can indemnify them against lawsuits related to the software, resolve problems promptly, and extend the software in specific directions. Businesses want a vendor that�s financially solid so that it can rely on it, now and into the future. While businesses may want a reliable vendor, they don�t want to be locked-in to that vendor�s software as they often are with proprietary software. Open source can inhibit such lock-in, since different vendors can support the same software, and businesses can then select a vendor that provides them the greatest value outside of that software. Cloudera is a vendor of the open source Apache Hadoop software platform.�We provide support services for that platform, and with those, some proprietary software.�We don�t want to lock our customers in, so we don�t provide proprietary software that�s used directly by their applications. All application code is based only on the open source platform, so applications will never be locked-in to Cloudera. Instead we provide proprietary software that our customers use to install, configure and monitor the open source platform. This software is an extension of our support services. We give our customers tools that make it easier for them to deploy the open source software, and also for us to support it. These tools are an integral part of our commercial support, validating supported installations and configurations, and automatically supplying context to problem reports. They are the �secret sauce� of our service business, helping to make our commercial support the best in the industry. We believe that this combination best permits us to become a financially viable company that our customers can rely on to support their mission-critical needs for the long term. A rock band could limit itself to a pure services model, surviving only on tour revenues, eschewing all copyright income. Or they could try to reach the widest audience possible, combining service income from concerts with copyright income from sales, giving its fans what they want. In 50 years, Cloudera doesn�t want to be like that guy who sang at your wedding, but instead hopes to be like the Rolling Stones, as a core figure of the technology world as the Stones are of Rock and Roll. Doug Cutting is Cloudera’s chief architect, a founder of the Apache Lucene and Apache Hadoop projects, and the current chair of the Apache Software Foundation.</snippet></document><document id="242"><title>How-to: Use the Apache HBase REST Interface, Part 2</title><url>http://blog.cloudera.com/blog/2013/04/how-to-use-the-apache-hbase-rest-interface-part-2/</url><snippet>This how-to is the second in a series that explores the use of the Apache HBase REST interface.�Part 1�covered HBase REST fundamentals, some Python caveats, and table administration. Part 2 below will show you how to insert multiple rows at once using XML and JSON. The full code samples can be found on GitHub. Adding Rows With XML The REST interface would be useless without the ability to add and update row values.�The interface gives us this ability with the POST verb. By posting new rows, we can add new rows or update existing rows using the same row key. First, let’s step through how to do this using the XML and JSON data formats. Let’s start with XML. We’ll have to add two import statements: import base64
from xml.etree.ElementTree import Element, SubElement, tostring, fromstring
   For the XML data format, all values and column names need to be base64 encoded because values can be binary data.�We can’t have binary data messing up our nicely formed XML. We also need to import our XML modules.�These modules will help us create the XML DOM to hold our new rows. To work with the column’s name, we need to base64 encode them.�I recommend doing this at the start of the script and reuse the variable as needed: linenumbercolumnencoded = base64.b64encode(cfname + ":" + linenumbercolumn)
usernamecolumnencoded = base64.b64encode(cfname + ":" + usernamecolumn)
messagecolumnencoded = base64.b64encode(cfname + ":" + messagecolumn)
   Let’s take a look at the code to create the rows: cellset = Element('CellSet')

linenumber = 0;
  ���� �
  for line in shakespeare:������
  ���� rowKey = username + "-" + filename + "-" + str(linenumber).zfill(6)
  ���� rowKeyEncoded = base64.b64encode(rowKey)
  ����
  ���� row = SubElement(cellset, 'Row', key=rowKeyEncoded)
  ����
  ���� messageencoded = base64.b64encode(line.strip())
  ���� linenumberencoded = encode(linenumber)
  ���� usernameencoded = base64.b64encode(username)
  ����
  ���� # Add bleet cell
  ���� cell = SubElement(row, 'Cell', column=messagecolumnencoded)
  ���� cell.text = messageencoded
  ����
  ���� # Add username cell
  ���� cell = SubElement(row, 'Cell', column=usernamecolumnencoded)
  ���� cell.text = usernameencoded
  ����
  ���� # Add Line Number cell
  ���� cell = SubElement(row, 'Cell', column=linenumbercolumnencoded)
  ���� cell.text = linenumberencoded
  ��������������
  ���� linenumber = linenumber + 1
  ����
  # Submit XML to REST server
request = requests.post(baseurl + "/" + tablename + "/fakerow", data=tostring(cellset), headers={"Content-Type" : "text/xml", "Accept" : "text/xml"})
   The first line creates the root XML element.�The CellSet node will contain all of the child elements to be inserted as rows. The for loop iterates over all of the entries we want to insert.�At the start of the loop, a row key is created and then base64 encoded.�This key will uniquely identify the row in HBase.�Next, a row element is created.�This row element contains an attribute called “key” with the base64 encoded row key as the value.�Each separate cell is base64 encoded and added to “Cell” elements.� The “Cell” elements all have the column name as an attribute.�The “Cell”s element text is set to the base64 encoded value. Note that the URL is set to “fakerow”.�Since this is a multi-store request, the REST interface will be using the key supplied in the “Cell” element.�For a single store request, you can use the URL.�Doing a multi-store request is much more efficient than doing a single request at a time.�The overhead is a lot less by doing a larger request fewer times. Once the entire XML DOM is created, it can be passed to the REST interface using the POST verb.�The POST‘s data is set to a string representation of the DOM.�The Content-Type and Accept headers are set to XML.�The REST server is expecting XML input and will pass back XML. The success of the call can be ascertained by looking at the status code. To update a row’s values in XML, simply make the key’s value the same as it was before. HBase will find the row and update the values contained in the call. Adding Rows With JSON Working with JSON is very similar to working with XML.�I find that the code for JSON is more straightforward though.�JSON also has the benefit of being better suited to data formats and creates a much smaller footprint.�I’ve found that the JSON calls take less time than the XML calls. Here are the imports for using JSON:   import json
  import base64
  import requests
from ordereddict import OrderedDict
   Note that OrderedDict�is one of the imports. We’ll discuss the reason why shortly. Now let’s see the JSON multi-store code: rows = []
jsonOutput = { "Row" : rows }

for line in shakespeare:
	rowKey = username + "-" + filename + "-" + str(lineNumber).zfill(6)
	rowKeyEncoded = base64.b64encode(rowKey)

	line = base64.b64encode(line.strip())
	lineNumberEncoded = encode(lineNumber)
	usernameEncoded = base64.b64encode(username)

	cell = OrderedDict([
		("key", rowKeyEncoded),
		("Cell",
		[
			{ "column" : messagecolumnencoded, "$" : line },
			{ "column" : usernamecolumnencoded, "$" : usernameEncoded },
			{ "column" : linenumbercolumnencoded, "$" : lineNumberEncoded },
		])
	])

	rows.append(cell)

	lineNumber = lineNumber + 1

# Submit JSON to REST server
request = requests.post(baseurl + "/" + tablename + "/" + rowKey, data=json.dumps(jsonOutput), headers={"Content-Type" : "application/json", "Accept" : "application/json"})
   First we create a row array to store all of the rows we want to store.�Then, we add the row array to a dict with the name “Row”. We enter a for loop that goes through all of the data we want to add to HBase.�First, we create the row key that will uniquely identify the row in HBase.�Once again, we have to base64 encode the value.�Next, we take the data we want to store and base64 encode it too. Now comes the OrderedDict.�We have to use an OrderedDict instead of a normal dict because we have to maintain the order of the keys in the dictionary.�This works around an issue in the REST daemon for JSON.�The bug is that the “key” entry must come before the “Cell” entry.�If it doesn’t, the REST interface won’t find the key and will use the URL’s key over and over.�In this case, the row key would be “fakekey” and every column would be added to the same row. We add the “key” to the OrderedDict and add the “Cell” which is an array of dictionaries.�The column key is the base64 encoded name of the column and the dollar sign ($) is the base64 encoded value of the column. Once we have finished creating the cell object, we can append that to our row array. The final step is to submit the JSON to the REST server using the POST verb.�We are using the multi-store so the URL’s key will be fake and the cell’s key will be used.�The data is set to a string representation of the JSON. The headers are changed so that the REST server is expecting JSON and will pass back JSON.�The success of the call can be ascertained by looking at the status code. To update a row’s values in JSON, simply make the key’s value the same as it was before.�HBase will find the row and update the values contained in the call. In the third and final how-to in this series, we’ll cover getting the rows that we’ve just inserted. Jesse Anderson is an instructor with Cloudera University.</snippet></document><document id="243"><title>Where to Find Cloudera Tech Talks Through June 2013</title><url>http://blog.cloudera.com/blog/2013/04/where-to-find-cloudera-tech-talks-through-june-2013/</url><snippet>It’s time for me to give you a quarterly update (here’s the one for Q1) about where to find tech talks by Cloudera employees in 2013. Committers, contributors, and other engineers will travel to meetups and conferences near and far to do their part in the community to make Apache Hadoop a household word! (Remember, we’re always ready to assist your meetup by providing speakers, sponsorships, and schwag.) A couple highlights: As follow-ups on�Cloudera Impala –�the open source, MPP query engine that runs natively on Apache Hadoop –�Impala architect Marcel Kornacker will be visiting the New York HUG (May 7) and Berlin Buzzwords (June 7). There are several other Impala talks on the list, too. This is a great time of year for the Apache�HBase community (especially in Europe), with PMC Member/HBase: The Definitive Guide author Lars George dropping in at NoSQL Matters and JAX in Germany, HBase Committer/PMC Member JD Cryans presenting at GOTO, and HBaseCon 2013 on tap for June 13. We’ll be sure to update the grid about Cloudera speakers at HBaseCon and Hadoop Summit whenever we have that info! Date City Venue Speaker(s) April 11 Stockholm Stockholm HUG James Kinley on Impala April 17 LA LA HBase Users Group HBase Committer/PMC Member Jon Hsieh on “The Future of HBase” April 23 Mainz, Germany JAX Germany HBase Committer/PMC Member Lars George on HBase use cases and schema design April 26 Moscow Big Data Week Moscow Justin Erickson on Impala April 26-27 Cologne, Germany NoSQL Matters HBase Committer/PMC Member Lars George on HBase use cases and schema design April 30 Flemington, NJ NJ Hadoop Meetup Linden Hillenbrand on “Hadoop 101″ May 2 Ballerup, Denmark Miracle Open World Greg Rahn on “Hadoop 101″ May 7 New York New York HUG Marcel Kornacker on Impala May 7 Nashville, Tenn. Nashville JUG Ian Wrigley on MapReduce Programming May 14 London HUG UK Paul Wilkinson on Impala May 16 Sunnyvale, Calif. BayLISA Eric Sammer in Hadoop innovations May 22 Boulder, Colo. Boulder/Denver Big Data Meetup Greg Rahn on Impala May 22 Pleasanton, Calif. NCOUG Spring Conference Justin Erickson on Impala May 28 Madison, Wisc. Big Data Madison Matt Harris on Impala May 31 Berkeley, Calif. DataEDGE Conference Glynn Durham on “Intro to Data Science” June 3 Berlin Berlin Buzzwords Marcel Kornacker on Impala June 4 Santa Clara, Calif. JAX US Apache Sqoop Committer/PMC Member Kate Ting on Apache ZooKeeper availability June 7 Paris dotScale Hadoop Founder Doug Cutting on the Hadoop Ecosystem June 12 Brooklyn, NY QCon NYC Hadoop Committer/PMC Member Eli Collins on building apps with Hadoop June 13 San Francisco HBaseCon See conference Website June 18 Amsterdam GOTO Conference HBase Committer/PMC Member JD Cryans on HBase use cases June 26-27 San Jose, Calif. Hadoop Summit Alejandro Abdelnur and Aaron T. Myers on Hadoop security, Nong Li on Parquet, and Jon Hsieh and Kevin O’Dell on trends in supporting HBase clusters</snippet></document><document id="244"><title>How-to: Use Vagrant to Set Up a Virtual Hadoop Cluster</title><url>http://blog.cloudera.com/blog/2013/04/how-to-use-vagrant-to-set-up-a-virtual-hadoop-cluster/</url><snippet>This guest post comes to us from David Greco, CTO of Eligotech. Vagrant is a very nice tool for programmatically managing many virtual machines (VMs) on a single physical machine. It natively supports VirtualBox and also provides plugins for VMware Fusion and Amazon EC2, supporting the management of VMs in those environments as well. Vagrant provides a very easy-to-use, Ruby-based internal DSL that allows the user to define one or more virtual machines together with their configuration parameters. Furthermore, it offers different mechanisms for automatic provisioning: You can use Puppet, Chef, or shell scripts for automating software installation and configuration on the machines defined in the Vagrant configuration file. So, using Vagrant, it’s possible to define complex virtual infrastructures based on multiple VMs running on your system. Pretty cool, no? A typical use case for Vagrant is to build working/development environments in a simple and consistent way. At my company, Eligotech, we are developing a product aimed to simplify the usage of Apache Hadoop, and CDH, Cloudera’s open source distribution, is our reference Hadoop distribution. We often need to set up a Hadoop environment on our machine for testing purposes, and we found Vagrant to be a very handy tool for that purpose. I put together an example of a Vagrant configuration file that you can test for yourself. You’ll need to download and install Vagrant (instructions) and VirtualBox. Once everything has been installed, just copy-and-paste the text below to a file named Vagrantfile and put it in a directory named, for example, VagrantHadoop. This configuration file assumes you have at least 32GB of memory on your box; if that’s not the case, you can edit the file to suit your environment (to run fewer slaves, for example, by commenting out some of the slave configurations). # -*- mode: ruby -*-
# vi: set ft=ruby :

$master_script = &lt;&lt;SCRIPT
#!/bin/bash
cat &gt; /etc/hosts &lt;&lt;EOF
127.0.0.1       localhost

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

10.211.55.100   vm-cluster-node1
10.211.55.101   vm-cluster-node2
10.211.55.102   vm-cluster-node3
10.211.55.103   vm-cluster-node4
10.211.55.104   vm-cluster-node5
10.211.55.105   vm-cluster-client
EOF

apt-get install curl -y
REPOCM=${REPOCM:-cm4}
CM_REPO_HOST=${CM_REPO_HOST:-archive.cloudera.com}
CM_MAJOR_VERSION=$(echo $REPOCM | sed -e 's/cm\\([0-9]\\).*/\\1/')
CM_VERSION=$(echo $REPOCM | sed -e 's/cm\\([0-9][0-9]*\\)/\\1/')
OS_CODENAME=$(lsb_release -sc)
OS_DISTID=$(lsb_release -si | tr '[A-Z]' '[a-z]')
if [ $CM_MAJOR_VERSION -ge 4 ]; then
  cat &gt; /etc/apt/sources.list.d/cloudera-$REPOCM.list &lt;&lt;EOF
deb [arch=amd64] http://$CM_REPO_HOST/cm$CM_MAJOR_VERSION/$OS_DISTID/$OS_CODENAME/amd64/cm $OS_CODENAME-$REPOCM contrib
deb-src http://$CM_REPO_HOST/cm$CM_MAJOR_VERSION/$OS_DISTID/$OS_CODENAME/amd64/cm $OS_CODENAME-$REPOCM contrib
EOF
curl -s http://$CM_REPO_HOST/cm$CM_MAJOR_VERSION/$OS_DISTID/$OS_CODENAME/amd64/cm/archive.key &gt; key
apt-key add key
rm key
fi
apt-get update
export DEBIAN_FRONTEND=noninteractive
apt-get -q -y --force-yes install oracle-j2sdk1.6 cloudera-manager-server-db cloudera-manager-server cloudera-manager-daemons
service cloudera-scm-server-db initdb
service cloudera-scm-server-db start
service cloudera-scm-server start
SCRIPT

$slave_script = &lt;&lt;SCRIPT
cat &gt; /etc/hosts &lt;&lt;EOF
127.0.0.1       localhost

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

10.211.55.100   vm-cluster-node1
10.211.55.101   vm-cluster-node2
10.211.55.102   vm-cluster-node3
10.211.55.103   vm-cluster-node4
10.211.55.104   vm-cluster-node5
10.211.55.105   vm-cluster-client
EOF
SCRIPT

$client_script = &lt;&lt;SCRIPT
cat &gt; /etc/hosts &lt;&lt;EOF
127.0.0.1       localhost

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

10.211.55.100   vm-cluster-node1
10.211.55.101   vm-cluster-node2
10.211.55.102   vm-cluster-node3
10.211.55.103   vm-cluster-node4
10.211.55.104   vm-cluster-node5
10.211.55.105   vm-cluster-client
EOF
SCRIPT

Vagrant.configure("2") do |config|

  config.vm.define :master do |master|
    master.vm.box = "precise64"
    master.vm.provider "vmware_fusion" do |v|
      v.vmx["memsize"]  = "4096"
    end
    master.vm.provider :virtualbox do |v|
      v.name = "vm-cluster-node1"
      v.customize ["modifyvm", :id, "--memory", "4096"]
    end
    master.vm.network :private_network, ip: "10.211.55.100"
    master.vm.hostname = "vm-cluster-node1"
    master.vm.provision :shell, :inline =&gt; $master_script
  end

  config.vm.define :slave1 do |slave1|
    slave1.vm.box = "precise64"
    slave1.vm.provider "vmware_fusion" do |v|
      v.vmx["memsize"]  = "5120"
    end
    slave1.vm.provider :virtualbox do |v|
      v.name = "vm-cluster-node2"
      v.customize ["modifyvm", :id, "--memory", "5120"]
    end
    slave1.vm.network :private_network, ip: "10.211.55.101"
    slave1.vm.hostname = "vm-cluster-node2"
    slave1.vm.provision :shell, :inline =&gt; $slave_script
  end

  config.vm.define :slave2 do |slave2|
    slave2.vm.box = "precise64"
    slave2.vm.provider "vmware_fusion" do |v|
      v.vmx["memsize"]  = "5120"
    end
    slave2.vm.provider :virtualbox do |v|
      v.name = "vm-cluster-node3"
      v.customize ["modifyvm", :id, "--memory", "5120"]
    end
    slave2.vm.network :private_network, ip: "10.211.55.102"
    slave2.vm.hostname = "vm-cluster-node3"
    slave2.vm.provision :shell, :inline =&gt; $slave_script
  end

  config.vm.define :slave3 do |slave3|
    slave3.vm.box = "precise64"
    slave3.vm.provider "vmware_fusion" do |v|
      v.vmx["memsize"]  = "5120"
    end
    slave3.vm.provider :virtualbox do |v|
      v.name = "vm-cluster-node4"
      v.customize ["modifyvm", :id, "--memory", "5120"]
    end
    slave3.vm.network :private_network, ip: "10.211.55.103"
    slave3.vm.hostname = "vm-cluster-node4"
    slave3.vm.provision :shell, :inline =&gt; $slave_script
  end

  config.vm.define :slave4 do |slave4|
    slave4.vm.box = "precise64"
    slave4.vm.provider "vmware_fusion" do |v|
      v.vmx["memsize"]  = "5120"
    end
    slave4.vm.provider :virtualbox do |v|
      v.name = "vm-cluster-node5"
      v.customize ["modifyvm", :id, "--memory", "5120"]
    end
    slave4.vm.network :private_network, ip: "10.211.55.104"
    slave4.vm.hostname = "vm-cluster-node5"
    slave4.vm.provision :shell, :inline =&gt; $slave_script
  end

  config.vm.define :client do |client|
    client.vm.box = "precise64"
    client.vm.provider "vmware_fusion" do |v|
      v.vmx["memsize"]  = "4096"
    end
    client.vm.provider :virtualbox do |v|
      v.name = "vm-cluster-client"
      v.customize ["modifyvm", :id, "--memory", "4096"]
    end
    client.vm.network :private_network, ip: "10.211.55.105"
    client.vm.hostname = "vm-cluster-client"
    client.vm.provision :shell, :inline =&gt; $client_script
  end

end
   This file defines six machines to be assigned the following CDH roles: vm-cluster-node1: This is the master; besides running the CM master, it should run the namenode, secondary namenode, and jobtracker. vm-cluster-node2: This is a slave, it should run a datanode and a tasktracker. vm-cluster-node3: This is a slave, it should run a datanode and a tasktracker. vm-cluster-node4: This is a slave, it should run a datanode and a tasktracker. vm-cluster-node5: This is a slave, it should run a datanode and a tasktracker. vm-cluster-client: This machine plays the role of gateway for the cluster. Click here to learn the meaning of the different items in the configuration file. In particular, you can see that depending on the particular provider, either VirtualBox or VMware Fusion, the memory size is changed in a different way. Observe how simple it is to switch between providers for customizing environment-specific things! This Vagrant file does another very important thing: It installs Cloudera Manager automatically on the master node, vm-cluster-node1. To create the virtual cluster, open a shell and just go to the directory holding the Vagrant file, i.e. VagrantHadoop. Under that directory, run: &gt; vagrant up --provider=virtualbox
   After a while, depending� on how fast your machine is, Vagrant will return control — meaning that all the VMs are up and running. At this point you are ready to configure your cluster through CM’s web UI via http://vm-cluster-node1:7180. Have fun!</snippet></document><document id="245"><title>Demo: HDFS File Operations Made Easy with Hue</title><url>http://blog.cloudera.com/blog/2013/04/demo-hdfs-file-operations-made-easy-with-hue/</url><snippet>Managing and viewing data in HDFS is an important part of Big Data analytics. Hue, the open source web-based interface that makes�Apache Hadoop easier to use, helps you do that through a GUI in your browser — �instead of logging into a Hadoop gateway host with a terminal program and using the command line. The first episode in a new series of Hue demos, the video below demonstrates how to get up and running quickly with HDFS file operations via Hue’s File Browser application. Other Features In addition to the above scenario, File Browser lets you perform more advanced file operations: Download Rename Move Copy Recursive change of permissions Recursive change of ownership Sort by attributes (e.g. name, size, date…) View content of zip/gz text View content of huge files Drag &amp; drop files to upload   What�s Next? Thanks to File Browser, file operations in HDFS are only a few clicks away. Hue�s other apps leverage the File Browser as well and offer direct links to the outputs of your MapReduce jobs, Hive queries, or Pig scripts so that you can share or take a glance/visualize in no time. Thank you for watching, and stay tuned for upcoming episodes! Feel free to ask questions in comments or via the Hue group.</snippet></document><document id="246"><title>Congrats to OSCON 2013 Speakers!</title><url>http://blog.cloudera.com/blog/2013/04/congrats-to-oscon-2013-speakers/</url><snippet>Cloudera will be a proud exhibitor at O’Reilly OSCON 2013 (July 22-26 in Portland, OR), which in our opinion is a shining light in the open source community. So be sure to look for us at Booth #420! We also want to take this opportunity to congratulate all speakers who will be presenting at OSCON. Furthermore, we want to highlight the talks led by Clouderans for your personal schedule: Tutorial: Introduction to Apache Hadoop Tom Wheeler, Cloudera University Instructor Mon. July 22, 10:30am This tutorial will present a mix of lecture and instructor-led demonstrations to explain what Apache Hadoop is and why it�s becoming a standard for large-scale data storage and processing. � Building an Impenetrable ZooKeeper Kate Ting, Software Engineer/Apache Sqoop PMC Member Weds. July 24, 10:40am ZooKeeper provides reliable and timely coordination of processes. Given the many cluster resources leveraged by distributed ZooKeeper, it �s frequently the first to notice issues affecting cluster health, which explains its moniker, �the canary in the Hadoop coal mine.� Attend this session and learn (1) how to configure ZooKeeper reliably, (2) how to monitor ZooKeeper closely, and (3) how to resolve ZooKeeper errors efficiently. � Doing Data Science on NFL Play by Play Jesse Anderson, Cloudera University Instructor Weds. July 24, 2:30pm Advanced NFL stats released the play by play data for the 2002 to 2012 seasons. The play data is human generated. Doing any Data Science on it will be difficult until you transform it. After that you can merge it with other dataset to get even more insight. Ideally, you want an easily query-able dataset that you can use Hive, Pig or Impala to gain more insight. � Getting Hadoop, Hive and HBase up and running in less than 15 minutes Mark Grover, Software Engineer/Apache Bigtop Committer and Apache Hive Contributor Weds. July 24, 4:10pm This talk introduces the audience to Apache Bigtop � a project aimed at developing packaging and tests within the Hadoop ecosystem. By making use of various packages available through Apache Bigtop, we would learn how to set up a cluster with Hadoop, Hive and HBase installed and configured in under 15 minutes. Subsequently, we will run some example MapReduce jobs, Hive and HBase queries to validate the setup. � Introducing Locksmith – an open source tool for detecting potential deadlocks in C and C++ Colin McCabe, Software Engineer/Apache Hadoop Committer Thurs. July 25, 5pm Locksmith is a tool which can help detect potential deadlocks before they happen. It can detect common concurrency mistakes, such as taking locks in an inconsistent order, or unlocking a mutex from a different thread than the one which locked it.�This talk will describe some challenges in debugging concurrent applications, some useful approaches, and how Locksmith can help. Again, congratulations to all newly-minted OSCON speakers!</snippet></document><document id="247"><title>For a Limited Time: Live Impala Demo on EC2</title><url>http://blog.cloudera.com/blog/2013/04/for-a-limited-time-live-impala-demo-on-ec2/</url><snippet>As a follow-up to a previous post about the Impala demo he built during Data Hacking Day, Alan Gardner from Pythian has deployed the app for a limited time on Amazon EC2. We republish his original post below. A little while ago�I blogged about�(and open sourced) a Cloudera Impala-powered soccer visualization demo, designed to demonstrate just how responsive Impala queries can be. Since not everyone has the time or resources to run the project themselves, we�ve decided to host it ourselves on an EC2 instance. [Note: instance live only for one week!]�You can�try the visualization; we�ve also opened up the�Impala web interface,�where you can see query profiles and performance numbers, and�Hue�(username and password are both �test�), where you can run your own queries on the dataset. Deploying Impala on EC2 While there are many tools to deploy a Hadoop cluster on EC2 � like�Apache Whirr, or even�Cloudera Manager�� I only wanted to use a single instance for the entire cluster. Starting from the base Ubuntu (Precise) �image, I added Cloudera�s apt repos, and�installed the single node configuration. Impala doesn�t support using Derby for the Hive metastore, so I installed MySQL and�configured Hive to use it instead. Then I installed Impala using�Cloudera�s instructions. Impala, and all of the Hadoop daemons, are running comfortably on one M3 2XLarge EC2 instance. Given our modest demands, this may actually be overkill; I over-spec’ed the server trying to find a (now-obvious) performance problem involving short-circuit reads. Short-Circuit Reads On the Pythian cluster, we could consistently �return a query in around half a second. On EC2 queries took closer to 5 seconds. A bit of investigation showed that in getting the server up and running, I had disabled short-circuit reads, which slows down Impala considerably. While�Impala isn�t supposed to start without short-circuit reads, it only throws an error if you have short-circuit reads enabled but misconfigured. If short-circuits are off in the hdfs-site configuration, it will happily start and run very slowly. With the default DEB install, the libhadoop library isn�t installed to the LD_PATH on Ubuntu, which prevents short-circuit read from working. The easiest solution was to create symlinks for libhadoop to /usr/lib/, then run ldconfig: ln -s /usr/lib/hadoop/lib/native/libhadoop.so /usr/lib/
ln -s /usr/lib/hadoop/lib/native/libhadoop.so.1.0.0 /usr/lib/
ldconfig
   To confirm whether your cluster has short-circuit reads enabled, you can visit the Impala web interface (by default, port 25000 on any system running impalad) and click on the �/varz� tab. Search for �dfs.client.read.shortcircuit� � it should be set to �true�. Partitioning With libhadoop installed and short-circuit reads enabled, the next greatest performance improvement came from partitioning the table on the sensor id. Since all of our web interface queries filter by sensor id, Impala can perform some serious partition elimination: looking at the query profiles, partitioning the table reduced the amount of data read from HDFS from 4GB to 50MB, and the query time from 2.6s to 130ms. The�README on Github�has instructions on how to use dynamic partitioning in Hive to quickly partition the soccer data; these steps can be generalized to any dataset.</snippet></document><document id="248"><title>We Honor the Champions of Big Data!</title><url>http://blog.cloudera.com/blog/2013/04/we-honor-the-champions-of-big-data/</url><snippet>In the technology business, building a thriving and progressive user ecosystem around a platform is about as Mom-and-apple-pie as you can get. We all intuitively acknowledge that it�s one of the metrics for success. Perhaps the most under-appreciated aspect of any platform ecosystem is the recognition that it is fundamentally built by real people. Without enthusiastic users of a platform engaging as evangelists on its behalf, the growth of the ecosystem around it will eventually slow to a crawl. So, to express our appreciation for the passionate users of CDH, our 100% open source distribution of Apache Hadoop and related projects, we are proud to announce Cloudera�s�Champions of Big Data program. Cloudera�s Champions of Big Data are passionate users of CDH with proven track records of contributing to the ecosystem by sharing knowledge and experiences, fostering skill set development, encouraging participation, or any combination of the above, whether internally or externally to their organizations. They are the Ecosystem Builders, the people on whom the platform depends, and who in turn depend on the platform. Therefore, we are proud to honor�these individuals for becoming our charter Champions of Big Data! Click here to learn more�about them or to nominate another CDH user who has been active in the Big Data community.�   Mats-Olov Eriksson, King.com   Emad Georgy, Experian Marketing Services   Bob Gourley, CTOVision.com   Oliver Guinan, Skybox Imaging   Sam Hamilton, PayPal   Erich Hochmuth, Monsanto   Adam Kawa, Spotify/Compendium   Jeremy Lizt, LiveRamp   Doug Meil, Explorys   Wayne Wheeles, Six3 Systems   Hugh Williams, eBay   John Zantey, Contexti</snippet></document><document id="249"><title>Seven Thoughts on Hadoop’s Seventh Birthday</title><url>http://blog.cloudera.com/blog/2013/04/seven-thoughts-on-hadoops-seventh-birthday/</url><snippet>On this special April 1 – the seven-year anniversary of the Apache Hadoop project’s first release – Hadoop founder Doug Cutting (also Cloudera’s chief architect and the Apache Software Foundation chair) offers seven thoughts on Hadoop: Open source accelerates adoption. If Hadoop had been created as proprietary software it would not have spread as rapidly. We�ve seen incredible growth in the use of Hadoop. Partly that�s because it�s useful. But many would have been cautious to make a vendor-controlled platform part of their infrastructure, useful or not. �Apache builds collaborative communities. The Hadoop ecosystem has hundreds of developers working for tens of organizations. Competitors productively collaborate on a daily basis, improving the software we all share. The Apache Software Foundation gives us the methodology that enables this. (Thanks, Apache!) The timing is right. Folks flock to Hadoop not just because it is open-source and works, but also because it fills a need. Moore�s law provides us with a bounty of affordable hardware. This has led to computing devices spreading through our world. Cars and tractors have computers. Phones, and cash registers and more have become computers. Data flows through each of these. Hadoop gives us tools to save and analyze more of this data, improving our understanding of the world. Random names are good names. When we started out, I had no idea what Hadoop would become, so I proposed a name for it that didn�t have any connotation. The project has grown, giving that name meaning. Not everyone may pronounce the word �Hadoop� the same, but we all know what it is. A whimsical name also helps remind us to have fun. People love a story. People love to hear me tell the story of my son�s toy elephant. They love to see that toy. The story has become the mythological prehistory of Hadoop. I guess every movement needs its origin story! Hadoop is a phase transition in computing. Hadoop�s developers did not invent distributed computing, nor is Hadoop its most advanced form, but Hadoop has brought distributed computing to the mainstream. Hadoop gets thousands of unreliable computers to work together reliably. As the number of computers grows, we no longer think about them individually but instead as parts of a whole. This is a fundamentally different way of using computers that�s rapidly becoming commonplace. The sky’s the limit. Hadoop was originally created to help build search engines. It�s still used for that, but its uses have grown far beyond that. Its core features have advanced tremendously, but even more dramatic is the range of systems being built on top of Hadoop. From machine learning to real-time queries, Hadoop is becoming a great platform for nearly any task folks imagine involving large amounts of data. The trends that gave rise to Hadoop continue, and Hadoop is evolving and growing to meet new challenges. We are still in the early days of this revolution. Here’s to the next seven years!</snippet></document><document id="250"><title>Cloudera is the Top Big Data Influencer in Social Media</title><url>http://blog.cloudera.com/blog/2013/03/cloudera-is-the-top-big-data-influencer-in-social-media/</url><snippet>Thanks to our friends at KDNuggets for pointing out that Cloudera is the top influencer in the “Big Data”�area, according to social media measurement service Klout – with a Klout Score of 81. (Klout is also a CDH user!) Klout, by the way, defines “influence” as�”the ability to drive action, such as sharing a picture that triggers comments and likes, or tweeting about a great restaurant and causing your followers to go try it for themselves.” Why is that significant? Because based on this metric (with which we agree), we’re nothing without our followers – just like we’re nothing without our users and customers. And we’re proud that we may have inspired them to try Big Data technologies and techniques for themselves. Thanks for the follow!�  </snippet></document><document id="251"><title>Meet the HBaseCon 2013 Program Committee</title><url>http://blog.cloudera.com/blog/2013/03/meet-the-hbasecon-2013-program-committee/</url><snippet>With HBaseCon 2013�(Early Bird registration now open!) preparations in full swing, you may be interested in learning a bit about the personalities behind the Program Committee, who are tasked with formulating a compelling, community-focused agenda.� Recently I had a chance to ask committee members Gary Helmling (Twitter), Lars Hofhansl (Salesforce.com), Jon Hsieh (Cloudera), Doug Meil (Explorys), Andrew Purtell (Intel), Enis S�ztutar (Hortonworks), Michael Stack (Cloudera), and Liyin Tang (Facebook) a few questions: How did you get involved in the HBase community? Hsieh: While working at Cloudera and being involved in the Hadoop ecosystem on other projects in 2009, I met a bunch of the folks in the Apache HBase community. When the opportunity came up to work on HBase and help take it to the next level, I jumped at it! Meil: I also got involved with HBase in 2009. Explorys was looking for a back-end datastore that would scale for our aggressive data storage and processing needs cost-effectively. By integrating into the Hadoop stack, we could leverage HDFS as well as MapReduce. S�ztutar: I was working on a social aggregator, also in 2009, and having been involved in the Hadoop community made it a no-brainer to go with HBase as the data storage layer. I was contributing to it by 2011.� Purtell: At a former employer in 2008, we were looking at an exponential trend�in the volume of data we would need to process daily for both production and research tasks. For the latter especially flexibility in schema management was a big�plus. Excellent Hadoop integration meant we could set up a full analytics pipeline.�My�involvement increased over time as we came to depend on HBase more. The great thing about basing your infrastructure on an open source foundation is you both can grow by working�together.� Stack: I was working at the Internet Archive on their crawler (Heritrix) and on search using early versions of Hadoop when the Google Bigtable paper came out (in 2006).�I soon learned that Powerset, a San Francisco natural-language search engine startup, wanted to build an open source Bigtable clone.�I went to work for them to help get what eventually became HBase off the ground.   Gary Helmling Lars Hofhansl Jon Hsieh Doug Meil Andrew Purtell Enis S�ztutar Michael Stack Liyin Tang Helmling: In 2009, I was working on a project at Meetup and looking for a storage system that would provide transparent scalability with a flexible schema. While doing some prototyping, I came across HBase, which seemed to fit the bill nicely. I contributed a couple bug fixes back to the HBase community, and I’ve been working on HBase ever since.�� Hofhansl: In 2010 it became clear that Salesforce could no grow the amount of data we can store on our traditional, relational storage. So we set out to find alternative, scalable stores. After vetting most existing NoSQL/KeyValue stores we settled on HBase, and it is being rolled out to production today. What unique perspectives do you think you bring to the PC? Hsieh: As I mentioned previously, I’ve worked on several other early Hadoop related projects. Furthermore, since I’m at Cloudera, I get to see a wide variety of customers’ HBase deployments and apps in production — and encounter a wide variety of challenges from supporting them. S�ztutar: Like Jon, I have been involved in Hadoop and other eco-system projects for a long time, and am a part of a larger group working on HBase and Hadoop development full time. Tang: I have worked on HBase development for more than two years at Facebook, and as you might expect, have some experience about how to build a reliable service on top of HBase. Meil: Similar to Liyin, I have real-life experience with HBase and I think like a user (smile). I’ve done a lot of work technical documentation with HBase and on the dist-lists and I understand the questions that people have when getting started with HBase. Helmling: I think I still retain some end user perspective on how changes we make in HBase can be applied in applications, and I have a strong interest in real world usage. I have also been around long enough to have some perspective on how HBase has evolved to where it is. � Purtell: Some of my background has�been�in the enterprise.�I�have brought in�some�concerns from that space and thought about what might make sense for�HBase: replication,�security, RESTful integration.�Some of my background has also�been�in research,�so I see the value in�HBase�being a�flexible�platform (via coprocessors) for solving challenges creatively.� Hofhansl: I have been involved in the database (relational, object oriented) and non-database (industrial software such as oil-refinery automation) world for over 24 years. So I think I also bring a broad perspective to this project. Why do you think HBaseCon is a positive thing for the community? Hofhansl: HBaseCon brings folks together, and gives them a chance to learn what is out there and to collaborate and make connections. Hsieh: As Lars said, there are folks from around the world working on this project, so it�s a great opportunity to meet them and our users in person. It�s also an opportunity for folks to tell their �war stories� and share their hard-earned wisdom with others. Helmling: It’s great to bring together so many parts of the community under one roof, from people successfully running applications, to those just starting to look at HBase, to those creating new features in HBase itself. �There is always a tremendous exchange of information, both in the sessions and in the hallways. Purtell: I echo what the others have said. HBaseCon brings developers and users of HBase together in a large-scale way that isn’t otherwise possible. I’ve been around HBase for a long time, but�I was still�surprised by the�interesting and unexpected�details of many�talks�at HBaseCon 2012. S�ztutar: As others point out, meeting people face-to-face and discussing how they use HBase in their organization is really valuable. What were some of your favorite things about HBaseCon 2012? Meil: I was at the first Hadoop World in 2009 and there was exactly one HBase presentation that day. Last year it was great to see how interest had grown to support an entire conference! There are so many different use cases now. Stack: In 2012 there was a really good vibe that came of having all the HBase brothers and sisters together under the one roof, as well as the girth, range, and quality of the talks.�There was everything from Karthik on the nosebleed scale at Facebook to a personal favorite: OCLC moving WorldCat, a 40-year-old library services project that has 25k libraries from 170 countries participating, to HBase. Hofhansl:�For me, HBaseCon was great for meeting people I had previously only known via email. I also learned a lot about what other people do with HBase! Helmling: I really enjoyed talking to people building applications on HBase about the ways they are applying it, as well as talking to other committers about the problems they are trying to solve. Even as someone who actively follows the community discussions, I came away with a new understanding of where HBase is going. �There is a depth to the face-to-face discussions that you just can’t get on the mailing lists.� Tang: I agree; I particularly enjoyed the talk about HBase schema last year, and I also had a chance to share some ideas, lessons, and experience of my own. Hsieh: The HBase community is a technical bunch and a great example of a good open source community. I really like the technical dev-centric and ops-centric focus of the talks, war stories, and�conversations. There were relatively few pure market-ecture or sales pitches, and I hope it stays that way.� Why should people attend HBaseCon 2013? Tang: For users, HBaseCon is a very good opportunity to learn about use cases of HBase and also get�familiar�with HBase internals and features. For HBase developers, it is a good time to share the insights into the feature�developments�and future roadmaps. Hofhansl: Yes, to be even more emphatic about it, HBaseCon is the best opportunity you will have to get to know the committers and other users, and learn about what is possible with HBase. Hsieh: Furthermore, the number and variety of contributors and users of HBase has grown significantly in the past year. �There are new stories, new applications, and a whole set of new systems being built on top of HBase. Lots of what Stack calls “good stuff.” Purtell: My view is that if you are curious about HBase, come and see what it is all about. If you are a user of HBase, come�to learn from the experience of others and perhaps share your own. If you are an IT professional,�entrepreneur,�researcher, or student,�come see what HBase can do for you�today and what it will be capable of in the near future. Stack: If HBaseCon 2013 is even a quarter as good as the 2012 version, it will be a great day. Helmling: What Stack said. This will be the biggest and best HBaseCon yet! Early Bird registration closes on April 23, so take advantage of discounted pricing now!</snippet></document><document id="252"><title>Meet the Engineer: Mark Grover</title><url>http://blog.cloudera.com/blog/2013/03/meet-the-engineer-mark-grover/</url><snippet>In this installment, meet Cloudera Software Engineer/Apache Bigtop Committer Mark Grover (@mark_grover). What do you do at Cloudera and in which Apache project are you involved? I�m a Software Engineer at Cloudera, involved mostly with Apache Bigtop, an open source project aimed at building a community around packaging and interoperability testing of projects in the Apache Hadoop ecosystem. In addition, I contribute to Apache Hive, a data warehousing system built on top of Apache Hadoop that allows users to structure and query their Hadoop data using familiar SQL-like syntax. I have also written a section in O�Reilly�s book on Hive, Programming Hive. Why do you enjoy your job? The Hadoop ecosystem is comprised of many different projects, each with their own specific problem space and use cases. Not only do I get a chance to take a deep-dive into some of these projects and the complex technical problems associated with them, but I also get an opportunity to integrate these projects so they interface well with each other, providing our customers with an easily deployable and well-integrated platform. While one day I may be working on creating a new datatype in Hive, the next day I may be looking into compatibility issues among two projects in the ecosystem. It is this unique mix of depth and breadth that makes me enjoy my job. To top that, I also get to contribute to open source software, collaborate with smart people both within and outside of Cloudera, and share and present open source projects at conferences and meetups. What is your favorite thing about Hadoop? Hadoop has become the de-facto framework for scalable storage and processing of large datasets. It allows users to store, access and process data that they didn�t previously have access to. Consequently, this allows them to gain more insight into their data and make better, data driven business decisions. Gaining new insights for a business is almost like giving someone eyeglasses who never had them. What�s more important though is that Hadoop brings this power to a much larger market. It enables users who don�t have the resources to invest in expensive data warehousing systems to make the most of their data in a much more cost effective manner. Today, Hadoop is used in finance, healthcare, bioinformatics, advertising, business intelligence, retail, government, social sciences, and many other avenues. Hadoop doesn�t just enable users to make better use of their data, it also opens it up to a much larger section of population — a section that hasn�t been catered to, up until now. What is your advice for someone who is interested in participating in any open source project for the first time? When I first got involved with Apache Hive, I was dealing with the problem of scalably storing web click and impression logs in a data warehouse. We were using MySQL but soon realized that it wouldn�t scale. I did an analysis of various open source technologies out there and Hive (along with Hadoop) emerged to be the winner. Consequently, I deployed Hadoop and Hive on a cluster and became a user of those projects. Eventually, our data warehouse moved over entirely to Hadoop and Hive and scalability was just a matter of adding more hardware. During this process, I ran into certain pain points with Hive. I created a few JIRAs for the same and uploaded patches wherever I could. Also, in the meantime, I started helping other users on the mailing lists with their questions. Therefore, based on my experience, I would suggest becoming a user of the project first. It helps to have a problem that the project tries to address but it�s also completely fine if you are using the project just for the sake of learning it. Join the user mailing lists and the IRC channel so you can post any questions that you have along the way, become aware of the problems other users are having and maybe even help others out whenever possible. Soon enough, you will find out pain points while using the project and from other users on the mailing lists. Some of these pain points are low hanging fruits and easy to fix, so get started on those. I have had conversations with other committers on the projects and asked them about issues that are most important to them. Create issues on the project JIRA for such issues and started posting patches. You may get some feedback from other committers, be open to it, their intent is the same as yours – to make the project better. Be a good community citizen, be genuinely interested in improving the project and show the same by posting patches, helping other users and expanding the community. You will be a committer before you know it! On a related note, projects like Apache Bigtop are always looking for new and exciting ideas to make things better for our users. If you have any ideas, or would like to contribute in shaping an open source integrated distribution of projects in the Hadoop ecosystem, I would strongly encourage you to try out�Apache Bigtop. At what age did you become interested and programming, and why? It all started for me in high school. I was learning C/C++, and to do so I wrote a Point-of-Sale system. The vision there was to build a system that would be used in the retail industry by cashiers. It was a rather old-school console application that allowed the cashier to add items to an invoice, print the invoice, and save/retrieve the invoice on demand, backed by a simple file-based database. That project provided me with a great holistic introduction to the software craft. I learned not just what a programmer does but also about the roles of an architect, program manager, tester, and release manager. From there, I never looked back! If you�re attending Big Data TechCon in Boston (April 8-10), you can catch Mark�s half-day tutorial �Introduction and Best Practices for Storing and Analyzing Your Data with Apache Hive� on April 8. (See full list of Cloudera speakers here.)  </snippet></document><document id="253"><title>Phoenix in 15 Minutes or Less</title><url>http://blog.cloudera.com/blog/2013/03/phoenix-in-15-minutes-or-less/</url><snippet>The following FAQ is provided by James Taylor of Salesforce, which recently open-sourced its Phoenix client-embedded JDBC driver for low-latency queries over HBase. Thanks, James! What is this new�Phoenix�thing I’ve been hearing about? Phoenix is an open source SQL skin for HBase. You use the standard JDBC APIs instead of the regular HBase client APIs to create tables, insert data, and query your HBase data. Doesn’t putting an extra layer between my application and HBase just slow things down? Actually, no. Phoenix achieves as good or likely better�performance�than if you hand-coded it yourself (not to mention with a heck of a lot less code) by: compiling your SQL queries to native HBase scans determining the optimal start and stop for your scan key orchestrating the parallel execution of your scans bringing the computation to the data by pushing the predicates in your where clause to a server-side filter executing aggregate queries through server-side hooks (called co-processors) In addition to these items, we’ve got some interesting enhancements in the works to further optimize performance: secondary indexes to improve performance for queries on non row key columns stats gathering to improve parallelization and guide choices between optimizations skip scan filter to optimize IN, LIKE, and OR queries optional salting of row keys to evenly distribute write load Ok, so it’s fast. But why SQL? It’s so 1970s Well, that’s kind of the point: give folks something with which they’re already familiar. What better way to spur the adoption of HBase? On top of that, using JDBC and SQL: Reduces the amount of code users need to write Allows for performance optimizations transparent to the user Opens the door for leveraging and integrating lots of existing tooling But how can SQL support my favorite HBase technique of x,y,z Didn’t make it to the last HBase Meetup did you? SQL is just a way of expressing�what you want to get�not�how you want to get it. Check out my�presentation�for various existing and to-be-done Phoenix features to support your favorite HBase trick. Have ideas of your own? We’d love to hear about them: file an�issue�for us and/or join our�user group. Blah, blah, blah – I just want to get started! Ok, great! Just follow our�install instructions: download and expand our installation tar copy the phoenix jar into the HBase lib directory of every region server restart the region servers add the phoenix client jar to the classpath of your HBase client download and�setup SQuirrel�as your SQL client so you can issue adhoc SQL against your HBase cluster I don’t want to download and setup anything else! Ok, fair enough – you can create your own SQL scripts and execute them using our command line tool instead. Let’s walk through an example now. In the bin directory of your install location: Create us_population.sql file 
CREATE TABLE IF NOT EXISTS us_population (
state CHAR(2) NOT NULL,
city VARCHAR NOT NULL,
population BIGINT
CONSTRAINT my_pk PRIMARY KEY (state, city)); Create us_population.csv file NY,New York,8143197
CA,Los Angeles,3844829
IL,Chicago,2842518
TX,Houston,2016582
PA,Philadelphia,1463281
AZ,Phoenix,1461575
TX,San Antonio,1256509
CA,San Diego,1255540
TX,Dallas,1213825
CA,San Jose,912332
 Create us_population_queries.sql file SELECT state as "State",count(city) as "City Count",sum(population) as "Population Sum"
FROM us_population
GROUP BY state
ORDER BY sum(population) DESC;    Execute the following command from a command terminal ./psql.sh  us_population.sql us_population.csv us_population_queries.sql Congratulations! You’ve just created your first Phoenix table, inserted data into it, and executed an aggregate query with just a few lines of code in 15 minutes or less! Big deal – 10 rows! What else you got? Ok, ok – tough crowd. Check out our�bin/performance.sh�script to create as many rows as you want, for any schema you come up with, and run timed queries against it. Why is it called Phoenix anyway? Did some other project crash and burn and this is the next generation? I’m sorry, but we’re out of time and space, so we’ll have to answer that next time! Thanks for your time, James Taylor http://phoenix-hbase.blogspot.com/ @JamesPlusPlus If you’re interested in HBase, be sure to register for HBaseCon 2013 (June 13, San Francisco) – THE community event for HBase contributors, developers, admins, and users. Early Bird registration is open until April 23.</snippet></document><document id="254"><title>How-to: Create a CDH Cluster on Amazon EC2 via Cloudera Manager</title><url>http://blog.cloudera.com/blog/2013/03/how-to-create-a-cdh-cluster-on-amazon-ec2-via-cloudera-manager/</url><snippet>Editor’s Note (added Feb. 28, 2014): The instructions below are deprecated for Cloudera Manager releases beyond 4.5. Please refer to this doc for instructions pertaining to releases 4.6 and later. Cloudera Manager�includes a new express installation wizard for Amazon Web Services (AWS) EC2. Its goal is to enable Cloudera Manager users to provision CDH clusters and Cloudera Impala (the open source distributed query engine for Apache Hadoop) on EC2 as easily as possible (for testing and development purposes only, not supported for production workloads) -�and thus is currently the fastest way to provision a Cloudera Manager-managed cluster in EC2. The new distinguishing feature introduced in version 4.5 is that Cloudera Manager can now launch and configure the instances for you, so you don�t have to worry about launching the instances, authorizing SSH keys, and configuring a firewall. All this can now be done from within Cloudera Manager!� Since Cloudera Manager and the nodes running CDH use internal hostnames to communicate, the Cloudera Manager server must run on EC2 as well. In fact, the Cloud Express Wizard only appears when installing Cloudera Manager on EC2. Here�s what you can do with Cloud Express Wizard: Provision new EC2 instances (AWS credentials required) Choose between CentOS and Ubuntu images (or a custom AMI) Choose your EC2 instance type Install the most recently released CDH, Cloudera Impala, and Cloudera Manager agent packages on them And here�s what you cannot do: Use pre-existing EC2 instances Install older (earlier�) versions of CDH and Cloudera Manager, or use Parcels� I am excited to show you how this feature works. These instructions will set up a fully configured CDH cluster (all services with embedded PostgreSQL) from scratch in less than 15 minutes. Step 1: Install Cloudera Manager Server on EC2 First, you will need to� launch an EC2 instance for the Cloudera Manager server, which will require an AWS Access Key ID and AWS Secret Key — please follow these instructions if you need help getting them. To launch the EC2 instance, go to �EC2� in the AWS web console and select �Instances� in the left menu. Before you provision the instance, select the EC2 region you want your instance to be in (dropdown in top right corner of the web console). For his demo, you can simply use the default �N. Virginia (us-east-1)� region. Click on �Launch Instance� and select the Classic Wizard. On the next page, pick the �Ubuntu Server 12.04 LTS� 64-bit image. You need one instance of type �m1.large.� You can keep the default values of other settings and proceed to the �Create Key Pair� page. If you don�t have an SSH key imported to EC2 already, select �Create a new Key Pair.� Enter the name of your new key pair, and click �Create and Download your key pair.� This will download a .pem file to your computer. (Important: AWS does not store the private SSH keys, so save this file or you won�t be able to SSH into the instance we�re about to launch.) It is very important to configure the EC2 firewall correctly. On the �Configure Firewall� page choose �Create a new Security Group,� and authorize all the ports listed below: TCP 22 SSH TCP 7180 Cloudera Manager web console TCP 7182 Agent heartbeat TCP 7183 (optional, Cloudera Manager web console with TLS) TCP 7432 Embedded PostgreSQL icmp -1 ping echo Next, go to the last page of the wizard and launch the instance! How to Install the Latest Version of Cloudera Manager Once the state of the instance is �running� (provisioning takes usually less than 5 minutes), you� can SSH in and install Cloudera Manager 4.5. The public hostname of the instance is listed in the instance details in the AWS console. $ ssh -i your-key.pem ubuntu@ec2-xx-xx-xx-xx.compute-1.amazonaws.com
   Download the Cloudera Manager 4.5 installer and execute it on the remote instance: $ wget http://archive.cloudera.com/cm4/installer/latest/cloudera-manager-installer.bin
$ chmod +x cloudera-manager-installer.bin
$ sudo ./cloudera-manager-installer.bin
   Once the installer finishes, use the public hostname of your server instance to navigate in your browser to http://ec2-xx-xx-xx-xx.compute-1.amazonaws.com:7180, and then log into the web console (the default username and password are both �admin�). If you�re successfully logged in, congratulations! Step 2: Installing a CDH Cluster with Cloud Express Wizard After logging in, Cloudera Manager will detect that it runs on EC2, and it will greet you with the welcome screen of the new wizard (see below). There is a warning that the instances started by this installer are instance store-based, which implies that stopping or terminating these instances results in losing all data stored on them. Remember to back-up� important data from the cluster before terminating the instances! Figure 1: Cloud Express Wizard Why does Cloudera Manager prefer instance store-backed over EBS-backed AMIs? Although EBS volumes offer persistent storage, they are network-attached and charge per I/O request, so they are not suitable for Hadoop deployments. If you wish to experiment with EBS-backed instances, you can always use a custom EBS AMI. Figure 2: Cloud Express Wizard – instance specifications Go to the second page of the wizard (Figure 2) to specify the details about the hosts we are about to launch. Cloudera Manager detects the region it runs in, and the new instances will be installed there as well. The following attributes can be specified: OS (Amazon Machine Image, AMI): Cloudera supports Ubuntu 12.04 and CentOS 6.3 images. Cloudera Manager knows which AMI to use for the specified region. If you choose to use a custom AMI (this is especially handy if you want to pre-install some tools or authorize SSH keys on your hosts), make sure the AMI is available in the specified region. Instance Type: Only instance types matching the minimum requirements for CDH hosts are available. m1.medium will be sufficient for this demo. The high-storage instances (hs1.8xlarge) are not yet available but will be included in a future release of Cloudera Manager�. Number of Instances: You will create four instances for this demo. Although there is no limit on the number of instances, you�re likely to exceed the EC2 API request limit� if you try to create more than ~20 instances at once. Group name: The optional �group name� is there to help you identify the instances launched by the wizard, and it will be used as suffix for the name, Security Group, and Key Pair of the instances. The next page (Figure 3) shows you the credentials page. You need to paste in the AWS Access ID and AWS Secret Key. Then you can choose an SSH key for the hosts; in this demo I will let Cloudera Manager generate a new key pair for my instances, and the private key will be available for download on the next page once the instances are launched. If you upload an existing private SSH key, Cloudera Manager will extract the public part and authorize it in your AWS account. Figure 3: Cloud Express Wizard – Credentials Proceed to the review page (Figure 4), where you can double-check your installation settings. You can easily go back to modify the settings. However, once the instances are provisioned, you must terminate� them in order to make changes. Note that when provisioning the instance fails on �503 Error: Api Request Limit exceeded�, it�s likely because other applications (or users) are issuing API calls to the same AWS account at the same time, or because you are launching a large number of instances at once. (In testing we successfully spun up as many as 20 instances� simultaneously.) This limitation will be removed in a future Cloudera Manager release. Figure 4: Cloud Express Wizard – Review Installation The review page indicates you are about to install the latest packages of CDH and Impala. Currently this is the only supported option in this installation wizard. If everything looks right, click the �Start Installation� button. (Note: if node installation fails because �CM failed to receive a heartbeat from Agent�, Confirm that port 7182 is authorized in the Security Group of Cloudera Manager server and re-try the installation.) Figure 5: AWS web console – EC2 instance started by Cloudera Manager Cloudera Manager uses jclouds to create new key pair and security group, and to launch the EC2 instances. The new instances will also appear in your AWS EC2 console (Figure 5). You can see that the security group and the key pair starts with �jclouds#� prefix. Also, all ports required for CDH have already been enabled. Provisioning new instances takes usually less than five minutes. Once the instances are successfully provisioned, you can download the private SSH key (Figure 6). It�s a good idea to download the key in case something goes wrong and you need to SSH in to investigate the issue. However, this installation path won�t require us to do anything manually on the remote hosts. Figure 6: Cloud Express Wizard – Instances successfully provisioned The next screen looks familiar if you�ve used the classic express wizard in Cloudera Manager. It shows the progress of package installation on the newly provisioned hosts (Figure 7). Figure 7: Cloud Express Wizard – Package installation After finishing the package installation, you can proceed to the Host Inspector and Services First Run page � you�re done.�Congratulations, the CDH cluster is up and running now! Note: The hosts cannot be terminated from Cloudera Manager, so to do that you�ll need to use EC2 CLI tools or the AWS web console instead. Go to the Instances page in�https://console.aws.amazon.com/ec2, select the instance you created for the server and all the instances launched by the wizard (hint: use the Group Name string to filter them out), and click �Actions &gt; Terminate�. Emanuel Buzek is a Software Engineer on the Enterprise team. Editor’s Note (added Feb. 28, 2014): The instructions above are deprecated for Cloudera Manager releases beyond 4.5. Please refer to�this doc�for instructions pertaining to releases 4.6 and later.  </snippet></document><document id="255"><title>How-to: Analyze Twitter Data with Hue</title><url>http://blog.cloudera.com/blog/2013/03/how-to-analyze-twitter-data-with-hue/</url><snippet>Hue 2.2 , the open source web-based interface that makes Apache Hadoop easier to use, lets you interact with Hadoop services from within your browser without having to go to a command-line interface. It features different applications like an Apache Hive editor and Apache Oozie dashboard and workflow builder. This post is based on our �Analyzing Twitter Data with Hadoop� sample app and details how the same results can be achieved through Hue in a simpler way. Moreover, all the code and examples of the previous series have been updated to the recent CDH4.2 release. Collecting Data The first step is to create the “flume” user and his home on the HDFS where the data will be stored. This can be done via the User Admin application. The second step consists of collecting some tweet data from the live Twitter stream. Apache Flume is an elegant solution for taking care of this. The configuration of Flume is detailed in the readme and previous blog post. However, if you want to skip this step, some data is available on GitHub. Just upload it as a zip file in the home directory of the flume user and the “tweets” directory will show up after a few seconds. If you are not taking this shortcut, create the tweets directory in the File Browser with the New Folder action. Then, when the Flume agent is started, the data will start appearing: Clicking on a file will display its content in the built-in viewer: Preparing Hive It is time to prepare the analysis of the tweet data. We’ll use Apache Hive, which can query the data with SQL-like syntax in a scalable way. The detailed description of the Hive setup is detailed in the readme. When Hive is ready, the tweet table can be created in the query editor of Beeswax. Notice that the Hive SerDe (to download or compile here) must be included as a jar in the query. You can read more about Hive SerDe in this previous post. To do this, just click on “Add” &gt; “File Resources”, click on the path chooser button, click on the “Home” button, and upload hive-serdes-1.0-SNAPSHOT.jar. Then just enter the CREATE TABLE statement and execute it: CREATE EXTERNAL TABLE tweets (
  id BIGINT,
  created_at STRING,
  source STRING,
  favorited BOOLEAN,
  retweet_count INT,
  retweeted_status STRUCT&lt;
    text:STRING,
    user:STRUCT&lt;screen_name:STRING,name:STRING&gt;&gt;,
  entities STRUCT&lt;
    urls:ARRAY&lt;STRUCT&lt;expanded_url:STRING&gt;&gt;,
    user_mentions:ARRAY&lt;STRUCT&lt;screen_name:STRING,name:STRING&gt;&gt;,
    hashtags:ARRAY&lt;STRUCT&lt;text:STRING&gt;&gt;&gt;,

  text STRING,
  user STRUCT&lt;
    screen_name:STRING,
    name:STRING,
    friends_count:INT,
    followers_count:INT,
    statuses_count:INT,
    verified:BOOLEAN,
    utc_offset:INT,
    time_zone:STRING&gt;,
  in_reply_to_screen_name STRING
)
PARTITIONED BY (datehour INT)
ROW FORMAT SERDE 'com.cloudera.hive.serde.JSONSerDe'
LOCATION '/user/flume/tweets'
   Now that the table is created, let’s insert some data in the table. First, select the table in the “Table” tab and click “Import data”. Enter the path “/user/flume/tweets/2013/02/25/17″ and “201302251″ as the key: Depending on the partition picked, a query similar to this will be generated: LOAD DATA INPATH '/user/flume/tweets/2013/02/25/16'
INTO TABLE `default.tweets`
PARTITION (datehour='2013022516')
   After the query executes, the table �tweets� will be available. Beeswax can access the Hive metastore and its list of tables. A description of their schema and partitions with some example of data contained in each table are helpful while designing your queries. Moreover, a wizard can guide you step-by-step to create new tables. � Analysis with Beeswax It becomes now possible to perform some SELECT queries on the data. Here is an example below but most of interesting ones are described in Parts�1 and 3�of the “Analyzing Twitter with Hadoop” series. SELECT
  � t.retweeted_screen_name,
  � sum(retweets) AS total_retweets,
  � count(*) AS tweet_count
  FROM (SELECT
  ������� retweeted_status.user.screen_name as retweeted_screen_name,
  ���� ������ retweeted_status.text,
  ���� ������ max(retweet_count) as retweets
  ����� FROM tweets
  ����� GROUP BY retweeted_status.user.screen_name,
  �������������� retweeted_status.text) t
  GROUP BY t.retweeted_screen_name
  ORDER BY total_retweets DESC
  LIMIT 10;
   Beeswax possesses multiple features for providing a better user experience than the command line shell. For example you can save queries and share them with other users. The result of a query can be exported into a new table or an HDFS file or downloaded to your desktop. Some other good examples are: Ajax refresh of the logs Quick column navigation on the result page MapReduce jobs listing with a direct access to their logs �Email me on completion� setting Multi-database support Example of the screen while running query: Seeing the result of the query: Note: if your queries are failing and you are seeing an error like below, it means that you forgot to add the �/user/flume/hive-serdes-1.0-SNAPSHOT.jar� to the query: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
   Conclusion In this post we focused on how the Beeswax application can make it easy to execute Hive queries. New features such as multi-query (HUE-159), autocomplete, and syntax highlighting (HUE-1063) are going to improve the usability even more. The next article in this series will elaborate on this topic and describe how Hue’s Apache Oozie application can be used for scheduling Hive queries in a few clicks. Thank you for reading and feel free to post comments here or on the hue-user list. We also hope to see you at the first Hue meetup�(this Wednesday, March 27)! Romain Rigaux is a Software Engineer on the Platform team.</snippet></document><document id="256"><title>One User’s Impala Experience at Data Hacking Day</title><url>http://blog.cloudera.com/blog/2013/03/one-users-impala-experience-at-data-hacking-day/</url><snippet>The following guest post comes to you from Alan Gardner of remote database services and consulting company�Pythian, who participated in Data Hacking Day (and was on the winning team!)�at Cloudera’s offices in February. Last Feb. 25, just prior to attending Strata, Alex Gorbachev (our CTO) and I had the chance to visit Cloudera�s Palo Alto offices for Data Hacking Day. The goal of the event was to produce something cool that�leverages Cloudera Impala – the new open source, low-latency platform for querying data in Apache Hadoop. Our hosts helpfully suggested some datasets, including the DEBS 2013 Grand Challenge data. This dataset contains the position of all the players and ball during a football match; our project was to map the data for a given span of time and player onto a map of the field, to create a heatmap of how much time that player spent at different positions. The Data The full-game�CSV describes the position of all the players and the ball on the field, in the following format: sid, ts, x, y, z, |v|, |a|, vx, vy, vz, ax, ay, az sid uniquely identifies the sensor: two are attached to each player (one per leg), four are attached to the ball, and others are attached to the goalies� arms and the referees. ts is the timestamp in picoseconds, and (x,y,z) is the position of the ball in space. The dataset also provides the acceleration a, and velocity v in each axis, and their absolute magnitude � we don�t use these values in our application. In all, the Hive statement to create the table is: CREATE TABLE soccer ( sid int, ts bigint, x double, y double, z double, v double, a double, vx double, vy double, vz double, ax double, ay double, az double )
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
   DEB doesn�t provide an easy, machine-readable mapping of the sensors to the players, so we wrote our own, called sensors.csv. �The definition of the sensors table is: CREATE TABLE sensors ( sid int, sensor_position string, team string, player_type string, player_name string )
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
   By joining the sensors table in, we can do ad hoc queries about specific players� positions, rather than working with sensor ids. Querying The web component is written in Ruby, and leverages Colin Marc�s impala-ruby gem,�which he�s blogged about here. Colin�s gem provides a simple interface to run SQL queries on Impala; combined with Sinatra, the server-side came together very quickly. The main query we use to populate the heatmap is: SELECT (round(x/2650)*17)+440 as x, (round(-1*y/3400)*24)+280 as y, count(*)/200 val from soccer_part
WHERE sid =#{params[:sid]}
AND ts &lt; #{params[:max]}
AND ts &gt; #{params[:min]} group by x,y
   This query splits the field into 20-by-20 grid and counts the number of sensor readings which land within a given square � since the readings are at regular intervals, this is proportional to how much time the player spent in that area. We have to approximate player position like this to reduce the load on the web server and client system � a straight SELECT would produce about 50MB of data per query, versus the 10KB the aggregation uses.�The multiplication and addition roughly map the x and y values provided by DEBS onto the football field image in the web UI.� Performance At the event we ran the web interface against the single-node demo VM image Cloudera provides to test Impala. Running on a Macbook Pro with 8GB of RAM, Impala was consistently capable of producing 1-to-2 second response times, which was fast enough that we could iterate on queries and quickly test them in the console. Once we got home, we loaded the same data into Pythian�s in-house Hadoop cluster, to better assess performance on a �real-world� system. Querying on the console, the benefits of not spinning up a JVM are appreciable: a typical query to aggregate all the sensor data ran in about 1 second in Impala, versus 90 seconds in Hive. In the web interface we saw similar performance: the entire request – including the web server and Beeswax – takes about 1.5 seconds on the first SELECT for a player, and about 800ms on subsequent queries over the same data. It�s worth noting that partitioning the table produced a meaningful performance boost, since we almost always scan all rows for a single sid � we gained about 100ms on average, or about a 10% boost. The queries to set up the partitioned table are included on the Github page (see below). Get The Code All of the code, including the CSV of player-sensor mappings, is available on Github. Feel free to direct any feedback to gardner@pythian.com.</snippet></document><document id="257"><title>Cloudera’s Jeff Hammerbacher on Charlie Rose</title><url>http://blog.cloudera.com/blog/2013/03/clouderas-jeff-hammerbacher-on-charlie-rose/</url><snippet>In this Charlie Rose interview that aired on March 22, 2013, Cloudera’s Chief Scientist Jeff Hammerbacher (@hackingdata) offers fascinating insights into the origins of Big Data and data science techniques at Google and their re-implementation into open source used by consumer Web companies. Furthermore, he offers great detail about their positive application across healthcare diagnostics and delivery – as well as the overall need for better balance between “numerical imagination” and “narrative imagination” in everything we do (in order to “ask bigger questions”, as some would say). It’s an incredibly valuable look into where Big Data came from, where it’s going, and how Cloudera is helping it get there.</snippet></document><document id="258"><title>Cloudera ML: New Open Source Libraries and Tools for Data Scientists</title><url>http://blog.cloudera.com/blog/2013/03/cloudera_ml_data_science_tools/</url><snippet>Editor’s note (12/19/2013): Cloudera ML has been merged into the Oryx project. The information below is still valid though. Last month, Apache Crunch�became the fifth project (along with Sqoop, Flume, Bigtop, and MRUnit) to go from Cloudera’s github repository through the Apache Incubator and on to graduate as a top-level project within the Apache Software Foundation. As the founder of the project and a newly minted Apache VP, I wanted to take this opportunity to express my gratitude to the Crunch community, who have taught me that leadership in the Apache Way means service, humility, and investing more time in building a community than I spend writing code. Working with you all on our shared vision is the highlight of every work week. Creating Analytical Applications with Crunch: Cloudera ML The Crunch Java libraries operate at a lower level of abstraction than other tools for creating MapReduce pipelines, like Apache Pig, Apache Hive, or Cascading. Crunch does not make any assumptions about the data model in your pipeline, which makes it easy to create data pipelines over non-relational data sources such as time series, Avro records, and Mahout Vectors. In fact, I originally wrote Crunch while I was working on Seismic Hadoop, a command line tool for processing time series of seismic measurements on Hadoop. When the data science team sat down with our training team to begin planning our next data science course, we quickly discovered that there weren’t any open-source tools in the Hadoop ecosystem that would allow students to perform the data preparation and model evaluation techniques that we wanted them to learn. For example, it wasn’t possible to quickly summarize a CSV file of numerical and categorical variables via a single MapReduce job, and then use that summary to convert the CSV file into the distributed matrix format that is used as input to many of Mahout’s algorithms. We were also concerned that there wasn’t a lot of guidance as to how to choose values for many of the parameters that Mahout’s algorithms require, and that this might discourage new data scientists from using these models effectively. Today, I’m pleased to introduce Cloudera ML, an Apache licensed collection of Java libraries and command line tools to aid data scientists in performing common data preparation and model evaluation tasks. Cloudera ML is intended to be an educational resource and reference implementation for new data scientists that want to understand the most effective techniques for building robust and scalable machine learning models on top of Hadoop. Clustering for Fun The first algorithm that we’re focusing on in Cloudera ML is�k-means clustering, the problem of creating K groups from a collection of N points such that each point is assigned to the group whose center is closest to that point. It is one of the most widely-used analytical algorithms in industry because of its simplicity, versatility, and performance. I have also found it to be one of the most misunderstood and misused techniques in the machine learning toolkit, especially by new data science teams. Clustering can be appealing because it belongs to the class of unsupervised learning techniques: you do not need to spend any time or money to acquire the labeled data that is necessary to perform supervised learning tasks, such as building a predictive model. But the ease of getting started with clustering can be a double-edged sword, especially if it leads new data science teams to work on problems that seem approachable instead of problems that are important to the business. Let’s consider the following scenario: the data science team at Company X is looking around for a proof-of-concept project, and decides to use k-means clustering in order to develop a new customer segmentation model for the marketing department. The team gathers a large quantity of data, and then spends a month cleansing it, vectorizing it, normalizing it, and experimenting with different choices of distance metrics and values of K. At the end of the month, they present their best clustering to the marketing department, who proceed to respond in one of two ways: Response #1: “Yep, that’s exactly how we think about our customers. Thank you for telling us something we already know.” Response #2: “No, that doesn’t really match up with how we think about our customers. You must have done something wrong– go back and try it again.” (The data science team that receives this response spends several months iterating on distance metrics and values of K until they manage to converge to Response #1.) Don’t let this situation happen to you: always start a new project by taking the time to understand and document how the performance of your modeling and analysis will be evaluated, and avoid situations where personal opinions matter more than business metrics. A data scientist’s time is a terrible thing to waste. Clustering for Profit One of my favorite applications of k-means clustering is for finding outliers in data sets, which can be useful for detecting fraudulent transactions, network intrusions, or simply as part of the process of data cleansing. When working on these types of problems, my goal is to find a “good” clustering of the data, and then examine the points that didn’t fit into any of the larger clusters that I found. During this process, I typically need to resolve three major issues, especially when working with large data sets: For a given set of input features, how do I quickly create a good partition of the data into K clusters? What value of K should I use? Once I have some good candidate clusterings, how do I examine the outliers? Historically, Hadoop has not been particularly great for helping me solve these problems. Although there are many implementations of�Lloyd’s algorithm (which is what most people are referring to when they talk about k-means clustering) as an iterative series of MapReduce jobs, they all suffer from the same two limitations: Lloyd’s algorithm requires a large (and unknown) number of passes over the data set in order to converge. In fact, it has been shown that the theoretical worst-case running time of the algorithm is super-polynomial in the size of the input [PDF]. If we choose a random set of points for our initial clusters, it is possible for Lloyd’s algorithm to become stuck in a local optimum that is arbitrarily bad relative to the optimal clustering. As you can imagine, there isn’t a whole lot of appeal in running an algorithm that takes an arbitrarily long time to return an arbitrarily bad answer. But don’t despair: a number of academic papers have introduced modifications to the basic k-means algorithm that provide good bounds on both the number of passes over the data that we need to perform as well as on the quality of the solution that we ultimately find. At the 2011 NIPS conference, Shindler et al. introduced “Fast and Accurate k-means for Large Data Sets [PDF],” an an approach to k-means that requires only a single pass over the data, provides reasonable guarantees on the quality of the clusters it finds, and �is the basis for MAHOUT-1154, an eagerly awaited component of Mahout’s 0.8 release. Last summer, Bahmani et al. introduced “Scalable k-means++ [PDF],” which provides somewhat better guarantees on the quality of the clusters in exchange for performing a few extra passes over the data set. Both of these algorithms appear to find good clusters in practice and can both be used to create small sketches of a large data set that reflect its overall structure, but are small enough to be analyzed interactively on a single machine. Data, Big and Small If you were paying at least somewhat close attention, you may have noticed that the algorithms I’m describing above are essentially clever sampling techniques. With all of the hype surrounding big data, sampling has gotten a bit of a bad rap, which is unfortunate, since most of the work of a data scientist involves finding just the right way to turn a large data set into a small one. Of course, it usually takes a few hundred tries to find that right way, and Hadoop is a powerful tool for exploring the space of possible features and how they should be weighted in order to achieve our objectives. Whenever possible, we want to minimize the amount of parameter tuning required during model fitting. At the very least, we should try to provide feedback on the quality of the model that is created by different parameter settings. For k-means, we want to help data scientists choose a good value of K, the number of clusters to create. In Cloudera ML, we integrate the process of selecting a value of K into the data sampling and cluster fitting process by allowing data scientists to evaluate multiple values of K during a single run of the tool and reporting statistics about the stability of the clusters,�such as the prediction strength. Finally, we want to investigate the anomalous events in our clustering- those points that don’t fit well into any of the larger clusters. Cloudera ML includes a tool for using the clusters that were identified by the scalable k-means algorithm to compute an assignment of every point in our large data set to a particular cluster center, including the distance from that point to its assigned center. This information is created via a MapReduce job that outputs a CSV file that can be analyzed interactively using Cloudera Impala or your preferred analytical application for processing data stored in Hadoop. Cloudera ML is under active development, and we are planning to add support for pivot tables, Hive integration via HCatalog, and tools for building ensemble classifers over the next few weeks. We’re eager to get feedback on bug fixes and things that you would like to see in the tool, either by opening an issue or a pull request on our github repository. We’re also having a conversation about training a new generation of data scientists next Tuesday, March 26th, at 2pm ET/11am PT, and I hope that you will be able to join us. Josh Wills is Cloudera’s senior director of data science.</snippet></document><document id="259"><title>Video Premiere: Training a New Generation of Data Scientists</title><url>http://blog.cloudera.com/blog/2013/03/video-premiere-training-a-new-generation-of-data-scientists/</url><snippet>Data scientists drive data as a platform to answer previously unimaginable questions. These multi-talented data professionals are in demand like never before because they identify or create some of the most exciting and potentially profitable business opportunities across industries. However, a scarcity of existing external talent will require companies of all sizes to find, develop, and train their people with backgrounds in software engineering, statistics, or traditional business intelligence as the next generation of data scientists. Join us for the premiere of Training a New Generation of Data Scientists on Tuesday, March 26, at 2pm ET/11am PT. In this video, Cloudera’s Senior Director of Data Science, Josh Wills, will discuss what data scientists do, how they think about problems, the relationship between data science and Hadoop, and how Cloudera training can help you join this increasingly important profession. Following the video, Josh will answer your questions about data science, Hadoop, and Cloudera’s Introduction to Data Science: Building Recommender Systems course. Register today!</snippet></document><document id="260"><title>How-to: Import a Pre-existing Oozie Workflow into Hue</title><url>http://blog.cloudera.com/blog/2013/03/how-to-import-a-pre-existing-oozie-workflow-into-hue/</url><snippet>Hue is an open-source web interface for Apache Hadoop�packaged with CDH that focuses on improving the overall experience for the average user. The�Apache Oozie�application in Hue provides an easy-to-use interface to build workflows and coordinators. Basic management of workflows and coordinators is available through the dashboards with operations such as killing, suspending, or resuming a job. Prior to Hue 2.2�(included in CDH 4.2), there was no way to manage workflows within Hue that were created outside of Hue. As of Hue 2.2, importing a pre-existing Oozie workflow by its XML definition is now possible. How to import a workflow Importing a workflow is pretty straightforward. All it requires is the workflow definition file and access to the Oozie application in Hue. Follow these steps to import a workflow: Go to Oozie Editor/Dashboard &gt; Workflows and click the �Import� button.     Provide at minimum a name and workflow definition file.       Click �Save�. This will redirect you to the workflow builder with a message in blue near the top stating �Workflow imported�.     How It Works The definition file describes a workflow well enough for Hue to infer its structure. It also provides the majority of the attributes associated with a node, with the exception of some resource references. Resource reference handling is detailed in the following paragraphs. A workflow is imported into Hue by uploading the XML definition. Its nodes are transformed into Django serialized objects, and then grok�d by Hue: Workflow transformation pipeline (Without hierarchy resolution) Workflow Definitions Transformation Workflow definitions have a general form, which make them easy to transform. There are several kinds of nodes, all of which have a unique representation. There are patterns that simplify the task of transforming the definition XML: &lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;workflow-app xmlns="uri:oozie:workflow:0.4" name="fs-test"&gt;
  &lt;start to="Fs" /&gt;
  &lt;action name="Fs"&gt;
    &lt;fs&gt;
      &lt;delete path="${nameNode}${output}/testfs" /&gt;
      &lt;mkdir path="${nameNode}${output}/testfs" /&gt;
      &lt;mkdir path="${nameNode}${output}/testfs/source" /&gt;
      &lt;move source="${nameNode}${output}/testfs/source" target="${nameNode}${output}/testfs/renamed" /&gt;
      &lt;chmod path="${nameNode}${output}/testfs/renamed" permissions="700" dir-files="false" /&gt;
      &lt;touchz path="${nameNode}${output}/testfs/new_file" /&gt;
    &lt;/fs&gt;
    &lt;ok to="end" /&gt;
    &lt;error to="kill" /&gt;
  &lt;/action&gt;
  &lt;kill name="kill"&gt;
    &lt;message&gt;Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;
  &lt;/kill&gt;
  &lt;end name="end" /&gt;
&lt;/workflow-app&gt;
   Nodes are children of the root element workflow-app. Every node has a unique representation varying in at least their name. Every action is defined by the element action with a unique name. Its immediate children consist of the action type and links. The children of the node type tag are various properties associated with the action. The start, end, fork, decision, join, and kill nodes have their own transformation, where actions are transformed using a general Extensible Stylesheet Language Transformation, or XSLT. The different attributes are generally not unique to an action. For instance, the Hive action and Sqoop action both have the prepare attribute. Hue provides an XSLT for every action type, but only to import non-unique attributes and to define transformations for unique attributes. In the XSLT below, the sqoop action is defined by importing all of the general fields and defining any Sqoop-specific fields: &lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;xsl:stylesheet xmlns:xsl="http://www.w3.org/1999/XSL/Transform" xmlns:workflow="uri:oozie:workflow:0.1" xmlns:sqoop="uri:oozie:sqoop-action:0.2" version="1.0" exclude-result-prefixes="workflow sqoop"&gt;
  &lt;xsl:import href="../nodes/fields/archives.xslt" /&gt;
  &lt;xsl:import href="../nodes/fields/files.xslt" /&gt;
  &lt;xsl:import href="../nodes/fields/job_properties.xslt" /&gt;
  &lt;xsl:import href="../nodes/fields/job_xml.xslt" /&gt;
  &lt;xsl:import href="../nodes/fields/params.xslt" /&gt;
  &lt;xsl:import href="../nodes/fields/prepares.xslt" /&gt;
  &lt;xsl:template match="sqoop:sqoop"&gt;
    &lt;object model="oozie.sqoop" pk="0"&gt;
      &lt;xsl:call-template name="archives" /&gt;
      &lt;xsl:call-template name="files" /&gt;
      &lt;xsl:call-template name="job_properties" /&gt;
      &lt;xsl:call-template name="job_xml" /&gt;
      &lt;xsl:call-template name="params" /&gt;
      &lt;xsl:call-template name="prepares" /&gt;
      &lt;field name="script_path" type="CharField"&gt;
        &lt;xsl:value-of select="*[local-name()='command']" /&gt;
      &lt;/field&gt;
    &lt;/object&gt;
  &lt;/xsl:template&gt;
  &lt;xsl:output method="xml" version="1.0" encoding="UTF-8" indent="yes" /&gt;
&lt;/xsl:stylesheet&gt;
   The above XSLT imports transformation definitions for the archives, files, job properties, job XML, params, and prepares attributes. If a Sqoop action XML definition were to be transformed by the above XSLT, the resulting XML would take on the following form: &lt;object model="oozie.sqoop" pk="0"&gt;
  &lt;field name="archives" type="TextField"&gt;...&lt;/field&gt;
  &lt;field name="files" type="TextField"&gt;...&lt;/field&gt;
  &lt;field name="job_properties" type="TextField"&gt;...&lt;/field&gt;
  &lt;field name="job_xml" type="TextField"&gt;...&lt;/field&gt;
  &lt;field name="params" type="TextField"&gt;...&lt;/field&gt;
  &lt;field name="prepares" type="TextField"&gt;...&lt;/field&gt;
  &lt;field name="script_path" type="CharField"&gt;...&lt;/field&gt;
&lt;/object&gt;
   Workflow Structure Resolution The structure of the workflow is created after the nodes are imported. Internally, the workflow hierarchy is represented as a set of �links� between nodes. The workflow definition contains references to next nodes in the graph through the tags ok, error, and start. These references are used to create transitions. The following code snippet illustrates a transition that goes to a node called end and an error transition that goes to a node named kill: &lt;ok to="end" /&gt;
&lt;error to="kill" /&gt;
   Workflow definitions do not have resources, such as a jar file used when running a MapReduce action. Hue intentionally leaves this information out when performing the transformation because it is not in the workflow definition. This forces users to update any resource-specific information within actions. An imported workflow. Note that its resource information is missing. Summary and Next Steps Hue can manage workflows with its dynamic workflow builder and now, officially, can import predefined workflows into its system. Another benefit of parsing the XML definition is it enables all workflows to be displayed as a graph in the dashboard: Dashboard graph of an imported workflow The workflow import process is good, but not perfect yet. Ideally, as detailed above, resources will be found on the system and validated before being imported or resources should be optionally provided. Have any suggestions? Feel free to tell us what you think via hue-user. Abraham Elmahrek is a Software Engineer on the Platform team.</snippet></document><document id="261"><title>How To: Use Oozie Shell and Java Actions</title><url>http://blog.cloudera.com/blog/2013/03/how-to-use-oozie-shell-and-java-actions/</url><snippet>Apache Oozie, the workflow coordinator for Apache Hadoop, has actions for running MapReduce, Apache Hive, Apache Pig, Apache Sqoop, and Distcp jobs; it also has a Shell action and a Java action. These last two actions allow us to execute any arbitrary shell command or Java code, respectively. In this blog post, we�ll look at an example use case and see how to use both the Shell and Java actions in more detail. Please follow along below; you can get a copy of the full project at Cloudera’s GitHub as well. This how-to assumes some basic familiarity with Oozie. Example Use Case Suppose we�d like to design a workflow that determines which earthquakes from the last 30 days have a magnitude greater than or equal to that of the largest earthquake in the last hour; also, we�d like to run this workflow every hour. One last requirement for our workflow is that in order to save bandwidth and time, we�d like to be able to skip downloading and processing the 30 days of earthquake data if there were no �large� earthquakes within the last hour; because �large� is subjective, we�ll just go with 3.2 for this example but we should make this easy to configure. We don�t have the earthquake data ourselves, but luckily the US Geological Survey (USGS) maintains earthquake datasets in a few formats that are updated quite frequently here. We�re interested in their �Past Hour� and �Past 30 Days� datasets. We�ll use the CSV (comma-separated value) format because it�s simple. The URL for the �Past Hour� dataset is http://earthquake.usgs.gov/earthquakes/feed/csv/all/hour and the URL for the �Past 30 Days� is http://earthquake.usgs.gov/earthquakes/feed/csv/2.5/month. Unfortunately, the USGS doesn�t make all earthquake data available for 30 days, only ones with magnitudes greater than 2.5. Going back to our requirements, we can create a simple Pig script to process the 30 days of earthquake data. But what about checking the last hour of data or downloading the data from the USGS website?� We can use the Shell action to run a shell script to check the last hour of data and the Java action to run some Java code to handle downloading the data. We�ll go into more detail on this later, but first let�s learn about some information that we�ll find useful for this workflow. Capturing Data from an Action We�ll need a way to get some small pieces of data (i.e. the magnitude of the largest earthquake from the past hour) from our shell script into the workflow itself. We can tell Oozie to capture any data that the shell action outputs to STDOUT so it�s available in the workflow by simply adding the &lt;capture-output/&gt; element. The outputted data must be in Java Properties file format and it must not exceed 2KB (by default). Then all we have to do is have our shell script echo the necessary information to STDOUT; we�ll also need to make sure that it doesn�t echo anything else. While we won�t need to do this in our example, &lt;capture-output/&gt; can also be used to get data from the Java action. The only difference is that instead of STDOUT, we have to write to a (local) file whose path must be obtained from�System.getProperty("oozie.action.output.properties"). Shell Action Caveats The Shell action has the following caveats: Interactive commands are not supported. In an unsecure cluster, everything is run as the user who started the TaskTracker where our shell script is running (mapred user in CDH4); in a “Kerberized” cluster, it will run as the UNIX user of whomever submitted the workflow. This is in contrast to MapReduce-based actions, which, for the purposes of interaction with Hadoop, are run as the user who submitted the workflow –although the UNIX process for the task still runs as mapred. The Shell action is executed on an arbitrary node in the cluster. Different operating systems may have different versions of the same shell commands. The implications of that third caveat are very important. Oozie executes the shell action in the same way it executes any of the other actions: as a MapReduce job. In this case, it’s a 1-mapper-0-reducer job, which is why it can be executed on any node in the cluster. This means that any command or script that we want to execute has to be available on that node; because we don�t know which node the shell action will be executed on, the command or script has to be available on all nodes!�This is fine for typical built-in shell commands like echo or grep, but can be more problematic for programs such as matlab, which must not only be installed but may also require a license. Instead, we�ll be putting our script in the same directory as the workflow.xml and taking advantage of the &lt;file&gt;�tag to have Oozie copy it to the proper node for us. Even though two operating systems, or even two different versions of the same operating system, may have the same built-in commands or programs, they may behave differently or accept different arguments. For example, we�ll be using the tail�command later; on Mac OS 10.7.5 we can specify the number of lines with the following arguments, but this won�t work properly on CentOS 6.2: tail +2 hour.txt
   This doesn�t mean that we can�t use the tail�command though; it just means that we have to be careful to ensure that all of the machines on which our Shell action could possibly run have compatible versions of the built-in commands. For example, the following arguments work correctly on both Mac OS 10.7.5 and CentOS 6.2: tail -n +2 hour.txt
   That said, the script has been tested on Mac OS 10.7.5 and CentOS 6.2, but may require minor tweaking on other operating systems or versions. Java Action Caveats The Java action has the following caveats: The Java action is executed on an arbitrary node in the cluster. Calling System.exit(int n) will always make the Java action do an �error to� transition. The Java action is also executed on an arbitrary node in the cluster for the same reason the Shell action is; however, this is typically less problematic for the Java action because external resources would be JAR files that we�d be already including anyway. It is also important that our Java code not call System.exit(int n) as this will make the Java action do an �error to� transition, even if the exit code was 0; instead, an �ok to� transition is indicated by gracefully finishing main and an �error to� transition is indicated by throwing an exception. Our Shell Script We�ll use a shell script to download the �Past Hour� data, get the magnitude of the largest earthquake (in the last hour), and determine if we need to go on to process the �Past 30 Days� data. Let�s call the script check-hour.sh. We�ll need to take in the minimum earthquake magnitude that we�re interested in as an argument (for deciding whether or not to process the “Past 30 Days� data): earthquakeMin=�$1�
   Now we�ll need the script to download the �Past Hour� data to a file; we can use curl�for this: curl http://earthquake.usgs.gov/earthquakes/feed/csv/all/hour -o "hour.txt" -s -S -f
   The -o argument tells curl where to download the file, the -s and -S arguments together make curl silent except for errors, and the -f argument makes curl return an error instead of outputting server error HTML pages (e.g. error 404). The -s argument is important here because we don�t want to pollute STDOUT. The working directory, where the hour.txt file will be written to, is on the TaskTracker that�s running our script; it will be automatically removed once the action ends, so we don�t have to worry about temporary files or anything like that. Here�s what the downloaded file looks like: DateTime,Latitude,Longitude,Depth,Magnitude,MagType,NbStations,Gap,Distance,RMS,Source,EventID,Version
2013-02-19T23:32:25.000+00:00,33.472,-116.881,15.6,1.1,Ml,,79,0.1,0.23,ci,ci11246978,1361316922097
2013-02-19T23:22:34.400+00:00,33.576,-117.598,7.7,1.8,Ml,,130,0.4,0.51,ci,ci11246970,1361316305852
   The next step is to determine the magnitude of the largest earthquake from the file we just downloaded. We�ll use a combination of standard Linux tools for this. The first line in the file is always the header information for the columns, which we don�t actually want; we can use the tailcommand to keep everything except for the first line: tail -n +2 hour.txt
   which looks like this: 2013-02-19T23:32:25.000+00:00,33.472,-116.881,15.6,1.1,Ml,,79,0.1,0.23,ci,ci11246978,1361316922097
2013-02-19T23:22:34.400+00:00,33.576,-117.598,7.7,1.8,Ml,,130,0.4,0.51,ci,ci11246970,1361316305852
   Going by the headers we just removed, the fifth column is the magnitude. We can get just the fifth column by using the cut�command: tail -n +2 hour.txt | cut -f 5 -d �,�
   The -f 5 tells it to get the fifth column and the -d �,�tells it to use comma as the column separator. This gives us: 1.1
1.8
   We can now use the sort�command to put the magnitudes in sorted order. In this case, they are already sorted, but that�s not always guaranteed, especially if there happens to be more than two earthquakes in the past hour. tail -n +2 hour.txt | cut -f 5 -d �,� | sort -n
   We can then use the tail�command again to get just the last magnitude, which must be the largest because we just sorted them: tail -n +2 hour.txt | cut -f 5 -d �,� | sort -n | tail -n -1
   Finally, we get just this: 1.8
   We now need to compare the largest magnitude with earthquakeMin to determine whether or not to process the “Past 30 Days� data. It�s not recommended to use bash directly for comparing floats, but we can use the bc�command instead: compare=`echo $largest '&gt;=' $earthquakeMin | bc -l`
   where largest is the variable where we stored the largest magnitude from the �Past Hour� data. We can then check the value of compare and echo to STDOUT�accordingly:   if [ "$compare" == "1" ]
  then
  ��� echo "largest=$largest"
  ��� echo "isLarger=true"
  else
  ��� echo "isLarger=false"
  fi
   Here we�re echoing, in Java Properties file format, whether or not largest is greater than or equal to earthquakeMin and if so, what largest is. We�ll see how these values can be picked up by the workflow later. One last thing that we should take care of in our shell script: what happens if there were no earthquakes in the past hour?� (standard_in) 1: parse error
   Our script will give the above error message (which would cause Oozie to FAIL the workflow). We can prevent this from happening by adding a simple check for this edge case:   numLines=`cat hour.txt | wc -l`
  if [ $numLines \&lt; 2 ]
  then
  ��� echo "isLarger=false"
  else
  ...
   This will use the cat command to echo the contents of the hour.txt file and pass it to the wc command. The -l argument tells wc to count the number of lines. We know that we need at least two lines (the header and one line of data) to not get the error message, so we can check if numLines is less than 2. We can then echo the property from before as false, instead of continuing with the script. Our final script (with some additional minor details) looks like this: #!/bin/bash -e

earthquakeMin="$1"

curl http://earthquake.usgs.gov/earthquakes/feed/csv/all/hour -o "hour.txt" -s -S -f

numLines=`cat hour.txt | wc -l`
if [ $numLines \&lt; 2 ]
then
  ��� echo "isLarger=false"
else
  ��� largest=`tail -n +2 hour.txt | cut -f 5 -d "," | sort -n | tail -n -1`
  ��� compare=`echo $largest '&gt;=' $earthquakeMin | bc -l`
  ��� if [ "$compare" == "1" ]
  ��� then
  ������� echo "largest=$largest"
  ������� echo "isLarger=true"
  ��� else
  ������� echo "isLarger=false"
  ��� fi
fi
   Note that we set -e to make sure that the script exits with an error if any of these commands had an error, instead of continuing. If the script exits with a non-zero exit code, the shell action will follow the �error to� transition instead of the �ok to� transition. Making a Decision Once our workflow finishes running our shell script, we need to make a decision. We can use the aptly-named Decision Control Node for this. Decision nodes are useful when we want our workflow to transition to a different node based on some criteria; it�s essentially identical to the switch�statement available in many programming languages. Here�s what ours will look like: &lt;decision name="decide"&gt;
  ��&lt;switch&gt;
  ������&lt;case to="get-data"&gt;
  ����������${wf:actionData('shell-check-hour')['isLarger']}
  ������&lt;/case&gt;
  ������&lt;default to="end"/&gt;
  ��&lt;/switch&gt;
&lt;/decision&gt;
   Earlier we saw in our shell script that we outputted whether or not we wanted the workflow to process the �Past 30 Days� data by echoing either isLarger=true or isLarger=false. We can use the wf:actionData EL function to retrieve this key=value pair from our shell action anywhere in the workflow. This EL function�s argument is the name of the action (which we�ll be calling �shell-check-hour� later); this then gives us an array of all of the key=value pairs that Oozie captured from that action. So here we�re checking if isLarger is true. If so, then Oozie will transition the workflow to the �get-data� action; otherwise, the �default to� transition will be followed, which goes to �end� to finish the workflow and skipping the processing of the �Past 30 Days� data. Downloading the Data to HDFS Before we can run a Pig job on the �Past 30 Days� data, we need to get it into HDFS. We could use the shell action again with curl and the Hadoop CLI, but there are some issues with this approach, including the second caveat with the Shell action that we saw earlier about which user is executing the script. We�ll instead use the Java action, which, as we mentioned earlier, doesn�t have this limitation. The Java action is particularly powerful because it allows us to execute any Java code we want. Here�s what our Java main function looks like: public static void main(String[] args) throws IOException {
  ��� URL url = new URL("http://earthquake.usgs.gov/earthquakes/feed/csv/2.5/month");
  ��� HttpURLConnection conn = (HttpURLConnection)url.openConnection();
  ��� conn.connect();
  ��� InputStream connStream = conn.getInputStream();

  ��� FileSystem hdfs = FileSystem.get(new Configuration());
  ��� FSDataOutputStream outStream = hdfs.create(new Path(args[0], "month.txt"));
���   IOUtils.copy(connStream, outStream);

  ��� outStream.close();
  ��� connStream.close();
  ��� conn.disconnect();
}
   The first four lines open a connection to the USGS website where the dataset is located and get an InputStream. We then get the HDFS FileSystem and create an FSDataOutputStream, which we can use to write a new file, month.txt. We can then use Apache Commons IOUtils to simplify writing the input stream from the connection to the output stream writing to month.txt. And finally, we close the two streams and disconnect the connection. The folder that we want to put month.txt in is passed in as an argument (i.e. args[0]) to our Java program; we�ll see how to do this in the workflow later. Though we�ll need to make sure that this location is set to be the input of our Pig job, which we�ll look at next. Processing the Data The final step is to process the data that we downloaded to HDFS in the previous section. The processing that we want to do is to filter out any earthquakes whose magnitude is less than that of the largest earthquake we found in the �Past Hour� data earlier. As before, we can get this magnitude in the workflow with this EL function call: ${wf:actionData('shell-check-hour')['largest']}
   We�ll see how to pass this to the Pig script from our workflow later. For now, let’s just assume we have a variable in our Pig script named $MINMAG. We�ll also need to have variables for the input file (the �Past 30 Days� data we downloaded to HDFS) and the output directory: $INPUT and $OUTPUT respectively. First we need to LOAD�the data to form a relation: A = LOAD '$INPUT' using PigStorage(',') AS(a1,a2,a3,a4,a5:float,a6,a7,a8,a9,a10,a11,a12,a13);
   The USING PigStorage(�,�) is to tell it to parse the input data as comma-delimited instead of tab-delimited, which is the default. The AS (a1, �) is to tell it the schema that we want it to use. When we looked at the header from the input file, we saw that the magnitude of the earthquake in a data row was the fifth column; hence why we need to make sure to let Pig know that a5 is a float — the rest of the columns we won�t be using so we don�t need to bother specifying data types for them. Now that we have the data loaded into Pig, we can simply use the FILTER�operator: B = FILTER A BY (a5 &gt;= $MINMAG);
   The expression (a5 &gt;= $MINMAG) is telling Pig to filter out any data row whose a5 (i.e. the magnitude of the earthquake) is less than $MINMAG (i.e. the largest earthquake in the last hour). The resulting rows that pass the filter are put into relation B. What happens to the first row of our data, the one with the header information?�Its fifth column is obviously not a float. (DateTime,Latitude,Longitude,Depth,,MagType,NbStations,Gap,Distance,RMS,Source,EventID,Version)
   Note that the fifth column is an empty string instead of �Magnitude�. Conveniently, the FILTER operator rejects null values, so the header row won�t pass the filter and won�t be a problem. Now that we�ve filtered the data, we need to output it back to HDFS. We can do that with the STORE�operator: STORE B INTO '$OUTPUT' USING PigStorage(',');
   As before with the LOAD operator, we specify USING PigStorage(�,�) to keep the data in a comma-delimited format. The end result is that the output from this Pig job has all of the data rows from the last 30 days whose magnitude is greater than or equal to that of the largest earthquake from the last hour. Here�s our final Pig script: A = LOAD '$INPUT' USING PigStorage(',') AS (a1,a2,a3,a4,a5:float,a6,a7,a8,a9,a10,a11,a12,a13);
B = FILTER A BY (a5 &gt;= $MINMAG);
STORE B INTO '$OUTPUT' USING PigStorage(',');
   Our Workflow We�re almost done: let�s take a quick look at the 3 actions in our workflow. First, we have our shell action: &lt;action name="shell-check-hour"&gt;
  ��&lt;shell xmlns="uri:oozie:shell-action:0.2"&gt;
  ������&lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
  ������&lt;name-node&gt;${nameNode}&lt;/name-node&gt;
  ������&lt;configuration&gt;
  ����������&lt;property&gt;
  ��������������&lt;name&gt;mapred.job.queue.name&lt;/name&gt;
  ��������������&lt;value&gt;${queueName}&lt;/value&gt;
  ����������&lt;/property&gt;
  ������&lt;/configuration&gt;
  ������&lt;exec&gt;check-hour.sh&lt;/exec&gt;
  ������&lt;argument&gt;${earthquakeMinThreshold}&lt;/argument&gt;
  ������&lt;file&gt;check-hour.sh&lt;/file&gt;�
  ������&lt;capture-output/&gt;
  �&lt;/shell&gt;
  �&lt;ok to="decide"/&gt;
  �&lt;error to="fail"/&gt;
&lt;/action&gt;
   The &lt;exec&gt; element tells Oozie what shell command to execute; this can be a script (like our check-hour.sh script) or an existing command such as echo or grep. We then use the &lt;argument&gt; element to pass the minimum earthquake magnitude that we want to consider to our script; here, we�re using ${earthquakeMinThreshold}, which we�ll define in our job.properties file. (You can have multiple &lt;argument&gt; elements for passing multiple arguments.)� The &lt;file&gt; argument tells Oozie to add our script to the distributed cache, making it available on whichever node the shell action gets executed on. Alternatively, we could place our script on every node in the cluster, but that is a lot more work and would require synchronizing updates to the script. We mentioned the &lt;capture-output/&gt; element earlier: it�s what tells Oozie to capture the shell command�s STDOUT so we can use it later in our workflow. Our shell action transitions to the decision node, which we looked at earlier. After the decision node, we have our Java action to download the �Past 30 Days� data to HDFS: &lt;action name="get-data"&gt;
  ��&lt;java&gt;
  ������&lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
  ������&lt;name-node&gt;${nameNode}&lt;/name-node&gt;
  ����� &lt;prepare&gt;
  ����������&lt;delete path="${dataInputDir}"/&gt;
  ����������&lt;mkdir path="${dataInputDir}"/&gt;
  ����� &lt;/prepare&gt;
  ������&lt;configuration&gt;
  ��������� &lt;property&gt;
  ������������� &lt;name&gt;mapred.job.queue.name&lt;/name&gt;
  ��������������&lt;value&gt;${queueName}&lt;/value&gt;
  ��������� &lt;/property&gt;
  ����� &lt;/configuration&gt;
  ����� &lt;main-class&gt;com.cloudera.earthquake.GetData&lt;/main-class&gt;
  ����� &lt;arg&gt;${dataInputDir}&lt;/arg&gt;
  � &lt;/java&gt;
  � &lt;ok to="filter-data"/&gt;
  � &lt;error to="fail"/&gt;
&lt;/action&gt;
   We use the &lt;main-class&gt; element to set the class name of our Java code. The JAR file containing the class will need to be made available to the action; in my previous blog post about the Oozie Sharelib, we saw a few ways to include additional JARs with our workflow. For simplicity, we�ll just put the JAR file in the lib folder next to our workflow.xml so Oozie will pick it up automatically. The &lt;arg&gt; element is essentially the same as the &lt;argument&gt; element we saw in the Shell action. Here we pass ${dataInputDir} to be the path where we want to store the downloaded data in HDFS; it would be defined in our job.properties. In the &lt;prepare&gt; section, we delete ${dataInputDir} and then recreate it to make sure that it exists (our Java code would throw an exception otherwise) and that its clean before we download the data to it. Lastly, we have our Pig action to process the data: &lt;action name="filter-data"&gt;
  ��&lt;pig&gt;
  ������&lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
  ������&lt;name-node&gt;${nameNode}&lt;/name-node&gt;
  ������&lt;prepare&gt;
  ����������&lt;delete path="${outputDir}"/&gt;
  ������&lt;/prepare&gt;
  ������&lt;configuration&gt;
  ��������� &lt;property&gt;
  ��������������&lt;name&gt;mapred.job.queue.name&lt;/name&gt;
  ��������������&lt;value&gt;${queueName}&lt;/value&gt;
  ��������� &lt;/property&gt;
  ����� &lt;/configuration&gt;
  ������&lt;script&gt;filter-data.pig&lt;/script&gt;
  ��� ��&lt;param&gt;INPUT=${dataInputDir}&lt;/param&gt;
  ������&lt;param&gt;OUTPUT=${outputDir}&lt;/param&gt;
  ����� &lt;param&gt;MINMAG=${wf:actionData('shell-check-hour')['largest']}&lt;/param&gt;
  � &lt;/pig&gt;
  ��&lt;ok to="end"/&gt;
  ��&lt;error to="fail"/&gt;
 &lt;/action&gt;
   The &lt;script&gt; element tells the Pig action the name of our Pig script, which we can put next to our workflow.xml. The &lt;param&gt; elements can then be used to pass variable names to the Pig script, such as the INPUT, OUTPUT, and MINMAG ones we used earlier. For INPUT, we�ll use ${dataInputDir}; note that this is where we told the Java action to download the data to. For OUTPUT, we�ll use ${outputDir}, which we can define in our job.properties to be where to output the final data. We�re also setting MINMAG to the largest variable from our shell script as previously discussed. Running the Workflow Before creating a coordinator job to run the workflow every hour, we can first try running the workflow directly to make sure that it works. If you downloaded the full project from Cloudera’s GitHub, you should be able to follow the README.txt to build the JAR file and you�ll have your workflow folder already prepared. Once you�ve done that, you can upload the workflow folder to your home directory in HDFS: hadoop fs -put Earthquake-WF Earthquake-WF
   In HDFS, you should now have something like this: $ hadoop fs -ls -R Earthquake-WF
-rw-r--r--�� 1 rkanter supergroup������ 1278 2013-02-25 15:54 Earthquake-WF/check-hour.sh
-rw-r--r--�� 1 rkanter supergroup������� 987 2013-02-25 15:54 Earthquake-WF/filter-data.pig
-rw-r--r--�� 1 rkanter supergroup������ 1155 2013-02-25 15:54 Earthquake-WF/job.properties
drwxr-xr-x�� - rkanter supergroup��������� 0 2013-02-25 15:54 Earthquake-WF/lib
-rw-r--r--�� 1 rkanter supergroup������ 3433 2013-02-25 15:54 Earthquake-WF/lib/Earthquake-J-1.0-SNAPSHOT.jar
-rw-r--r--�� 1 rkanter supergroup������ 3406 2013-02-25 15:54 Earthquake-WF/workflow.xml
   Because the workflow uses a Pig action, you�ll need to make sure the sharelib is properly installed or the workflow will fail; please see my previous blog post about the Oozie Sharelib. You can then submit the workflow to Oozie: oozie job -config path/to/job.properties -run -oozie http://some.hostname:11000/oozie
   It shouldn�t take more than a few minutes to run. If there wasn�t a �large� earthquake in the last hour, it won�t run the Java or Pig actions (and should complete very quickly). You can lower the value of earthquakeMinThreshold in job.properties to compensate or try waiting until after a large earthquake. Creating a coordinator job to run the workflow every hour is left as an exercise for the reader. Conclusion In this blog post we focused mostly on the Shell, Java, and Pig actions; we also looked at the decision node. You should now be able to write your own Shell and Java actions. The Shell and Java actions are great for times when there isn�t an appropriate action to accomplish a task in your Oozie workflow. While there are some limitations to the Shell action, its ease-of-use and lack of requirement to compile any code are quite useful; plus, you can use it to execute scripts from other languages such as python or perl. The Java action is also quite easy to use and has fewer caveats than the Shell action, but requires recompiling every time you change something. Ultimately, creating your own action type is the most flexible as you have more control over how it interacts with the outside world; plus, it’s directly reusable by other users and can even be contributed back to the Apache Oozie project. Creating your own action type is quite a bit more work though. Further Reading Earthquake project code on github Shell Action Documentation Java Action Documentation Java Action Cookbook Pig Action Documentation Decision Control Node Documentation Creating your own action type Documentation How to: Use the ShareLib in Apache Oozie blog post USGS Real-time Feeds &amp; Data Robert Kanter is a Software Engineer on the Platform team and an Apache Oozie Committer.</snippet></document><document id="262"><title>Cloudera Speakers at Hadoop Summit Europe</title><url>http://blog.cloudera.com/blog/2013/03/cloudera-speakers-at-hadoop-summit-europe/</url><snippet>Hadoop Summit Europe is coming up in Amsterdam next week, so this is an appropriate time to make you aware of the Cloudera speaker program there (all three talks on Thursday, March 21): Apache HBase Sizing Notes (Lars George, Solutions Architect/HBase PMC Member – also Track Chair of Hadoop Summit’s “Integrating Hadoop” track) This talk will address valuable lessons learned with the current versions of HBase. There are inherent architectural features that warrant for careful evaluation of the data schema and how to scale out a cluster. The audience will get a best practices summary of where there are limitations in the design of HBase and how to avoid those. In particular, we will discuss issues like proper memory tuning (for reads and writes), optimal flush file sizing, compaction tuning, and the number of write ahead logs required. Furthermore, there will be a discussion of the theoretical write performance, in comparison to those observed on real clusters. A collection of cheat sheets and example calculation for cluster sizing rounds out the talk towards the end.� � HBase Storage Internals, Present and Future (Matteo Bertozzi, Software Engineer/HBase Committer) Apache HBase is the Hadoop open-source, distributed, versioned storage manager well suited for random, realtime read/write access. This talk will give an overview on how HBase achieve random I/O, focusing on the storage layer internals, starting from how the client interacts with Region Servers and Master and going into WAL, MemStore, Compactions, and on-disk format details.� � Hadoop and the Enterprise Data Warehouse (Patrick Angeles, Solutions Architect) The Data Warehouse has been a staple in data-driven organizations for decades. As a result, the ecosystem, architecture, processes and methodologies around data warehousing is extremely mature. The arrival of Hadoop and Big Data has brought new life into traditional data warehousing by proposing new architectures and processes that upend existing norms. This presentation goes over several variants of how Hadoop interplays with existing data warehouses to solve modern problems.� There will be other Cloudera employees at the summit as well, so be on the lookout for anyone wearing Cloudera Blue!�</snippet></document><document id="263"><title>How Apache Hadoop Helps Scan the Internet for Security Risks</title><url>http://blog.cloudera.com/blog/2013/03/how-apache-hadoop-helps-scan-the-internet-for-security-risks/</url><snippet>The following guest post comes from Alejandro Caceres, president and CTO of Hyperion Gray LLC – a small research and development shop focusing on open-source software for cyber security. Imagine this: You�re an informed citizen, active in local politics, and you decide you want to support your favorite local political candidate. You go to his or her new website and make a donation, providing your bank account information, name, address, and telephone number. Later, you find out that the website was hacked and your bank account and personal information stolen. You�re angry that your information wasn�t better protected — but at whom should your anger be directed? Who is responsible for the generally weak condition of website security, today? It can�t be website operators, because there�s no prerequisite to know about blind SQL injection attacks or validation filters before spinning up a website. It can�t be website developers either — we definitely don�t equip them to evaluate website security for themselves. It�s a pretty small community that focuses on web development and web security, and that community is pretty opaque. I decided to change that dynamic by creating the open source PunkSPIDER project. PunkSPIDER gives users the ability to evaluate website security on their own, and via the most familiar medium possible: a search engine. Specifically, PunkSPIDER �scans the entire Internet for the most basic web vulnerabilities (bsqli, sqli, and xss), indexes the results for searchability, and then provides all this information out in the open, for free. Sound crazy? Sound hard? Sound expensive? Well, that�s where Apache Hadoop comes in — I never would have gotten the PunkSPIDER project off the ground without Hadoop. Hadoop is helping me create something literally as big as the Internet, with virtually no money and some old hardware. We�re actually using Hadoop in a pretty unique way. Sure, we do data analytics too, and the end goal is to provide rolled-up data to the end user, but Hadoop is flexible and powerful enough to do more than that. At its core, PunkSPIDER functions as your standard web spider, much like the one Google uses. It uses Apache Nutch to spider the Internet, collect domains, and keep this index updated. Nutch runs on top of a Hadoop cluster and provides out-of-the-box functionality to perform extremely quick crawls using MapReduce jobs. This spider is left running indefinitely, constantly updating the index and collecting new domains. But the coolest part is what happens after PunkSPIDER has some domains in its index: From there, it moves on to searching for vulnerabilities in the indexed domains. Why is this cool? �Well, web application vulnerability scanning is a fairly memory- and CPU-intensive process. Typical scanners can be unstable, they often get caught in infinite loops, and it can take a really long time to scan a single domain. Almost all of them only work on one website at a time and provide very little automation. (This is not to disparage other scanners out there – they simply have a different purpose. But for the millions of domains that we currently have in our index, and the hundreds of million that we expect in the future, this was simply not going to work.) So, I decided I needed to build my own scanner, called PunkSCAN, to be: Extremely stable – If a single scan fails, the entire job should continue gracefully. Extremely fast � Because the Internet is big. Built for massive scans � Again, because the Internet is big. Extremely cheap – Open source projects aren�t get-rich-quick schemes. With Hadoop, I was able to solve every one of the issues above. PunkSCAN essentially grabs a batch of domains from our index, scans them in parallel, and returns results, indexing them as metadata on a particular domain. Everything in PunkSCAN is a Hadoop MapReduce job. That means Hadoop takes care of a lot of the hard stuff. For example, if a job fails at any time or takes too long, it�s retried on another node in the cluster. It�s also infinitely scalable; the more machines we add to the cluster, the faster the scan goes. We were also able to build it on almost no budget; our machines are literally donated old laptops and desktops. If one of the machines dies, no big deal — Hadoop redistributes the job to another node! No data lost, and everything continues forward beautifully. The final step was to make all of this information easily searchable in the PunkSPIDER front-end, where users can search for specific websites of interest to them. And there you have it, an awesomely elegant and simple solution provided by our friendly neighborhood elephant. We�ve received a ton of positive feedback and most people are excited about the project, but it has understandably sparked a healthy debate. Some people have asked, �Aren�t you just giving script kiddies a gold mine of information for breaking into websites?� Well, we�re not giving malicious actors any new information that they can�t (or don�t already) get on their own. But we are giving average site owners and users access to this information, which they don�t have. Let�s face it, website vulnerabilities are rampant, and site owners and users aren�t equipped to do much about it. But we�re hoping that this project changes that by raising awareness about where the vulnerabilities are � because in website security, ignorance is the opposite of bliss. Since PunkSPIDER was released last week, we�ve received several requests for scans. We�ve also seen some community interest in building PunkSPIDER�based browser plug-ins to alert users when they visit a vulnerable site — which is exactly what we hoped to see happen. Hadoop technology and the incredible open-source support community around it allowed me to take my big idea and fit it into the small space of my spare bedroom, to build something new and innovative and powerful that may have a positive impact on the entire world wide web. And if you ask me, that is pretty cool.</snippet></document><document id="264"><title>Welcome, KijiMR</title><url>http://blog.cloudera.com/blog/2013/03/welcome-kijimr/</url><snippet>The following guest post is provided by Aaron Kimball, CTO of�WibiData. The Kiji ecosystem has grown with the addition of a new module, KijiMR. The Kiji framework is a collection of components that offer developers a handle on building Big Data Applications. In addition to the first release, KijiSchema, we are now proud to announce the availability of a second component: KijiMR. KijiMR allows KijiSchema users to use MapReduce techniques including machine-learning algorithms and complex analytics to develop many kinds of applications using data in KijiSchema. Read on to learn more about the major features included in KijiMR and how you can use them. KijiMR offers developers a set of new processing primitives explicitly designed for interacting with complex table-oriented data. The low-level batch interfaces available in MapReduce include basic InputFormat and OutputFormat implementations. The raw APIs are designed for processing key-value pairs stored in flat files in HDFS. Integrating MapReduce with HBase via InputFormat and OutputFormat APIs is hard to do from scratch in every algorithm. In KijiMR, we have extended the available MapReduce APIs to include: Bulk importers, which load data from external sources (like files in HDFS) into tables managed by KijiSchema Gatherers that allow you to scan over columns of a table, and emit key-value pairs for processing with a conventional MapReduce pipeline Producers, which implement computation functions that update individual rows in a Kiji table Several features of KijiMR make developing MapReduce pipelines more friendly for developers. Using these specialized processing metaphors in addition to the more conventional map and reduce makes it easier to focus on building applications: Command-line tools in Kiji support specific kinds of jobs, making it easier to launch and test MapReduce jobs without writing tedious boilerplate code. With flexible parameters, users can specify complex options like input and output data formats as well as data to include in the distributed cache. Builder APIs make it easier to programmatically construct MapReduce jobs. Key-value store lookups enable distributed jobs to efficiently perform map-side joins, foreign key lookups, and machine learning model scoring tasks. KijiMR also includes a library of stock implementations for bulk importing from a variety of formats, KeyValueStore implementations for common file formats, standard aggregation reducers and more. Producers and Gatherers The core components of KijiMR are called producers and gatherers. A producer executes a function over a subset of the columns in a table row and produces output to be injected back into a column of that row. Producers can be run in a MapReduce job that operates over a range of rows from a Kiji table. Common tasks for producers include parsing, profiling, recommending, predicting, and classifying. For example, you might run a LocationIPProducer to compute and store the location of each user into a new column, or a PersonalizationProfileProducer to compute a personalization profile. A producer updates each row with new information in an output column. A Kiji Gatherer scans over the rows of a Kiji table using the MapReduce framework and outputs key-value pairs. Gatherers are a flexible job type and can be used to extract or aggregate information into a variety of formats based on the output specification and reducer used. Common tasks for gatherers include calculating sums across an entire table, extracting features to train a model, and pivoting information from one table into another. You should use a gatherer when you need to pull data out of a Kiji table into another format or to feed it into a reducer. A gatherer scans over a set of columns in each row of a Kiji table, emitting key-value pairs to a reducer. Model scoring with KeyValueStores KeyValueStores allow processing pipeline elements like producers and gatherers to load data in external sources, beyond the specific record being processed. Users can specify data sets as key-value stores using the KeyValueStore API. User programs then use a KeyValueStoreReader to look up values associated with input keys. These input keys may be defined by the records in a data set that the user is processing with MapReduce. In conjunction with a KijiProducer, users can access KeyValueStores to apply the results of a trained machine learning model to their primary data set. The output of a machine learning model is often expressed as (key, value) pairs stored in files in HDFS, or in a secondary Kiji table. For each entry in a table, users can compute a new recommendation for the entry by applying the model to the information in the target row. A value in the user�s row may be a key into some arbitrary key-value store representing the model; the returned value is the recommendation. KeyValueStores also support ordinary map-side joins in a MapReduce program, e.g., for denormalization of data. The smaller data set is loaded into RAM in each map task in the form of a KeyValueStore instance. For each record in the larger dataset, users look up the corresponding record, and emit the concatenation of the two to the reducer. Ready to Check Out KijiMR? These features, as well as the ones provided in KijiSchema, are all available for download now. Get started with Kiji today by downloading a Bento Box. The Kiji Bento Box is a complete SDK for building Big Data Applications. All the components in Kiji are Apache 2.0-licensed open source. The Bento Box contains KijiSchema, KijiMR, developer tools, a standalone Hadoop/HBase instance, as well as example applications with source code that show how all the components fit together. The Kiji quick start guide can be completed in 15 minutes or less. For more information, see www.kiji.org.</snippet></document><document id="265"><title>Introducing Parquet: Efficient Columnar Storage for Apache Hadoop</title><url>http://blog.cloudera.com/blog/2013/03/introducing-parquet-columnar-storage-for-apache-hadoop/</url><snippet>Below you’ll find the official announcement from Cloudera and Twitter about Parquet, an efficient general-purpose columnar file format for Apache Hadoop. Parquet is designed to bring efficient columnar storage to�Hadoop.�Compared to, and learning from, the initial work done toward this goal in Trevni, Parquet includes the following enhancements: Efficiently encode nested structures and sparsely populated data based on the Google Dremel definition/repetition levels Provide extensible support for per-column encodings (e.g. delta, run length, etc) Provide extensibility of storing multiple types of data in column data (e.g. indexes, bloom filters, statistics) Offer better write performance by storing metadata at the end of the file Based on feedback from the Impala beta and after a joint evaluation with Twitter, we determined that these further improvements to the Trevni design were necessary to provide a more efficient format that we can evolve going forward for production usage. Furthermore, we found it appropriate to host and develop the columnar file format outside of the Avro project (unlike Trevni, which is part of Avro) because Avro is just one of many input data formats that can be used with Parquet. We’d like to introduce a new columnar storage format for Hadoop called Parquet, which started as a joint project between Twitter and Cloudera engineers. We created Parquet to make the advantages of compressed, efficient columnar data representation available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model, or programming language. Parquet is built from the ground up with complex nested data structures in mind. We adopted the repetition/definition level approach to encoding such data structures, as described in Google�s Dremel paper; we have found this to be a very efficient method of encoding data in non-trivial object schemas. Parquet is built to support very efficient compression and encoding schemes. Parquet allows compression schemes to be specified on a per-column level, and is future-proofed to allow adding more encodings as they are invented and implemented. We separate the concepts of encoding and compression, allowing Parquet consumers to implement operators that work directly on encoded data without paying decompression and decoding penalty when possible. Parquet is built to be used by anyone. The Hadoop ecosystem is rich with data processing frameworks, and we are not interested in playing favorites. We believe that an efficient, well-implemented columnar storage substrate should be useful to all frameworks without the cost of extensive and difficult to set up dependencies. The initial code defines the file format, provides Java building blocks for processing columnar data, and implements Hadoop Input/Output Formats, Pig Storers/Loaders, and an example of a complex integration � Input/Output formats that can convert Parquet-stored data directly to and from Thrift objects. A preview version of Parquet support will be available in Cloudera�s Impala 0.7. Twitter is starting to convert some of its major data source to Parquet in order to take advantage of the compression and deserialization savings. Parquet is currently under heavy development. Parquet�s near-term roadmap includes: Hive SerDes (Criteo) Cascading Taps (Criteo) Support for dictionary encoding, zigzag encoding, and RLE encoding of data (Cloudera and Twitter) Further improvements to Pig support (Twitter) Company names in parenthesis indicate whose engineers signed up to do the work � others can feel free to jump in too, of course. We’ve also heard requests to provide an Avro container layer, similar to what we do with Thrift. Seeking volunteers! We welcome all feedback, patches, and ideas; to foster community development, we plan to contribute Parquet to the Apache Incubator when the development is farther along. Regards, Nong Li (Cloudera) Julien Le Dem (Twitter) Marcel Kornacker (Cloudera) Todd Lipcon (Cloudera) Dmitriy Ryaboy (Twitter) Jonathan Coveney (Twitter) Justin Coffey (Criteo) Micka�l Lacour (Criteo) and friends….</snippet></document><document id="266"><title>How-to: Use the Apache HBase REST Interface, Part 1</title><url>http://blog.cloudera.com/blog/2013/03/how-to-use-the-apache-hbase-rest-interface-part-1/</url><snippet>There are various ways to access and interact with Apache HBase.�The Java API�provides the most functionality, but many people want to use HBase without Java. There are two main approaches for doing that:�One is the Thrift interface, which is the faster and more lightweight of the two options.�The other way to access HBase is using the REST interface, which uses HTTP verbs to perform an action, giving developers a wide choice of languages and programs to use. This series of how-to’s will discuss the REST interface and provide Python code samples for accessing it.�The first post will cover HBase REST, some Python caveats, and table administration. The second post will explain how to insert multiple rows at a time using XML and JSON.�The third post will show how to get multiples rows using XML and JSON. The full code samples can be found on my GitHub account. HBase REST Basics For both Thrift and REST to work, another HBase daemon needs to be running to handle these requests.�These daemons can be installed in the hbase-thrift and hbase-rest packages. The diagram below illustrates where Thrift and REST are placed in the cluster.�Note that the Thrift and REST clients usually don’t run any other services services like DataNode or RegionServers to keep the load down, and responsiveness high, for REST interactions. Be sure to install and start these daemons on nodes that have access to both the Hadoop cluster and the web application server.�The REST interface doesn’t have any built-in load balancing; that will need to be done with hardware or in code. Cloudera Manager makes it really easy to install and manage the HBase REST and Thrift services.�(You can download and try it out for free!) The downside to REST is that it is much heavier-weight than Thrift or Java.� A REST interface can use various data formats: XML, JSON, and protobuf.�By specifying the Accept and Content-Type headers, you can choose the format you want to pass in or receive back. To start using the REST interface, you need to figure out which port it’s running on.�The default port for CDH is port 8070.�For this post, you’ll see the baseurl variable used, and here is the value I’ll be using:: baseurl = "http://localhost:8070" The REST interface can be set up to use a Kerberos credential to increase security. For your code, you’ll need to use the IP address or fully qualified domain name DNS�of the node running the REST daemon.�Also, confirm that the port is correct.�I highly recommend making this URL a variable, as it could change with network changes. Python and HBase Bug Workarounds There are two bugs and workarounds that need to be addressed.�The first bug is that the built-in Python modules don’t support all of the HTTP verbs.�The second is an HBase REST bug when working with JSON. The built-in Python modules for REST interaction don’t easily support all of the HTTP verbs needed for HBase REST.�You’ll need to install the Python requests module.�The requests module also cleans up the code and makes all of the interactions much easier. The HBase REST interface has a bug when adding data via JSON: it is required that the fields maintain their exact order.�The built-in Python dict type doesn’t support this feature, so to maintain the order, we’ll need to use the OrderedDict class.�(Those with Python 2.6 and older will need to install the ordereddict module.) I’ll cover the bug and workaround later in the post, too. It was also difficult to use base64 encode and decode integers, so I wrote some code to do that: # Method for encoding ints with base64 encoding
def encode(n):
���� data = struct.pack("i", n)
���� s = base64.b64encode(data)
���� return s

# Method for decoding ints with base64 encoding
def decode(s):
���� data = base64.b64decode(s)
���� n = struct.unpack("i", data)
���� return n[0]
   To make things even easier, I wrote a method to confirm that HTTP responses come back in the 200s, which indicates that the operation worked.�The sample code uses this method to check the success of a call before moving on.�Here is the method: # Checks the request object to see if the call was successful
def issuccessful(request):
	if 200   Working With Tables Using the REST interface, you can create or delete tables.�Let’s take a look at the code to create a table. content =  '&lt;?xml version="1.0" encoding="UTF-8"?&gt;'
content += '&lt;TableSchema name="' + tablename + '"&gt;'
content += '  &lt;ColumnSchema name="' + cfname + '" /&gt;'
content += '&lt;/TableSchema&gt;'

request = requests.post(baseurl + "/" + tablename + "/schema", data=content, headers={"Content-Type" : "text/xml", "Accept" : "text/xml"})
   In this snippet, we create a small XML document that defines the table schema in the content variable.�We need to provide the name of the table and the column family name.�If there are multiple column families, you create some more ColumnSchemanodes. Next, we use the requests module to POST the XML to the URL we create.�This URL needs to include the name of the new table.�Also, note that we are setting the headers for this POST call.�We are showing that we are sending in XML with the Content-Type set to “text/xml” and that we want XML back with the Accept set to “text/xml”. Using the request.status_code, you can check that the table create was successful.�The REST interface uses the same HTTP error codes to detect if a call was successful or errored out.�A status code in the 200s means that things worked correctly. We can easily check if a table exists using the following code: request = requests.get(baseurl + "/" + tablename + "/schema")
   The calls uses the GET verb to tell the REST interface we want to get the schema information about the table in the URL.�Once again, we can use the status code to see if the table exists.�A status code in the 200s means it does exist and any other number means it doesn’t. Using the curl command, we can check the success of a REST operation without writing code.�The following command will return a 200 showing the success of the call because the messagestabletable does exist in HBase.�Here is the call and its output: [user@localhost]$ curl -I -H "Accept: text/xml" http://localhost:8070/messagestable/schema
HTTP/1.1 200 OK
Content-Length: 0
Cache-Control: no-cache
Content-Type: text/xml
   This REST call will error out because the tablenottheretable doesn’t exist in HBase.�Here is the call and its output: [user@localhost]$ curl -I -H "Accept: text/xml" http://localhost:8070/tablenotthere/schema
HTTP/1.1 500 org.apache.hadoop.hbase.TableNotFoundException: tablenotthere
Content-Type: text/html; charset=iso-8859-1
Cache-Control: must-revalidate,no-cache,no-store
Content-Length: 10767
   We can delete a table using the following code: request = requests.delete(baseurl + "/" + tablename + "/schema")
   This call uses the DELETE verb to tell the REST interface that we want to delete the table.�Deleting a table through the REST interface doesn’t require you to disable it first.�As usual, we can confirm success by looking at the status code. In the next two posts in this series, we’ll cover inserting and getting rows, respectively. Jesse Anderson is an instructor with Cloudera University. If you’re interested in HBase, be sure to register for HBaseCon 2013 (June 13, San Francisco) – THE community event for HBase contributors, developers, admins, and users. Early Bird registration is open until April 23.</snippet></document><document id="267"><title>Apache Hadoop Developer Training Helps Query Massive Telecom Data</title><url>http://blog.cloudera.com/blog/2013/03/apache-hadoop-developer-training-helps-query-massive-telecom-data/</url><snippet>This guest post is provided by Rohit Menon, Product Support and Development Specialist at Subex. I am a software developer in Denver and have been working with C#, Java, and Ruby on Rails for the past six years. Writing code is a big part of my life, so I constantly keep an eye out for new advances, developments, and opportunities in the field, particularly those that promise to have a significant impact on software engineering and the industries that rely on it.� In my current role working on revenue assurance products in the telecom space for Subex, I have regularly heard from customers that their data is growing at tremendous rates and becoming increasingly difficulty to process, often forcing them to portion out data into small, more manageable subsets. The more I heard about this problem, the more I realized that the current approach is not a solution, but an opportunity, since companies could clearly benefit from more affordable and flexible ways to store data. Better query capability on larger data sets at any given time also seemed key to derive the rich, valuable information that helps drive business. Ultimately, I was hoping to find a platform on which my customers could process all their data whenever they needed to. As I delved into this Big Data problem of managing and analyzing at mega-scale, it did not take long before I discovered Apache Hadoop. Mission: Hands-On Hadoop My initial reading about Hadoop on the various blogs and forums had me convinced that it is easily one of the best tools out there for handling and processing large volumes of data. At first, I thought I�d be able to learn Hadoop on my own by reading Hadoop: The Definitive Guide and the Hadoop Tutorial from Yahoo! However, after only a few days of reading, it became clear that I would benefit greatly from direct interaction with Hadoop experts, supervised experimentation, and interaction with practical examples of Hadoop challenges from the field.� Almost all of my research into Hadoop developer training led me to Cloudera University. As I learned more, I was really impressed by Cloudera�s contributions to the Hadoop community, its early entry into the Hadoop support and services space, its unmatched experience serving a recognizable customer base, and the enthusiasm for and popularity of CDH as the world�s leading Hadoop distribution. And all that was even before I realized Cloudera offers public training classes conveniently located in Denver! Moreover, given my aspirations to become a full-time Big Data professional, I knew Cloudera Certified Developer for Apache Hadoop (CCDH) status would be an important next step. After reviewing some of Cloudera�s online tutorial videos, enrollment in Cloudera�s next available public training class felt like the obvious choice. Training as a Catalyst Cloudera Developer Training for Apache Hadoop�is a four-day course designed to give software developers and engineers a complete understanding of HDFS and MapReduce, the other tools that make up the Apache Hadoop ecosystem, and the skills to develop applications on Hadoop right out the gate. My instructor not only had a true level of expertise in Hadoop and the curriculum, but was able to draw on both his years as an engineer and his time as a dedicated training specialist to give helpful guidance, answer all my questions, and instill a sense that I was getting the best training experience possible. The course provided me with insights into why Hadoop was built�it should be noted that Hadoop�s inventor and the Chairman of the Apache Software Foundation, Doug Cutting, is also Cloudera�s Chief Architect�and the problems it addresses. Importantly, the classes went beyond the basic curriculum to look at real-world Hadoop cases, which provided a compelling platform on which the labs and hands-on exercises were conducted. Completion of the Cloudera Developer Training and Cloudera Certification not only guarantee entry into the Big Data domain, but also assure a level of expertise and fluency that is nearly impossible to gain anywhere else, particularly in such a short amount of time. I truly felt prepared to use these tools to start working on Big Data analytics. Productizing with Help from Hive Based on the knowledge I gained from Cloudera Developer Training, I plan to use Hadoop as a solution to the Big Data problems so many of my customers face, as I call on my training to implement proposed programs that place Hadoop in development at the core of key data management and analytics products. One of the great things about CDH is that, in the case that a developer hasn�t yet learned to run MapReduce jobs, I will still be able to work with analysts to set up and query Hive tables using the SQL-like language that immediately breaks down the bottleneck previously faced when trying to access large quantities of data. For users interested in writing custom code, Pig, the scripting framework for Hadoop, becomes the ideal choice and allows data mining the analysts with whom I work could previously only imagine. I should note that Cloudera also offers Training for Apache Hive and Pig. It�s clear to me that Hadoop training has not only helped me identify solutions for the most interesting problems I face at work from day to day, but has also put me on the learning path to achieving even loftier goals, not least of which is completing additional training towards gaining my Cloudera Data Scientist Certification.</snippet></document><document id="268"><title>What the Hack! The Story of the Cloudera Hackathon</title><url>http://blog.cloudera.com/blog/2013/03/what-the-hack-the-story-of-the-cloudera-hackathon/</url><snippet>Every growing, dynamic engineering culture needs a hackathon every once in a while.� Earlier this week, Cloudera put that thought into action with a two-day, around-the-clock “What the Hack!” internal hackathon in our Palo Alto offices, with our friends from Accel Partners underwriting the omnipresent food and beverage (thanks!). The carrot: “Fun surprise awards, and most important, the rights to brag about your cool hacking ideas.” The morning began with a warm and festive welcome: The hacking proceeded forthwith. I was careful to keep my distance, so as not to disturb the Hacker Zen State: They’re everywhere! Dinner arrived in true hipster fashion, via The Chairman truck — with Mao’s advice that “A revolution is not a dinner party” ignored: After 30 straight hours, 29 different projects were presented to our celebrity judges (including CEO Mike Olson, Original Data Hacker Jeff Hammerbacher, Eli Collins, CTO Amr Awadallah, and friends from Accel and Greylock Partners) – with no singing involved, thankfully – and “People’s Choice” and “Critics’ Choice” winners were named (credit for awesome pano-shot to Alex Moundalexis): A handful of hard-working, hard-playing engineers, some of whom had hacked throughout the night, won iPad Minis for their efforts – but even better, they acquired those all-important bragging rights. “Critics’ Choice” winners pictured below: And the proud “People’s Choice” winner is pictured here: And that was the Story of the Cloudera Hackathon. Keep calm and hack on!</snippet></document><document id="269"><title>Introduction to Apache HBase Snapshots</title><url>http://blog.cloudera.com/blog/2013/03/introduction-to-apache-hbase-snapshots/</url><snippet>The current (4.2) release of CDH — Cloudera’s 100% open-source distribution of Apache Hadoop and related projects (including Apache HBase) — introduced a new HBase feature, recently landed in trunk, that allows an admin to take a snapshot of a specified table. Prior to CDH 4.2, the only way to back-up or clone a table was to use Copy/Export Table, or after disabling the table, copy all the hfiles in HDFS. Copy/Export Table is a set of tools that uses MapReduce to scan and copy the table but with a direct impact on Region Server performance. Disabling the table stops all reads and writes, which will almost always be unacceptable. In contrast, HBase snapshots allow an admin to clone a table without data copies and with minimal impact on Region Servers. Exporting the snapshot to another cluster does not directly affect any of the Region Servers; export is just a distcp with an extra bit of logic. Here are a few of the use cases for HBase snapshots: Recovery from user/application errors Restore/Recover from a known safe state. View previous snapshots and selectively merge the difference into production. Save a snapshot right before a major application upgrade or change. Auditing and/or reporting on views of data at specific time Capture monthly data for compliance purposes. Run end-of-day/month/quarter reports. Application testing Test schema or application changes on data similar to that in production from a snapshot and then throw it away. For example: take a snapshot, create a new table from the snapshot content (schema plus data), and manipulate the new table by changing the schema, adding and removing rows, and so on. (The original table, the snapshot, and the new table remain mutually independent.) Offloading of work Take a snapshot, export it to another cluster, and run your MapReduce jobs. Since the export snapshot operates at HDFS level, you don�t slow down your main HBase cluster as much as CopyTable does. What is a Snapshot? A snapshot is a set of metadata information that allows an admin to get back to a previous state of the table. A snapshot is not a copy of the table; it�s just a list of file names and doesn�t copy the data. A full snapshot restore means that you get back to the previous �table schema� and you get back your previous data losing any changes made since the snapshot was taken. Operations Take a snapshot:�This operation tries to take a snapshot on a specified table. The operation may fail if regions are moving around during balancing, split or merge. Clone a snapshot:�This operation creates a new table using the same schema and with the same data present in the specified snapshot. The result of this operation is a new fully functional table that can can be modified with no impact on the original table or the snapshot. Restore a snapshot:�This operation brings the table schema and data back to the snapshot state.�(Note: this operation discards any changes made since the snapshot was taken.)� Delete a snapshot:�This operation removes a snapshot from the system, freeing unshared disk space, without affecting any clones or other snapshots. Export a snapshot:�This operation copies the snapshot data and metadata to another cluster. The operation only involves HDFS so there�s no communication with the Master or the Region Servers, and thus the HBase cluster can be down. Zero-copy Snapshot, Restore, Clone The main difference between a snapshot and a CopyTable/ExportTable is that the snapshot operations write only metadata. There are no massive data copies involved. One of the main HBase design principles is that once a file is written it will never be modified. Having immutable files means that a snapshot just keeps track of files used at the moment of the snapshot operation, and during a compaction it is the responsibility of the snapshot to inform the system that the file should not be deleted but instead it should be archived. The same principle applies to a Clone or Restore operation. Since the files are immutable a new table is created with just �links� to the files referenced by the snapshot. Export Snapshot is the only operation that require a copy of the data, since the other cluster doesn�t have the data files. Export Snapshot vs Copy/Export Table Aside from the better consistency guarantees that a snapshot can provide compared to a Copy/Export Job, the main difference between Exporting a Snapshot and Copying/Exporting a table is that ExportSnapshot operates at HDFS level. This means that Master and Region Servers are not involved in this operations. Consequently, no unnecessary caches for data are created and there is no triggering of additional GC pauses due to the number of objects created during the scan process. The performance impact on the HBase cluster stems from the extra network and disk workload experienced by the DataNodes. HBase Shell: Snapshot Operations Confirm that snapshot support is turned on by checking if the hbase.snapshot.enabled property in hbase-site.xml is set to true. To take a snapshot of a specified table, use the snapshot command. (No file copies are performed) hbase&gt; snapshot �tableName�, �snapshotName�
   To list all the snapshots, use the list_snapshot�command. it will display the snapshot name, the source table, and the creation date and time.� hbase&gt; list_snapshots
SNAPSHOT               TABLE + CREATION TIME
 TestSnapshot          TestTable (Mon Feb 25 21:13:49 +0000 2013)
   To remove a snapshot, use the delete_snapshot�command. Removing a snapshot doesn�t impact cloned tables or other subsequent snapshots taken. hbase&gt; delete_snapshot 'snapshotName'
   To create a new table from a specified snapshot (clone), use the clone_snapshot�command. No data copies are performed, so you don�t end up using twice the space for the same data. hbase&gt; clone_snapshot 'snapshotName', 'newTableName'
   To replace the current table schema/data with a specified snapshot content, use the restore_snapshot�command. hbase&gt; restore_snapshot 'snapshotName'
   To export an existing snapshot to another cluster, use the ExportSnapshot tool. The export doesn�t impact the RegionServers workload, it works at the HDFS level and you have to specify an HDFS location (the hbase.rootdir of the other cluster). hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot
SnapshotName -copy-to hdfs:///srv2:8082/hbase
   Current Limitations Snapshots rely on some assumptions, and currently there are a couple of tools that are not fully integrated with the new feature: Merging regions referenced by a snapshots causes data loss on the snapshot and on cloned tables. Restoring a table with replication on for the table restored ends up with the two cluster out of synch. The table is not restored on the replica. Conclusion Currently the snapshot feature includes all the basic required functionality, but there�s still much work to do, including metrics, Web UI integration, disk usage optimizations and more. To learn more about how to configure HBase and use snapshots, review the documentation. Matteo Bertozzi is a Software Engineer on the Platform team, and an HBase committer.  </snippet></document><document id="270"><title>How-to: Set Up Cloudera Manager 4.5 for Apache Hive</title><url>http://blog.cloudera.com/blog/2013/03/how-to-set-up-cloudera-manager-4-5-for-apache-hive/</url><snippet>Last week Cloudera released the 4.5 release of Cloudera Manager, the leading framework for end-to-end management of Apache Hadoop clusters. (Download Cloudera Manager�here, and see install instructions here.) Among many other features, Cloudera Manager 4.5 adds support for Apache Hive. In this post, I’ll explain how to set up a Hive server for use with Cloudera Manager 4.5 (and later). For details about other new features in this release, please see the full release notes: Free Edition Enterprise Edition Introducing the Hive Metastore Server Starting with Cloudera Manager 4.5, there is a new role type called the Hive Metastore Server. This role manages the metastore process when Hive is configured with a Remote Metastore. I strongly encourage you to read the documentation about the Hive Remote Metastore and Local Metastore. Cloudera recommends using a Remote Metastore with Hive, especially for CDH 4.2 or later. Since the Remote Metastore is recommended, Cloudera Manager treats the Hive Metastore Server as a required role for all Hive services. Here are a couple key reasons why the Remote Metastore setup is advantageous, especially in production settings: The Hive Metastore Database password and JDBC drivers don�t need to be shared with every Hive client; only the Hive Metastore Server does. Sharing passwords with many machines is a security concern. You can control activity on the Hive Metastore Database. To stop all activity on the database, just stop the Hive Metastore Server. This makes it easy to perform tasks such as backup and upgrade, which require all Hive activity to stop. The Hive Metastore Server should not be used with CDH3. If you are using CDH3 or you�d like to use the Local Metastore mode, you can control this process by enabling the Bypass Hive Metastore Server mode in the Hive Service Configuration. See the discussion in the �Hive Services Created During Cloudera Manager Upgrade� below. Hive Setup Made Easy Whether adding Hive for the first time or importing an existing Hive install to be managed by Cloudera Manager, the steps are very similar. If you are upgrading from a previous Cloudera Manager release, then see the section below on upgrade. To help you set up and manage Hive, Cloudera Manager will: Create a database for the Hive Metastore if you are using the Cloudera Manager Embedded PostgreSQL Database. Create Hive Metastore tables. Create the Hive Warehouse Directory in HDFS. Manage Hive Metastore Server. Manage HiveServer2 (CDH4.2 only). Manage client configurations (/etc/hive/conf). Manage Hive configuration for services that depend on Hive (Cloudera Impala and Hue). Note that using Apache Derby for the Hive Metastore Database is not recommended for production use. The wizards do not support adding a new Hive service configured with Derby. Instead, I recommend leveraging the Cloudera Manager Embedded PostgreSQL Database for an easy, production-quality setup. Now let�s walk through adding a Hive service to an existing cluster. If you are creating a new Cloudera Manager cluster from scratch, the steps are extremely similar to the steps outlined below. If you are importing an existing Hive install to be managed by Cloudera Manager: Back up your Hive Metastore Database and any hive configuration files (hive-site.xml). Stop any running hive processes such as Hive Metastore, HiveServer, HiveServer2, or any Hive clients that are running commands. Get your Hive Metastore Database login info handy. You�ll need it in a moment. From main page, click “Actions� -&gt; “Add Service”. Select Hive. Select dependencies. Hive depends on HDFS and MapReduce (including YARN). Select roles Add a �Gateway� to any host from which you will run the Hive CLI or the Beeline CLI. Pick one host for the �Hive Metastore Server�. Although Hive can be configured to use more than one Hive Metastore Server, Cloudera does not support having multiple Hive Metastore Servers. This may result in problems such as concurrency errors. For performance reasons, it�s generally a good idea to have the Hive Metastore Server on the same host as the database that it�ll be talking to. This is not required. If you�re using CDH4.2, you can also pick �HiveServer2�. HiveServer2 differs from HiveServer, and Beeline is the supported CLI to communicate with HiveServer2. HiveServer2 supports multiple clients making many simultaneous requests, which is an improvement over HiveServer. (See HiveServer2 Documentation.) Test the database connection. If you�d like to leverage the Cloudera Manager Embedded PostgreSQL Database for your Hive Metastore Database, select �Use Embedded Database�. Don�t use this when adopting an existing Hive setup. If you’d like to configure an external database, then select “Use Custom Databases” and enter the appropriate database login information. If you are importing an existing Hive setup, enter the same information that your existing Hive setup uses. Click �Test Connection� and make sure there are no errors (skipped tests are ok), then click �Continue�. � Review Configuration Changes For newly created Hive setups, the defaults are normally appropriate. If you are adopting an existing Hive setup, then be sure to pick the Hive Warehouse Directory�that matches what your existing Hive setup uses. Cloudera Manager sets up your service. Note the following steps: Create Hive Metastore Database – creates the user and database in the Cloudera Manager Embedded PostgreSQL server. If you selected a custom database, then this step will not appear in the workflow. Create Hive Metastore Database Tables – creates all of the tables in the Hive Metastore Database for the current Hive version. This command will only run if the schema is empty. Create Hive Warehouse Directory – creates the Hive Warehouse directory in HDFS with 1777 permissions if it doesn�t already exist. Deploy Client Configuration – updates /etc/hive/conf on all hosts that have a Hive role, using the alternatives mechanism. Once this is done, then you can run the shell command hive on any of these hosts and your hive commands will all go through the Hive Metastore Server. � (Optional) Review configuration: You can now review the configuration to make sure it matches what you�d like. If you are importing an existing setup, you should compare the configuration with your backup copy of hive-site.xml. If there�s any config that�s missing from your hive-site.xml, but there�s no option in the UI for this config, then you can use the property Hive Service Configuration Safety Valve for hive-site.xml to specify these configs for all CM-managed processes (includes roles from Hive, Impala, and Hue). If you need the config to be used by Hive CLI (which normally reads from /etc/hive/conf/hive-site.xml), then use the property Hive Client Configuration Safety Valve for hive-site.xml. Be sure to restart Hive, restart any dependent services (Hue and Impala), and deploy client configs after making a configuration change. Hive Services Created During Cloudera Manager Upgrade When upgrading to Cloudera Manager 4.5, if there are any Hue or Impala services in the existing setup, one or more Hive services will automatically be created. Prior to Cloudera Manager 4.5, Hue and Impala both specified Hive configurations such as warehouse directory and database configuration. When upgrading to Cloudera Manager 4.5, this information from the Hue and Impala configurations is used to generate a new Hive service with the same warehouse directory and database configuration. The old Hue Impala services are then linked to the new Hive service(s). Cloudera Manager will attempt to merge the configuration, so if Hue and Impala had an identical Hive configuration, then only a single Hive service will be created. After upgrading to Cloudera Manager 4.5, at the end of the Upgrade Wizard, you will be asked to add a Hive Metastore Server role to each Hive Service that was automatically created. Select one host for the Hive Metastore Server. Cloudera recommends using the Hive Metastore Server, so Cloudera Manager requires that each Hive service has one. For performance reasons, it�s good to have the Hive Metastore Server on the same host as the database that it�ll be talking to. If you manually (not using Cloudera Manager) created an account for Hive in the Embedded PostgreSQL Database, then you need to make sure that the database and user have the same name. This can easily be done through the commands: CREATE ROLE &lt;dbname&gt; LOGIN PASSWORD �&lt;password&gt;�;
ALTER DATABASE &lt;dbname&gt; SET OWNER &lt;dbname&gt;;
REASSIGN OWNED BY &lt;old username&gt; TO &lt;dbname&gt;;
   After running these commands, edit your Hive Metastore Database configuration in Cloudera Manager with the new username and password, restart Hive, restart Hue/Impala, and deploy the client configuration. (Thanks to Benjamin Kim on the scm-users group for pointing this out!) Hive has the configuration Bypass Hive Metastore Server. When this configuration is enabled, Hive clients, Hue, and Impala connect directly to the Hive Metastore Database. Prior to Cloudera Manager 4.5, Hue and Impala talked directly to the Hive Metastore Database, so the Bypass mode is enabled by default when upgrading to Cloudera Manager 4.5. This is to make sure the upgrade doesn�t disrupt your existing setup. You should plan to disable the Bypass Hive Metastore Server mode, especially when using CDH 4.2 or later. Using the Hive Metastore Server is the recommended configuration (as discussed in �Introducing the Hive Metastore Server� previously). To switch between using the Hive Metastore Server or talking directly to the Metastore Database, use the Hive service configuration Bypass Hive Metastore Server. You can find this option by using the search feature on the Hive Service Configuration page: After toggling this Bypass option, restart Hive and all services that depend on Hive (Hue and Impala), then re-deploy client configuration. Troubleshooting Here are some common issues I hope you�ll now easily avoid. It�s always a good idea to look at the latest Cloudera Manager Installation Documentation and Known Issues before performing an install or upgrade. CDH Upgrades When upgrading to CDH 4.1 or 4.2, a manual Metastore Database upgrade is required. When performing any CDH upgrade, be sure to read the Cloudera Manager upgrade guides to make sure the Hive Metastore Database is properly backed up and upgraded before starting Hive. See “Upgrading CDH in a Cloudera Managed Deployment” in this doc. CDH3 You�ll probably need to chown /user/hive in HDFS to hive:hive after the warehouse directory is created, otherwise you may see errors in creating /user/hive/.Trash when you drop a table. There are various issues with using the Hive Metastore Server in CDH3. It�s easier to just always enable the �Bypass Hive Metastore Server� mode when running in CDH3. (See the Upgrade section for discussion on using this option.) CDH4.0 and CDH4.1 Secure Clusters Hue has trouble talking to the Hive Metastore Server in CDH4.0 and CDH 4.1 secure clusters. Details can be found at the “Known Issues and Workarounds” section of this doc. �Darren Lo is a Software Engineer working on the Enterprise team.</snippet></document><document id="271"><title>How-to: Set Up a Hadoop Cluster with Network Encryption</title><url>http://blog.cloudera.com/blog/2013/03/how-to-set-up-a-hadoop-cluster-with-network-encryption/</url><snippet>Hadoop network encryption is a feature introduced in Apache Hadoop 2.0.2-alpha and in CDH4.1. In this blog post, we�ll first cover Hadoop�s pre-existing security capabilities. Then, we�ll explain why network encryption may be required. We�ll also provide some details on how it has been implemented. At the end of this blog post, you’ll get step-by-step instructions to help you set up a Hadoop cluster with network encryption. A Bit of History on Hadoop Security Starting with Apache Hadoop 0.20.20x and available in Hadoop 1 and Hadoop 2 releases (as well as CDH3 and CDH4 releases), Hadoop supports Kerberos-based authentication. This is commonly referred to as Hadoop Security. When Hadoop Security is enabled it requires users to authenticate (using Kerberos) in order to read and write data in HDFS or to submit and manage MapReduce jobs. In addition, all Hadoop services authenticate with each other using Kerberos. Kerberos authentication is available for Hadoop’s RPC protocol as well as the HTTP protocol. The latter is done using the Kerberos HTTP SPNEGO authentication standard, and it can be used to protect Hadoop�s Web UIs, WebHDFS, and HttpFS. Why Network Encryption? While Hadoop Security provides Kerberos authentication, it does not protect data as it travels through the network; all network traffic still goes on the clear. Hadoop is a distributed system typically running on several machines, which means that data must be transmitted over the network on a regular basis. If your Hadoop cluster holds sensitive information (financial data, credit card transactions, healthcare information, etc.), it may be required to ensure that data is also protected while in transit through the network (to avoid eavesdropping and man-in-the-middle attacks). This is no different than accessing your bank�s website using a secure connection (using HTTPS) when you connect to it. To address these kinds of use cases, network encryption was added to Hadoop. Securing Hadoop�s Network Interactions First, let’s review the different types of network interactions found in Hadoop: Hadoop RPC calls – These are performed by clients using the Hadoop API, by MapReduce jobs, and among Hadoop services (JobTracker, TaskTrackers, NameNodes, DataNodes). HDFS data transfer – Done when reading or writing data to HDFS, by clients using Hadoop API, by MapReduce jobs and among Hadoop services. HDFS data transfers are done using TCP/IP sockets directly. MapReduce Shuffle – The shuffle part of a MapReduce job is the process of transferring data from the Map tasks to Reducer tasks. As this transfer is typically between different nodes in the cluster, the shuffle is done using the HTTP protocol. Web UIs – The Hadoop daemons provide Web UIs for users and administrators to monitor jobs and the status of the cluster. The Web UIs use the HTTP protocol. FSImage operations – These are metadata transfers between the NameNode and the Secondary NameNode. They are done using the HTTP protocol. This means that Hadoop uses three different network communication protocols: Hadoop RPC Direct TCP/IP HTTP Hadoop RPC already had support for SASL for network encryption. Thus, we only needed to worry about securing HDFS data transfers and securing HTTP. Hadoop RPC Encryption When authentication support was added to Hadoop�s RPC protocol, SASL was used as the security protocol. SASL, or the Simple Authentication and Security Layer, is a framework which abstracts away the actual security implementation details for those higher-level protocols which want to support authentication, message integrity verification, or encryption. SASL does not specify a wire format or the protocols that implement it, but rather it specifies a handshake system whereby the parties involved in a SASL connection (the client and server) will iteratively exchange messages when a connection is first established. SASL allows for the use of several different security mechanisms for different contexts, e.g. MD5-DIGEST, GSSAPI, or the SASL PLAIN mechanism. For protocols that properly implement the SASL framework, any of these mechanisms can be used interchangeably. Separately from the details of how a given SASL mechanism implements its security constructs, most SASL mechanisms are capable of providing several different levels of quality of protection, or QoP. This allows a single SASL mechanism, e.g. MD5-DIGEST, to optionally provide only authentication (auth), message integrity verification (auth-int), or full message confidentiality/encryption (auth-conf). What does all of this mean in the context of Hadoop RPC? Since the Hadoop RPC system implements the SASL framework, we can utilize all of these features without any additional implementation complexity. When a Hadoop client that has Kerberos credentials available (e.g. a user running hadoop fs -ls ...) connects to a Hadoop daemon, the SASL GSSAPI mechanism will be used for authentication. Not all clients have direct access to Kerberos credentials; however, and in these cases, those clients will use Delegation Tokens issued by either the JobTracker or NameNode. When a Hadoop client which has Hadoop token credentials available (e.g. a MapReduce task reading/writing to HDFS) connects to a Hadoop daemon, the SASL MD5-DIGEST mechanism will be used for authentication. Though the default is only to authenticate the connection, in either of these cases the QoP can be optionally set via the hadoop.rpc.protection configuration property to additionally cause the RPCs to have their integrity verified, or fully encrypted while being transmitted over the wire. Direct TCP/IP (HDFS Data Transfer Encryption) Though Hadoop RPC has always supported encryption since the introduction of Hadoop Security, the actual reading and writing of file data between clients and DataNodes does not utilize the Hadoop RPC protocol. Instead, file data sent between clients and DNs is transmitted using the Hadoop Data Transfer Protocol, which does not utilize the SASL framework for authentication. Instead, clients are issued Block Tokens by the NameNode when requesting block locations for a given file, and the client will then present these tokens to the DN when connecting to read or write data. The DN is capable of verifying the authenticity of these Block Tokens by utilizing a secret key shared between the NameNode and DataNodes in the cluster. Though this system is perfectly sufficient to authenticate clients connecting to DataNodes, the fact that the Data Transfer Protocol does not use SASL means that we do not get the side benefit of being able to set the QoP and trivially add integrity or confidentiality to the protocol. To address this deficiency of the Data Transfer Protocol, we chose to wrap the existing Data Transfer Protocol with a SASL handshake. We wrapped the existing protocol, instead of wholesale replacing the existing protocol with SASL, in order to maintain wire compatibility in the normal case where wire encryption is disabled. This optional SASL wrapper can be enabled by setting dfs.encrypt.data.transfer to true in the NameNode and DN configurations and restarting the daemons. When this is enabled, the NameNode will generate and return to clients Data Encryption Keys which can be used as credentials for the MD5-DIGEST SASL mechanism and whose authenticity can be verified by DataNodes based on a shared key between the NameNode and DataNodes, similarly to the way Block Tokens are when only authentication is enabled. Note that, since the Data Encryption Keys are themselves sent to clients over Hadoop RPC, it is necessary to also set the hadoop.rpc.protection setting to�privacy in order for the system to actually be secure. For the full details of the HDFS encrypted transport implementation, check out HDFS-3637. HTTP Encryption The solution for HTTP encryption was clear: HTTPS. HTTPS is a proven and widely adopted standard for HTTP encryption. Java supports HTTPS, browsers support HTTPS, and many libraries and tools in most operating systems have built in support for HTTPS. As mentioned before, Hadoop uses HTTP for its Web UIs, for the MapReduce shuffle phase and for FSImage operations between the NameNode and the Secondary Name node. Because the Hadoop HttpServer (based on Jetty) already supports HTTPS, the required changes were minimal. We just had to ensure the HttpServer used by Hadoop� is started using HTTPS. Nodes in a cluster are added and removed dynamically. For each node that is added to the cluster the certificate public key has be added to all other nodes. Similarly, for each node that is removed from the cluster its certificate public key has to be removed from all other nodes.� This addition and removal of public key certificates must be done in running tasks without any disruption. Because the default Java keystore only loads the certificates at initialization time, we needed to implement a custom Java keystore to reload certificate public keys if the certificate’s keystore file changed. In addition, we had to modify the Hadoop HttpServer, the MapReduce shuffle HTTP client, and the FSImage HTTP client code to use the new custom Java keystore. The HTTP encryption work was done in two Apache JIRAs, MAPREDUCE-4417 and HADOOP-8581. Setting up Network Encryption with CDH4 Now we�ll explain how to create HTTPS certificates using the Java Keytool for a Hadoop cluster. Once the certificates have been created and made available to all nodes in the Hadoop cluster, refer to CDH4 documentationto complete Hadoop configuration for network encryption. The following steps explain how to create the HTTPS certificates and the required keystore and truststore files, assuming we have a cluster consisting of two machines — h001.foo.com and h002.foo.com — and that all Hadoop services are started by a user belonging to the hadoop group. Each keystore file contains the private key for each certificate, the single truststore file contains all the keys of all certificates. The keystore file is used by the Hadoop HttpServer while the truststore file is used by the client HTTPS connections. For each node create a certificate for the corresponding host in a different keystore file: $ keytool -genkey -keystore� h001.keystore -keyalg RSA -alias h001 \
-dname "CN=h001.foo.com,O=Hadoop"� -keypass pepepe -storepass pepepe
$ keytool -genkey -keystore� h002.keystore -keyalg RSA -alias h002 \&gt;
-dname "CN=h002.foo.com,O=Hadoop"� -keypass pepepe -storepass pepepe
   For each node export the certificate’s public key in a different certificate file: $ keytool -exportcert -keystore� h001.keystore -alias h001 \
-file h001.cert -storepass pepepe�
$ keytool -exportcert -keystore� h002.keystore -alias h002 \
-file h002.cert -storepass pepepe
   Create a single truststore containing the public key from all certificates: $ keytool -import -keystore hadoop.truststore -alias h001 \
-file h001.cert -noprompt -storepass pepepe
$ keytool -import -keystore hadoop.truststore -alias h002 \
-file h002.cert -noprompt -storepass pepepe
   Copy the keystore and trutstore files to the corresponding nodes: $ scp h001.keystore hadoop.truststore root@h001.foo.com:/etc/hadoop/conf/
$ scp h002.keystore hadoop.truststore root@h002.foo.com:/etc/hadoop/conf/
   Change the permissions of the keystore files to be read only by user and group. Make their group the hadoop group; the truststore file should be made readable by everybody: $ ssh root@h001.foo.com "cd /etc/hadoop/conf;chgrp hadoop h001.keystore;\
chmod 0440 h001.keystore;chmod 0444 hadoop.truststore"
$ ssh root@h002.foo.com "cd /etc/hadoop/conf;chgrp hadoop h002.keystore;\
chmod 0440 h002.keystore;chmod 0444 hadoop.truststore"
   Generate public key certificates to install in your browser: $ openssl x509 -inform der -in h001.cert &gt;&gt; hadoop.PEM
$ openssl x509 -inform der -in h002.cert &gt;&gt; hadoop.PEM
   Follow the instructions of your browser/OS to install the hadoop.pem certificates file. Now that the HTTPS certificates have been created and distributed to the machines in the cluster, you’ll need to configure the Hadoop cluster with network encryption. Please refer to the CDH4 documentation, Configuring Encrypted Shuffle, Encrypted Web UIs, and Encrypted HDFS Transport, for detailed instructions on how to configure network encryption in Hadoop. You have now set-up your cluster to use network encryption! Alejandro Abdelnur is a Software Engineer on the Platform team, and a PMC member of the Apache Oozie, Hadoop, and Bigtop projects. Aaron T. Myers (ATM) is also a Software Engineer on the Platform team and a PMC member for Hadoop.</snippet></document><document id="272"><title>Meet the Instructor: Glynn Durham</title><url>http://blog.cloudera.com/blog/2013/03/meet-the-instructor-glynn-durham/</url><snippet>In this installment of �Meet the Instructor,� we speak to San Francisco-based Glynn Durham, one of the big brains behind Cloudera�s Introduction to Data Science training and certification.� What is your role at Cloudera? I am a Senior Instructor with Cloudera University, which means I am a road warrior: I will travel anywhere to teach anything to anyone. I teach all the courses Cloudera offers, including custom private training events that I run at customer sites. Right now, I�m especially enjoying teaching Cloudera�s new course, Introduction to Data Science: Building Recommender Systems. In tandem with the rollout of the course, we�re developing Cloudera Certified Professional: Data Scientist exams, which will include a challenging performance-based lab component in addition to the written test. Prior to Cloudera, I primarily came from a database background. My first corporate job was at Oracle just before it went public. I spent a year producing Oracle�s first batch of course materials for developers and database administrators and then spent several years teaching all kinds of people all over the world. For some time, I was an Oracle Database Administrator. I eventually moved on to the LAMP code stack, and I later worked for MySQL. What do you enjoy most about training and/or curriculum development? For me, Hadoop represents the continuation of a lifelong professional interest in digital data as a useful abstraction of the real world. As an instructor, there�s nothing better than observing an “ah-ha” moment. Even better yet, I love being part of the circuit that causes the proverbial cartoon light bulb to switch on above a person�s head. Training allows me to contribute to that process all the time�I�m even the one on the learning end now and again! I enjoy meeting people whose eyes light up when they start to think about what a platform like Hadoop enables. Cloudera�s training is dedicated to opening up a new realm of possibilities for everyone involved. I get to learn a lot, not just about Cloudera�s technology (which is awesome, by the way), but also about what customers are finding to do with their data. It just goes on and on! Describe an interesting application you�ve seen or heard about for Apache Hadoop. I get my healthcare coverage from Kaiser Permanente, which has offered members the opportunity to participate in a huge project sequencing individuals� genomes from blood samples. Kaiser hopes to collect the genomes of 500,000 members, which, paired with members’ health records, can provide the data for countless new discoveries about the genetic component of health processes.�I don’t know for sure that Kaiser uses Hadoop in this project, but I know this is an eminently Hadoop-able application. What advice would you give to an engineer, system administrator, or analyst who wants to learn more about Big Data? The term Big Data refers at least in part to the acceleration of data growth in organizations of all sorts. Our more mature technologies such as RDBMS and spreadsheets remain useful as ways to work with data. However, with the acceleration we see now, it�s critical to adapt and find new ways to store and process data at profoundly higher scales than before. One of my favorite points about Cloudera is that our mission statement is about data. Hadoop is clearly an important platform that helps users succeed in more ways with more data.�But let�s be clear: it�s the data that�s most important. Hadoop is an open-source project, and Cloudera�s CDH distribution is completely Apache-licensed: free and open-source forever. Go to Downloads on Cloudera�s website and get your own cluster up and running using Cloudera Manager, or get a Virtual Machine image running a single-node cluster, all ready to go. Watch our videos! Read a few blogs or white papers! It�s the nature of open-source projects to make intellectual property available to anyone who wants it. So that�s my main advice: jump in and start swimming! Cloudera�s business�including training�is to help accelerate users� time to mastery of the Hadoop platform (and, I expect, other exciting data technologies going forward). Our full support subscription, Cloudera Enterprise, is especially meant to help users get production clusters working and keep them working optimally to get the most value from data. How did you become involved in technical training and Hadoop? My enthusiasm for teaching, broadly, goes way back to junior high school in Louisiana, where I tutored a classmate who was failing geometry. Like me, he later went on to become a successful engineer. I find it rewarding to think that I actually made a difference in that guy�s life. I like to think that, all these years later, I am still doing the same thing: with a few days� mutual commitment, we can provide real value to individuals in our classrooms. What�s one interesting fact or story about you that a training participant would be surprised to learn? In a previous chapter of my life, I interned with the recording engineers at Tiny Telephone, a famous recording studio in San Francisco that favors legacy analog techniques. I spent many, many hours aligning tape decks and setting up signal chains, then listening to all sorts of musicians work their magic. Great fun!  </snippet></document><document id="273"><title>What�s New in Hue 2.2?</title><url>http://blog.cloudera.com/blog/2013/03/whats-new-in-hue-2-2/</url><snippet>This post is about the new release of Hue, an open source web-based interface that makes Apache Hadoop easier to use, that’s included in�CDH4.2. Hue lets you interact with Hadoop services from within your browser without having to go to a command-line interface. It features a file browser for HDFS, an Apache Oozie Application for creating workflows of data processing jobs, a job designer/browser for MapReduce, Apache Hive and Cloudera Impala query editors, a Shell, and a collection of Hadoop APIs. The goal of this release was to add a set of new features and improve the user experience. Read on for a list of the major changes (from 304 commits). Oozie Application With the Oozie Application you can chain jobs and schedule them repeatedly without having to write XML anymore. Workflow and Coordinator management got extra focus and now matches all the Oozie functionalities: The workflow editor supports Drag &amp; Drop and was restyled. The coordinator page was redesigned with a wizard and data can be specified by range. The dashboard displays any workflow as a graph and refreshes itself dynamically. All the Oozie actions are supported (addition of Sub-workflow, DistCp, Email, Fs, Generic). Forks can be converted to decision nodes. A read-only user can access the dashboard. A workflow or a coordinator can be resubmitted from specific steps. Existing XML workflow definition can be imported. The dashboard provides direct access to task logs of any action. � Drag &amp; Drop Workflow Editor � Workflow Dashboard � Coordinator Wizard Rerun a Workflow (left) or a Coordinator (right) Beeswax/Hive Query Editor A number of user experience improvements make it simpler to query your data with SQL: Multiple databases are supported (tackling one of the most popular requests HUE-535). Query editor is bigger, has line numbers and shows lines with error(s). Running queries shows logs in Ajax and lets you scroll through them. Query results page has a horizontal scroll bar and a quick column name lookup for accessing a certain column when they are many.   Query Editor � Wide result page with column lookup Impala Editor Impala can now be queried from a new interface. More features will be supported when Impala is GA. Cloudera Impala query FileBrowser FileBrowser lets you navigate and manage HDFS files in a UI. Its front end was totally redesigned and new filesystem operations were added. You do not need to use the hadoop fs command anymore! Bulk operations for multiple deletions, changing of permissions or owner Supports bulks operation recursively or not (e.g. chmod recursively a folder) Upload archives (e.g. upload multiple files at once like the Oozie sharelib) Create a file and edit it Bulk editing JobBrowser JobBrowser lists MapReduce jobs with their statistics and statuses. It was prettified and now supports MR2 jobs running over YARN: MR2 jobs and their logs can be browsed. Job logs can be accessed with one click. Other apps like Beeswax and Oozie can now show the MR2 logs. MR1/MR2 job list and direct log access UserAdmin Groups and Hue permissions can be assigned to the users through the UserAdmin application. Access to Hue applications can be customized for each user. (For example, Bob can only see and use the Oozie and Impala applications.) The application has been restyled and simplified, and is no longer accessible by default to non-superuser: HDFS home of first/new/imported users is created automatically. LDAP support now has wildcard search, user import by wildcard expression and group syncing by distinguished name. Group permission editing Desktop Desktop is the core library of Hue and every application is built on top of it. In this release, the user experience has been improved with more informative errors (now with stack traces and line numbers) and new status messages (such as when critical services like Oozie are down). On the technical side, users can now upload files to a federated cluster, some XSS vulnerabilities were fixed, and database greenlet support was introduced for more performance. Hue now fully supports transactional databases like MySQL InnoDB and PostgreSQL. Hue is also internationalized and available in English, French, German, Spanish, Portuguese (and Brazilian Portuguese), Korean, Japanese and simplified Chinese. Conclusion With this 2.2 release, a big part of the Hadoop user experience gap was filled in. The next 2.3 release will target users who wish to better leverage the multiple query solutions in CDH (Beeswax/Hive, Impala, and Pig). A new document model (HUE-950) would make each query (e.g. Hive query) searchable and shareable with your colleagues or importable into an Oozie workflow without any duplication. A trash and source control versioning system (HUE-951) is also discussed as well as Oozie bundle (HUE-869) integrations. Many past feature and bugs were discussed on the hue-user list, so feel free to chime in! A Hue meetup group was created and it would be a pleasure to meet you in person and see how analyzing your data with Hadoop could be made easier. Romain Rigaux is a Software Engineer on the Platform team.</snippet></document><document id="274"><title>What’s New in Cloudera Manager 4.5?</title><url>http://blog.cloudera.com/blog/2013/02/whats-new-in-cloudera-manager-4-5/</url><snippet>It has been a while since I have blogged, primarily because we have been heads-down working toward the Cloudera Manager 4.5 release that we announced yesterday! Cloudera Manager has seen a rapid adoption among enterprise customers and as more clusters are deployed into production environments, the more feature requests we get from them. We have heard our customers and the Cloudera Manager 4.5 release aims to address many of these requests. Kudos to the engineering team for another feature-packed release. Some key features of CM4.5 release are as follows: Rolling Upgrades/Restarts Traditionally, upgrading a Hadoop/CDH cluster has been a cumbersome exercise for some customers. With Cloudera Manager 4.5, we are addressing the challenge of platform upgrades head-on: Users can now download, distribute, and activate the latest release of CDH completely from within Cloudera Manager. Cloudera Manager does all the orchestration of making sure the right bits are deployed on every node of the Hadoop cluster.�This feature is facilitated via a new packaging format that we are introducing, called “parcel“. Coupled with the Rolling Restarts workflow, Cloudera Manager 4.5 enables the cluster to be upgraded with minimal downtime. In addition, the Rolling Restarts workflow is especially useful for propagating configuration updates for services (HDFS, HBASE, MR etc.) with minimal/zero downtime, as it cycles through the entire cluster and updates the appropriate configurations on a node-by-node basis. Advanced Charting Monitoring has been one of the core value adds of Cloudera Manager. Hadoop has tons of metrics but making sense of them requires a fair degree of expertise. Cloudera Manager brings Hadoop intelligence right into the tool, and makes Cloudera�s collective experience of managing several Hadoop clusters directly available to the users. One of the gaps we noticed in previous releases was the inability to effectively correlate the various metrics across the stack, from services to roles to hosts. Customers ended up viewing the different related metrics on separate views and then tried to make sense of all of them. With 4.5′s new charting capabilities, you can now review all the relevant metrics (Services � HDFS, HBASE, MR etc, Roles � DATANODES, TT�s, RS�s �and Hosts) all on the same page. So next time you are experiencing a latency spike on your HBase cluster, the advanced charts will quickly help you pinpoint the problem from within a single view – �a genuine example of “breaking the silos”. The team has also delivered a fantastic set of additional capabilities that make the lives of Hadoop administrators so much easier. For example, we have built a simple query language that enables (advanced) users to query the myriad of metrics available within Cloudera Manager to create�custom charts that are relevant to their clusters. It also provides�the ability to share and save these dashboards to facilitate easy sharing — both within the organization and with Cloudera Support — to more quickly troubleshoot and diagnose problems. Node Templating This feature has been a direct result of several customer requests (especially those with several hundreds of Hadoop nodes). Typically, customers start off with a homogenous set of hardware for their Hadoop clusters but over a period of time end up acquiring newer generations of hardware with different specs (CPUs, memory etc.). This mixed setup necessitates a more streamlined approach to managing groups of disparate hardware. The Node Templating feature in 4.5 provides this ability to define specific role types and create custom templates based on these role types (example: Template.Large,� Template.Medium, Template.Small, etc). These templates can then be applied to the groups of hardware. This greatly simplifies the process of adding hosts and instantiating the roles that should run on those hosts. SNMP Integration From the beginning, we have emphasized the need for Cloudera Manager to integrate with the broader ecosystem of IT management tools in the data center. In CM3.7, we had alerts available via�SMTP. In CM4.0, we added a rich set of APIs to the application to enable integration with various popular tools like Zenoss and Nagios. With CM4.5, we are making it even easier to integrate with enterprise-management tools like IBM Tivoli, HP Openview, and others via SNMP (v2 or v3). In addition to the above core features, we have added whole set of new capabilities around Hive managament, resource management, support for auto-provisioning of clusters on Amazon EC2, automated clusterstats, and a ton of usability improvements. More details are available here. For current Cloudera Manager users, I am sure you are looking to upgrade to 4.5 right away!�For others, Cloudera Manager is the easiest and most effective way to build your Hadoop clusters. So get started! Bala Venkatrao is a product director at Cloudera.</snippet></document><document id="275"><title>Open Source, Flattery, and The Platform for Big Data</title><url>http://blog.cloudera.com/blog/2013/02/open-source-flattery-and-the-platform-for-big-data/</url><snippet>It has been a busy time for announcements coinciding with this week�s Strata conference. There�s no corner of the technology world that has not embraced Apache Hadoop as the new platform for big data.� Apache Hadoop began as a telegram from the future from Google, turned into real software by Doug Cutting while on a freelance assignment. While Hadoop�s origins are surprising, its ongoing popularity is not � open source has been a major contributing factor to Hadoop�s current ubiquity. Easy to trial, fast to evolve, inexpensive to own: open source makes a compelling case for itself. From the founding of the company, Cloudera recognized the importance of Apache open source to Hadoop�s continued evolution. We�re now entering our fifth year of shipping a 100% open source platform. Every significant advance we have added to the platform has stayed consistent to our open source strategy. In the process Cloudera has now sponsored the development of seven new open source projects including Apache Flume, Apache Sqoop, Apache Bigtop, Apache MRUnit, Cloudera Hue, Apache Crunch, and most recently, Cloudera Impala. Acknowledging the maxim �innovation happens elsewhere,� we�ve also managed to convince the founders and/or PMC chairs of Apache Hadoop, Apache Oozie, Apache Zookeeper, and Apache HBase to come join Cloudera. Our investment in open source is not altruistic — we think it is good business. Today, Cloudera employees contribute more patches to the Apache Hadoop ecosystem than every other software vendor combined. Meanwhile more enterprises have adopted our open source platform than every other Hadoop distribution combined. We do not think it is a coincidence that these two things are simultaneously true. I�m reminded of our open source strategy this week not only because of the further validation of Hadoop�s popularity but also because of the entry of a new round of proprietary imitators. At one point there were six distinct vendors all promoting proprietary filesystems as alternatives to HDFS, many of which included breathless claims of how they could make Apache Hadoop faster and �more powerful.� This year we get to see history repeat itself, this time with SQL engines. The marketing is nearly identical to that of the proprietary filesystem era: damning open source with faint praise, pointing out its limitations and extolling the virtues of some feature(s) proprietary to that particular vendor. Our bet continues to be that open source wins and Impala is evidence of that belief. We took an expensive and innovative R&amp;D project and released it for free as Apache-licensed open source. It is the first and only functioning open source interactive SQL engine for the Hadoop stack and one that will continue to rapidly evolve. Proprietary SQL vendors will pull a page from the proprietary storage playbook: damn open source Impala with faint praise and point out its limitations, both real and contrived. They will be equally ineffective. We will continue to bet on an open, integrated, and highly flexible big data platform. Saying you are �all in on Hadoop� while simultaneously promoting a proprietary platform means you are missing the point.</snippet></document><document id="276"><title>New Products and Releases: Cloudera Navigator, Cloudera Enterprise BDR, and More</title><url>http://blog.cloudera.com/blog/2013/02/new-products-releases/</url><snippet>Today is an exciting day for Cloudera customers and users. With an update to our 100% open source platform and a number of new add-on products, every software component we ship is getting either a minor or major update. There�s a lot to cover and this blog post is only a summary. In the coming weeks we�ll do follow-on blog posts that go deeper into each of these releases. New Products We�re now supporting several hundred production Hadoop clusters. In doing so we�ve had to make a lot of advances in the functionality, reliability and manageability of the Hadoop platform. Even with these improvements, customers have been traditionally reluctant to run certain data and applications on the Apache Hadoop platform. The new products we are announcing today were designed to remove these obstacles to adoption. Storing sensitive data in Hadoop: Cloudera Navigator 1.0.�Many of our customers have the need to work with data that is considered sensitive either because of internal company policies or because they operate in regulated industries. We�ve made some great strides in improving security in the Hadoop stack but auditability and data governance has been largely lacking. No longer. Today we�re announcing the general availability of Cloudera Navigator 1.0, a data governance suite for the Hadoop stack. This first release focuses on audit and access management for all of the key objects in the Hadoop stack from HDFS files and directories to Hive tables to HBase tables. Cloudera Navigator is also designed to work with your existing compliance systems &amp; processes. Entrusting business critical workloads to Hadoop: Cloudera Enterprise BDR 1.0. We�ve seen many customers who have been hesitant to trust business-critical workloads to Apache Hadoop because of the lack of a complete and consistent disaster recovery capability. Problem solved. Today we are announcing the general availability of Cloudera Enterprise BDR 1.0, a disaster recovery automation solution for the Hadoop stack. In its first version BDR provides the ability to create &amp; maintain a remote DR cluster. BDR builds on the existing data replication primitives of the Hadoop stack but adds automation, synchronization and transparency. (Editor’s note: As of Feb. 3, 2014, BDR is no longer a separate product but rather an included feature in all Cloudera Enterprise editions.) New Releases We�re releasing a major new version of Cloudera Manager 4.5. It contains many new enhancements including custom charting, templating, SNMP support and scheduling. One big new enhancement we�ve been working on for some while is rolling upgrades. With Cloudera Manager it is now possible to patch, update, or upgrade your Hadoop cluster without taking downtime. The complete process is automated in Cloudera Manager including the ability to restore old versions. Combined with the fact that Cloudera Manager supports multiple clusters and multiple versions and you can automate the entire dev-test-prod lifecycle from within the system. This has big implications for improving the uptime of a Hadoop cluster. It also has big implications for the security and quality of production Hadoop clusters. Before rolling upgrades it simply wasn�t realistic to keep a production cluster up to date with the latest bug fixes and security patches. Cloudera Manager Free Edition also gets an update today. Free Edition can now be used on an unlimited number of nodes. We will also add a number of new monitoring capabilities to free edition in the next few weeks as users shouldn�t have to stitch together a bunch of disparate tools to run their clusters. New Updates We are also releasing an update to our open source platform CDH. This update CDH 4.2 contains a number of enhancement and more than a hundred bug fixes. Key enhancements include HBase snapshots (multiple point in time recovery) and the ability to run a highly available JobTracker (failover will require a restart of jobs in flight). Apache Hive gets a number of enhancements including support for decimal data types, support for user impersonation and the addition of the HCatalog features that are now folded in as part of Hive. Impala users will benefit from all of these same enhancements. There are many smaller improvements to Flume, Sqoop and Hue, too numerous to mention here. Per usual, CDH updates are designed to maintain stability and application�compatibility. Users and customers may also skip CDH updates as they see fit. Lastly, we are releasing an update to Cloudera Impala 0.6. Impala is still in beta but we hope to be able to remove that label shortly. In the meantime 0.6 adds support for Avro and RCFILE as well as JDBC. Impala 0.6 will require CDH 4.2. As always we appreciate your feedback. You can download the latest updates to CDH, Impala, and Cloudera Manager here. Thanks so much for your support of Apache Hadoop and of Cloudera.</snippet></document><document id="277"><title>Apache Pig: It Goes to 0.11</title><url>http://blog.cloudera.com/blog/2013/02/apache-pig-it-goes-to-0-11/</url><snippet>This blog was originally published at blog.apache.org/pig and is republished here for your convenience by permission of its author, Pig Committer Dmitriy Ryaboy. After months of work, we are happy to announce the 0.11 release of Apache Pig. In this blog post, we highlight some of the major new features and performance improvements that were contributed to this release. A large chunk of the new features was created by Google Summer of Code (GSoC) students with supervision from the Apache Pig PMC, while the core Pig team focused on performance improvements, usability issues, and bug fixes. We encourage CS students to consider�applying for GSOC in 2013�– it�s a great way to contribute to open source software. This blog post hits some of the highlights of the release. Pig users may also find a�presentation by Daniel Dai, which includes code and output samples for the new operators, helpful. New Features DateTime Data Type The�DateTime�data type has been added to make it easier to work with timestamps. You can now do date and time arithmetic directly in a Pig script, use UDFs such as�CurrentTime,�AddDuration,�WeeksBetween, etc. PigStorage expects timestamps to be represented in the�ISO 8601 format. Much of this work was done by Zhijie Shen as part of his GSoC project. RANK Operator The new�RANK operator�allows one to assign an ordinal number to every tuple in a relation. A user can specify whether she wants exact rank (elements with the same sort value get the same rank) or �DENSE� rank (elements with the same sort value get consecutive rank values). One can also rank by a field value, in which case the relation is sorted by this field prior to ranks being assigned. Much of this work was done by Allan Avenda?o as part of his GSoC project. A = load 'data' AS (f1:chararray,f2:int,f3:chararray);

DUMP A;
(David,1,N)
(Tete,2,N)
(Ranjit,3,M)
(Ranjit,3,P)
(David,4,Q)
(David,4,Q)
(Jillian,8,Q)
(JaePak,7,Q)
(Michael,8,T)
(Jillian,8,Q)
(Jose,10,V)

B = rank A;

dump B;
(1,David,1,N)
(2,Tete,2,N)
(3,Ranjit,3,M)
(4,Ranjit,3,P)
(5,David,4,Q)
(6,David,4,Q)
(7,Jillian,8,Q)
(8,JaePak,7,Q)
(9,Michael,8,T)
(10,Jillian,8,Q)
(11,Jose,10,V)

C = rank A by f1 DESC, f2 ASC DENSE;

dump C;
(1,Tete,2,N)
(2,Ranjit,3,M)
(2,Ranjit,3,P)
(3,Michael,8,T)
(4,Jose,10,V)
(5,Jillian,8,Q)
(5,Jillian,8,Q)
(6,JaePak,7,Q)
(7,David,1,N)
(8,David,4,Q)
(8,David,4,Q)
   CUBE and ROLLUP Operators The new�CUBE�and�ROLLUP�operators of the equivalent SQL operators provide the ability to easily compute aggregates over multi-dimensional data. Here is an example: events = LOAD '/logs/events' USING EventLoader() AS (lang, country, app_id, event_id, total);
eventcube = CUBE events BY
 CUBE(lang, country), ROLLUP(app_id, event_id);
result = FOREACH eventcube GENERATE
  FLATTEN(group) as (lang, country, app_id, event_id),
  COUNT_STAR(cube), SUM(cube.total);
 STORE result INTO 'cuberesult';
   The�CUBE�operator produces all combinations of cubed dimensions. The�ROLLUP�operator produces all levels of a hierarchical group, meaning,�ROLLUP(country, region, city)�will produce aggregates by country, country and region, country, region, and city, but not country and city (without region). When used together as in the above example, the output groups will be the cross product of all groups generated by cube and rollup operation. That means that if there are�m�dimensions in cube operations and�n�dimensions in rollup operation then overall number of combinations will be�(2^m) * (n+1). Detailed documentation can be seen in�the CUBE Jira. This work was done by Prasanth Jayachandran as part of his GSoC project. He also did further work on optimizing the cubing computation to make it extremely scalable; this optimization will likely be added to the Pig 0.12 release. Groovy UDFs Pig has support for UDFs written in JRuby and Jython. In this release, support for�UDFs in Groovy�is added, providing an easy bridge for converting Groovy and Pig data types and specifying output schemas via annotations. This work was contributed by Mathias Herberts. Improvements Performance improvement of in-memory aggregation Pig 0.10 introduced in-memory aggregation for algebraic operators — instead of relying on Hadoop combiners, which involve writing map outputs to disk and post-processing them to apply the combine function, Pig can optionally buffer up map outputs in memory and apply combiners without paying the IO cost of writing intermediate data out to platters. While the initial implementation significantly improved performance of a number of queries, we found some corner cases where it actually hurt performance; furthermore, reserving a large chunk of memory for aggregation buffers can have negative effects on memory-intensive tasks. In Pig 0.11, we completely�rewrote the partial aggregation operator�to be much more efficient, and�integrated it with Pig�s Spillable Memory Manager, so it no longer requires dedicated space on the task heap. This feature is still considered experimental and is off by default; you can turn it on by setting�pig.exec.mapPartAgg�to�true. With these changes in place, Twitter was able to turn this option on by default for all Pig scripts they run on their clusters — thousands of Map-Reduce jobs per day (they also dropped�pig.exec.mapPartAgg.minReduction�to 3, to be even more aggressive with this feature). Performance improvement related to Spillable management Speaking of the�SpillableMemoryManager�– it also saw some significant improvements. The default collection data structure in Pig is a �Bag�. Bags are spillable, meaning that if there is not enough memory to hold all the tuples in a bag in RAM, Pig will spill part of the bag to disk. This allows a large job to make progress, albeit slowly, rather than crashing from �out of memory� errors. The way this worked before Pig 0.11 was as follows: Every time a Bag is created from the�BagFactory, it is registered with the�SpillableMemoryManager. The�SpillableMemoryManager�keeps a list of�WeakReferences�to�Spillable�objects Upon getting a notification that GC is about to happen, the�SMM�iterates through its list of�WeakReferences, deleting ones that are no longer valid (pointing to null), and looking for the largest�Spillable�it can find. It then asks this�Spillable�to spill, and relies on the coming GC to free up spilled data. Some users reported seeing large amounts of time taken up by traversing the�WeakReference�list kept by the�SMM. A large�WeakReference�list affected both performance, since we had to iterate over large lists when GC was imminent, and memory, since each�WeakReference�adds 32 bytes of overhead on a 64-bit JVM. In Pig 0.11 we modified the Bag code so that instead of registering all bags in case they grow, we have Bags register themselves if their contents exceed 100KB, the logic being that a lot of bags will never reach this size, and would not be useful to spill anyway. This drastically reduced the amount of time and memory we spend on the�SpillableMemoryManager. Improvements to AvroStorage and HBaseStorage HBase: Added the ability to set HBase scan maxTimestamp, minTimestamp and timestamp in HBaseStorage. Significant performance optimization for filters over many columns Compatibility with HBase 0.94 + secure cluster Avro: AvroStorage can now optionally skip corrupt Avro files Added support for recursively defined records Added support for Avro 1.7.1 Better support for file globbing Faster, leaner Schema Tuples Pig uses a generic�Tuple�container object to hold a �row� of data. Under the covers, it�s simply a�List, where the objects might be�Strings,�Longs, other�Tuples,�Bags, etc. Such generality comes with overhead. We found that we can achieve significant performance gains in both size and speed of Tuples if, when the schema of a tuple is known, custom classes are auto-generated on the fly for working with particular schemas. You can see the�results of our benchmarks�(�Tuple� is the default generic tuple implementation; �Primitive� is an earlier attempt at tuple optimization, which streamlined handling of schemas consisting only of longs, ints, etc — this approach was abandoned after the codegen work was complete; �Schema� is the codegen work). To turn on schema tuples, set the�pig.schematuple�property to�true. This feature is considered experimental and is off by default. New APIs for Algebraic, Accumulator UDFs, and Guava We added a number of helper classes to make creating new UDFs easier, and improved some of the APIs around UDFs. Pig Tuple object is now a Java Iterable, so you can easily loop over all of the fields if you like (PIG-2724 has details) Accumulators can now terminate early. This can be a big performance win for accumulators that can bail out upon reaching some success condition. One simply has to implement the�TerminatingAccumulator interface, which has just one method:�isFinished(). A new abstract class,�IteratingAccumulatorEvalFunc, has been added to make it easier to write Accumulator functions. To write an Accumulator, one can simply extend this abstract class and implement a single method which takes an Iterator and returns the desired result. If you�ve implemented Accumulators before, you�ll see why the sample code in�PIG-2651�is much cleaner and simpler to write. Instead of implementing a getOutputSchema function, UDF authors can tell Pig their output schema by annotating the UDF with an�@OutputSchema�annotation Before Pig 0.11 if you wanted to implement an Algebraic or Accumulative UDF, you still had to implement the regular exec() method, as well. We�ve introduced a couple of new abstract classes, AlgebraicEvalFunc�and�AccumulatorEvalFunc, which give you the derivable implementations for free. So if you implement�AlgebraicEvalFunc, you automatically get the regular and�Accumulator�implementations. Saves code, saves sanity! We�ve found that many Pig users are interested in being able to share their UDF logic with non-Pig programs.�FunctionWrapperEvalFunc�allows one to easily wrap Guava functions which contain the core logic, and keep UDF-specific code minimal. We�ve found that many UDFs work on just a single field (as opposed to a multi-field tuple), and return a single value. For those cases, extending�PrimitiveEvalFunc&lt;IN, OUT&gt;�allows the UDF author to skip all the tuple unwrapping business and simply implement�public OUT exec(IN input), where IN and OUT are primitives. We find ourselves wrapping�StoreFuncs�often and have created�StoreFuncWrapper�and�StoreFuncMetadataWrapper�classes to make this easier. These classes allow one to subclass and decorate only selectStoreFunc�methods. mock.Storage, a helper�StoreFunc�to simplify writing JUnit tests for your pig scripts, was quietly added in 0.10.1 and got a couple of bug fixes in 0.11. See details in�mock.Storage docs. Other Changes A number of other changes were introduced — optimizations, interface improvements, small features, etc. Here is a sampling: Penny, a debugging tool introduced as an experimental feature in Pig 0.9, has been removed due to lack of adoption and complexity of the codebase StoreFuncs can now implement a new method,�cleanupOnSuccess, in addition to the previously existing�cleanupOnFailure Pig Streaming can be passed values from the JobConf via environment variables. Rather than pass all the variables from the JobConf, which can cause Java�s�ProcessBuilder�to croak for large enough JobConfs, Pig 11 allows the user to explicitly specify which properties should be passed in by setting the value of�pig.streaming.environment�property The logic used to estimate how many reducers should be used for a given Map-Reduce job is now pluggable, with the default implementation remaining as it was. Setting the�pig.udf.profile�property to true will turn on counters that approximately measure the number of invocations and milliseconds spent in all UDFs and Loaders. Use this with caution, as this feature can really bloat the number of counters your job uses! Useful for lightweight debugging of jobs. Local mode is now significantly faster Merge Join previously only worked immediately after loading. It is now allowed after an�ORDER�operator Grunt prints schemas in human-friendly JSON if you set�pig.pretty.print.schema=true Better HCatalog integration Pluggable�PigProgressNotificationListeners�allow custom tool integration for monitoring Pig job progress — for an example of what can be possible with this, check out�Twitter Ambrose Extensive work went into making sure that Pig works with JDK 7 and Hadoop 2.0 A lot of work went into this release, and we are grateful to all the contributors. We hope you like all the new stuff! Let us know what you think –�users@pig.apache.org. - Dmitriy Ryaboy (@squarecog) on behalf of the Pig team</snippet></document><document id="278"><title>How-to: Resample from a Large Data Set in Parallel (with R on Hadoop)</title><url>http://blog.cloudera.com/blog/2013/02/how-to-resample-from-a-large-data-set-in-parallel-with-r-on-hadoop/</url><snippet>UPDATED 20130424: The new RHadoop treats output to Streaming a bit differently, so do.trace=FALSE must be set in the randomForest call. UPDATED 20130408: Antonio Piccolboni, the author of RHadoop, has improved the code somewhat using his substantially greater experience with R. The most material change is that the latest version of RHadoop can bind multiple calls to keyval correctly. Internet-scale data sets present a unique challenge to traditional machine-learning techniques, such as fitting random forests or “bagging“. In order to fit a classifier to a large data set, it�s common to generate many smaller data sets derived from the initial large data set (i.e.,resampling). There are two reasons for this: Large data sets typically live in a cluster, so any operations should have some level of parallelism. Separate models are fit on separate nodes that contain different subsets of the initial data. Even if you could use the entire initial data set to fit a single model, it turns out that ensemble methods, where you fit multiple smaller models using subsets of the data, generally outperform single models. Indeed, fitting a single model with 100M data points can perform worse than fitting just a few models with 10M data points each (so less total data outperforms more total data; e.g., see this paper). Furthermore, bootstrapping is another popular method that randomly chops up an initial data set in order to characterize distributions of statistics and also to build ensembles of classifiers (e.g., bagging). In fact, parallelizing bootstrap sampling or ensemble learning can provide significant performance gains even when your data set is not so large that it must live in a cluster. The gains from purely parallelizing the random number generation are still significant. Sampling-with-replacement is the most popular method for sampling from the initial data set to produce a collection of samples for model fitting. This method is equivalent to sampling from a multinomial distribution where the probability of selecting any individual input data point is uniform over the entire data set. Unfortunately, it is not possible to sample from a multinomial distribution across a cluster without using some kind of communication between the nodes (i.e., sampling from a multinomial is not embarrassingly parallel). But do not despair: we can approximate a multinomial distribution by sampling from an identical Poisson distribution on each input data point independently, lending itself to an embarrassingly parallel implementation. Below, we will show you how to implement such a Poisson approximation to enable you to train a random forest on an enormous data set. As a bonus, we�ll be implementing it in R and RHadoop, as R is many people�s statistical tool of choice. Because this technique is broadly applicable to any situation involving resampling a large data set, we begin with a fully general description of the problem and solution. Formal Problem Statement Our situation is as follows: We have N data points in our initial training set {xi}, where N is very large (106–109) and the data is distributed over a cluster. We want to train a set of M different models for an ensemble classifier, where M is anywhere from a handful to thousands. We want each model to be trained with K data points, where typically K &lt;&lt; N. (For example, K may be 1–10% of N.) The number of training data points available to us, N, is fixed and generally outside of our control. However, K and M are both parameters that we can set and their product KM determines the total number of input vectors that will be consumed in the model fitting process. There are three cases to consider: KM &lt; N, in which case we are not using the full amount of data available to us. KM = N, in which case we can exactly partition our data set to produce totally independent samples. KM &gt; N, in which case we must resample some of our data with replacement. The Poisson sampling method described below handles all three cases in the same framework. (However, note that for the case KM = N, it does not partition the data, but simply resamples it as well.) (Note: The case where K = N corresponds exactly to bootstrapping the full initial data set, but this is often not desired for very large data sets. Nor is it practical from a computational perspective: performing a bootstrap of the full data set would require the generation of MN data points and M scans of an N-sized data set. However, in cases where this computation is desired, there exists an approximation called a “Bag of Little Bootstraps”.) So our goal is to generate M data sets of size K from the original N data points where N can be very large and the data is sitting in a distributed environment. The two challenges we want to overcome are: Many resampling implementations perform M passes through the initial data set. which is highly undesirable in our case because the initial data set is so large. Sampling-with-replacement involves sampling from a multinomial distribution over the N input data points. However, sampling from a multinomial distribution requires message passing across the entire data set, so it is not possible to do so in a distributed environment in an embarrassingly parallel fashion (i.e., as a map-only MapReduce job). Poisson-approximation Resampling Our solution to these issues is to approximate the multinomial sampling by sampling from a Poisson distribution for each input data point separately. For each input point xi, we sample M times from a Poisson(K / N) distribution to produce M values {mj}, one for each model j. For each data point xi and each model j, we emit the key-value pair &lt;j, xi&gt; a total of mj times (where mj can be zero). Because the sum of multiple Poisson variables is Poisson, the number of times a data point is emitted is distributed as Poisson(KM / N), and the size of each generated sample is distributed as Poisson(K), as desired. Because the Poisson sampling occurs for each input point independently, this sampling method can be parallelized in the map portion of a MapReduce job. (Note that our approximation never guarantees that every single input data point is assigned to at least one of the models, but this is no worse than multinomial resampling of the full data set. However, in the case where KM = N, this is particularly bad in contrast to the alternative of partitioning the data, as partitioning will guarantee independent samples using all N training points, while resampling can only generate (hopefully) uncorrelated samples with a fraction of the data.) Ultimately, each generated sample will have size K on average, and so this method will approximate the exact multinomial sampling method with a single pass through the data in an embarrassingly parallel fashion, addressing both of the big data limitations described above. Because we are randomly sampling from the initial data set, and similarly to the �exact� method of multinomial sampling, it�s possible that some of the initial input vectors will never be chosen for any of the samples. In fact, we expect that approximately exp{–KM / N} of the initial data will be entirely missing from any of the samples (see figure below). Amount of missed data as a function of KM / N. The value for KM = N is marked in gray. Finally, the MapReduce shuffle distributes all the samples to the reducers and the model fitting or statistic computation is performed on the reduce side of the computation. The algorithm for performing the sampling is presented below in pseudocode. Recall that there are three parameters — N, M, and K — where one is fixed; we choose to specify T = K / N as one of the parameters as it eliminates the need to determine the value of N in advance. # example sampling parameters
T = 0.1  # param 1: K / N; average fraction of input data in each model; 10%
M = 50   # param 2: number of models

def map(k, v):  // for each input data point
    for i in 1:M  // for each model
        m = Poisson(T)  // num times curr point should appear in this sample
        if m &gt; 0
            for j in 1:m  // emit current input point proper num of times
                emit (i, v)

def reduce(k, v):
    fit model or calculate statistic with the sample in v
   Note that even more significant performance enhancements can be achieved if it is possible to use a combiner, but this is highly statistic/model-dependent.   Example: Kaggle Data Set on Bulldozer Sale Prices We will apply this method to test out training of a random forest regression model on a Kaggle data set found here. The data set comprises ~400k training data points. Each data point represents a sale of a particular bulldozer at an auction, for which we have the sale price along with a set of other features about the sale and about the bulldozer. (This data set is not especially large, but will illustrate our method nicely.) The goal will be to build a regression model using an ensemble method (specifically, a random forest) to predict the sale price of a bulldozer from the available features. Could be yours for $141,999.99 The data are actually supplied as two tables: a transaction table that includes the sale price (target variable) and some other features, including a reference to a specific bulldozer; and a bulldozer table, that contains additional features for each bulldozer. As this post does not concern itself with data munging, we will assume that the data come pre-joined. But in a real-life situation, we�d incorporate the join as part of the workflow by, for example, processing it with a Hive query or a Pig script. Since in this case the data are relatively small, we simply use some R commands. The code to prepare the data can be found here. Quick Note on R and RHadoop As so much statistical work is performed in R, it is highly valuable to have an interface to use R over large data sets in a Hadoop cluster. This can be performed with RHadoop, which is developed with the support of Revolution Analytics. (Another option for R and Hadoop is the RHIPE project.) One of the nice things about RHadoop is that R environments can be serialized and shuttled around, so there is never any reason to explicitly move any side data through Hadoop�s configuration or distributed cache. All environment variables are distributed around transparently to the user. Another nice property is that Hadoop is used quite transparently to the user, and the semantics allow for easily composing MapReduce jobs into pipelines by writing modular/reusable parts. The only thing that might be unusual for the �traditional� Hadoop user (but natural to the R user) is that the mapper function should be written to be fully vectorized (i.e., keyval() should be called once per mapper as the last statement). This is to maximize the performance of the mapper (since R�s interpreted REPL is quite slow), but it means that mappers receive multiple input records at a time and everything the mappers emit must be grouped into a single object. Finally, I did not find the RHadoop installation instructions (or the documentation in general) to be in a very mature state, so here are the commands I used to install RHadoop on my small cluster. Fitting an Ensemble of Random Forests with Poisson Sampling on RHadoop We implement our Poisson sampling strategy with RHadoop. We start by setting global values for our parameters: frac.per.model &lt;- 0.1  # 10% of input data to each sample on avg
num.models &lt;- 50
   As mentioned previously, the mapper must deal with multiple input records at once, so there needs to be a bit of data wrangling before emitting the keys and values: # MAP function
poisson.subsample &lt;- function(k, input) {
  # this function is used to generate a sample from the current block of data
  generate.sample &lt;- function(i) {
    # generate N Poisson variables
    draws &lt;- rpois(n=nrow(input), lambda=frac.per.model)
    # compute the index vector for the corresponding rows,
    # weighted by the number of Poisson draws
    indices &lt;- rep((1:nrow(input)), draws)
    # emit the rows; RHadoop takes care of replicating the key appropriately
    # and rbinding the data frames from different mappers together for the
    # reducer
    keyval(i, input[indices, ])
  }

  # here is where we generate the actual sampled data
  c.keyval(lapply(1:num.models, generate.sample))
}
   Because we are using R, the reducer can be incredibly simple: it takes the sample as an argument and simply feeds it to our model-fitting function, randomForest(): # REDUCE function
fit.trees &lt;- function(k, v) {
  # rmr rbinds the emited values, so v is a dataframe
  # note that do.trace=T is used to produce output to stderr to keep
  # the reduce task from timing out
  rf &lt;- randomForest(formula=model.formula,
                        data=v,
                        na.action=na.roughfix,
                        ntree=10,
                        do.trace=FALSE)
  # rf is a list so wrap it in another list to ensure that only
  # one object gets emitted. this is because keyval is vectorized
  keyval(k, list(forest=rf))
}
   Keep in mind that in our case, we are actually fitting 10 trees per sample, but we could easily only fit a single tree per �forest�, and merge the results from each sample into a single real forest. Note that the choice of predictors is specified in the variable model.formula. R�s random forest implementation does not support factors that have more than 32 levels, as the optimization problem grows too fast. For the purpose of illustrating the Poisson sampling method, we chose to simply ignore those features, even though they probably contain useful information for regression. In a future blog post, we will address various ways that we can get around this limitation. The MapReduce job itself is initiated like so: mapreduce(input="/poisson/training.csv",
               input.format=bulldozer.input.format,
               map=poisson.subsample,
               reduce=fit.trees,
               output="/poisson/output")
   and the resulting trees are dumped in HDFS at /poisson/output. Finally, we can load the trees, merge them, and use them to classify new test points: raw.forests &lt;- values(from.dfs("/poisson/output"))
forest &lt;- do.call(combine, raw.forests)
   Each of 50 samples produced a random forest with 10 trees, so the final random forest is an ensemble of 500 trees, fitted in a distributed fashion over a Hadoop cluster. The full set of source files is available here. Hopefully, you have now learned a scalable approach for training ensemble classifiers or bootstrapping in a parallel fashion by using a Poisson approximation to multinomial sampling. Uri Laserson (@laserson) is a data scientist at Cloudera.</snippet></document><document id="279"><title>Call for Speakers and Early Bird Registration: HBaseCon 2013</title><url>http://blog.cloudera.com/blog/2013/02/call-for-papers-hbasecon-2013/</url><snippet>(Added Feb. 25 2013: Early Bird registration is now open – closes April 23, 2013!) HBaseCon 2012 was such a stunning success�- blowing past all expectations about attendance – that�we want to double-down on the joy in 2013: The HBaseCon 2013 Call for Speakers is now open! Taking place on Thurs., June 13, at the San Francisco Marriott Marquis (and hosted by Cloudera), HBaseCon is the�community event for Apache HBase�contributors, developers, admins, and users of all skill levels.�Registration will open shortly, but for now, we�re asking all members of the community to submit abstracts for tech talks (closes April 1, 2013). What type of abstracts is HBaseCon looking for? Well, deep dives into internals and futures, best practices for running HBase in production, use cases and applications, how to contribute to HBase, and other topics we may not have thought of yet are all on the table. Plus, we’ll be featuring a full track of “thunder talks” this year (20-minute sessions) to complement three other tracks of full-length breakouts. Many thanks in advance to the Program Committee�for what is sure to be the hard work of selecting speakers from a very long list of quality candidates!�</snippet></document><document id="280"><title>Big Data’s New Use Cases: Transformation, Active Archive, and Exploration</title><url>http://blog.cloudera.com/blog/2013/02/big-datas-new-use-cases-transformation-active-archive-and-exploration/</url><snippet>Now that Apache Hadoop is seven years old, use-case patterns for Big Data have emerged. In this post, I�m going to describe the three main ones (reflected in the post�s title) that we see across Cloudera’s growing customer base. Transformation Transformations (T, for short) are a fundamental part of BI systems: They are the process through which data is converted from a source format (which can be relational or otherwise) into a relational data model that can be queried via BI tools. In the late 1980s, the first BI data stacks started to materialize, and they typically looked like Figure 1. Figure 1 � Traditional ETL In this setup the ETL tool is responsible for pulling the data from the source systems (Extract), then it did the transformation into target data (Transform), then it finally loads the data into the target data mart (Load). Traditional ETL tools have four main components: Connectors to read/write from/to many different types of sources/targets Business logic and metadata for what transformations need to be done Transformation libraries for how to perform different types of transformations A transformation engine that does that actual data processing In the mid-to-late 1990s, the data sizes started to grow beyond the ability of ETL tools to finish the transformation work within allocated time (the ETL window) — that is, they�couldn’t�execute the T fast enough, which forced the industry to move from ETL to ELT (more about ELT later). Matters were further complicated by human error. For example, if it is discovered that part of the ETL logic is incorrect for the last three months, the transformations have to be redone. This problem often takes weeks to be corrected, as the ETL tool has to redo all that past work while continuing to do the work for new daily data as it arrives. In summary, ETL transformation engines became a significant bottleneck. ELT is similar to ETL but with one core difference: The heavy lifting of executing the transformation logic is pushed to the databases. Some of the work is pushed to the source databases (downstream), and some to the target databases (upstream). The transformation of unstructured data (i.e. non-relational) is handled by creating a BLOB (Binary Large Object) column in the database, then relying on UDFs (User Defined Functions) and stored procedures that implement the transformation libraries that the ETL also requires. In this model, the ETL tool is primarily a maintainer of the business logic and an orchestrator of the transformation work. Basically, the ETL tool is no longer in the critical path of data flow, and is simply there to manage the execution as opposed to perform it. In ELT, the BI data stack starts to look more like Figure 2. Figure 2: ELT: Source DB -&gt; partial T in Source DB -&gt; Target DB -&gt; partial T in target DB -&gt; BI Reporting (ETL tool orchestrates the execution) The move to ELT worked because years ago, several RDBM systems started to evolve into MPP architectures (Massively Parallel Processing) supporting execution across a number of servers. That parallelism made the RDBMS a much better place to do the heavy lifting of the transformation versus the ETL tool. It is worth noting that some of the ETL tool vendors tried to parallelize their systems but few succeeded. Furthermore, enterprises had already invested significant money in creating the MPP RDBMS clusters, and they�didn’t�want to maintain two separate large clusters – one for the ETL grid, and another for the RDBMS. However, the RDBMSs were created to do queries (Q for short), and not batch transformations. So, as data sizes started to grow again, not only did this approach lead to missing ETL SLA windows (Service Level Agreements), it started missing the Q performance SLAs, too. So, this approach eventually led to a double whammy: the Ts and the Qs slowed down. As the industry struggled with database Q performance getting bogged down by T, and by missing T completion SLAs, a new solution evolved: Apache Hadoop, a distributed data storage and processing system built from the ground up to be massively scalable (thousands of servers) using industry-standard hardware. Furthermore, Hadoop was built to be very flexible in terms of accepting data of any type, regardless of structure. Now that Hadoop has been commercially available in the market for years, hundreds of organizations are moving the T function from their databases to Hadoop. This movement achieves a number of key benefits: Hadoop can perform T much more�effectively�than RDBMSs. Besides the performance benefits it is also very fault tolerant and elastic. If you have a nine-hour transformation job running on 20 servers, and at the eighth hour four of these servers go down, the job will still finish — you will not need to rerun it from scratch. If you discover a human error in your ETL logic and need to rerun T for the last three months, you can temporarily add a few nodes to the cluster to get extra processing speed, and then decommission those nodes after the ETL catch-up is done. When the T moves to Hadoop the RDBMSs are free to focus on doing the Q really well. It is surprising how much faster these systems get once voluminous unstructured data is moved out of them. The new data architecture with Hadoop doing the T looks like Figure 3. Figure 3: Source Systems -&gt; Hadoop -&gt; do T in Hadoop -&gt; Parallel copy to DB -&gt; do Q in DB� A number of ETL tools already support orchestrating execution inside of Hadoop (e.g. Informatica and Pentaho). This allows you to take your existing business transformation logic and define it through the ETL tool as usual, then push the T execution to happen inside of Hadoop. Active Archive We define Return on Byte (ROB) as the ratio of the value of a byte divided by the cost of storing that byte. Analytical RDBMSs are built to be very low-latency OLAP systems for running repetitive queries on extremely high-value bytes (hence they cost a significant premium on a per-TB basis). However, the original unstructured data before going through the transformation phase is too voluminous by nature leading to a low value per byte. This is also true for historical data, since as data gets older, the value on a per-byte basis gets lower. Figure 4: The Return-on-Byte Iceberg While the unstructured and historical bytes in aggregate have a very high value, the individual bytes have very little value. When the ROB drops below 1 — the byte value is less than the cost — what most organizations do is archive that data (to tape or otherwise). Archival systems are very cheap for data storage (especially when compared to RDBMSs) but they are astronomically expensive for data retrieval (since you can�t run processing/queries in archival systems, you have to recover the data back into an RDBMS). Typically once data is archived you never see it again, it literally takes an Act of God (disaster recovery) or Government (auditing/subpoenas) to bring that data back. So in many ways, archival systems are where data goes to die, despite still having a ton of value. In addition to being a great system for doing T, Hadoop is also very good at doing Q for high-granularity and historical data (i.e. data with low value density). In October 2012, Cloudera announced a new open-source system called Cloudera Impala. Impala provides low-latency OLAP capabilities (in SQL) on top of HDFS and Apache HBase. This new system complements the MapReduce batch-transformation system so that you can get the best of both worlds: fast T and economical Q. Impala turns Hadoop into an �active archive�; now you can keep your historical data accessible, but more importantly, you can extract the latent value from it (as opposed to keeping it dormant in a passive archival system). In layman terms, not all of your data deserves to fly in �first class�. Obviously, the critically imperative data, the data with the highest value density (lots of dollars per byte), deserves to bypass all the long lines to arrive to the decision makers as soon as possible. The high cost to store and query that first-class data is justified since its value is congruently high on a per byte basis. However, with Big Data, you are collecting and keeping all of the raw most granular events over many years of history. Though the value of that data in aggregate is very high, the value on a per-byte basis is very small and�doesn’t�justify a first-class ticket. Instead of grounding that data, it is much better to give it an �economy-class� ticket, which enables it to eventually arrive albeit a bit behind the first-class data. This economy-class option is much better than not arriving at all (passive tape archive), and hence suffering the opportunity cost of not letting that data �fly�. Figure 5 shows what such a hybrid BI environment looks like. Figure 5: Active Archive: Hadoop contains a lot of granular historical data, the DB has aggregate recent data, and BI tools on top can query either system using ODBC/JDBC and SQL. Exploration Flexibility is one of the three key differences between traditional RDBMSs and Big Data platforms like Hadoop and Impala (the other two differences being massive scalability and storage economics). At a high level, the flexibility benefit is about allowing the schemas to be dynamic/descriptive as opposed to static/prescriptive. For RDBMSs a schema has to be defined first, then the input data converted from its original native format to the proprietary storage format of the database. If any new data shows up that�wasn’t�pre-defined in that static schema, this new data can�t flow in until: (1) the schema is amended, (2) the ETL logic is changed to parse/load that new data, and (3) the BI data model is amended to recognize it. This adds a lot of rigidity, especially when dealing with quickly evolving data. It does have benefits though — by parsing the data and laying it out efficiently at write-time, the RDBMSs are able to do many optimizations to enable the OLAP queries to finish extremely fast. Furthermore, by having a well-governed static schema across the enterprise, different groups can collaborate and know what the different columns mean. So we want the OLAP optimization and governance of traditional RDBMSs, but we need to augment that with the ability to be agile. The traditional RDBMS schema-on-write model is good at answering the “unknown known” questions — those that we could model the schema for ahead of time. But there is a very large class of exploratory questions that fall under the category of �unknown unknowns� — questions that we�didn’t�know/expect ahead of time. Hadoop with Impala allows us to augment RDBM system with the capability to do such explorations. For example, in your day-to-day BI job, you might get a new question from your business analyst that�isn’t�modeled appropriately in your data model. You are now faced with a dilemma: How can I be sure this question is important enough for me to change all the business logic necessary to expose the new underlying data for that question? It�s a chicken-and-egg problem because you can�t know the true value of the question without having the capability to ask the question at first place! Impala with Hadoop gives you the capability to ask these types of questions. Then, once you find the true value, and decide that this is a question you want to ask over and over again, then you will have the justification necessary to go through adding this new attribute or dimension to your pristine data model. It takes the guesswork out of the process, it allows us to iterate quickly, fail-fast, and finally pick the winning insights in a much more agile way. Impala, and Hadoop at large, get their flexibility from employing a schema-on-read model versus schema-on-write. In other words, the schema is described at the latest stage possible (aka �late binding�) when the question is being asked as opposed to the earliest stage possible when the data is being born. The underlying storage system in Hadoop accepts files in their original native format just like any other filesystem; the files are just copied as-is (e.g. tab-delimited text files, CSV, XML, JSON, syslog files, images,� �). When you are running a query inside Impala it then parses the file and extracts the relevant schema at run time. (Think of it as ETL on the fly.) You can see how this approach of parsing data during query runtime can lead to slower speed for the interactive queries compared to an RDBMS that paid the cost once at write time, but the benefit is extreme agility. New data can start flowing into the system in any shape or form, and months later you can change your schema parser to immediately expose the new data elements without having to go through an extensive database reload or column recreation. Conclusion You learned about three core use cases for Hadoop here: Transformations are bogging down the query performance of RDBM systems. RDBMSs were built to do Queries very well. T is much better suited for a batch processing system like Hadoop which offers the agility of absorbing data of any type, scalably processing that data, and at an economical cost matched to the value of such raw data. Moving data with low value density to passive archival systems, aka data graveyards, leads to a significant loss of value since while that data is of low value on a per byte basis, it is extremely valuable in aggregate. Impala allows you to continue to extract the latent value from your data by powering an active archive that is economically suited for the value of the data. The descriptive data-model agility of Hadoop and Impala makes them a very powerful tool for exploring all of your data and extracting new value that you�couldn’t�have been able to find before due to the rigid schemas of RDBMSs. You truly gain the ability to explore the unknown unknowns, and not just the known unknowns. Remember, this�isn’t�an either-or proposition; RDBM systems have their place when it comes to high-value known questions and rigid governance constraints. Amr Awadallah is the CTO of Cloudera.</snippet></document><document id="281"><title>Apache Hadoop 2.0.3-alpha Released</title><url>http://blog.cloudera.com/blog/2013/02/apache-hadoop-2-0-3-alpha-released/</url><snippet>Last week the Apache Hadoop PMC voted to release Apache Hadoop 2.0.3-alpha, the latest in the Hadoop 2 release series. This release fixes over 500 issues (covering the Common, HDFS, MapReduce and YARN sub-projects) since the 2.0.2-alpha release in October last year. In addition to bug fixes and general improvements the more noteworthy changes include: HDFS High Availability (HA) can now use a Quorum Journal Manager (QJM) for sharing namenode edit logs (HDFS-3077). QJM runs a quorum of journal nodes (typically three), and is designed to replace the reliance on shared storage such as an NFS filer for the edit logs. There is a QJM guide available. There is a new C client for HDFS that uses the WebHDFS REST interface rather than libhdfs, which uses JNI (HDFS-2656). The shuffle and sort in MapReduce are now pluggable, allowing third parties to try out their own implementations for improved performance (MAPREDUCE-2454). The documentation has more details on this feature. There is a new option (mapreduce.job.classloader) to run MapReduce tasks in a custom classloader to isolate user classes from system classes, like in Java Application Servers (MAPREDUCE-1700). The YARN Resource Manager (RM) supports application recovery across restarts (YARN-128). Recovery is off by default, but by setting yarn.resourcemanager.recovery.enabled to true any YARN apps (including MR jobs) that were running on the cluster will be recovered after the RM is restarted. This is the first step in providing HA for the RM (YARN-149). YARN’s Capacity Scheduler now supports CPU in resource requests, in addition to memory (YARN-2). This means that applications may ask for containers with a desired number of “virtual cores”. The allowable range of virtual cores for a container is determined by yarn.scheduler.minimum-allocation-vcores and yarn.scheduler.maximum-allocation-vcores, the capacity of a Node Manager is set by yarn.nodemanager.resource.cpu-cores, and a virtual core is defined by yarn.nodemanager.vcores-pcores-ratio. MAPREDUCE-4520 covers the changes to MapReduce to allow the number of cores to be specified for jobs. YARN can optionally use Linux Control Groups (cgroups) to enforce resource limits for containers (YARN-3). See yarn.nodemanager.resource-enforcer.class in yarn-default.xml for how to enable this feature. The YARN Fair Scheduler now supports all the features of the Fair Scheduler in MR1, plus some new features like support for hierarchical queues. Its UI has been improved to support the new UI framework in YARN (YARN-145). AMRMClient was added as a convenience class for YARN application writers to manage Application Master-Resource Manager communication. (YARN-103 and YARN-277.) Getting to Hadoop 2 GA The next release, Apache Hadoop 2.0.4-beta, is planned for in a couple of months, with a Hadoop 2.0 GA release following sometime in the middle of the year. The remaining work before the GA release is focused on stabilizing YARN so that it can run production MapReduce workloads at scale, and ensuring its APIs are stable. Yahoo! recently reported good progress in this area: they have run over 14 million jobs on YARN, on tens of thousands of nodes. (They run the 0.23.x code line, which contains all the YARN stabilization fixes that are in 2.0.3-alpha.) Try It Out! You can download the release from an�Apache mirror. The forthcoming CDH 4.2.0 release will include most of the changes from Apache Hadoop 2.0.3-alpha. (Note that QJM has been available since CDH 4.1.0.) Like the Apache release, MR2 (running on YARN) is still experimental in CDH 4.2.0, but HDFS and MR1 are stable and fully supported in production since CDH 4.0. Acknowledgements Thanks to everyone who contributed to this release�every contribution is appreciated. Also, thanks go to Arun C Murthy who acted as release manager.</snippet></document><document id="282"><title>Cloudera Speakers at Big Data TechCon (+ $200 Off Registration)</title><url>http://blog.cloudera.com/blog/2013/02/cloudera-speakers-at-big-data-techcon-200-off-registration/</url><snippet>Cloudera is proud to be a sponsor of Big Data TechCon�in Boston (April 8-10), in particular because this is a new conference (run by the good folks at BZ Media) in a Big Data hotbed city.� Even better, attendees will have a bonanza of technical sessions led by Cloudera engineer-rock stars from which to choose: Extending Your Data Infrastructure with Hadoop – Jonathan Seidman Hadoop Backup and Disaster Recovery 101 – Jai Ranganathan 7 Deadly Hadoop Misconfigurations – Kate Ting Building an Impenetrable ZooKeeper – Kate Ting Mastering Sqoop for Data Transfer for Big Data – Kate Ting &amp; Jaroslav Cecho Introduction and Best Practices for Storing and Analyzing Your Data in Apache Hive�(Half-Day Tutorial) – Mark Grover How to Fit a Petabyte in Apache HBase – JD Cryans Using Apache HBase’s API for Complex Real-Time Queries – JD Cryans Building Applications using HBase – Amandeep Khurana HBase Schema and Table Design Principles – Amandeep Khurana Finally, being a sponsor, Cloudera has the opportunity to offer you a $200 discount – just use discount code “cloudera” at registration. This discount has a limited timeframe, so best not to wait too long.�</snippet></document><document id="283"><title>How-To: Run a MapReduce Job in CDH4 using Advanced Features</title><url>http://blog.cloudera.com/blog/2013/02/how-to-run-a-mapreduce-job-in-cdh4-using-advanced-features/</url><snippet>In my previous post, you learned how to write a basic MapReduce job and run it on Apache Hadoop. In this post, we�ll delve deeper into MapReduce programming and cover some of the framework�s more advanced features. In particular, we�ll explore: Combiner functions, a feature that allows you to aggregate map outputs before they are passed to the reducer, possibly greatly reducing the amount of data written to disk and sent over the network for certain types of jobs Counters, a way to track how often user-defined events occur across an entire job – for example, count the number of bad records your MapReduce job encounters in all your data and feed it back to you, without any complex instrumentation on your part Custom Writables, go beyond the basic data types that Hadoop provides as keys and values for your mappers and reducers MRUnit, a framework that facilitates unit testing of MapReduce programs The full code and short instructions for how to compile and run it are available at https://github.com/sryza/traffic-reduce. In addition, this time we�ll write our MapReduce program using the �new� MapReduce API, a cleaned-up take on what MapReduce programs should look like that was introduced in Hadoop 0.20. Note that the difference between the old and new MapReduce API is entirely separate from the difference between MR1 and MR2: The API changes affect developers writing MapReduce code, while MR2 is an architectural change that differs from MR1 by, under the hood, extracting out the scheduling and resource management aspects into YARN, which allows Hadoop to support other parallel execution frameworks and scale to larger clusters. Both MR1 and MR2 support the old and new MapReduce API. The Use Case It�s 11pm on a Thursday, and while Los Angeles is known for its atrocious traffic, you can usually count on being safe five from heavy traffic hours after rush hour. But when you merge onto the I-10 going west, it�s bumper to bumper for miles!� What�s going on? It has to be the Clippers game. With tens of thousands of cars leaving from the Staples Center after a home-team basketball game, of course it�s going to be bad. But what about for a Lakers game?� How bad does it for those?� And what about holidays and during political events? It would be great if you could enter a time and determine how far traffic deviated from average for every road in the city. CalTrans� Performance and Measurement System (PeMS) provides detailed traffic data from sensors placed on freeways across the state, with updates coming in every 30 seconds. The Los Angeles area alone contains over 4,000 sensor stations.� While this is frankly a boatload of data, MapReduce allows you to leverage a cluster to process it in a reasonable amount of time. In this post, we�ll write a MapReduce program that computes the averages, and next time, we�ll write a program that uses this information to build an index of this data, so that a program may query it easily to display data from the relevant time. The TrafficInduce Program For our first MapReduce job, we would like to find the average traffic for each sensor station at each time of the week. While the data is available every 30 seconds, we don�t need such fine granularity, so we will use the five-minute summaries that PeMS also publishes. Thus, with 4,370 stations, we will be calculating 4,370 * (60 / 5) * 24 * 7 = 8,809,920 averages. Each of our input data files contains the measurements for all the stations over a month. Each line contains a station ID, a time, some information about the station, and the measurements taken from that station during that time interval. Here are some example lines. The fields that are useful to us are the first, which tells the time; the second, which tells the station ID; and the 10th, which gives a normalized vehicle at that station at that time. 01/01/2012 00:00:00,312346,3,80,W,ML,.491,354,33,1128,.0209,71.5,0,0,0,0,0,0,260,.012,73.9,432,.0273,69.2,436,.0234,72.3,,,,,,,,,,,,,,,
01/01/2012 00:00:00,312347,3,80,W,OR,,236,50,91,,,,,,,,,91,,,,,,,,,,,,,,,,,,,,,,,
01/01/2012 00:00:00,312382,3,99,N,ML,.357,0,0,629,.0155,67,0,0,0,0,0,0,330,.0159,69.9,299,.015,63.9,,,,,,,,,,,,,,,,,,
01/01/2012 00:00:00,312383,3,99,N,OR,,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
01/01/2012 00:00:00,312386,3,99,N,ML,.42,0,0,1336,.0352,67.1,0,0,0,0,0,0,439,.0309,70.4,494,.039,67.4,403,.0357,63.4,,,,,,,,,,,,,,,
   The mappers will parse the input lines and emit a key/value pair for each line, where the key is an ID that combines the station ID with the time of the week, and the value is the number of cars that passed over that sensor during that time. Each call to the reduce function receives a station/time of week and the vehicle count values over all the weeks, and computes their average. Combiners An interesting inefficiency to note is that if a single mapper processes measurements over multiple weeks, it will end up with multiple outputs going to the same reducer.�As these outputs are going to be averaged by the reducer anyway, we would be able to save I/O by computing partial averages before we have the complete data. To do this, we would need to maintain a count of how many data points are in each partial average, so that we can weight our final average by that count.� For example, we could collapse a set of map outputs like 5, 6, 9, 10 into (avg=7.5, count=4). As each map output is written to disk on the mapper, sent over the network, and then possibly written to disk on the reducer, reducing the number of outputs in this way can save a fair amount of I/O. MapReduce provides us with a way to do exactly this in the form of combiner functions. The framework calls the combiner function in between the map and reduce phase, with the combiner�s outputs sent to the reducer instead of the map outputs that it�s called on. The framework may choose to call a combiner function zero or more times – generally it is called before map outputs are persisted to disk, both on the map and reduce side. Thus, from a high level, our program looks like this: map(line of input file) {
  parse line of input file, emit (station id, time of week) -&gt; (vehicle count, 1)
}

combine((station id, time of week), list of corresponding (vehicle count, 1)s) {
  take average of the input values, emit (station id, time of week) -&gt; (average vehicle count, size(list))
}

reduce((station id, time of week), list of corresponding (vehicle count, size)s) {
  take weighted average of the input values, emit (station id, time of week) -&gt; average vehicle count
}
   Custom Writables MapReduce key and value classes implement Hadoop�s Writable interface so that they can be serialized to and from binary. While Hadoop provides a set of classes that implement Writable to serialize primitive types, the tuples we use in your pseudo-code don�t map efficiently onto any of them. For our keys, we can concatenate the station ID with the time of week to represent them as strings and use the Text type.� However, as our value tuple is composed of primitive types, a float and an integer, it would be nice not to have to convert them to and from strings each time you want to use them. We can accomplish this by implementing a Writable for them. public class AverageWritable implements Writable {

� private int numElements;
� private double average;
�
� public AverageWritable() {}
�
� public void set(int numElements, double average) {
��� this.numElements = numElements;
��� this.average = average;
� }
�
� public int getNumElements() {
��� return numElements;
� }
�
� public double getAverage() {
��� return average;
� }
�
� @Override
� public void readFields(DataInput input) throws IOException {
��� numElements = input.readInt();
��� average = input.readDouble();
� }

��@Override
� public void write(DataOutput output) throws IOException {
��� output.writeInt(numElements);
��� output.writeDouble(average);
� }

� [toString(), equals(), and hashCode() shown in the the github repo]
}
   We deploy our Writable by including it in our job jar. To instantiate our Writable, the framework will call its no-argument constructor, and then fill it in by calling its readFields method. Note that if we wanted to use a custom class as a key, it would need to implement WritableComparable so that it would be able to be sorted. At Last, the Program With our custom data type in hand, we are at last ready to write our MapReduce program. Here is what our mapper looks like: public class AveragerMapper extends Mapper&lt;LongWritable, Text, Text, AverageWritable&gt; {
�
� private AverageWritable outAverage = new AverageWritable();
� private Text id = new Text();
�
� @Override
� public void map(LongWritable key, Text line, Context context)
����� throws InterruptedException, IOException {
��� String[] tokens = line.toString().split(",");
��� if (tokens.length &lt; 10) {
����� return;
��� }
��� String dateTime = tokens[0];
��� String stationId = tokens[1];
��� String trafficCount = tokens[9];

��� if (trafficCount.length() &gt; 0) {
����� id.set(stationId + "_" + TimeUtil.toTimeOfWeek(dateTime));
����� outAverage.set(1, Integer.parseInt(trafficCount));
�����
����� context.write(id, outAverage);
��� }
� }
}
   You may notice that this mapper looks a little bit different than the mapper used in the last post.�This is because in this post we use the �new� MapReduce API, a rewrite of the MapReduce API that was introduced in Hadoop 0.20.� The newer one is a little bit cleaner, but Hadoop will support both APIs far into the future. An astute observer will notice that our combiner and reducer are doing exactly the same thing – i.e. outputting a weighted average of the inputs.� Thus, we can write the following reducer function, and pass it as a combiner as well: public class AveragerReducer extends Reducer&lt;Text, AverageWritable, Text, AverageWritable&gt; {
�
� private AverageWritable outAverage = new AverageWritable();
�
� @Override
� public void reduce(Text key, Iterable&lt;AverageWritable&gt; averages, Context context)
����� throws InterruptedException, IOException {
��� double sum = 0.0;
��� int numElements = 0;
��� for (AverageWritable partialAverage : averages) {
����� // weight partial average by number of elements included in it
����� sum += partialAverage.getAverage() * partialAverage.getNumElements();
����� numElements += partialAverage.getNumElements();
��� }
��� double average = sum / numElements;
���
��� outAverage.set(numElements, average);
��� context.write(key, outAverage);
� }
}
   Using the new API, our driver class looks like this: public class AveragerRunner {
� public static void main(String[] args) throws IOException, ClassNotFoundException,
����� InterruptedException {
��� Configuration conf = new Configuration();
��� String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
��� Job job = new Job(conf);
��� job.setJarByClass(AveragerRunner.class);
��� job.setMapperClass(AveragerMapper.class);
��� job.setReducerClass(AveragerReducer.class);
��� job.setCombinerClass(AveragerReducer.class);
��� job.setMapOutputKeyClass(Text.class);
��� job.setMapOutputValueClass(AverageWritable.class);
��� job.setInputFormatClass(TextInputFormat.class);
���
��� FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
��� FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));

��� job.waitForCompletion(true);
��}
}
   Note that unlike last time, when we used KeyValueTextInputFormat, we use TextInputFormat for our input data. While KeyValueTextInputFormat splits up the line into a key and a value, TextInputFormat passes the entire line as the value, and uses its position in the file (as an offset from the first byte) as the key.� The position is not used, which is fairly typical when using TextInputFormat. Counters In the real world, data is messy. Traffic sensor data, for example, contains records with missing fields all the time, as sensors in the wild are bound to malfunction at times. Running our MapReduce job, it is often useful to count up and collect metrics on the side about what our job is doing. For a program on a single computer, we might just do this by adding in a count variable, incrementing it whenever our event of interest occurs, and printing it out at the end, but when our code is running in a distributed fashion, aggregating these counts gets hairy very quickly.� Luckily, Hadoop provides a mechanism to handle this for us, using Counters. MapReduce contains a number of built-in counters that you have probably seen in the output on completion of a MapReduce job. Map-Reduce Framework
�������Map input records=10
�������Map output records=7
�������Map output bytes=175
�������[and many more]
   This information is also available in the web UI, both per-job and per-task. To use our own counter, we can simply add a line like context.getCounter("Averager Counters", "Missing vehicle flows").increment(1);
   to the point in the code where the mapper comes across a record with a missing count. Then, when our job completes, we will see our count along with the built-in counters: Averager Counters � �Missing vehicle flows=2329 It�s often convenient to wrap your entire map or reduce function in a try/catch, and increment a counter in the catch block, using the exception class�s name as the counter�s name for a profile of what kind of errors come up. Testing Running a MapReduce program on a cluster, if we even have access to one, can take a while.�However, if we want to make sure that our basic logic works, we have no need for all the machinery. Enter Apache MRUnit, an Apache project that makes writing JUnit tests for MapReduce programs probably as easy as it could possibly be. Through MRUnit, we can test our mappers and reducers both separately and as a full flow.� To include it in our project, we add the following to the dependencies section Maven�s pom.xml: &lt;dependency&gt;
���&lt;groupId&gt;org.apache.mrunit&lt;/groupId&gt;
���&lt;artifactId&gt;mrunit&lt;/artifactId&gt;
���&lt;version&gt;0.9.0-incubating&lt;/version&gt;
���&lt;classifier&gt;hadoop2&lt;/classifier&gt;
&lt;/dependency&gt;
   The following contains a test for both the mapper and reducer, verifying that with sample inputs, they produce the expected outputs: public class TestTrafficAverager {
� private MapDriver&lt;LongWritable, Text, Text, AverageWritable&gt; mapDriver;
� private ReduceDriver&lt;Text, AverageWritable, Text, AverageWritable&gt; reduceDriver;
�
� @Before
� public void setup() {
��� AveragerMapper mapper = new AveragerMapper();
��� AveragerReducer reducer = new AveragerReducer();
��� mapDriver = MapDriver.newMapDriver(mapper);
��� reduceDriver = ReduceDriver.newReduceDriver(reducer);
� }
�
� @Test
� public void testMapper() throws IOException {
��� String line = "01/01/2012 00:00:00,311831,3,5,S,OR,,118,0,200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,";
��� mapDriver.withInput(new LongWritable(0), new Text(line));
��� Text outKey = new Text("311831_" + TimeUtil.toTimeOfWeek("01/01/2012 00:00:00"));
��� AverageWritable outVal = new AverageWritable();
��� outVal.set(1, 200.0);
��� mapDriver.withOutput(outKey, outVal);
��� mapDriver.runTest();
� }
�
� @Test
� public void testReducer() {
��� AverageWritable avg1 = new AverageWritable();
��� avg1.set(1, 2.0);
��� AverageWritable avg2 = new AverageWritable();
��� avg2.set(3, 1.0);
��� AverageWritable outAvg = new AverageWritable();
��� outAvg.set(4, 1.25);
��� Text key = new Text("331831_86400");
���
��� reduceDriver.withInput(key, Arrays.asList(avg1, avg2));
��� reduceDriver.withOutput(key, outAvg);
��� reduceDriver.runTest();
� }
   We can run our tests with �mvn test� in the project directory.� If there are failures, information on why is available in the project directory in target/surefire-reports. A more in depth MRUnit tutorial is available here: https://cwiki.apache.org/confluence/display/MRUNIT/MRUnit+Tutorial. Running Our Program on Hadoop Like last time, we can build the jar with mvn install
   The full data is available at http://pems.dot.ca.gov/?dnode=Clearinghouse, but like last time, the github repo contains some sample data to run our program on.� To place it on the cluster, we can run: hadoop fs -mkdir trafficcounts
hadoop fs -put samples/input.txt trafficcounts
   To run our program, we can use hadoop jar target/trafficinduce-1.0-SNAPSHOT.jar AveragerRunner trafficcounts/input.txt trafficcounts/output
   We can inspect the output with: hadoop fs -cat /trafficcounts/output/part-00000
   Thanks for reading! Next time, we�ll delve into some more advanced MapReduce features, like the distributed cache, custom partitioners, and custom input and output formats. Sandy Ryza is a Software Engineer on the Platform team.</snippet></document><document id="284"><title>The Cloudera Sessions Across the U.S.: Where Are You on the Road to Big Data?</title><url>http://blog.cloudera.com/blog/2013/02/the-cloudera-sessions-across-the-u-s-where-are-you-on-the-road-to-big-data/</url><snippet>Organizations of all types and sizes are waking up to the idea that integrating the Apache Hadoop stack into their IT infrastructure solves very common, near-term data management problems. At the same time, deploying Hadoop offers the long-term promise of rapid innovation via Big Data analytics. But, how do you get from Point A to Point Z with the least possible exposure to risk? Coming to a U.S. city near you, The Cloudera Sessions are single-day, interactive events – with presentations in the morning, and�technical breakouts in the afternoon – designed to help you identify where you are on your journey with Apache Hadoop, and how to keep that journey going in a low-risk, productive way. You’ll benefit not only from Cloudera’s experiences with real-world deployments, but also hear directly from some of the Hadoop users who planned and manage them. If you’re the business owner of a data warehouse, an enterprise architect, an IT leader, or an analyst or developer, you can’t afford to miss this opportunity to learn about the very real and widely applicable benefits of Hadoop. Register today for a Cloudera Session near you, because seats are limited: March 5 – Chicago March 19 – Atlanta March 20 – Charlotte, N.C. April 16 – Seattle� April 17 – Houston April 18 – Dallas April 22 – Toronto April 23 – Boston April 23�- Los Angeles May 7 – Washington, D.C. May 8 – Columbus, Ohio May 22�- Minneapolis� Register</snippet></document><document id="285"><title>How-to: Use Apache ZooKeeper to Build Distributed Apps (and Why)</title><url>http://blog.cloudera.com/blog/2013/02/how-to-use-apache-zookeeper-to-build-distributed-apps-and-why/</url><snippet>It�s widely accepted that you should never design or implement your own cryptographic algorithms but rather use well-tested, peer-reviewed libraries instead. The same can be said of distributed systems: Making up your own protocols for coordinating a cluster will almost certainly result in frustration and failure. Architecting a distributed system is not a trivial problem; it is very prone to race conditions, deadlocks, and inconsistency. Making cluster coordination fast and scalable is just as hard as making it reliable. That�s where Apache ZooKeeper, a coordination service that gives you the tools you need to write correct distributed applications, comes in handy. With ZooKeeper, these difficult problems are solved once, allowing you to build your application without trying to reinvent the wheel. ZooKeeper is already used by Apache HBase, HDFS, and other Apache Hadoop projects to provide highly-available services and, in general, to make distributed programming easier. In this blog post you’ll learn how you can use ZooKeeper to easily and safely implement important features in your distributed software. How ZooKeeper Works ZooKeeper runs on a cluster of servers called an ensemble that share the state of your data. (These may be the same machines that are running other Hadoop services or a separate cluster.) Whenever a change is made, it is not considered successful until it has been written to a quorum (at least half) of the servers in the ensemble. A leader is elected within the ensemble, and if two conflicting changes are made at the same time, the one that is processed by the leader first will succeed and the other will fail. ZooKeeper guarantees that writes from the same client will be processed in the order they were sent by that client. This guarantee, along with other features discussed below, allow the system to be used to implement locks, queues, and other important primitives for distributed queueing. The outcome of a write operation allows a node to be certain that an identical write has not succeeded for any other node. A consequence of the way ZooKeeper works is that a server will disconnect all client sessions any time it has not been able to connect to the quorum for longer than a configurable timeout. The server has no way to tell if the other servers are actually down or if it has just been separated from them due to a network partition, and can therefore no longer guarantee consistency with the rest of the ensemble. As long as more than half of the ensemble is up, the cluster can continue service despite individual server failures. When a failed server is brought back online it is synchronized with the rest of the ensemble and can resume service. It is best to run your ZooKeeper ensemble with an odd number of server; typical ensemble sizes are three, five, or seven. For instance, if you run five servers and three are down, the cluster will be unavailable (so you can have one server down for maintenance and still survive an unexpected failure). If you run six servers, however, the cluster is still unavailable after three failures but the chance of three simultaneous failures is now slightly higher. Also remember that as you add more servers, you may be able to tolerate more failures, but you also may begin to have lower write throughput. (Apache�s documentation�has a nice illustration of the performance characteristics of various ZooKeeper ensemble sizes.) Installing ZooKeeper You need to have Java installed before running ZooKeeper (client bindings are available in several other languages). Cloudera currently recommends Oracle Java 6 for production use, but these examples should work just fine on OpenJDK 6 / 7. You�ll then need to install the correct CDH4�package repository for your system and install the zookeeper package (required for any machine connecting to ZooKeeper) and the zookeeper-server package (required for any machine in the ZooKeeper ensemble). Be sure to look at the instructions for your specific system to get the correct URL and commands, but the installation on an Ubuntu system will look something like this: $ wget http://archive.cloudera.com/.../cdh4-repository_1.0_all.deb
$ sudo dpkg -i cdh4-repository_1.0_all.deb
$ sudo apt-get update
$ sudo apt-get install zookeeper zookeeper-server
...
JMX enabled by default
Using config: /etc/zookeeper/conf/zoo.cfg
ZooKeeper data directory is missing at /var/lib/zookeeper fix the path or run initialize
invoke-rc.d: initscript zookeeper-server, action "start" failed.
   The warnings you will see indicate that the first time ZooKeeper is run on a given host, it needs to initialize some storage space. You can do that as shown below, and start a ZooKeeper server running in a single-node/standalone configuration. $ sudo service zookeeper-server init
No myid provided, be sure to specify it in /var/lib/zookeeper/myid if using non-standalone
$ sudo service zookeeper-server start
JMX enabled by default
Using config: /etc/zookeeper/conf/zoo.cfg
Starting zookeeper ... STARTED
   The ZooKeeper CLI ZooKeeper comes with a command-line client for interactive use, although in practice you would use one of the programming language bindings�directly from your application. We�ll just demonstrate the basic principles of using ZooKeeper with the command-line client. Launch the client by executing the zookeeper-client command. The initial prompt may be hidden by some log messages, so just hit &lt;ENTER&gt; if you don�t see it. Try typing ls / or help to see the other possible commands. $ zookeeper-client
  ...
  [zk: localhost:2181(CONNECTED) 0] ls /
  [zookeeper]
  [zk: localhost:2181(CONNECTED) 1] help
   This is similar to the shell and file system from UNIX-like systems. ZooKeeper stores its data in a hierarchy of znodes. Each znode can contain data (like a file) and have children (like a directory). ZooKeeper is intended to work with small chunks of data in each znode: the default limit is 1MB. Reading and Writing Data Creating a znode is as easy as specifying the path and the contents. Create an empty znode to serve as a parent �directory�, and another znode as its child: [zk: localhost:2181(CONNECTED) 2] create /zk-demo ''
Created /zk-demo
[zk: localhost:2181(CONNECTED) 3] create /zk-demo/my-node 'Hello!'
Created /zk-demo/my-node
   You can then read the contents of these znodes with the get command. The data contained in the znode is printed on the first line, and metadata is listed afterwards (most of the metadata in these examples has been replaced with the text &lt;metadata&gt;for brevity). [zk: localhost:2181(CONNECTED) 4] get /zk-demo/my-node
'Hello!'
&lt;metadata&gt;
dataVersion = 0
&lt;metadata&gt;
   You can, of course, modify znodes after you create them. Notice that the dataVersion values have been modified (as well as the modified timestamp – other metadata has been omitted for brevity). [zk: localhost:2181(CONNECTED) 5] set /zk-demo/my-node 'Goodbye!'
&lt;metadata&gt;
dataVersion = 1
&lt;metadata&gt;
[zk: localhost:2181(CONNECTED) 6] get /zk-demo/my-node
'Goodbye!'
&lt;metadata&gt;
dataVersion = 1
&lt;metadata&gt;
   You can also delete znodes. znodes that have children cannot be deleted (unless their children are deleted first). There is an rmr command that will do this for you. [zk: localhost:2181(CONNECTED) 7] delete /zk-demo/my-node
[zk: localhost:2181(CONNECTED) 8] ls /zk-demo
[]
   Sequential and Ephemeral znodes In addition to the standard znode type, there are two special types of znode: sequential and ephemeral. You can create these by passing the -s and -e flags to the create command, respectively. (You can also apply both types to a znode.) Sequential nodes will be created with a numerical suffix appended to the specified name, and ZooKeeper guarantees that two nodes created concurrently will not be given the same number. [zk: localhost:2181(CONNECTED) 9] create -s /zk-demo/sequential one
Created /zk-demo/sequential0000000002
[zk: localhost:2181(CONNECTED) 10] create -s /zk-demo/sequential two
Created /zk-demo/sequential0000000003
[zk: localhost:2181(CONNECTED) 11] ls /zk-demo
[sequential0000000003, sequential0000000002]
[zk: localhost:2181(CONNECTED) 12] get /zk-demo/sequential0000000002
one
&lt;metadata&gt;
   Note that the numbering is based on previous children that have had the same parent – so the first sequential node we created was actually # 2. This feature allows you to create distributed mutexes. If a client wants to hold the mutex, it creates a sequential node. If it is then the lowest number znode with that name, it holds the lock. If not, it waits. To release the mutex, it deletes the node, allowing the next znode in order to hold the lock. You can implement a very simple master election system by making sequential znodes ephemeral. Ephemeral nodes are automatically deleted when the client that created them disconnects (which means that ZooKeeper can also help you with failure detection – another hard problem in distributed systems). Clients can disconnect intentionally when they shut down, or they can be considered disconnected by the cluster because the client exceeded the configured timeout without sending a heartbeat. The node that created the lowest-numbered sequential ephemeral node assumes the �master� role. If the machine crashes, or the JVM pauses too long for garbage collection, the ephemeral node is deleted and the next eligible node can assume its place. [zk: localhost:2181(CONNECTED) 13] create -e -s /zk-demo/ephemeral data
Created /zk-demo/ephemeral
[zk: localhost:2181(CONNECTED) 14] ls /zk-demo
[sequential0000000003, sequential0000000002, ephemeral0000000003]
[zk: localhost:2181(CONNECTED) 15] quit
Quitting...
$ zookeeper-client
Connecting to localhost:2181
Welcome to ZooKeeper!
[zk: localhost:2181(CONNECTED) 0] ls /zk-demo
[sequential0000000003, sequential0000000002]
   Watches ZooKeeper can also notify you of changes in a znode�s content or changes in a znode�s children. To register a �watch� on a znode�s data, you need to use the get or stat commands to access the current content or metadata, and pass an additional parameter requesting the watch. To register a �watch� on a znode�s children, you pass the same parameter when getting the children with ls. [zk: localhost:2181(CONNECTED) 1] create /zk-demo/watch-this data
Created /watch-this
[zk: localhost:2181(CONNECTED) 2] get /zk-demo/watch-this true
data
&lt;metadata&gt;
   Modify the same znode (either from the current ZooKeeper client or a separate one), and you will see the following message written to the terminal: WATCHER::

WatchedEvent state:SyncConnected type:NodeDataChanged path:/watch-this
   Note that watches fire only once. If you want to be notified of changes in the future, you must reset the watch each time it fires. Watches allow you to use ZooKeeper to implement asynchronous, event-based systems and to notify nodes when their local copies of the data in ZooKeeper is stale. Versioning and ACLs If you look at the metadata listed in previous commands, you will see items that are common in many file systems and features that have been discussed above: creation time, modification time (and corresponding transaction IDs), the size of the contents in bytes, and the creator of the node (if ephemeral). You will also see some metadata for features that help safeguard the integrity and security of the data: data versioning and ACLs. For example: cZxid = 0x00
ctime = Sun Jan 00 00:00:00 UTC 2013
mZxid = 0x00
mtime = Sun Jan 00 00:00:00 UTC 2013
pZxid = 0x00
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 0
numChildren = 0
   The current version of the data is provided every time you read or write to it, and it can also be specified as part of a write command (a test-and-set operation). If a write is attempted with an out-of-date version specified, it will fail – which is useful to make sure you do not overwrite changes that your client has not yet processed. ZooKeeper also supports the use of Access Control Lists (ACLs) and various authentication systems. ACLs allow you to specify finely-grained permissions to define which users and groups are allowed to create, read, update or delete each znode. Using ACLs in ZooKeeper is beyond the scope of this article, but you can read more about them on the�project�s website. Ideas for Using ZooKeeper All of the mechanisms described above are accessible through various programming language bindings, allowing you to use ZooKeeper to write better distributed applications. Several Hadoop projects are already using ZooKeeper to coordinate the cluster and provide highly-available distributed services. Perhaps most famous of these is Apache HBase, which uses ZooKeeper to track the master, the region servers, and the status of data distributed throughout the cluster. Here are some other examples of how ZooKeeper might be useful to you, and you can find details of the algorithms required for many of these use cases�here. Group membership and name services By having each node register an ephemeral znode for itself (and any roles it might be fulfilling), you can use ZooKeeper as a replacement for DNS within your cluster. Nodes that go down are automatically removed from the list, and your cluster always has an up-to-date directory of the active nodes. Distributed mutexes and master election We discussed these potential uses for ZooKeeper above in connection with sequential nodes. These features can help you implement automatic fail-over within your cluster, coordinate concurrent access to resources, and make other decisions in your cluster safely. Asynchronous message passing and event broadcasting Although other tools are better suited to message passing when throughput is the main concern, I�ve found ZooKeeper to be quite useful for building a simple pub/sub system when needed. In one case, a cluster needed a long sequence of actions to be performed in the hours after a node was added or removed in the cluster. On demand, the sequence of actions was loaded into ZooKeeper as a group of sequential nodes, forming a queue. The �master� node processed each action at the designated time and in the correct order. The process took several hours and there was a chance that the master node might crash or be decommissioned during that time. Because ZooKeeper recorded the progress on each action, another node could pick up where the master left off in the event of any problem. Centralized configuration management Using ZooKeeper to store your configuration information has two main benefits. First, new nodes only need to be told how to connect to ZooKeeper and can then download all other configuration information and determine the role they should play in the cluster for themselves. Second, your application can subscribe to changes in the configuration, allowing you to tweak the configuration through a ZooKeeper client and modify the cluster�s behavior at run-time. Wrapping Up Done wrong, distributed software can be the most difficult to debug. Done right, however, it can allow you to process more data, more reliably and in less time. Using Apache ZooKeeper allows you to confidently reason about the state of your data, and coordinate your cluster the right way. You�ve seen how easy it is to get a ZooKeeper server up and running. (In fact, if you�re a CDH user, you may already have an ensemble running!) Think about how ZooKeeper could help you build more robust systems, and have a look at CDH documentation�and Apache�s project website�for more information about running these packages in a production environment. Sean Mackrory is a Software Engineer on Cloudera’s infrastructure and packaging team.</snippet></document><document id="286"><title>Meet the Engineer: Kathleen Ting</title><url>http://blog.cloudera.com/blog/2013/02/meet-the-engineer-kathleen-ting/</url><snippet>In this installment of �Meet the Engineer�, get to know Customer Operations Engineering Manager/Apache Sqoop committer Kathleen Ting (@kate_ting). What do you do at Cloudera, and in what open-source projects are you involved? I�m a support manager at Cloudera, and an Apache Sqoop committer and PMC member. I also contribute to the Apache Flume and Apache ZooKeeper mailing lists and organize and present at meetups, as well as speak at conferences, about those projects. My role is a hybrid �player/coach� model: in addition to doing managerial things like leading a team and addressing customer escalations, I also answer customer support cases directly, which is a fairly unique combination. This is an effective approach: giving me direct insights into customer concerns that I otherwise wouldn�t get, helping me stay grounded, and ensuring I appreciate the work the team is doing, first-hand. Why do you enjoy your job? The best thing about my job, beyond the privilege of helping customers become successful with Big Data technology, is that I can stay deeply involved in the developer community � speaking at meetups and conferences, answering questions on the mailing lists, contributing code upstream, and so on � just like any other engineer at Cloudera. At most other companies, engineering and support (known at Cloudera as Customer Operations Engineering) are in separate silos, but here, Customer Operations Engineers (COEs) are just as involved in the community and the product roadmap as developers are � maybe even more so, because we hear what customers care about, every day. In fact, the COE organization at Cloudera is highly respected internally, because supporting customers is literally half our business. This is why at Cloudera, COEs have a career path that rivals that of developers. The technical bar for COEs is quite high because we encounter issues across development and operations, and have to know a lot about both. And with the Apache Hadoop stack being so complex, it takes a lot of knowledge and experience to correctly deduce the root cause of a customer issue. There would be literally no way to follow a script even if we wanted to, because problems can present themselves in so many unforeseen ways. (A very, very small percentage of customer problems are repeatable.) I enjoy that, because it makes every day, and every customer ticket, interesting in a different way. What is your favorite thing about Apache Hadoop? In a former life, I wrote software for storage on the mainframe, and in those days, you needed really deep pockets for pricey software and a lot of expensive big iron to get much value from your data. But now, with Hadoop serving as the �Great Equalizer� for storing and processing Big Data, many, many more companies have the ability to do the same thing a lot more economically using open-source software and industry-standard hardware. I think the time is now for Big Data – there are more and more use cases every day, and for that reason, everyone wants a piece of the action. It�s an exciting time to be involved! What advice can you offer about getting involved in open source for the first time? Basically, find a way to regularly contribute with good intentions. You don�t need to have written Hadoop to use it – you can be active on the mailing lists, attend meetups, and work on JIRAs without being a committer. Furthermore, fresh eyes can really be helpful in open-source projects, so write a troubleshooting guide, update the documentation, help triage broken builds, and pick up JIRAs tagged �n00b�. Just supporting committers in those areas can be really helpful. The important thing is to make some kind of contribution every day, no matter how minor. if you do that, you�ll build a solid open-source rep pretty quickly. If you�re looking for guidance, I�m happy to help. At what age did you become interested and programming, and why? When I was growing up I wanted to be just like my older brother, an attorney. That was until my sophomore year of high school when on a whim, I attended an engineering camp hosted by Santa Clara University. I found myself happily holed up in the computer lab coding functional programming exercises in LISP. (Tiger got to hunt. Bird got to fly. Lisper got to sit and wonder, (Y (Y Y))? – with apologies to Kurt Vonnegut and hat tip to Darius Bacon.) If you�re attending ApacheCon North America 2013 (Feb. 26-28, Portland), see Kathleen present on �7 Deadly Hadoop Misconfigurations� and �Mastering Sqoop for Data Transfer for Big Data�.</snippet></document><document id="287"><title>Inside Cloudera Impala: Runtime Code Generation</title><url>http://blog.cloudera.com/blog/2013/02/inside-cloudera-impala-runtime-code-generation/</url><snippet>Cloudera Impala, the open-source real-time query engine for Apache Hadoop, uses many tools and techniques to get the best query performance.�This blog post will discuss how we use runtime code generation to significantly improve our CPU efficiency and overall query execution time. We�ll explain the types of inefficiency that code-generation eliminates and go over in more detail one of the queries in the TPCH workload where code generation improves overall query speeds by close to 3x. Why Code Generation? The baseline for “optimal” query engine performance is a native application that is written specifically for your data format, written only to support your query. For example, it would be ideal if a query engine could execute this query: select count(*)
from tbl
where col like %XYZ%
   as fast as grep -c "XYZ" tbl. As another example, consider the query select sum(col) from tbl. If the table had only one int64 column, encoded as little endian, this could be implemented by a dedicated application as: int64_t sum = 0;
int64_t* values = (int64_t*)buffer;
for (int i = 0; i &lt; num_rows; ++i) {
��� sum += values[i];
� }
   Both these queries are reasonable (as is the encoding on the second for a columnar format) and probably much slower when run through most existing query engines. (This is assuming brute force execution; a database could certainly use indices or save precomputed values to perform much better than a simple application. Of course, the techniques described here would be applied to non-brute force execution strategies as well.)�This is because most query engines in use today follow an interpretive approach that adds various forms of execution overhead. Overhead comes in the form of: Virtual function calls.�Without codegen, expressions (e.g. col1 + col2 &lt; col3) are interpreted, resulting in a virtual function call per expression. (This certainly depends on the implementation, but ours, and probably most others, would involve something akin to a virtual generic �Eval� function that each operator would implement.)�In this case, the expression itself is very cheap (a single add) but the virtual function call overhead is very high. Large switch statements for types, operators, functions that are not referenced by the query.�While the branch predictor can alleviate this problem, branch instructions still prevents effective instruction pipelining and instruction-level parallelism.� Inability to propagate all constants. Impala computes a fixed-width tuple format during planning (e.g. col3 is at byte offset 16). It would be beneficial if these constants could be folded into the code itself rather than having to do additional memory lookups. The goal of code generation is to have each query use the same number of instructions as custom-written code that implements exactly the logic of the query, with no overhead due to the query execution supporting broader functionality. Introduction to LLVM LLVM (Low-Level Virtual Machine) is a set of libraries that constitute the building blocks of a compiler (the clang compiler is built using these libraries).�The key components are: Abstract Syntax Tree (AST) ? Intermediate Representation (IR) generation IR optimization (i.e. compiler optimizations) IR ? machine code generation IR is an intermediate, typed assembly language that LLVM uses as the input and output for most of its internal components; it’s comparable to Java byte code.�LLVM also exposes higher-level code objects (e.g. Instruction object, Function object) that makes it easy to manipulate IR programmatically.�This includes operations like inlining a function call, removing instructions, removing a computation with a constant, and so on. Impala utilizes only (2) and (3) at runtime since we have our own version of the AST in the form of the query plan. LLVM gives us a better balance of low overhead, being able to leverage an optimizing compiler and having an actual API to generate the code. There are other techniques that can be used for code generation.�We believe LLVM is superior.�Other techniques include: Generating assembly on the fly and running that.�While this is very fast to do, writing assembly (as ASCII text) is error-prone and difficult, especially as the number of functions generated in assembly increases.�It also does not benefit from optimization passes that a compiler is able to do.� Generating C++ (as text) on the fly, compiling it (by exec-ing a compiler) and dynamically loading the resulting binary.�This has the benefit of being able to use an optimizing compiler and writing in a higher level language (although writing as text is not great) but the overhead of compiling can be prohibitive (multiple seconds).� Impala’s Use of IR After the SQL semantic analysis phase, we generate code for the �kernels� for the individual operators of the query: i.e. the inner loops of the code in which the query spends the vast majority of the cpu cycles.�At the time of code generation, we know all the types, tuple layouts, SQL operations and expressions that will be used by this query.�The result is a very tight inner loop with all function calls inlined and no extraneous instructions. We first need to get IR function objects for pieces of the code path.�LLVM provides two mechanism for generating IR. The first is to use LLVM�s IrBuilder (c++) API which lets us programmatically generate IR, instruction by instruction.�The second is to use the clang compiler to compile c++ source code into IR.� Impala uses both methods. At a high level, to perform code generation we: Generate IR using the IRBuilder where more efficient code can be generated with� additional runtime information. Load precompiled IR for functions that we need but don’t benefit from runtime information. Combine the IR from 1 and 2 by replacing called functions.�This allows us to inline across what would have been virtual function calls. Run the LLVM optimizer along with some of our custom optimizations. This is very similar to compiling your code with optimizations enabled and takes care of many things for you.�In addition to having less code, this step also does subexpression elimination, constant propagation, more function inlining, instruction reordering, dead code elimination, and other compiler optimization techniques. JIT-compile the optimized IR to machine code.�LLVM returns this as a function pointer that the query engine uses instead of the interpreted function. Example &amp; Results The most common question we get when talking about code generation is how much does this speed things up?�What’s the performance difference?� Here�s an example running the TPCH-Q1 query on a test cluster that comprises 10 data nodes each with 10 disks, 48GB RAM and 8 cores (16 hyper threads). The query is: select
� ��� l_returnflag,
� ��� l_linestatus,
sum(l_quantity),
� ��� sum(l_extendedprice),
� ��� sum(l_extendedprice * (1 - l_discount)),
� ��� sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)),
� ��� avg(l_quantity),
� ��� avg(l_extendedprice),
����� avg(l_discount),
����� count(1)
from
tpch.lineitem
where
 	l_shipdate&lt;='1998-09-02'
group by
 	l_returnflag,
 	l_linestatus   Details of how Impala executes queries will be provided in a future blog post.�As a quick overview, Impala processes batches of tuples through an operator tree.�In this case, there are two operators: a scan which reads the input from disk, and a hash aggregation, which computes the sum and averages.� We�ll focus on the aggregation step.�For hash aggregation, we iterate through the tuples in the batch, evaluate and hash the grouping columns (l_returnflags and l_linestatus), do a hash table lookup and then evaluate the aggregation expressions (the sums, avgs, and count in the select list).�For the aggregation operator, the code generation phase compiles all the logic for evaluating batches of rows into a single fully inlined loop. We�ll run this query on two differently sized datasets; the first is at the 1TB scale factor and the second is at the� 100GB scale factor.�The files are stored as sequence files with Snappy block compression.�For the 100GB dataset, the dataset is small enough to easily fit in the OS buffer cache of the cluster.�This prevents the disks from being a possible bottleneck. For both datasets, enabling codegen improves the query time by just under 3x.�The total code generation time for this query is about 150ms. (Code generation can be toggled on and off through the query options so you can try the same experiment.�To see the list of query options, just type ‘set’ in the impala shell.) To further quantify the benefit of codegen, we can compare some more detailed metrics. In this case, the query is run on a single machine on a much smaller dataset (700MB) using perf stat, which is a linux perf tool that lets you gather hardware metrics with minimal setup requirements. The results are the totals from running the query 5 times. Codegen on? Time Instructions Branches Branch Misses Miss % Yes .63s 52,605,701,380 9,050,446,359 145,461,106 1.607 No 1.7s 102,345,521,322 17,131,519,396 370,150,103 2.161 ������� As you can see, without codegen, we are running about twice as many instructions and over twice as many branch misses.�� Conclusion The time we’ve invested in code generation has already paid dividends and we expect the benefits to grow even bigger as we continue to improve the query engine.�With a columnar file format, more efficient encodings, and larger (memory) caches, we expect IO performance to improve dramatically, making CPU efficiency more and more important. Code generation is most beneficial for queries that execute simple expressions and the interpretation overhead is most pronounced.�For example, a query that is doing a regular expression match over each row is not going to benefit from code generation much because the interpretation overhead is low compared to the regex processing time.�We expect cases like this to be much less common and our initial users have confirmed this. There are still code paths in the current version of Impala (0.5) that are not yet code generated; we simply have not had the time to do that.�A lot of these code paths will be finished up for our upcoming GA release.�We have more ideas on how to better take advantage of code generation for after GA. Nong Li is a Software Engineer on the Impala team.</snippet></document><document id="288"><title>From Zero to Impala in Minutes</title><url>http://blog.cloudera.com/blog/2013/02/from-zero-to-impala-in-minutes/</url><snippet>This was post was originally published by U.C. Berkeley AMPLab developer (and former Clouderan) Matt Massie, on his personal blog. Matt has graciously permitted us to re-publish here for your convenience. Note: The post below is valid for Impala version 0.6 only and is not being maintained for subsequent releases. To deploy Impala 0.7 and later using a much easier (and also free) method,�use�this how-to. Cloudera Impala provides fast, interactive SQL queries directly on your Apache Hadoop data stored in HDFS or Apache HBase. This post will explain how to use Apache Whirr to bring up a Cloudera Impala multi-node cluster on EC2 in minutes. When the installation script finishes, you�ll be able to immediately query the sample data in Impala without any more setup needed. The script also sets up Impala for performance (e.g. enabling direct reads). Since Amazon�s Elastic Compute Cloud (Amazon EC2) is a resizable compute capacity, you can easily choose any size Impala cluster you want. In addition, your Impala cluster will be automatically setup with Ganglia: a lightweight and scalable metric-collection framework that provides a powerful web UI for analyzing trends in cluster and application performance. The installation scripts represent a day of work so I�m sure there are ways they can be improved. Please feel free to comment at the end of the post if you have any ideas (or issues). These scripts could also easily be used as a basis for a proper Whirr service if someone had the time. If you�re planning to deploy Impala in production, I highly recommend that you use Cloudera Manager. Installing Whirr If you haven�t already installed Apache Whirr, download and install using the following instructions. If you already have Whirr 0.8.1 installed, feel free to skip ahead. Note: I like to install things in /workspace on my machine but you can install Whirr anywhere you like of course. $ cd /workspace
$ wget http://www.apache.org/dist/whirr/stable/whirr-0.8.1.tar.gz
$ gunzip &lt; whirr-0.8.1.tar.gz | tar -xvf -
$ cd whirr-0.8.1
$ mkdir ~/.whirr
$ cp conf/credentials.sample ~/.whirr/credentials
   Add the following line to your .bashrc replacing /workspace with the path you installed Whirr into. "/workspace/whirr-0.8.1/bin:$PATH"
   Once you�ve edited your .bashrc, source it and check that whirr is in your path. $ . ~/.bashrc
$ whirr version
Apache Whirr 0.8.1
jclouds 1.5.1
   Edit your ~/.whirr/credential file (created above) to set EC2 (aws-ec2) as your cloud provider and add your AWS identity and credentials, e.g. PROVIDER=aws-ec2
IDENTITY=[Put your AWS Access Key ID here]
CREDENTIAL=[Put your AWS Secret Access Key here]
   For the last step, you need to create an SSH RSA (not DSA!) keypair. You�ll use this keypair whenever you launch a cluster on EC2 using Whirr (more on that soon). $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa_whirr
   Note that this keypair has nothing to do with your AWS keypair that is generated in the AWS Management Console or by running ec2-add-keypair. Preparing for Impala Installation You will need three files to install Impala: impalacluster.properties, installer.sh, and setup-impala.sh.� The impalacluster.properties file will be passed to Whirr as a recipe for creating your cluster. This cluster will be built to satify Impala requirements, e.g. CentOS 6.2, CDH, etc. The installer.sh script will use the information provided by Whirr about your cluster to scp and ssh the setup-impala.sh script to each machine and run it. The installer.sh will pass the address of the machine to house the Hive metadata store as well as a randomly generated password for the �hive� user. The setup-impala.sh script does the actual installation on each machine in your cluster. This script will completely configure Impala and Hive on your cluster for optimal performance. Once complete, you will immediately be able to query against Impala (and Hive). Let�s go through each of these files in detail. impalacluster.properties The Impala installation guide lists the following requirements, e.g. Red Hat Enterprise Linux (RHEL)/CentOS 6.2 (64-bit) CDH 4.2.0 or later Hive MySQL Sufficient memory to handle join operation The RightImage CentOS_6.2_x64 v5.8.8 EBS image (ami-51c3e614) will satisfy the CentOS 6.2 requirement and Whirr will do all the work to install CDH 4.2.x on your cluster. The installation scripts provided in this post will handle setting up Hive, MySQL and Impala. Here is the Impala-ready Apache Whirr recipe, impalacluster.properties, to use a starting point�for your deployment: #
# NOTE: EDIT THE FOLLOWING PROPERTIES TO MATCH YOUR ENVIRONMENT AND DESIRED CLUSTER
#

# The private key you created during the Whirr installation above (you'll need to change this path)
whirr.private-key-file=/Users/matt/.ssh/id_rsa_whirr
# The public key you created during the Whirr installation above (you'll need to change this path)
whirr.public-key-file=/Users/matt/.ssh/id_rsa_whirr.pub
# The size of EC2 instances to run (see http://aws.amazon.com/ec2/instance-types/). Keep in mind that some
# joins can require quite a bit of memory. We'll use the m2.xlarge (High-Memory Extra Large Instance) for extra memory.
# You can use any size instance you like here (except micro).
whirr.hardware-id=m2.xlarge
# You can modify the number of machines in the cluster here. The first machine type is the master and the second
# machine type are the workers.  To change you cluster size, change the number of workers in the cluster.
whirr.instance-templates=1 hadoop-namenode+hadoop-jobtracker+ganglia-metad,5 hadoop-datanode+hadoop-tasktracker+ganglia-monitor

#
# NOTE: DO NOT CHANGE THE PROPERTIES FROM HERE DOWN OR THE INSTALLER MAY BREAK
#

# This name will be used by Amazon to create the security group name. Using any string you like.
whirr.cluster-name=myimpalacluster
# Impala should not be run as root since root is not allowed to do direct reads
whirr.cluster-user=impala
# The RightImage CentOS 6.2 x64 image
whirr.image-id=us-west-1/ami-51c3e614
# The following two lines will cause Whirr to install CDH instead of Apache Hadoop
whirr.hadoop.install-function=install_cdh_hadoop
whirr.hadoop.configure-function=configure_cdh_hadoop
   The installer.sh will pass these properties to Whirr when you launch your cluster. You should edit the properties at the top of the file to match your environment and desired cluster characteristics (e.g. RSA key, cluster size and EC2 instance type). Do not edit the properties at the bottom of the file. Doing so, could break the installer. If you want to learn more about these whirr options, take a look at the Whirr Configuration Guide. There is also a recipes directory inside the Whirr distribution with example recipes. installer.sh The installer.sh file orchestrates the installation using the Whirr deployment information that is generated by the launch-cluster command. This information is found in the directory ~/.whirr/myimpalacluster. Here is the script: #!/bin/bash

# Please provide the path to the RSA private key you
# created as part of the Whirr installation
RSA_PRIVATE_KEY=$HOME/.ssh/id_rsa_whirr

# DO NOT MODIFY ANYTHING FROM HERE ON DOWN
CLUSTER_NAME=myimpalacluster
CLUSTER_USERNAME=impala
WHIRR_INSTANCES=$HOME/.whirr/$CLUSTER_NAME/instances

SETUP_IMPALA_SCRIPT=setup-impala.sh

# Generate a random password to secure the 'root' and 'hive' mysql users
RANDOM_PASSWORD=$(dd count=1 bs=16 if=/dev/urandom of=/dev/stdout 2&gt;/dev/null | base64)

# Use Whirr to bring up the CDH cluster
whirr launch-cluster --config impalacluster.properties

# Fetch the list of workers from the Whirr deployment
WORKER_NODES=$(egrep -v 'hadoop-namenode|hadoop-jobtracker|ganglia-metad' \
                    $WHIRR_INSTANCES | awk '{print $3}')

# Install the Hive metastore on the first worker node
# Hive box internal IP
HIVE_MYSQL_BOX_INTERNAL=$(head -1 $WHIRR_INSTANCES | awk '{print $4}')
# Hive box external IP
HIVE_MYSQL_BOX_EXTERNAL=$(head -1 $WHIRR_INSTANCES | awk '{print $3}')

# Copy the impala setup script to every machine in the cluster and run it
SSH_OPTS=" -i $RSA_PRIVATE_KEY -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no "
for WORKER_NODE in $WORKER_NODES
do
  scp $SSH_OPTS $SETUP_IMPALA_SCRIPT $CLUSTER_USERNAME@$WORKER_NODE:/tmp
        # Run the script in the background so the installation is in parallel
  ssh $SSH_OPTS $CLUSTER_USERNAME@$WORKER_NODE \
            sudo bash /tmp/$SETUP_IMPALA_SCRIPT $HIVE_MYSQL_BOX_INTERNAL $RANDOM_PASSWORD &gt; /tmp/impala-install.log 2&gt;&amp;1 &amp;
done

echo "Waiting for the installation scripts to finish on all the nodes. This will take about a minute per node in the cluster."
wait

echo "The password for your root and Hive account on the MySQL box is $RANDOM_PASSWORD"
echo "Please save this password somewhere safe."
   You will likely need to change the RSA_PRIVATE_KEY specified at the top of the script; otherwise, you should not need to modify anything else in this file. This script will generate a random password for the Hive metadatastore user, launch a cluster using the impalacluster.properties file, using the Whirr deployment to copy and run the setup-impala.sh script on every worker in the cluster. Note that, for performance, the installer runs the ssh calls in parallel and waits for them to complete. Using time ./installer.sh, I�ve found that it takes about a minutes/machine to bring up a cluster, e.g. an 11-node cluster (1 master, 10 workers) will take, e.g. real 11m41.172s
user  0m22.061s
sys   0m2.402s
   setup-impala.sh This is the setup-impala.sh script�that is run on each machine in your Impala cluster to install and configure Impala: #!/bin/bash

# Ip address of the box with the Hive metastore
HIVE_METASTORE_IP=$1
# Password to use for the hive user
HIVE_PASSWORD=$2

HADOOP_CONF_DIR=/etc/hadoop/conf
HIVE_CONF_DIR=/etc/hive/conf
IMPALA_CONF_DIR=/etc/impala/conf
IMPALA_REPO_FILE=http://beta.cloudera.com/impala/redhat/6/x86_64/impala/cloudera-impala.repo

function write_hive_site {
cat &gt; $1 &lt;&lt;HIVESITE
&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
  &lt;value&gt;jdbc:mysql://$HIVE_METASTORE_IP/metastore&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
  &lt;value&gt;hive&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
  &lt;value&gt;$HIVE_PASSWORD&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt;
  &lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;datanucleus.fixedDatastore&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
HIVESITE
}

# Some configuration only needs to be run the the box housing the hive metastore
/sbin/ifconfig -a | grep "addr:$HIVE_METASTORE_IP " &gt; /dev/null &amp;&amp; {

# Install all the necessary packages
yum install -y hive mysql mysql-server mysql-connector-java
# Start the mysql server
/etc/init.d/mysqld start
# Create the Hive metastore and hive user
/usr/bin/mysql -u root &lt;&lt;SQL
-- Create the metastore database
create DATABASE metastore;
-- Use the metastore database
use metastore;
-- Import the metastore schema from hive
SOURCE /usr/lib/hive/scripts/metastore/upgrade/mysql/hive-schema-0.10.0.mysql.sql;
-- Secure the root accounts with the hive password
update mysql.user set password = PASSWORD('$HIVE_PASSWORD') where user = 'root';
-- Create a user 'hive' with random password for localhost access
CREATE USER 'hive'@'localhost' IDENTIFIED BY '$HIVE_PASSWORD';
-- Grant privileges on the metastore to the 'hive' user on localhost
GRANT ALL PRIVILEGES ON metastore.* TO 'hive'@'localhost' WITH GRANT OPTION;
-- Create a user 'hive' with random password
CREATE USER 'hive'@'%' IDENTIFIED BY '$HIVE_PASSWORD';
-- Grant privileges on the metastore to the 'hive' user
GRANT ALL PRIVILEGES ON metastore.* TO 'hive'@'%' WITH GRANT OPTION;
-- Load the new privileges
FLUSH PRIVILEGES;
SQL
# Write the hive-site to the Hive configuration directory
write_hive_site $HIVE_CONF_DIR/hive-site.xml
# Link the mysql connector to hive lib
ln -s /usr/share/java/mysql-connector-java.jar /usr/lib/hive/lib
# Load up a really basic tab-delimited table into hive for testing end-to-end functionality
cat &gt; /tmp/numbers.txt &lt;&lt;TABLE
1 one
2 two
3 three
4 four
TABLE
sudo -E -u impala hadoop fs -mkdir /user/impala
sudo -E -u impala hadoop fs -put /tmp/numbers.txt /user/impala/numbers.txt
sudo -E -u impala hive -e "CREATE TABLE numbers (num INT, word STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;"
sudo -E -u impala hive -e "LOAD DATA INPATH '/user/impala/numbers.txt' into table numbers;"
} # /end hive metadata store specific commands

# Fetch the Cloudera yum repo file
(cd /etc/yum.repos.d/ &amp;&amp; wget -N $IMPALA_REPO_FILE)
# Install the impala and impala-shell packages
yum -y install impala impala-shell impala-server impala-state-store

# Create the impala configuration directory
mkdir -p $IMPALA_CONF_DIR
# Install the hive-site.xml into the Impala configuration directory
write_hive_site $IMPALA_CONF_DIR/hive-site.xml

# Copy the Hadoop core-site.xml into the Impala config directory
# Make sure to prepend the some properties for performance
CORE_SITE_XML=core-site.xml
cat &gt; $IMPALA_CONF_DIR/$CORE_SITE_XML &lt;&lt;'EOF'
 &lt;property&gt;
     &lt;name&gt;dfs.domain.socket.path &lt;/name&gt;
     &lt;value&gt;/var/lib/hadoop-hdfs/socket._PORT &lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
     &lt;name&gt;dfs.client.read.shortcircuit.skip.checksum &lt;/name&gt;
     &lt;value&gt;false &lt;/value&gt;
   &lt;/property&gt;EOF
grep -v "&lt;configuration&gt;" $HADOOP_CONF_DIR/$CORE_SITE_XML &gt;&gt; $IMPALA_CONF_DIR/$CORE_SITE_XML

# Update the hdfs-site.xml file
HDFS_SITE_XML=hdfs-site.xml
cat &gt; /tmp/$HDFS_SITE_XML &lt;&lt;'EOF'
&lt;property&gt;
    &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;
    &lt;value&gt;/var/lib/hadoop-hdfs/socket._PORT&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.datanode.hdfs-blocks-metadata.enabled&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
  &lt;/property&gt;
EOF
grep -v "&lt;configuration&gt;" $HADOOP_CONF_DIR/$HDFS_SITE_XML &gt;&gt; /tmp/$HDFS_SITE_XML
mv /tmp/$HDFS_SITE_XML $HADOOP_CONF_DIR/$HDFS_SITE_XML
# Copy the hdfs-site.xml file into the Impala config directory
cp $HADOOP_CONF_DIR/$HDFS_SITE_XML $IMPALA_CONF_DIR/$HDFS_SITE_XML
# Copy the log4j properties from Hadoop to Impala
cp $HADOOP_CONF_DIR/log4j.properties $IMPALA_CONF_DIR/log4j.properties

# Add Impala to the HDFS group
/usr/sbin/usermod -G hdfs impala

# Restart HDFS
/etc/init.d/hadoop-hdfs-datanode restart

# Start the impala services
# NOTE: It's important to run impala as a non-root user or performance will suffer (no direct reads)
sudo -E -u impala GVLOG_v=1 nohup /usr/bin/impalad \
-state_store_host=$HIVE_METASTORE_IP -nn=$NN_HOST -nn_port=$NN_PORT \
-ipaddress=$(host $HOSTNAME | awk '{print $4}') &lt; /dev/null &gt;
 /tmp/impalad.out 2&gt;&amp;1 &amp;
   You do not need to edit this file. This script is a bit long but I hope it�s easy to understand. This script is passed two arguments: the IP address of the Hive metadata store and the password to hive user. When run, this script will, e.g. Check if it�s running on the machine designated to be the Hive metadata store; if so, it will install and configure Hive and MySQL and drop in a very simple example table. Install the necessary Impala packages Configure impala for read.shortcircuit, skip.checksum, local-path-access.user and data locality tracking for performance Create an impala user Restart the datanode to pull in the modified configuration Start the statestored service Start impalad passing in the -state_store_host (all impalad use the state store running on the Hive metadata store machine), -nn (NameNode) and -nn_port (NameNode port) arguments Once you�ve modified your impalacluster.properties and installer.sh files, you�re ready to launch your Impala cluster. Launching your Impala cluster At this point, you should have a directory with your customized installation script and configuration file: $ ls
impalacluster.properties  installer.sh            setup-impala.sh
   To launch your cluster, simply run the installer.sh script. % time bash ./installer.sh
Running on provider aws-ec2 using identity ABCDEFGHIJKLMNOP
Bootstrapping cluster
Configuring template for bootstrap-hadoop-datanode_hadoop-tasktracker_ganglia-monitor
Configuring template for bootstrap-hadoop-namenode_hadoop-jobtracker_ganglia-metad
Starting 5 node(s) with roles [hadoop-datanode, hadoop-tasktracker, ganglia-monitor]
Starting 1 node(s) with roles [hadoop-namenode, hadoop-jobtracker, ganglia-metad]
...
   When installer.shcompletes, you should see the following messages, e.g. ...
Warning: Permanently added '50.18.85.89' (RSA) to the list of known hosts.
setup-impala.sh                                                                               100% 5675     5.5KB/s   00:00
Warning: Permanently added '54.241.114.18' (RSA) to the list of known hosts.
setup-impala.sh                                                                               100% 5675     5.5KB/s   00:00
Warning: Permanently added '184.169.189.144' (RSA) to the list of known hosts.
setup-impala.sh                                                                               100% 5675     5.5KB/s   00:00
Warning: Permanently added '50.18.132.190' (RSA) to the list of known hosts.
setup-impala.sh                                                                               100% 5675     5.5KB/s   00:00
Warning: Permanently added '184.169.237.144' (RSA) to the list of known hosts.
setup-impala.sh                                                                               100% 5675     5.5KB/s   00:00
Waiting for the installation scripts to finish on all the nodes. This will take about a minute per node in the cluster.
The password for your root and Hive account on the MySQL box is lBnn/HynCPcYNr/AUm5Hzg==
Please save this password somewhere safe.

real  6m42.620s
user  0m14.590s
sys   0m1.261s
   At this point, your Impala Cluster is up and ready for work. Using your Impala cluster You can find your deployment details in the file ~/.whirr/myimpalacluster/instances, e.g. $ cat ~/.whirr/myimpalacluster/instances
us-west-1/i-082ab151  hadoop-datanode,hadoop-tasktracker,ganglia-monitor  50.18.85.89 10.178.233.226
us-west-1/i-0a2ab153  hadoop-datanode,hadoop-tasktracker,ganglia-monitor  54.241.114.18   10.169.70.184
us-west-1/i-0c2ab155  hadoop-datanode,hadoop-tasktracker,ganglia-monitor  184.169.189.144 10.166.173.74
us-west-1/i-0e2ab157  hadoop-datanode,hadoop-tasktracker,ganglia-monitor  50.18.132.190   10.169.70.5
us-west-1/i-142ab14d  hadoop-datanode,hadoop-tasktracker,ganglia-monitor  184.169.237.144 10.166.250.208
us-west-1/i-162ab14f  hadoop-namenode,hadoop-jobtracker,ganglia-metad 54.241.85.84    10.166.123.235
   The columns are, in order, the EC2 instance id, the Whirr service template, the EC2 public IP of the machine and the EC2 private address of the machine. The Hive metadata store is always installed on the first machine in the list (that is not a master running the namenode, jobtracker, etc). To log into the Hive machine, use the public IP address of the first node: 50.18.85.89 in this example. $ ssh -i /Users/matt/.ssh/id_rsa_whirr -o "UserKnownHostsFile /dev/null" -o StrictHostKeyChecking=no impala@50.18.85.89
Warning: Permanently added '50.18.85.89' (RSA) to the list of known hosts.
Last login: Tue Nov 20 22:57:12 2012 from 136.152.39.187
-bash-4.1$
   Launch hive to ensure you can run queries, e.g. -bash-4.1$ hive
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/impala/hive_job_log_impala_201211202349_133443320.txt
hive&gt; show tables;
OK
numbers
Time taken: 2.883 seconds
hive&gt; select * from numbers;
OK
1 one
2 two
3 three
4 four
Time taken: 0.943 seconds
hive&gt; select word from numbers;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_201211210024_0003, Tracking URL = http://ec2-50-18-16-224.us-west-1.compute.amazonaws.com:50030/jobdetails.jsp?jobid=job_201211210024_0003
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=ec2-50-18-16-224.us-west-1.compute.amazonaws.com:8021 -kill job_201211210024_0003
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2012-11-21 00:29:11,916 Stage-1 map = 0%,  reduce = 0%
2012-11-21 00:29:15,940 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.73 sec
2012-11-21 00:29:16,949 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.73 sec
2012-11-21 00:29:17,961 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 0.73 sec
MapReduce Total cumulative CPU time: 730 msec
Ended Job = job_201211210024_0003
MapReduce Jobs Launched:
Job 0: Map: 1   Cumulative CPU: 0.73 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 730 msec
OK
one
two
three
four
Time taken: 10.687 seconds
hive&gt; quit;
   Now that you know Hive is running correctly, you can use Impala to query the same table. $ impala-shell
$ impala-shell
Welcome to the Impala shell. Press TAB twice to see a list of available commands.

Copyright (c) 2012 Cloudera, Inc. All rights reserved.

(Build version: Impala v0.1 (e50c5a0) built on Mon Nov 12 13:22:11 PST 2012)
[Not connected] &gt; connect localhost
[localhost:21000] &gt; show tables;
numbers
[localhost:21000] &gt; select * from numbers;
1 one
2 two
3 three
4 four
[localhost:21000] &gt; select word from numbers;
one
two
three
four
[localhost:21000] &gt;
   Destroying your Impala Cluster To destroy your Impala cluster, use the Whirr destroy-cluster command: $ whirr destroy-cluster --config impalacluster.properties
Running on provider aws-ec2 using identity ABCDEFGHIJKLMNOP
Finished running destroy phase scripts on all cluster instances
Destroying myimpalacluster cluster
   Taking a look at Ganglia For security, Whirr installs the Ganglia web interface to only be accessible by localhost, e.g.   #
  # Ganglia monitoring system php web frontend
  #

  Alias /ganglia /usr/share/ganglia

    Order deny,allow
    Deny from all
    Allow from 127.0.0.1
    Allow from ::1
    # Allow from .example.com
   In order to view ganglia, you will need to run the following script to create a secure SSH tunnel (in a separate terminal). #!/bin/sh
LOCAL_PORT=8080
CLUSTER_NAME=myimpalacluster
CLUSTER_USER=impala
GMETA_NODE=`grep ganglia-metad $HOME/.whirr/$CLUSTER_NAME/instances | awk '{print $3}'`
echo "Creating an SSH tunnel to $GMETA_NODE. Open your browser to localhost:$LOCAL_PORT. Ctrl+C to exit"
ssh -i $HOME/.ssh/id_rsa_whirr -o ConnectTimeout=10 -o ServerAliveInterval=60 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -L $LOCAL_PORT:localhost:80 -N $CLUSTER_USER@$GMETA_NODE
   This script to look at your Whirr deployment to find the ganglia-meta machine and start an SSH tunnel. To view your ganglia data, open your browser to http://localhost:8080/ganglia/ or use whatever port you set LOCAL_PORT to in the script. (An alternative is to use the Whirr SOCKS proxy � see the Whirr docs.) Ganglia tracks performance metrics for all your hosts and services. Keep in mind that it will take a few minutes for ganglia to distribute all metrics when it first starts. Initially, check that the Hosts up: number to make sure all the machines are reporting (meaning that ganglia heartbeats are getting through). That�s it I hope you find these bash scripts useful. Feel free to contact me using the comment box below. Matt Massie is the lead developer at the�UC Berkeley AMP Lab, and previously worked in the Cloudera engineering team. He founded the Ganglia project in 2000.</snippet></document><document id="289"><title>Cloudera is the Second "Sexiest Enterprise Startup" for 2012</title><url>http://blog.cloudera.com/blog/2013/02/cloudera-is-the-second-sexiest-enterprise-startup-for-2012/</url><snippet>Last Thursday, I had the pleasure�of attending the Crunchies�with Alan Saldich (our VP of Marketing) and Sarah Mustarde (our Senior Director of Corporate Marketing). The Crunchies is an awards event a la “the Oscars” but for startups.� Cloudera was nominated for the “Sexiest Enterprise Startup” award. This was the 6th Annual Crunchies but the first time that TechCrunch had a slot for enterprise software, as Aneel Bhusri emphasized during the intro for the enterprise award. Historically, the Crunchies focused on social and mobile startups, aka the non-sexy startups :) Our co-nominees for the award were a stellar bunch:�Asana, Box, Plexxi, and Zendesk. The Crunchie went to Box (congrats), and Cloudera was named runner-up. As many of you might know, Aneel represents one of our major investors,�Greylock Partners, and sits on�Cloudera’s Board of Directors. He was tasked with announcing the runner-up and winner, and his reaction when he opened the envelope was simply priceless. You can watch Aneel’s reaction here: The highlight of the event for me was the host:�John Oliver from The Daily Show. John totally knew how to appease a nerdy crowd; he was dead-on. In particular, he paid homage to ancient Egyptian civilization, which�I (being an�Egyptian-American)�found quite entertaining. My eyes gushed with tears of laughter upon seeing his reaction when one of the audience participants replied to his question about “What is the first thing that comes to your mind when thinking of Egyptian civilization?” by saying “mummies” (he expected “pyramids”). You can watch that funny clip here: Obviously, inside my head, as the nominees for the previous categories were giving their acceptance speeches, I was thinking what I would say. I will say those words here anyway :) “Hurr-rrah, it is great to be recognized for creating not just a successful startup, but a whole new industry. To Jeff, Mike, and all the Clouderans, it is official ….. you are now sexy :). To all the new Big Data startups getting insane valuations because of the wave we created, I would like to say ….. You’re welcome! Finally, not to be outdone by Airbnb’s acceptance speech, I would like to thank not one but two men upstairs [looks up to the ceiling a la Brian Chesky's speech]. These two men were very instrumental in bringing the Cloudera founding team together, and frankly we wouldn’t be here without them….. Andrew Braccia and Ping Li,�you’re too officially sexy :)” – amr</snippet></document><document id="290"><title>How Syncsort Leverages Training to Optimize Hadoop Scalability</title><url>http://blog.cloudera.com/blog/2013/02/how-syncsort-leverages-training-to-optimize-hadoop-scalability/</url><snippet>This guest post is provided by Dave Nahmias, Pre-Sales and Partner Solutions Engineer at Syncsort, with an introduction by Patty Crowell, Director of Global Education Services at Syncsort. Introduction: Training is Key Apache Hadoop is extremely important to maximizing the value Syncsort�s technology delivers to our customers. That value promise starts with a solid foundation of knowledge and skills among key technical staff across the company. We chose Cloudera University�s private training option to ensure Syncsort�s cross-functional team of engineering, support, services, and technical sales professionals had the expertise to optimize our data products for the end-user. Because the members of our team had different levels of prior Hadoop experience, the private class enabled us to freely share information and ask tough questions, resulting in a high level of engagement throughout the course. Everyone benefited tremendously from the experience and attention of Cloudera�s instructor, since the private training was tailored to our particular needs and was set up to promote collaboration during the entire learning process. Lessons ranged from core Hadoop knowledge for the less experienced members of the team all the way up to tight focus on specific roles and skills for those who had been using Hadoop all along, all supported by relevant and challenging labs. Moreover, the opportunity to host the training at our own location according to our own schedule made Cloudera training a highly convenient solution. Read on for the technical insights gained by a Syncsort engineer. Why I Attended Cloudera�s Training I am the technical interface to Syncsort�s partners. I have been working with large data environments for the past 20 years and specifically with Hadoop for the last year-and-a-half. As the first line of defense for a software vendor in the Hadoop marketplace, it is critical that I understand all the inner-workings of the technology. Armed with the proper level of product knowledge, I can give partners and customers a guided tour through the menagerie of animal names associated with the Hadoop ecosystem, helping them to better understand the solution components and to build successful applications. Ultimately, it helps me evangelize the benefits of Big Data solutions. Attending Cloudera�s Developer Training�was a no-brainer, since Cloudera is well recognized in the industry as a primary source of Hadoop knowledge and insight. What Syncsort Brings to the Hadoop Party Syncsort has been a thought leader in high-efficiency sort solutions on mainframes and open systems for more than 30 years. Syncsort�s unique product architecture allowed us to contribute changes to the core Hadoop code, which allows any sort algorithm (including Syncsort�s) to be called instead of Hadoop�s native sort. This interface also provides a vehicle for Syncsort to integrate our native ETL engine directly into the MapReduce architecture, bringing the graphical development environment, efficiencies, and throughput of native execution across the mainframe, open systems, and Hadoop environments. We extend Hadoop�s flexibility and ease of use, essentially leveling the playing field for any of these environments. What I Learned I had acquired most of my prior Hadoop knowledge by reading and experimenting.�Having a knowledgeable instructor validate and fine-tune my understanding at a deeper level as well as articulate real-world use cases from his personal experience was incredibly valuable.�Although there are plenty of blogs and articles attempting to build the Hadoop knowledge base, I have found they tend to misrepresent how Hadoop actually works, so the full Cloudera training course�proved to be both the fastest and the most reliable way to clarify cloudy areas, get first-hand experience, and transition from conceptualization to development with Hadoop. The details about how Hadoop actually reads and writes data were particularly interesting to me. I knew Hadoop stored multiple replicas of data blocks, but I didn�t previously have any in-depth knowledge of the exact algorithms used or how they relate to scheduling and data integrity with respect to the checksums maintained. It�s also very helpful to understand the difference in scheduling techniques. The course also clarified how the map and reduce tasks are scheduled across nodes to minimize network traffic. Additionally, in my day-to-day responsibilities, I can leverage what I learned about the way Hadoop clients are configured, the interrelationship of the XML configuration files, and how those files are organized to define the architecture across the cluster.�The lessons about how the schedulers work and the impact they might have on various workloads, such as benchmarking, will also be of immediate value. Finally, I will derive immediate benefit from the lessons on the Hadoop client architecture, which gave me a better understanding of how Flume and Sqoop work. What the Syncsort Team Learned My colleagues agree the training enabled us to be much more productive when working with Hadoop and to better visualize the possibilities created by Hadoop. Specifically, it helped us understand how Syncsort might leverage the Hadoop architecture in conjunction with our DMExpress software�to help our customers extend their usage of Hadoop. It will also help us position Syncsort to contribute to accelerating Hadoop acceptance by providing well-architected Big Data ETL and intra-platform data movement solutions. Even at a fraction of those expected outcomes we will have realized measurable return on investment from Cloudera training. From a professional standpoint, the knowledge we gained also prepares us for Cloudera Certification, a necessity for anyone who needs to provide tangible evidence of Hadoop expertise in the field.�Last, but certainly not least, we believe the training improved our collective ability to intelligently communicate with our partners and customers about Hadoop�s functionality, a skill that should never be underestimated. Employing the Expanded Skill Set Data in mainframes and open systems will continue to be important, and integrating that data together with data stored in HDFS will be a challenge for our customers that I believe Syncsort is now well-positioned to address. Thanks to Cloudera training, I can use my increased Hadoop knowledge to provide even better service to my partners and customers and provide more value with a technology that will grow increasingly important to my company�s business.</snippet></document><document id="291"><title>Ph.D. Interns at Cloudera: Bringing Big Data Back to School</title><url>http://blog.cloudera.com/blog/2013/02/ph-d-interns-at-cloudera-bringing-big-data-back-to-school/</url><snippet>The following is a series of stories from people who in the recent past worked as Engineering Interns at Cloudera. These experiences concretely illustrate how collaboration between commercial companies like Cloudera and academia, such as in the form of these internships, helps promote big data research at universities.�(These experiences were previously published in the ACM student journal, XRDS.) Yanpei Chen (Intern 2011) I Interned with Cloudera during my last summer of grad school. My dissertation was on �Workload Driven Design and Evaluation of Large-Scale Data-Centric Systems�, and I already had collaborations with Facebook and NetApp, two other big data companies. The goal of my work was to develop and demonstrate a set of empirical, workload-driven design and evaluation methods that complemented the traditional, subjective approach of designing by intuition and experience. It was very important that these methods generalized across many types of customer workloads. Hence, when Cloudera offered me an internship, I leapt at the unique opportunity to collect insights from customers in traditional industries who were still dealing with big data. My internship project was to collect and analyze CDH traces from Cloudera customers. Cloudera�s cutting edge knowledge allowed it to realize that several MapReduce �benchmarks�, popular even now, were not at all representative of real-world use cases. As Cloudera�s customers increased their MapReduce expertise, they voiced similar concerns. Thus, the lack of empirical, real-life cluster traces created huge barriers for quality assurance and performance testing of the core CDH product, and technology certification for Cloudera�s partner vendors. Furthermore, Cloudera�s customers and prospects in non-technology industries were beginning to lament the limited attention paid to their use cases. Thus, empirical insights of real-life use cases also assisted Cloudera�s customer support and marketing efforts. The actual process of collecting customer cluster traces proved necessarily difficult. Customers were rightly concerned about leaking proprietary information. I was fortunate to have members of the support, marketing, and partner-relations teams helping me initiate and moderate my discussions with customers. Cloudera�s internal infrastructure teams helped set up special file transfers to comply with our customers� firewall policies. Cloudera executives also occasionally stepped in to offer encouragement and support. In the end, we collected an unprecedented set of real-world MapReduce cluster traces from both technology and traditional enterprises. Insights from this data set has led to key publications of my dissertation, while helping Cloudera�s ongoing efforts in quality assurance, performance testing, technology certification, customer support, and marketing. The internship was truly a collaborative, multi-disciplinary experience. It also led to a full-time job offer, which I accepted after finishing my dissertation. Andrew Wang (Intern 2012) As a part of the AMPLab at Berkeley, much of my research revolves around big data and the components of the Hadoop software ecosystem. More specifically, I�m interested in providing high-level service-level objectives (SLOs) for distributed storage systems. Working at Cloudera has been an eye-opening experience in three major ways. First, my internship has provided incredible perspective on how storage systems like HDFS and Apache HBase are used in practice. Getting to talk directly with customers and developers has helped me refine my understanding of the problems faced in practice, sometimes in surprising ways. This has influenced my research agenda in terms of both problem selection and approach. Second, my internship solidified in my mind the importance of academic systems research. The continual stream of support tickets and ship dates in industry can preclude full examination of a design space. Researchers have the luxury of a more measured approach to problem solving which focuses on examining fundamental tradeoffs, methodology, and quantifying differences with other solutions. Part of what impresses me about Cloudera is how closely they watch the output of academic research conferences. If a paper thoroughly solves a real-world problem, it�s likely to be quickly applied to actual code used in production. Third, Cloudera is a great place to learn and practice open-source software development. Too often, research code is left to succumb to �bit rot� after the associated paper is published. Open-source is an opportunity for researchers to have additional impact and is also a way of further publicizing your research. Overall, I strongly enjoyed my experience at Cloudera. I was able to disseminate my research ideas within the open-source community, as well as work on directly applying them to a product that will ultimately be used by hundreds of companies. [Ed: Andrew�s internship also led to a full-time offer, which he accepted.] Brian Martin (Intern 2012) At UMass, I am a student in machine learning and natural language processing; specifically, I research parallel inference and learning in graphical models for large-scale information extraction. Cloudera is not only about systems design. In my first month, working directly with the Director of Data Science Josh Wills, I have developed several new statistics and machine-learning tools for advanced analytics on Hadoop. First, I wrote a tool for calculating distance correlation over giant tables of data (e.g. all Chicago crimes and building permits in the last decade grouped by location, or all purchase histories grouped by various demographic variables). Distance correlation is a recent statistical measure of dependence, linear and nonlinear, between variables. This tool will soon be available open-source and as a Cloudera product. Second, I implemented a recently proposed solver for very large-scale linear regression problems using the new Hadoop feature, YARN.� YARN allows for running non-MapReduce applications on a Hadoop cluster, while playing well with the resource manager. The biggest advantage of doing these projects at Cloudera was the insight into customer needs. Coming from academia, it is difficult to know what companies are doing behind closed doors. With so many customers and so much experience, Cloudera has provided me with a more complete picture of the diversity of industry�s data and needs.� In academia it is all too easy to pursue a novel or cute idea over one with more tangible benefit. Another advantage of doing this work at Cloudera is the amount of input I was able to have on the tools I was using. For example, I used Apache Crunch, a library for composing pipelines of MapReduce jobs, which was written by my manager. This makes for a very productive loop.� That is, while the project is open-source, being able to meet with the lead developer in person reduces a lot of the usual friction of submitting to open-source. Whenever I was missing a feature or found a bug, it was very easy to write up the fix and have it integrated quickly. Andrew Ferguson (Intern 2012) As a student at Brown University, I work on software defined networks (SDNs) and platforms for Big Data processing, such as Hadoop and Microsoft�s Dryad/Cosmos. In both these areas, the core technologies were developed in academic labs and Internet-based companies, and are now reaching new markets via more traditional companies. As technologies are adopted by new users, new use cases and new problems arise — opening fresh avenues for systems research. This exposure to new types of Hadoop customers drew me to Cloudera for the Summer of 2012. My internship at Cloudera also allows me to study a young company that transformed a small project into a mature product, and is using it to disrupt a large and established market for data processing. While the company may have started with a few engineers and sales staff, it now employs teams dedicated to all phases of a product�s lifecycle, from training, marketing, sales, and installation, to support and development, all within an organization still small enough for an intern to get to know. As the development of software-defined networks is still several years behind that of Big Data platforms, my summer at Cloudera lets me look into the future of SDN companies. Finally, I would encourage any Ph.D. student, and particularly those in systems research, to consider spending a summer at a start-up or other small company, even if they are set on joining the academy after graduation. Numerous faculty members start companies during their careers, as it can be an effective way to change the world through research. And even for those who don�t start companies, the experience will help when advising future students on career options and selecting their own internships. As graduate students, we often have the twin luxuries of unstructured time and an ease of moving, so pick a city and an interesting company, and explore a new side of your life and research! Patrick Wendell (Intern 2012) At U.C. Berkeley, I work on resource management for large-scale data processing systems. My summer work at Cloudera, however, was off the beaten path for most academic engineers: I spent spending three months travelling out in the field with Cloudera�s engineers and working directly with customers as they assess, prototype, and deploy Hadoop in live environments. This experience put me right where the �rubber meets the road� in large-scale data management, and led to several insights about the problems faced day-to-day in big data deployments. It also provided perspective on which types of engineering solutions are most successful in the wild, which I will take this back with me as I continue my research degree. The most salient lesson I took from the summer is that when companies are evaluating a new technology, performance with respect to alternative solutions is but one of many criteria considered. Factors like deployment complexity, interoperability with existing systems, cost, fitness for a particular business problem, and overall user-friendliness combine to influence adoption of new technologies. This is even ignoring �human� elements like trust in particular brands, history of prior relationships, and internal company politics. Interoperability, simplicity, and ease-of-use are rarely stated goals in systems research projects — and indeed, these are partially the responsibility of �productizing� engineers at companies like Cloudera — but they should be considered first-class for any researchers who want to have impact. As de-facto standards arise around storage and processing of big data, the responsibility falls on researchers to inter-operate new technology with existing solutions, or at least propose a path towards integration or evolution in the long term. Simplicity also reigns supreme: given the unavoidable complexity of administering and deploying distributed systems, users will always opt for a simpler, more stable design at the cost of some performance. Finally, ease-of-use remains a major pain point for state-of-the-art big data solutions such as Hadoop. At a minimum, Hadoop�s processing abstraction, MapReduce, is too low-level for most technology consumers. Higher-level abstractions and languages exist, but these are still nascent and don�t sufficiently take advantage of obvious performance optimizations available lower in the stack. Finding the right abstractions for data processing at scale remains an open problem, and also one that I plan to directly focus on this coming year. The question then, is how to simultaneously meet the canonical requirements for great research (innovative and groundbreaking progress) with the more mundane requirements for viable technology solutions (compatibility and simplicity). A key challenge for any great systems researcher is to walk this line adeptly! These experiences illustrate how Ph.D. students benefit from understanding real-life big data problems, and accessing a broad spectrum of industrial engineers, partners, and customers. Conversely, Cloudera benefits from the industry-academia cross-pollination of ideas, and the methodical approach to problem solving brought by the Ph.D. interns. Here is a list of current internship and full-time job openings at Cloudera.</snippet></document><document id="292"><title>A Ruby Client for Impala</title><url>http://blog.cloudera.com/blog/2013/02/a-ruby-client-for-impala/</url><snippet>Thanks to Stripe’s Colin Marc (@colinmarc) for the guest post below, and for his work on the world’s first Ruby client for Cloudera Impala! Like most other companies, at Stripe it has become increasingly hard to answer the big and interesting questions as datasets get bigger. This is pretty insidious: the set of potential interesting questions also grows as you acquire more data. Answering questions like, “Which regions have the most developers per capita?” or “How do different countries compare in how they spend online?” might involve hours of scripting, waiting, and generally lots of lost developer time. Up to now, the answer has often been Apache Hive, which at least made it easy to express many of these queries. Unfortunately, Hive queries are typically very slow. Cloudera Impala provides a similar front-end while being orders of magnitude faster, and we’ve found it immensely useful in many different situations at Stripe. With the near real-time results, the notion of performing programmatic (and not just ad-hoc) queries has now become more attractive. Programmatic Access with Ruby We have a pretty hefty set of administrative and analytical tools and dashboards. Because most of Stripe is written in Ruby, we’ve had no way to integrate Impala into those tools, or even write basic scripts that make use of our Impala cluster. To address that, I’ve spent some time over the last few weeks developing a Ruby client for Impala. It’s now available as an open-source gem, with documentation on rubydoc. It’s also available on GitHub. Using the gem To install the gem, run 'gem install impala'. Here’s what it looks like to run a query (all the examples use the sample data that Cloudera provides with its Impala demo VM): require 'rubygems'
require 'impala' 

Impala.connect('hostname', 21000) do |conn|
  conn.query('SELECT zip, income FROM zipcode_incomes ORDER BY income DESC LIMIT 5')
end
   which returns an array of hashes: [
  {:zip=&gt;"10514", :income=&gt;189570},
  {:zip=&gt;"98243", :income=&gt;188363},
  {:zip=&gt;"10577", :income=&gt;187019},
  {:zip=&gt;"11568", :income=&gt;184298},
  {:zip=&gt;"94028", :income=&gt;181337}
]
   If your result is too large to fit into memory, you can also use cursors: conn = Impala.connect('hostname', 21000)
cursor = conn.execute('SELECT zip, income FROM zipcode_incomes')
cursor.each do |row|
  # do something with the row here
end
   The idea is to make Impala just as simple to use as any other SQL-based datastore like MySQL or PostgreSQL. What’s Next? I look forward to seeing more tools and frameworks built on top of Impala. By way of example, I whipped up a sample project that provides a web interface for running queries and saving results for later inspection. It took about two hours to build with Sinatra and Bootstrap. The source, along with instructions for getting it running, are on GitHub. If you build something cool with impala-ruby, or if you have any feedback or questions, I’d love to hear from you! Colin Marc is a developer at Stripe.</snippet></document><document id="293"><title>Understanding MapReduce via Boggle, Part 2: Performance Optimization</title><url>http://blog.cloudera.com/blog/2013/01/understanding-mapreduce-via-boggle-part-2-performance-optimization/</url><snippet>In Part 1 of this series, you learned about MapReduce’s ability to process graphs via the example of Boggle*.�The project’s full source code can be found on my GitHub account. The example comprised a 4×4 matrix of letters, which doesn’t come close to the number of relationships in a large graph.�To calculate the number of possible combinations, we turned off the Bloom Filter with “-D bloom=false“.�This enters a brute-force mode where all possible combinations in the graph are traversed. In a 4×4 or 16-letter matrix, there are 11,686,456 combinations, and a 5×5 or 25-letter matrix has 9,810,468,798 combinations. As previously discussed, increasing matrix sizes is an important part of scaling up.�We also want to effectively use the cluster when processing the graph.�In this post, I’ll describe some of the performance optimizations I used to improve performance and scalability. Although Boggle is sold in only two versions, with 4×4 and 5×5 (�Big Boggle�) matrices, in our code you can create arbitrarily-sized matrices with the rollversion parameter. For example, passing in "-D rollversion=10" to the driver will create a random 10×10 matrix with the dice randomly chosen from the Big Boggle dice. Bloom Filter A Bloom Filter is a memory-constrained hash function.�Instead of taking up several MB of RAM, the Bloom Filter can be constrained to whatever size or memory limits the programmers wants to use.�A Bloom Filter is used for quick membership tests.�This allows a program to quickly answer the question: “Could this object possibly be a member or part of this dataset?” For the Boggle code, I use the Bloom Filter to avoid or filter out possible dead ends. Let’s take another look at the Boggle roll. We humans are very good at throwing out dead ends.�In this roll, for example, we wouldn’t spend much time on the “Z” at [2, 3].�Why not?�Because there aren’t as many letters in the English language that start with or have a “Z” in them.�Our time is better spent looking at other letters like “T” or “R”, which occur much more often in English. To weed out these dead ends, we could use a complex statistical model based on letter occurrence or we could use a simple Bloom Filter.�Since we know all the words in the English language, we can pass them through a Bloom Filter and it can tell whether the current node is a dead end. Consider this code from the program that creates the Bloom file. Note that for brevity, all of these code snippets will have portions taken out.�Please consult the code on GitHub for the unedited code. while ((line = dict.readLine()) != null) {
���� Matcher matcher = words.matcher(line);

���� if (matcher.matches()) {
��������� for (int i = 0; i &lt; line.length(); i++) {
�������������� String wordPiece = line.substring(0, i + 1);
�������������� bloomFilter.add(new Key(wordPiece.getBytes()));
��������� }
���� }
}
   This code goes through the dictionary file containing all words in the English language.�The word is then broken down into gradually bigger pieces with each iteration containing one more character of the word until the entire word is added.�This iteration needs to happen because this is how the graph is traversed.�A word is found one iteration at a time in the mapper. Here is the relevant code from the mapper.   if (bloomFilter.membershipTest(new Key(newWord.getBytes()))) {
  ���� context.write(new Text(newWord), nextGraphWritable);
  }
   The newly iterated word is passed through the Bloom Filter to see if the word might exist.� Notice that I said “might”.�That’s one of the compromises you make when using a Bloom Filter: you may get false positives, although you will not get false negatives.�In other words, if the Bloom Filter says “no,” it really means no; if it says “yes,” that’s more like a “maybe.” Now that we’ve learned a little about the Bloom Filter and its use in the Boggle algorithm, let’s talk about the benefits.�Here is some output from the Boggle program: 12/12/29 19:24:02 INFO Boggle: Traversed graph for 9 iterations.�Found 2943 potential words.�Bloom prevented 9835 traversals so far.
12/12/29 19:24:02 INFO Boggle: Finished traversing graph after 9 iterations. Found 2943 total words.�Bloom prevented 9835 traversals.
   This speaks to the savings in graph processing created by the Bloom Filter.�By throwing out dead ends early on and quickly, we saved a lot of processing time.�Looking at our Boggle roll, the Bloom filter allowed us to throw out words like “ZR”, “ZT” and “ZF”.�This saved us a significant amount of processing because the permutations have factorial growth.�Throwing out the word early means it and all of its grandchildren, great-grandchildren, great-great-grandchildren, and all descendants are never even processed.�Although this example says the Bloom saved 9,835 nodes, it saved significantly more when you count all descendants. To illustrate this, look at another set of outputs from two Boggle program runs on a 5×5 matrix with a three-node cluster: -- Unoptimized Bloom
13/01/04 13:37:36 INFO Boggle: Starting graph traversal.
...
13/01/04 15:04:07 INFO Boggle: Finished traversing graph after 15 iterations.�Found 9733548413 potential words.�Bloom prevented 48358169 traversals.
13/01/04 15:25:01 INFO Boggle: Finished traversing graph after 15 iterations.�Found 1134 total words.

-- Optimized Bloom
13/01/04 13:34:55 INFO Boggle: Starting graph traversal.
...
13/01/04 13:36:54 INFO Boggle: Finished traversing graph after 10 iterations.�Found 4376 potential words.�Bloom prevented 15059 traversals.
13/01/04 13:37:19 INFO Boggle: Finished traversing graph after 10 iterations.�Found 1672 total words.
   There are two main data items we see in the outputs.�The optimized Bloom took a lot less time to run and had a much lower false positive rate; the unoptimized Bloom took 1 hour, 26 minutes, and 31 seconds and the optimized Bloom took 3 minutes, 24 seconds.�The unoptimized Bloom took so much longer because that run pursued many more dead ends and couldn’t throw them out as early.�By comparison, the unoptimized Bloom run found 9,733,548,413 potential words and the optimized Bloom run found 4,376 potential words.�To put it another way, the unoptimized Bloom traversed 99.2% of all possible combinations and the optimized Bloom only did 0.00004%.�There are nowhere near 9.7 billion words in the roll and the vast majority of those words represented false positives (9,810,468,798). Although your use case may not use Bloom Filters, the lesson still applies.�Try to throw out data in your mapper as early as possible, especially when you’re dealing with iterative MapReduce algorithms. Compression Depending on your data, adding compression can improve your job speed and not just size on disk.�For the Boggle data, adding compression did both. Consider this code from the driver. FileOutputFormat.setOutputCompressorClass(job, SnappyCodec.class);
SequenceFileOutputFormat.setOutputCompressionType(job, CompressionType.BLOCK);
   With some relatively simple code, I added Snappy compression.�The second line sets this compression to run at block level.�The size of a snappy block will vary depending on the version of CDH but the default is 32K or 256K.�You can set this parameter with io.compression.codec.snappy.buffersize. If you remember, the mapper which traverses the graph is pretty chatty.�It will output every previous or final node as well as all adjacent nodes. It also does this by iterating over the graph until it is completely done.�The previous mapper’s output is fed into the next mapper’s input. By compressing the input on both sides, we save disk space and improve the speed.�In this case, the block compresses down to a smaller size while retaining all information.�On I/O-bound jobs with data that compresses well, this can overcome some of the speed issues related to hardware.�In this case, it’s faster to compress, write to disk, and decompress again during the next iteration than not compressing and simply writing to disk. Sequence Files Sequence files are flat files with binary keys and values that allow automatic serialization to the mapper and reducer. Before using sequence files, I was serializing the data out to plain text.�This caused extra time and space to be wasted doing my own serialization and deserialization when it’s built into Hadoop. It helped clean up some of my code, too.�You can look at the diff to see that it removed a lot of the parsing and error handling from the code. Using sequence files is easy. Here is the setup from the driver: job.setInputFormatClass(SequenceFileInputFormat.class);
job.setOutputFormatClass(SequenceFileOutputFormat.class);
   This section sets the graph traversal iterations to use sequence files as the input and output.�In order to prime the pump or write out the adjacency matrix of the initial Boggle roll, we use this code: SequenceFile.Writer writer = null;
for (int i = 0; i &lt; roll.rollCharacters.length; i++) {
���� writer = SequenceFile.createWriter(fileSystem, configuration, new Path(parent, i + ".txt"),
�������������� Text.class, RollGraphWritable.class);
����
���� for (int j = 0; j &lt; roll.rollCharacters[i].length; j++) {
��������� ArrayList&lt;Node&gt; nodes = new ArrayList&lt;Node&gt;();
��������� nodes.add(new Node(i, j));

��������� RollGraphWritable graphWritable = new RollGraphWritable(nodes, false);

��������� Text text = new Text(roll.rollCharacters[i][j]);

��������� // Mimic the adjacency matrix written by the mapper to start things off
��������� writer.append(text, graphWritable);
���� }
}
   This code creates a file per row of the matrix.�By creating a row per file, the cluster can distribute the load across multiple mappers. The final MapReduce job that finds the actual words needs to be changed, too. job.setInputFormatClass(SequenceFileInputFormat.class);
   The job is configured to use sequence files as the input.�This input comes from the final graph traversal mapper whose output is in sequence file format.�Note that the output file format is not set.�We want the output format to be a plain text representation of the nodes. I hope you enjoyed learning a little about Graph Theory using MapReduce.�While your project may not be using graphs, some of the performance tricks might benefit your MapReduce programs.�Most of these tricks require little extra coding and could give your program that added boost. Jesse Anderson is a Curriculum Developer and Instructor for Cloudera University. * Boggle is a trademark of Hasbro.</snippet></document><document id="294"><title>Webinar: Introduction to Hadoop Developer Training (Jan. 31)</title><url>http://blog.cloudera.com/blog/2013/01/webinar-introduction-to-hadoop-developer-training-jan-31/</url><snippet>Are you new to Apache Hadoop and need to start processing data fast and effectively? Have you been playing with CDH and are ready to move on to development supporting a technical or business use case? Are you prepared to unlock the full potential of all your data by building and deploying powerful Hadoop-based applications? If you’re wondering whether Cloudera’s Developer Training for Apache Hadoop is right for you and your team, sign up for this webinar now! In this live session on Thurs., Jan. 31, at 11am PT, you will learn who is best suited to attend the full training, what prior knowledge you should have, and what topics the course covers. Cloudera Curriculum Manager�Ian Wrigley�will discuss the skills you will attain during the course and how they will help you become a full-fledged Hadoop application developer. During the webinar, Ian will also present a short portion of the actual Cloudera Developer course, discussing the difference between new and old APIs, why there are different APIs, and which you should use when writing your MapReduce code. Following the presentation, Ian will answer your questions about this or any of Cloudera�s other training courses. Register today!</snippet></document><document id="295"><title>Cloudera at Strata Santa Clara 2013</title><url>http://blog.cloudera.com/blog/2013/01/cloudera-at-strata-santa-clara-2013/</url><snippet>Join us February 26 � 28 at Strata Santa Clara and learn how Cloudera has developed the de facto Platform for Big Data. Visit Cloudera in booth 701 to hear from our team in a series of presentations and partner-integrated demonstrations � agenda coming soon. We will also be hosting several celebrated authors of the Big Data community who will be available to sign copies of their published works and converse with you about the Big Data environment and specific projects within the Big Data space. Not only will our booth be packed with a full speaker and demonstration line-up and several authors, you will also have the opportunity to meet Doug Cutting. Doug helped found the Hadoop project and coined the project �Hadoop� after his son�s stuffed elephant. Within the Strata Santa Clara event agenda look for Cloudera speakers presenting a number of topics: Tuesday, February 26th Using HBase effectively – What You Need to Know as an Application Developer Ballroom CD @ 9:00am PT Cloudera Solution Architect and Co-Author of HBase In Action Amandeep Khurana will share his HBase expertise at building effective HBase applications with attendees. Bring your laptop to this session and build a fully functional HBase application by following instruction from Amandeep. Introduction to Apache Hadoop Ballroom AB @ 1:30pm PT Cloudera Director of Educational Services Sarah Sproehnle hosts this tutorial which provides a solid foundation for those seeking to understand large scale data processing with MapReduce and Hadoop, plus its associated ecosystem. If you are new to Hadoop, you definitely want to attend this session. Wednesday, February 27th Tricks for Distributed System Debugging and Diagnosis Ballroom CD @ 4:50pm PT Philip Zeyliger has been around Hadoop since the beginning and as one of Cloudera�s Senior Engineers he has seen it all. Learn from Philip as he shares tricks and tips for diagnosing and solving many of the tricky issues that come about with distributed systems. Thursday, February 28th Impala: A Modern SQL Engine for Hadoop Great America Ballroom J @ 2:20pm PT Cloudera Senior Product Manager, Justin Erickson, is the Cloudera product lead for Impala � a modern SQL engine for Hadoop. The Impala project has introduced the availability of SQL-like real-time queries in your Hadoop system. Attend this session to learn how to turn your Hadoop cluster into a fast and efficient machine while using the familiar SQL query language. Building Recommendation Platforms with Hadoop Ballroom CD @ 4:00pm PT Cloudera Engineer Jayant Shekhar shares Cloudera�s experience with building recommendation systems. If you are interested in using Hadoop to create smarter, more efficient processes for your business attend this session. Jayant will explain a bit of the science behind the art of building recommendation system that is right for your business.� Register Register for Strata Santa Clara with the code �CLOUDERA� to receive a 25% discount! We look forward to seeing you there!</snippet></document><document id="296"><title>Save 15% on Multi-Course Public Training Enrollments in January and February</title><url>http://blog.cloudera.com/blog/2013/01/save-15-on-multi-course-public-training-enrollments-in-january-and-february/</url><snippet>Cloudera University is the world leader in Apache Hadoop training and certification. Our full suite of live courses and online materials is the best resource to get started with your Hadoop cluster in development or advance it towards production.� We offer deep industry insight into the skills and expertise required to establish yourself as a leading Developer or Administrator�managing and processing Big Data in this fast-growing field. But did you know Cloudera training can also help you plan for the advanced stages and progress of your Hadoop cluster?�In addition to core training for Developers and Administrators, we also offer the best (and, in some cases, only) opportunity to get up to speed on lifecycle projects within the Hadoop ecosystem in a classroom setting.�Cloudera University’s course offerings go beyond the basics to include Training for Apache HBase, Training for Apache Hive and Pig, and Introduction to Data Science: Building Recommender Systems.�Depending on your Big Data agenda, Cloudera training can help you increase the accessibility and queryability of your data, push your data performance towards real-time, conduct business-critical analyses using familiar scripting languages, build new applications and customer-facing products, and conduct data experiments to improve your overall productivity and profitability. For a limited time, Cloudera University is offering a 15% discount when you register for two or more Hadoop training courses to help you build out and realize your Big Data plan.�Cover the basics with Developer or Administrator training, move beyond the HDFS and MapReduce core by pairing Developer and HBase training, work towards machine learning with Hive and Pig training and Introduction to Data Science, or customize your own learning path.� Just use discount code 15off2 when you register for multiple public training classes from Cloudera University.�This offer is only available for new enrollments and is only valid for classes delivered by Cloudera and scheduled to begin before March 1, 2013. Discount Code: 15off2 Expiration Date: Feb. 28, 2013 Register today!</snippet></document><document id="297"><title>How-to: Deploy a CDH Cluster in Skytap Cloud</title><url>http://blog.cloudera.com/blog/2013/01/how-to-deploy-a-cdh-cluster-in-the-skytap-cloud/</url><snippet>You may have seen the recent announcement from Skytap about the�availability of pre-configured CDH4 templates in the Skytap Cloud public template library.�So for anyone who wants to try out a Cloudera Hadoop cluster�from small to large�it can now be easily accomplished in Skytap Cloud. The how-to below from Skytap’s Matt Sousely explains how. The goal of this how-to will be to spin up a 10-node Cloudera Hadoop cluster in Skytap Cloud. To begin, let’s talk about the two new Cloudera Hadoop cluster templates. The first is Cloudera CDH4 Hadoop cluster: a 2-node Hadoop cluster template. It includes 2 nodes and a management node/server. The second is the Cloudera CDH4 Hadoop host template. This second template is not intended to run by itself in a configuration�rather, it contains a host VM that is ready to become another Hadoop node in the Cloudera CDH4 Hadoop cluster template-based configuration. To start, let’s spin up a Cloudera Hadoop cluster. Log in to Skytap Cloud Choose the Templates tab In the search box, type hadoop Select Cloudera CDH4 Hadoop cluster Click New Configuration Click Run Once all the VMs start (in about 90 seconds), you�ll have a working Cloudera Hadoop cluster with all the normal services (hdfs, hbase, hue, mapreduce, oozie, and zookeeper). This cluster is a 2-node cluster (host 1 and host 2) with a management server (manager). While a 2-node cluster is enough to get going with Cloudera Hadoop, it’s possible in Skytap Cloud to ratchet this up to a cluster of any size. For this blog post, we’ll expand this into a 10-node cluster. The Cloudera Manager is hosted on the manager VM on port 7180. However, none of the VMs in this configuration have a web browser, so we need a way to interact with Cloudera Manager. This can be accomplished in a few different ways: 1) Use a Skytap published service, 2) Use ICNR (inter-configuration network routing) with a configuration that has a graphical web browser, 3) Use a public IP, or 4) Use Skytap VPN to connect your local network to this configuration. For production use, VPN is probably your best bet, but for this blog post I’m going to use a published service. To add the published service, do the following: Click Settings. Click VM Settings. Select manager in the Select a VM menu. Under Network Adapters choose Add Published Service. In the dropdown, select By Port: Enter 7180 in the text box. Click Add Published Service. Expand the Show Published Services link and note the url and port number. Example – services.cloud.skytap.com:25693 Now you can put that URL into your local web browser and get the Cloudera Manager (Free Edition) login page. You should then be able to use the username of ‘admin’ and the password found in the credentials tab of the manager VM settings for the ‘admin’ account. Now that everything is running, the Cloudera Manager is accessible, and I’m logged in, it’s time to expand our cluster from 2 nodes to 10 nodes. To do that: Click Back to configuration to get back to the 2-node configuration. Click Add VMs. In the search box, type hadoop. Select Cloudera CDH4 Hadoop host. Click Add. Redo steps 2-5 another 7 times (to take our host count up to 10). Notice that although the titles for all of these new nodes are shown as ‘host-n’ their network names have been automatically incremented. Optionally, to make the configuration easier to view, I can rename all node hosts from host-n to their corresponding host-x number. Click Run. After about 90 seconds, everything will start up and we�ll have all the hosts we need for our 10-node cluster. It’s now time to go back to Cloudera Manager to finish setting up the nodes. Go back into Cloudera Manager. (Note: You may need to log in again.) Click Hosts at the top of the web page. Click Add Hosts. Click Continue. In the search form, type host-[3-10].hadoop.local This will search DNS to ping and find all the new host nodes. Leave all hosts selected and click Install CDH on selected hosts. Keep all defaults on the next page, then click Continue. Leave radio button defaults and use the root password found in the credentials tab of any of the host-n VMs. (Note: They all have the same password.) Click Start Installation. Wait for all of the nodes to finish installing. (Note: It could take 10-15 minutes for everything to install.) If for any reason the web page times out, or something just doesn�t seem right, you can redo steps 2-9 to validate that all the software was installed properly. When the installations are done, click Continue. The UI will now inspect all hosts. All hosts should resolve as green. (Note: It is OK if you have one yellow relating to mismatched versions.) If all looks good, click Continue. If not, run steps 2-11 again. Click Continue again to finish the Wizard. It should forward you to the hosts page where all your hosts (1 through 10 and manager) should show up in good health. At this point, we have a 10-node Cloudera Hadoop cluster, but we want to put these new nodes to work just like nodes 1 and 2. So, to accomplish that: Click Cloudera Manager (Free Edition) at the top left of the web UI. This will bring you back to the services page. Click the upside-down triangle next to each server, then click Instances. Click Add. In the Add Role Instances view, check the same boxes for hosts 3-10 that are checked for hosts 1 and 2. In the case of HDFS, this would be the ‘region server’ column. Click Continue. Click Accept. Wait for the commands to complete. Repeat steps 1-6 for all the different services. Note: Some services may not utilize nodes 1 and 2, in which case you can safely leave out nodes 3-10 as well. For example, the Hue service is only hosted on the manager VM and there are no settings for nodes 1 and 2. If you would like to make manager fault tolerant, you will want to follow all the steps in this blog post to create a second manager node and that is identical to the existing manager node. And there you have it�a 10-node Cloudera Hadoop cluster. Matt is the Manager/Developer of Public Templates at Skytap. He started building content for NetIQ on its Operations Manager product (later bought by Microsoft and renamed MOM) as well as its AppManager product. He has also worn many hats while working for iConclude/Opsware/HP and FullArmor.�  </snippet></document><document id="298"><title>Where to Find Cloudera Tech Talks in Early 2013</title><url>http://blog.cloudera.com/blog/2013/01/where-to-find-cloudera-tech-talks-in-early-2013/</url><snippet>Clouderans are traveling the United States (and beyond) in droves during the first quarter of 2013 to present at developer meetups and conferences. If you’re interested in attending one near you, we’ve listed them below – see links for specific topics (but note that some of the sites involved may not reflect complete event details yet; check back later for updates). Let me point out in particular that: Hadoop committer/PMC member Eli Collins Doug Cutting (Hadoop project founder, ASF chairman, Cloudera chief architect) will speak about the “Evolution of the Apache Hadoop Ecosystem” at the Silicon Valley JUG on Feb. 20. Eva Andreasson�brings a Hadoop session to the�Jfokus�Java developer conference in early February. Marcel Kornacker, Cloudera Impala architect and tech lead, will be doing Impala tech talks in Paris (HUG France) and Belgium (BigData.be) in late February. You can attend multiple Clouderan-led sessions at ApacheCon, Strata, and Hadoop Summit Europe on various topics. Date City Venue Speaker(s) Jan. 23 San Francisco Bay Area HBase User Group Elliott Clark Jan. 23 St. Louis St. Louis Machine Learning Tom Wheeler Jan. 24 Saint John, N.B. Leading Thinkers/Big Data Congress Jesse Anderson Jan. 28 Chicago Chicago Big Data Meetup Josh Wills Jan. 29 Research Triangle Park, N.C. TriHUG Ricky Saltzer Jan. 31 Los Angeles LA HBase User Group Ian Wrigley Jan. 31 Los Angeles LA Hadoop Users Group Mike Percy Feb. 3 Brussels FOSDEM Lars George Feb. 7 Stockholm Jfokus Eva Andreasson Feb. 20 Vancouver, B.C. Vancouver Hadoop Users Group Jeff Lord Feb. 20 Redwood Shores, Calif. Oracle Developer Day: Big Data (Admin track) Greg Rahn Feb. 20 Mountain View, Calif. Silicon Valley Java User Group Eli Collins Feb. 25 Leuven, Belgium Belgian Big Data Community Marcel Kornacker Feb. 26 Paris HUG France Marcel Kornacker Feb. 26-28 Santa Clara, Calif. Strata Conference Justin Erickson, Amandeep Khurana, Jayant Shekhar, Philip Zeyliger Feb. 26-28 Portland, Ore. ApacheCon NA Mark Grover, Arvind Prabhakar, Roman Shaposhnik,Kathleen Ting Feb. 27 Portland, Ore. PDX Hadoop/ Data Science Mark Grover March 7 St. Louis Lambda Lounge Tom Wheeler March 21 Amsterdam Hadoop Summit Europe Patrick Angeles, Matteo Bertozzi, Lars George</snippet></document><document id="299"><title>Cloudera Impala Beta (version 0.4) and Cloudera Manager 4.1.3 Now Available</title><url>http://blog.cloudera.com/blog/2013/01/cloudera-impala-beta-version-0-4-and-cloudera-manager-4-1-3-now-available/</url><snippet>I am pleased to announce the release of Cloudera Impala Beta (version 0.4) and Cloudera Manager 4.1.3. Key enhancements in each release are: Cloudera Impala Beta (version 0.4) Added support for Impala on RHEL5.7/Centos5.7 operating systems, in addition to RHEL6.2/Centos6.2 operating systems. Significant stability and performance enhancements to address feedback from the beta program. Additional metrics available on the Impala Debug Webpage. Impala Beta will be regularly updated with features, bug fixes, and performance enhancements. We will typically release such updates every 2-4 weeks. Please check the release notes to find out what’s new with each update. For more questions, visit the Impala FAQ page or email us at impala-user@cloudera.org. Cloudera Manager 4.1.3 Cloudera Manager 4.1.3 supports Impala 0.4. Cloudera Manager 4.1.3 can now be used to install Impala on RHEL5.7/Centos5.7, in addition to RHEL6.2/Centos6.2 operating systems. Bug fixes Detailed release notes Download CM 4.1.3 or upgrade to CM 4.1.3. For more questions, visit the CM FAQ page or email us at scm-users@cloudera.org. As a reminder, here is how you can get started with Impala: Use Cloudera Manager 4.1.3 to deploy Impala. Documentation can be found here. To manually install Impala, access the�download of Impala, install Impala and try it out. Please note that you need to have CDH 4.1.x installed on RHEL/CentOS 6.2 or RHEL/CentOS 5.7. Resources: Download the demo VM of Impala. The VM includes instructions that show you the power of Impala. Watch the e-learning course on “An Introduction To Impala“. Access the Impala source code at https://github.com/cloudera/impala. Download and review the Impala documentation. Feedback: Once you get started, we encourage you to provide feedback. We have the following mechanisms set up to do this: An Impala user group has been set up. Please use this to ask questions and provide feedback. An�Impala Jira project has been set up. Feature requests and bug reports are welcome. For questions related to Cloudera Manager, please use the� Cloudera Manager user group.</snippet></document><document id="300"><title>How-To: Schedule Recurring Hadoop Jobs with Apache Oozie</title><url>http://blog.cloudera.com/blog/2013/01/how-to-schedule-recurring-hadoop-jobs-with-apache-oozie/</url><snippet>Our thanks to guest author Jon Natkins (@nattyice) of WibiData for the following post! Today, many (if not most) companies have ETL�or data enrichment jobs that are executed on a regular basis as data becomes available. In this scenario it is important to minimize the lag time between data being created and being ready for analysis. CDH, Cloudera’s open-source distribution of Apache Hadoop and related projects, includes a framework called Apache Oozie that can be used to design complex job workflows and coordinate them to occur at regular intervals. In this how-to, you�ll review a simple Oozie coordinator job, and learn how to schedule a recurring job in Hadoop. The example involves adding new data to a Hive table every hour, using Oozie to schedule the execution of recurring Hive scripts. (For the full context of the example, see the “Analyzing Twitter Data with Apache Hadoop” series.) Adding Data to Hive Tables In Apache Hive, tables may be partitioned to make it easier to manage a rolling window of data, or to improve performance of queries that predicate on the partition column. (To learn more about Hive, and how partitions work, see “Querying Semi-structured Data with Apache Hive”.) For the purpose of this article, we�ll work with a table, introduced in a previous blog post, used to store Twitter tweets, and populated by a data stream ingested by Flume into HDFS: ADD JAR /tmp/hive-serdes-1.0-SNAPSHOT.jar

CREATE EXTERNAL TABLE tweets (
 ...
 retweeted_status STRUCT&lt;
   text:STRING,
   user:STRUCT&gt;,
 entities STRUCT&lt;
   urls:ARRAY&gt;,
   user_mentions:ARRAY&gt;,
   hashtags:ARRAY&gt;&gt;,
 text STRING,
 ...
)
PARTITIONED BY (datehour INT)
ROW FORMAT SERDE 'com.cloudera.hive.serde.JSONSerDe'
LOCATION '/user/flume/tweets';
   The important things to note here are that the root of the table�s data directory is located at /user/flume/tweets, and that the table has a partition column, which uses an integer to represent the date and hour of the partition. We�ve chosen to mimic the Java date format �yyyyMMddHH�. For example, the partition containing data for 2013-01-04 09:00 UTC would be identified by the partition value of 2013010409. For this example, we will design an Oozie coordinator job that will add a new partition to the table each hour. Writing the Hive Queries In order to add a partition to a Hive table, you�ll need a file containing the queries that need to be executed. In Hive, partitions are created through an ALTER TABLE statement: ADD JAR ${JSON_SERDE};
ALTER TABLE tweets
  �ADD IF NOT EXISTS
  �PARTITION (datehour = ${DATEHOUR})
  �LOCATION '${PARTITION_PATH}';
    �The commands are fairly straightforward. The table that was created uses a custom SerDe, which Hive will use when performing the add partition command. In order to ensure that Hive can access the SerDe class for the table, you must add the JAR file, and then add the partition. However, several of the values in the query are variables: ${JSON_SERDE}, ${DATEHOUR}, and ${PARTITION_PATH}will be replaced by Oozie with their actual values at execution time. Designing the Workflow In order to run a recurring job, you�ll need to have an Oozie workflow to execute. Oozie workflows are written as an XML file representing a directed acyclic graph (DAG). Let�s look at a simple workflow example, and dive into what�s actually going on. The following file is saved as add-partition-hive-action.xml, in a directory which is referenced as workflowRoot: &lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;workflow-app xmlns="uri:oozie:workflow:0.4" name="hive-add-partition-wf"&gt;
  &lt;start to="hive-add-partition"/&gt;

  &lt;action name="hive-add-partition"&gt;
    &lt;hive xmlns="uri:oozie:hive-action:0.4"&gt;
      &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
      &lt;name-node&gt;${nameNode}&lt;/name-node&gt;
      &lt;prepare&gt;
        &lt;delete path="${workflowRoot}/output-data/hive"/&gt;
        &lt;mkdir path="${workflowRoot}/output-data"/&gt;
      &lt;/prepare&gt;
      &lt;job-xml&gt;${workflowRoot}/hive-site.xml&lt;/job-xml&gt;
      &lt;configuration&gt;
        &lt;property&gt;
          &lt;name&gt;oozie.hive.defaults&lt;/name&gt;
          &lt;value&gt;${workflowRoot}/hive-site.xml&lt;/value&gt;
        &lt;/property&gt;
      &lt;/configuration&gt;
      &lt;script&gt;add_partition_hive_script.q&lt;/script&gt;
      &lt;param&gt;JSON_SERDE=${workflowRoot}/lib/hive-serdes-1.0-SNAPSHOT.jar&lt;/param&gt;
      &lt;param&gt;PARTITION_PATH=${workflowInput}&lt;/param&gt;
      &lt;param&gt;DATEHOUR=${dateHour}&lt;/param&gt;
    &lt;/hive&gt;
    &lt;ok to="end"/&gt;
    &lt;error to="fail"/&gt;
  &lt;/action&gt;

  &lt;kill name="fail"&gt;
    &lt;message&gt;Hive failed, error message[${wf:errorMessage(wf:lastErrorNode())}]&lt;/message&gt;
  &lt;/kill&gt;
  &lt;end name="end"/&gt;
&lt;/workflow-app&gt;
   The graph is made up of a series of control-flow nodes and Oozie actions. The control flow nodes are XML entities like &lt;start&gt;, &lt;kill&gt;, and &lt;end&gt;, which identify the beginning, error, and final states of the workflow. There are also several other more complex control flow nodes called &lt;decision&gt;, &lt;fork&gt;, and &lt;join&gt;. More information on those nodes can be found in the Oozie documentation. In the workflow above, we�ve designed a very simple workflow, which, when visualized as a DAG, looks like this: This is a very simple workflow that only executes a single action. Actions in Oozie are a discrete computation or processing task. They can do a number of different things: execute a MapReduce job, run a Java main class, or run a Hive or Pig script, to name a few possibilities. In this workflow, we identify the action as a Hive action with this node: &lt;hive xmlns="uri:oozie:hive-action:0.4"&gt;. As an aside, in order to use Hive actions, the Oozie Share Lib must be installed. Looking more closely at the Hive action, we can see how its functionality is defined. The &lt;job-tracker&gt; and &lt;name-node&gt; entities dictate the servers that the Hive job will connect to for executing its script: &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;
&lt;name-node&gt;${nameNode}&lt;/name-node&gt;
   You�ll see where the variable values ${jobTracker} and ${nameNode}come from a little later. The &lt;job-xml&gt; entity is used to specify a configuration file for Hive. Since most Hive installations will need to connect to a non-default metastore database, this configuration file will likely contain database connection credentials. You may also notice that the &lt;configuration&gt; section of the action defines a parameter called oozie.hive.defaults, which points to the same file as &lt;job-xml&gt;: &lt;job-xml&gt;${workflowRoot}/hive-site.xml&lt;/job-xml&gt;
      &lt;configuration&gt;
        &lt;property&gt;
          &lt;name&gt;oozie.hive.defaults&lt;/name&gt;
          &lt;value&gt;${workflowRoot}/hive-site.xml&lt;/value&gt;
        &lt;/property&gt;
      &lt;/configuration&gt;
   This approach is necessary because in the current version of Oozie (CDH4.1.2), Oozie Hive actions require a Hive defaults file. The file can be provided using the workflow configuration or, in certain versions of Hive, is provided by the Hive JAR itself. However, the CDH4.1.2 version of Hive no longer packages or uses that file, so you must include it in the workflow despite it having no effect on the action. (For more information, see OOZIE-1087.) In CDH4.2, it will no longer be necessary to include the oozie.hive.defaults configuration parameter. The most interesting pieces in this action are the &lt;script&gt; and &lt;param&gt; entities: &lt;script&gt;add_partition_hive_script.q&lt;/script&gt;
&lt;param&gt;JSON_SERDE=${workflowRoot}/lib/hive-serdes-1.0-SNAPSHOT.jar&lt;/param&gt;
&lt;param&gt;PARTITION_PATH=${workflowInput}&lt;/param&gt;
&lt;param&gt;DATEHOUR=${dateHour}&lt;/param&gt;
   The &lt;script&gt; entity specifies the location of the Hive script in HDFS, and the &lt;param&gt; entities contain declarations of variables that will be passed into the Hive script. You can see here that the parameters being passed in match the names of the variables from the Hive script. Again, for reference, the JSON_SERDEparameter specifies the HDFS path to a JAR file containing the custom SerDe class. Scheduling Recurring Workflows Oozie has another type of a job called a coordinator application. Coordinator applications allow users to schedule more complex workflows, including workflows that are scheduled regularly, or that have dependencies on the output from other workflows. For this application, which is stored on an HDFS cluster in a file named add-partition-coord-app.xml, the add partition workflow is executed on an hourly basis: &lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;coordinator-app name="add-partition-coord" frequency="${coord:hours(1)}"
������������ start="${jobStart}" end="${jobEnd}"
������������ timezone="UTC"
������������ xmlns="uri:oozie:coordinator:0.4"&gt;
� &lt;datasets&gt;
��� &lt;dataset name="tweets" frequency="${coord:hours(1)}"
�������� initial-instance="${initialDataset}" timezone="America/Los_Angeles"&gt;
����� &lt;uri-template&gt;
������� hdfs://hadoop1:8020/user/flume/tweets/${YEAR}/${MONTH}/${DAY}/${HOUR}
����� &lt;/uri-template&gt;
����� &lt;done-flag&gt;&lt;/done-flag&gt;
��� &lt;/dataset&gt;
� &lt;/datasets&gt;
�
&lt;input-events&gt;
��� &lt;data-in name="tweetInput" dataset="tweets"&gt;
����� &lt;instance&gt;${coord:current(coord:tzOffset() / 60)}&lt;/instance&gt;
��� &lt;/data-in&gt;
��� &lt;data-in name="readyIndicator" dataset="tweets"&gt;
����� &lt;instance&gt;${coord:current(1 + (coord:tzOffset() / 60))}&lt;/instance&gt;
��� &lt;/data-in&gt;
� &lt;/input-events&gt;
� &lt;action&gt;
��� &lt;workflow&gt;
����� &lt;!-- add-partition-hive-action.xml is the workflow containing the Hive action --&gt;
����� &lt;app-path&gt;${workflowRoot}/add-partition-hive-action.xml&lt;/app-path&gt;
����� &lt;configuration&gt;
�������
&lt;property&gt;
��������� &lt;name&gt;workflowInput&lt;/name&gt;
��������� &lt;value&gt;${coord:dataIn('tweetInput')}&lt;/value&gt;
������� &lt;/property&gt;
�������
&lt;property&gt;
����� ���&lt;name&gt;dateHour&lt;/name&gt;
����� ���&lt;value&gt;
����������� ${coord:formatTime(coord:dateOffset(coord:nominalTime(), tzOffset, 'HOUR'), 'yyyyMMddHH')}
��������� &lt;/value&gt;
��� �� �&lt;/property&gt;
����� &lt;/configuration&gt;
��� &lt;/workflow&gt;
� &lt;/action&gt;
&lt;/coordinator-app&gt;
   Note in the top-level entity that the app is configured to run once an hour using the coord:hours(1) method. Oozie includes a number of different methods for specifying frequency intervals, which can be found in the documentation. You also have to specify a start time and end time for the job, which are represented by the jobStart and jobEndvariables. You�ll see where those values come from a little later on. The second major component of the coordinator app is the datasets entity. A dataset specifies the location of a set of input data. In this case, there is a dataset called tweets, which is updated every hour, as specified by the frequency. For each execution of the Hive workflow, you will have a separate instance of the tweets dataset, starting with the initial instance specified by the dataset. A particular instance of a dataset is identified by its creation time in ISO-8601 format, such as 2013-01-04T09:00Z. In this case, the location of the data is hdfs://hadoop1:8020/user/flume/tweets/${YEAR}/${MONTH}/${DAY}/${HOUR}. YEAR, MONTH, DAY, and HOUR are special variables that can be used to parameterize the URI template for the dataset. The done flag specifies a file that determines when the dataset is done being generated. The default value for this is _SUCCESS, which is the name of the file that a MapReduce job generates when it completes. For the purposes of this application, data was being ingested into HDFS via Flume, which does not generate a _SUCCESS file. Instead, the done flag is empty, which instructs Oozie that the dataset is considered ready when the directory exists. For this application, Flume is configured to write events to a different directory each hour. Having a completed dataset is one of the criteria for executing an instance of the Hive workflow. The other requirement is for any input events to be satisfied. Currently, Oozie is restricted to input events in the form of available datasets. This means that an input event will not be satisfied until a particular instance of a dataset exists. In the context of a more complex data pipeline, this means that a job can be configured to execute only after all of its input data has been successfully generated. For this coordinator app, the input events look like this: &lt;input-events&gt;
  &lt;data-in name="tweetInput" dataset="tweets"&gt;
  ��&lt;instance&gt;${coord:current(coord:tzOffset() / 60)}&lt;/instance&gt;
  &lt;/data-in&gt;
  &lt;data-in name="readyIndicator" dataset="tweets"&gt;
  ��&lt;instance&gt;${coord:current(1 + (coord:tzOffset() / 60))}&lt;/instance&gt;
  &lt;/data-in&gt;
&lt;/input-events&gt;
   There are a number of things going on here: we have two distinct input events, and each of them is specified by an Oozie built-in function. Let�s break down the built-in function: ${coord:current(coord:tzOffset() / 60)}
   coord:tzOffset()is a function that returns the offset, in minutes, from UTC, of the timezone of the machine executing the workflow. In order to get the number of hours offset from UTC, we divide by 60. coord:current(n) will return the timestamp representing the nth instance for a dataset. For example, if the time were 2013-01-17 at midnight UTC, then coord:current(0) would return 2013-01-17T00:00Z. For the tweets dataset, which has an hourly frequency, coord:current(1) would evaluate to 2013-01-17T01:00Z. However, if the tweets dataset had a frequency of coord:days(1), coord:current(1) would evaluate to 2013-01-18T00:00Z. Putting these two pieces together, you can see that function used to determine the tweetInput input event is going to evaluate to the current dataset instance for your machine�s local timezone. Making a logical leap, the readyIndicator input event refers to the dataset instance immediately after the current one, both in your local timezone. In order to understand why the readyIndicator event exists, let’s revisit the done flag for the tweets dataset. Since Flume does not create an indicator file when it�s finished writing to a particular directory, we left the flag empty to specify that an instance of the tweets dataset would exist when a directory exists. Since Flume will immediately begin writing to a new directory each hour, this means is that the current instance of the tweets dataset is really not ready for processing until Flume has started writing the next hour�s directory. You can ensure that the current instance is ready by using the readyIndicator event to ensure that Flume has started writing the next instance of the tweets dataset. The final section of the coordinator app is the action, which contains the details of what will be executed: &lt;action&gt;
    &lt;workflow&gt;
      &lt;app-path&gt;${workflowRoot}/add-partition-hive-action.xml&lt;/app-path&gt;
      &lt;configuration&gt;
        &lt;property&gt;
          &lt;name&gt;workflowInput&lt;/name&gt;
          &lt;value&gt;${coord:dataIn('tweetInput')}&lt;/value&gt;
        &lt;/property&gt;
        &lt;property&gt;
      	   &lt;name&gt;dateHour&lt;/name&gt;
      	   &lt;value&gt;
            ${coord:formatTime(coord:dateOffset(coord:nominalTime(), tzOffset, 'HOUR'), 'yyyyMMddHH')}
          &lt;/value&gt;
    	 &lt;/property&gt;
      &lt;/configuration&gt;
    &lt;/workflow&gt;
  &lt;/action&gt;
   The action specifies a workflow that will be executed, as well as any configuration properties that the workflow will need in order to execute. For this worfklow, we�ve specified workflowInput and dateHour. The function used to generate workflowInput, coord:dataIn('tweetInput'), will evaluate to the URI of the current tweetInput instance. To illustrate this, let�s consider an example: The time is 2013-01-04 5:00 PM in the Pacific timezone. Then the current tweetInput instance time will be 2013-01-04T09:00Z, since Pacific time is offset by -8 hours from UTC. The tweetInput event is linked to the tweets dataset, so the HDFS URI that is returned by the coord:dataIn function is hdfs://hadoop1:8020/user/flume/tweets/2013/01/04/09. The dateHour parameter will be used to generate the partition that we will be adding. In the Hive table, the partition is an integer representing the year, month, day, and hour in the form �yyyyMMddHH�. To generate the appropriate value, we use another function: coord:formatTime(coord:dateOffset(coord:nominalTime(), tzOffset, 'HOUR'), 'yyyyMMddHH'). Considering the same example as above, the coord:nominalTime() function will return the time when the coordinator action was created. Since the coordinator app has a frequency of one hour, this time will be somewhere between 2013-01-04 5:00 PM and 2013-01-04 5:59 PM (Pacific Time), depending on when the coordinator app was started. The coord:dateOffset function will take that nominal time, and offset the hours by our timezone offset, which is -8, so we�ll end up with a timestamp of the form 2013-01-04T09:XXZ, where XX is something between 00 and 59. Finally, the coord:formatTime function will translate the timestamp to a string 2013010409, which Hive will be able to convert to the integer partition value. When all is said and done, this particular coordinator action would run these Hive commands: ALTER TABLE tweets
  � ADD IF NOT EXISTS
  � PARTITION (datehour = 2013010409)
  � LOCATION 'hdfs://hadoop1:8020/user/flume/tweets/2013/01/04/09';
   You still need one last piece to fill in the remaining variables, such as jobStart, jobEnd, and workflowRoot. That piece is the job.properties file: nameNode=hdfs://hadoop1:8020
jobTracker=hadoop1:8021
workflowRoot=${nameNode}/user/${user.name}/add-tweet-partitions

# jobStart and jobEnd should be in UTC, because Oozie uses UTC for
# processing coordinator jobs by default (and it is not recommended
# to change this)
jobStart=2013-01-04T22:00Z
jobEnd=2013-12-12T23:00Z

# Timezone offset between UTC and the server timezone
tzOffset=-8

# This should be set to an hour boundary, and should be set to a time
# of jobStart + tzOffset or earlier.
initialDataset=2013-01-04T14:00Z

# Adds a default system library path to the workflow�s classpath
# Used to allow workflows to access the Oozie Share Lib, which includes
# the Hive action
oozie.use.system.libpath=true

# HDFS path of the coordinator app
oozie.coord.application.path=${nameNode}/user/${user.name}/add-tweet-partitions/add-partition-coord-app.xml
   You can see here where the remaining variables are set: One variable of note, user.name, is actually an Oozie built-in variable, and evaluates to the name of the user running the coordinator app. A directory must exist in HDFS with all the necessary files and external JAR files for running the application. In this app, workflowRootpoints to the location where that directory exists. Running the Coordinator Application In order to run the coordinator app, you first have to stage the necessary files in HDFS. The files should be put in HDFS according to their defined locations in the XML files. In this example, all the files are located in the same directory: /user/${user.name}/add-tweet-partitions. That directory will also need to have a lib subdirectory in it. The lib directory can be used to make any dependency JARs available to the workflow. For example, Hive actions will usually require a JDBC driver, in order to communicate with a non-Derby metastore, and for the example in this article, the lib directory will contain the custom SerDe JAR file, as well. [natty@frumious ~]$ hadoop fs -ls /user/natty/add-tweet-partitions
  Found 5 items
  -rw-r--r--�� 3 natty natty������� 938 2012-12-27 09:37 /user/natty/add-tweet-partitions/add_partition_hive_script.q
  -rw-r--r--�� 3 natty natty������ 2533 2012-12-27 09:37 /user/natty/add-tweet-partitions/add-partition-coord-app.xml
  -rw-r--r--�� 3 natty natty������ 1918 2012-12-27 09:37 /user/natty/add-tweet-partitions/add-partition-hive-action.xml
  -rw-r--r--�� 3 natty natty������ 1386 2012-12-27 09:37 /user/natty/add-tweet-partitions/job.properties
  drwxr-xr-x�� - natty natty��������� 0 2012-12-27 09:38 /user/natty/add-tweet-partitions/lib

[natty@frumious ~]$ hadoop fs -ls /user/natty/add-tweet-partitions/lib
 Found 2 items
  -rw-r--r--�� 3 natty natty���� 236141 2012-12-27 09:38 /user/natty/add-tweet-partitions/lib/hive-serdes-1.0-SNAPSHOT.jar
  -rw-r--r--�� 3 natty natty���� 758452 2012-12-27 09:38 /user/natty/add-tweet-partitions/lib/mysql-connector-java.jar
   In order to run the coordinator app, the job.properties file should be stored locally, and the following command should be run: $ oozie job -oozie http://:/oozie -config add-tweet-partitions/job.properties -run
   That command will return the name of the job, which can be used to get status updates on the job, either via the web UI or the command line tool. Once executed, the coordinator app will create and run instances of the workflow each hour, and data will automatically become queryable through Hive as the partitions get created. Conclusion Oozie is a versatile system that can be used to set up and automate even the most complicated of data processing workflows. Also, check out the recently added workflow and coordinator app creation UI in Hue, which makes it much easier to design and execute complicated data flows. This how-to is certainly not comprehensive, and there are lots of other features in Oozie that haven�t been discussed. I encourage you to go and try to build your own Oozie workflows and coordinator apps, and explore some different features. Jon Natkins is a field engineer at WibiData. Formerly, he worked as a software engineer at Cloudera.</snippet></document><document id="301"><title>Meet the Instructor: Jesse Anderson</title><url>http://blog.cloudera.com/blog/2013/01/meet-the-instructor-jesse-anderson/</url><snippet>The Hadoop Community�is an invariably fascinating world. �After all, as Clouderan ATM put it in a past blog post, the user group meetups are adorably called �HUGs.� Just as the Cloudera blog has introduced you to some of the engineers, projects, and applications that serve as the head, heart, and hands of the Hadoop Community, we�re proud to add the circulatory system (to extend the metaphor), made up of Cloudera�s expert trainers and curriculum developers who bring Hadoop to new practitioners around the world every week. Welcome to the first installment of our �Meet the Instructor� series, in which we briefly introduce you to some of the individuals endeavoring to teach Hadoop far and wide. Today, we speak to Jesse Anderson (@jessetanderson)!� What is your role at Cloudera? I joined Cloudera about a year ago as a curriculum developer and instructor.�I get the best of both worlds in educational services:�I create and improve existing curriculum, such as the Cloudera Manager series, and I travel to teach the courses. What do you enjoy most about training and/or curriculum development? I enjoy passing on my knowledge about Hadoop and development.�I spent the previous 15 years as an engineer and programmer, and now I want to share the benefits of my experience.�As far as development goes, I�ve seen it all, from tiny startups to multinational corporations, working on projects ranging from distributed file systems to client/server applications, from enterprise services to the social/mobile web (where I was, unfortunately, required to wear skinny jeans). I bring my accumulated knowledge and experience to the classroom to demonstrate effective development methods in the clearest, most comprehensive way I can. During class, I tell stories from my years in the developer trenches to illustrate how certain techniques are optimally or sub-optimally applied.�Although I continue to code, I thankfully don’t get that call anymore at 2am, when there’s a bug that needs fixed. Instead, I get calls and emails from other Hadoop professionals around the world who are eager to discuss the state of the art. Now I focus on instructional code and even think of PowerPoint as my integrated development environment. In a sense, my job is to learn as much as I can about Hadoop as an effective and dynamic data platform and pass that insight on to others.�I’ve been thinking about things like that in my free time for years, but now I�m expected to do it full time. How cool is that? Describe an interesting application you’ve seen or heard about for Apache Hadoop. It’s interesting to see the wide array of production-worthy programs in the community. At Strata + Hadoop World 2012, some folks from LinkedIn presented about their Hadoop workflow, including a visualization of all its jobs and dependencies. It was a tangled, interconnected vision of enterprisey goodness!�One of the artifacts from this workflow is LinkedIn�s People You May Know feature, which uses Graph Theory to find people with whom each member would like to connect due to some organic association, but who are not necessarily closely linked by degrees of separation. I use that application all the time, so it was really cool to see how Hadoop was enabling it. I’m also partial to my own Million Monkeys Project. You may have heard of the Infinite Monkey Theorem or the saying “a million monkeys on a million typewriters will eventually recreate Shakespeare” (or maybe you�re a fan of The Simpsons). Using Hadoop, a program I wrote, some basic rules, and time, my virtual monkeys recreated the entirety of Shakespeare�s oeuvre. This project shows that Big Data isn’t always a massive file sitting in HDFS�sometimes it’s just a computationally intensive application that requires a cluster’s horsepower to finish. What advice would you give to an engineer, system administrator, or analyst who wants to learn more about Big Data? My two favorite quotes on the subject are: “An investment in knowledge pays the best interest.” � Benjamin Franklin “I have never let my schooling interfere with my education.” � Mark Twain � Get in the habit of investing in yourself with education and knowledge. Don’t let a college degree encumber continuous learning�a degree is only the foundational intelligence on which wisdom is built. Those working in technical fields tend to already be in the habit of learning, so getting up to speed on Big Data is pretty easy. However, although Hadoop is an open-source project, training is by far the quickest way to move from the conceptual to the productive�Cloudera University courses are intensive and are meant to propel participants towards necessary expertise. � How did you become involved in technical training and Hadoop? I’ve spent the past few years running the Reno-area local developers group, and Pragmatic Programmers recently published a series of my screencasts. These exercises were a way of proverbially dipping my toe in the training waters. I had been looking for a means to continue learning and coding, but untether myself from my workstation. I love to travel, teach, and meet end-users face-to-face, so training is an excellent way to get out on the road while keeping up my technical chops. I became involved with Hadoop when I was doing the Million Monkeys project. I’ve spent a lot of time writing and working with distributed systems and did some scalability and ROI research with Hadoop. �I like to tell people I have a visual representation in graph form for why I’m using Hadoop! What�s one interesting fact or story about you that a training participant would be surprised to learn? I used to moonlight as a karaoke D.J.�Between amateur renditions of �Gin and Juice� and �Stayin’ Alive,� I liked to play my own painfully tempo- and pitch-shifted versions of �True� by Spandau Ballet as bumper music. It�s nice to have an ace up my sleeve in case my Hadoop classes get out of line!�</snippet></document><document id="302"><title>Apache HBase Internals: Locking and Multiversion Concurrency Control</title><url>http://blog.cloudera.com/blog/2013/01/apache-hbase-internals-locking-and-multiversion-concurrency-control/</url><snippet>This following post was originally published via blog.apache.org; we republish it here for your convenience. NOTE: This blog post describes how Apache HBase does concurrency control. �This assumes knowledge of the HBase write path, which you can read more about in this other blog post. Introduction Apache HBase provides a consistent and understandable data model to the user while still offering high performance. �In this blog, we�ll first discuss the guarantees of the HBase data model and how they differ from those of a traditional database. �Next, we�ll motivate the need for concurrency control by studying concurrent writes and then introduce a simple concurrency control solution. �Finally, we�ll study read/write concurrency control and discuss an efficient solution called Multiversion Concurrency Control. Why Concurrency Control? In order to understand HBase concurrency control, we first need to understand why HBase needs concurrency control at all; in other words, what properties does HBase guarantee about the data that requires concurrency control? The answer is that HBase guarantees ACID semantics per-row. �ACID is an acronym for: Atomicity: All parts of transaction complete or none complete Consistency: Only valid data written to database Isolation: Parallel transactions do not impact each other�s execution Durability: Once transaction committed, it remains If you have experience with traditional relational databases, these terms may be familiar to you. �Traditional relational databases typically provide ACID semantics across all the data in the database; for performance reasons, HBase only provides ACID semantics on a per-row basis. �If you are not familiar with these terms, don�t worry. �Instead of dwelling on the precise definitions, let�s look at a couple of examples. Writes and Write-Write Synchronization Consider two concurrent writes to HBase that represent {company, role} combinations I�ve held: Image 1. �Two writes to the same row From the previously cited Write Path Blog Post, we know that HBase will perform the following steps for each write: (1) Write to Write-Ahead-Log (WAL) (2) Update MemStore: write each data cell [the (row, column) pair] to the memstore List 1. Simple list of write steps That is, we write to the WAL for disaster recovery purposes and then update an in-memory copy (MemStore) of the data. Now, assume we have no concurrency control over the writes and consider the following order of events: Image 2. �One possible order of events for two writes At the end, we are left with the following state: Image 3. �Inconsistent result in absence of write-write synchronization which is a role I�ve never held. �In ACID terms, we have not provided Isolation for the writes, as the two writes became intermixed. We clearly need some concurrency control. �The simplest solution is to provide exclusive locks per row in order to provide isolation for writes that update the same row. �So, our new list of steps for writes is as follows (new steps are in blue). (0) Obtain Row Lock (1) Write to Write-Ahead-Log (WAL) (2) Update MemStore: write each cell to the memstore (3) Release Row Lock List 2: List of write-steps with write-write synchronization Read-Write Synchronization So far, we�ve added row locks to writes in order to guarantee ACID semantics. �Do we need to add any concurrency control for reads? �Let�s consider another order of events for our example above (Note that this order follows the rules in List 2): Image 4. �One possible order of operations for two writes and a read Assume no concurrency control for reads and that we request a read concurrently with the two writes. �Assume the read is executed directly before �Waiter� is written to the MemStore; this read action is represented by a red line above. �In that case, we will again read the inconsistent row: Image 5. �Inconsistent result in absence of read-write synchronization Therefore, we need some concurrency control to deal with read-write synchronization. �The simplest solution would be to have the reads obtain and release the row locks in the same manner as the writes. �This would resolve the ACID violation, but the downside is that our reads and writes would both contend for the row locks, slowing each other down. Instead, HBase uses a form of Multiversion Concurrency Control (MVCC) to avoid requiring the reads to obtain row locks. �Multiversion Concurrency Control works in HBase as follows: For writes: (w1) After acquiring the RowLock, each write operation is immediately assigned a write number (w2) Each data cell in the write stores its write number. (w3) A write operation completes by declaring it is finished with the write number. For reads: (r1) �Each read operation is first assigned a read timestamp, called a read point. (r2) �The read point is assigned to be the highest integer such that all writes with write number &lt;= x have been completed. (r3) �A read r for a certain (row, column) combination returns the data cell with the matching (row, column) whose write number is the largest value that is less than or equal to the read point of r. List 3. Multiversion Concurrency Control steps Let�s look at the operations in Image 4 again, this time using MultiVersion Concurrency Control: Image 6. �Write steps with Multiversion Concurrency Control Notice the new steps introduced for Multiversion Concurrency Control. �Each write is assigned a write number (step w1), each data cell is written to the memstore with its write number (step w2, e.g. �Cloudera [wn=1]�) and each write completes by finishing its write number (step w3). Now, let�s consider the read in Image 4, i.e. a read that begins after step �Restaurant [wn=2]� but before the step �Waiter [wn=2]�. �From rule r1 and r2, its read point will be assigned to 1. �From r3, it will read the values with write number of 1, leaving us with: Image 7. �Consistent answer with Multiversion Concurrency Control A consistent response without requiring locking the row for the reads! Let�s put this all together by listing the steps for a write with Multiversion Concurrency Control: (new steps required for read-write synchronization are in red): (0) Obtain Row Lock (0a) Acquire New Write Number (1) Write to Write-Ahead-Log (WAL) (2) Update MemStore: write each cell to the memstore (2a) Finish Write Number (3) Release Row Lock Conclusion In this blog we first defined HBase�s row-level ACID guarantees. �We then demonstrated the need for concurrency control by studying concurrent writes and introduced a row-level locking solution. �Finally, we investigated read-write concurrency control and presented an efficient mechanism called Multiversion Concurrency Control (MVCC). This blog post is accurate as of HBase 0.92. �HBase 0.94 has various optimizations, e.g. HBASE-5541 that will be described in a future blog post. Gregory Chanan is a Software Engineer at Cloudera and an HBase committer.</snippet></document><document id="303"><title>Understanding MapReduce via Boggle</title><url>http://blog.cloudera.com/blog/2013/01/understanding-mapreduce-via-boggle/</url><snippet>Graph theory is a growing part of Big Data.�Using graph theory, we can find relationships in networks.� MapReduce is a great platform for traversing graphs.�Therefore, one can leverage the power of an Apache Hadoop cluster to efficiently run an algorithm on the graph. One such graph problem is playing Boggle*.�Boggle is played by rolling a group of 16 dice.�Each players’ job is find the most number of words spelled out by the dice.�These dice are six-sided with a single letter that faces up: �A Boggle roll creates a 4×4 matrix of nodes as the dice settle into their slots.�Below is an example of the nodes in a Boggle roll. Once we get into the dice relationships, things get interesting. Each die is related to the one above, below, to the left and right and the diagonals of each side.�The diagram below shows the relationships for each die in the roll. Let’s look at an example of traversing this graph.�In this matrix, let’s say that the letter “U” is at position [0,0]. The letter “L” directly beneath it is at position [0,1].�Let’s try to spell out a word that starts with the letter “L” at [0,1].� Looking at the relationships, we have five possible letters to be the next letter in our word.�Let’s go with the letter “A” at [1,0] as shown by the blue line.�Our word so far is “LA”. Continuing on from the letter “A” at [1,0], we only have four possible letters to be the next letter.�We can’t use the “L” at [0,1] because the rules of Boggle forbid reusing the same letter or node.�We choose the “T” at [1,1] again show by the blue line.� Our word so far is “LAT”.�Let’s skip ahead by choosing the “T” at [1,3], the “E” at [0,3], and “R” at [1,4] which are all shown by the blue line.�Our final word is “LATTER”. Looking at this graph problem with human eyes makes the problem seem easy. Turning this in to an algorithm that finds for all of the words and scales, though, is a difficult problem. The way the graph’s relationships expand, this is a complex challenge with factorial growth. However, using Hadoop and MapReduce, we can create an efficient and scalable solution. Coding The full and working solution to this problem is on my GitHub account.�The code includes an Eclipse project for trying out the code yourself. Let’s take a look at the driver code for the graph traversal.�Note that for brevity, all of these code snippets will have portions omitted.�Please consult the code on GitHub for the unedited code.   // Traverse the graph until it is completely traversed
do {
���� Job job = new Job(configuration);

���� FileInputFormat.setInputPaths(job, getPath(input, iteration));
���� FileOutputFormat.setOutputPath(job, getPath(input, iteration + 1));

���� // Create Map only job

���� job.setMapperClass(BoggleMapper.class);

���� job.setOutputKeyClass(Text.class);
���� job.setOutputValueClass(RollGraphWritable.class);

���� boolean success = job.waitForCompletion(true);

���� iteration++;
} while (true);
   This driver code is going to run a single node’s worth of relationships per iteration.�In the example above, the “L” going to the “A” is one iteration and the “A” going to the “T” is another iteration.�The iterations continue until the entire graph is traversed or all the possible words are found. Notice the iterations are Map-only jobs; the extra overhead of the reducer isn’t necessary.�The output from one iteration is fed into the next iteration to continue to traverse the graph. We need to create a custom writable for the value to handle the nodes in our graph.�This is the RollGraphWritable class. ArrayList nodes = new ArrayList();
boolean isFinal;
   RollGraphWritable keeps track of the nodes that make up the word ArrayList. It also keeps track of the RollGraphWritable‘s status to see if its child or adjacent nodes have been traversed.�If the RollGraphWritable‘s children have been emitted, the node is marked as true. The Node object just represents the place in the Boggle roll from which the individual letters are taken. public short row;
public short column; The Node object simply encodes the place in the matrix of the letter’s row and column position. This is just like the method I described previously when manually traversing the graph. Next comes the mapper code. if (!value.isFinal) {
  ���� processNonFinalNode(context, key.toString(), value);
  } else {
  ���� context.write(key, value);
  }
   If a RollGraphWritable is final, it is simply emitted for the next iteration. This ensures that the RollGraphWritableis around for the final reducer. If the RollGraphWritable is not final, we need to emit those nodes adjacent to the node.   // Emit the characters around the last node in the Boggle Roll
Node node = rollGraph.nodes.get(rollGraph.nodes.size() - 1);

for (int row = node.row - 1; row &lt; node.row + 2; row++) {
���� if (row &lt; 0 || row &gt;= roll.rollSize) {
��������� // Check if row is outside the bounds and skip if so
��������� continue;
���� }

���� for (int col = node.column - 1; col &lt; node.column + 2; col++) {
��������� if (col &lt; 0 || col &gt;= roll.rollSize) {
�������������� // Check if column is outside the bounds and skip if so
�������������� continue;
��������� }

��������� // Found viable row and column. See if node has already been traversed
��������� Node nextNode = new Node(row, col);

��������� if (!rollGraph.nodes.contains(nextNode)) {
�������������� // Node not found, see if it passes the membership test
�������������� String newWord = charsSoFar + roll.rollCharacters[row][col];

�������������������� ArrayList nextNodeList = (ArrayList) rollGraph.nodes.clone();
�������������������� nextNodeList.add(nextNode);

�������������������� RollGraphWritable nextGraphWritable = new RollGraphWritable(nextNodeList, false);

�������������������� context.write(new Text(newWord), nextGraphWritable);
��������� }
���� }
}
   This mapper code chunk handles iterating over the node’s relationships.�There is a nested for loop.�The first loop iterates over the node’s rows.�The second loop iterates over the node’s columns.� When a viable (not outside the bounds of the matrix) is found, we check to see if the Node is already in the RollGraphWritable.�This check is important because we can’t repeat the same letter twice as we’re traversing the graph. If that Node isn’t in the list, the next Node is added to the list and emitted. The driver will continue to run this mapper code until the entire graph is traversed or all possible words are found. Once this is done, the driver will start a different MapReduce job. Job job = new Job(configuration);

job.setNumReduceTasks(1);

job.setMapperClass(BoggleWordMapper.class);

job.setOutputKeyClass(Text.class);
job.setOutputValueClass(RollGraphWritable.class);

boolean success = job.waitForCompletion(true);
   The second MapReduce job is much simpler and only runs once: This job will use the built-in identity reducer that will simply emit whatever the mapper emitted.�This reduce will give us all words that appear in the Boggle roll in alphabetical order. The BoggleWordMapper code is straightforward.   String charsSoFar = key.toString();

   if (words.contains(charsSoFar)) {
  ���� // Word appears, emit
  ���� context.write(new Text(charsSoFar), value);
  }
   The incoming key is the entire word and the value is RollGraphWritablethat contains the nodes that make up the word. In the mapper’s setup method, a dictionary containing all the words in the English language are loaded into a HashMap for quick access. Every word that was found by traversing the graph is passed into the mapper.�If the word appears in the HashMap, it will be emitted and appear in the final list of words in the Boggle roll. In Part 2, we’ll explore optimizations to improve the performance of the job. Jesse Anderson is a Curriculum Developer and Instructor for Cloudera University. * Boggle is a trademark of Hasbro.</snippet></document><document id="304"><title>How-to: Do Apache Flume Performance Tuning (Part 1)</title><url>http://blog.cloudera.com/blog/2013/01/how-to-do-apache-flume-performance-tuning-part-1/</url><snippet>The post below was originally published via�blogs.apache.org�and is republished below for your reading pleasure. This is Part 1 in a series of articles about tuning the performance of Apache Flume, a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of event data. To kick off this series, I�d like to start off discussing some important Flume concepts that come into play when tuning your Flume flows for maximum performance: the channel and the transaction batch size. Setting Up a Data Flow Imagine you want to take a heavy stream of user activity from your application server logs and store that onto your Hadoop cluster for analytics. If you have a large deployment of application servers, you would likely want to build a fan-in architecture, where you are sending data from many nodes to relatively fewer nodes. If you are sending these user events one at a time, each time waiting for acknowledgment that it was delivered, your throughput may be limited by network latency. Naturally, you would want to batch up the events into larger transactions, so that you amortize the latency of the acknowledgment over a larger number of events and therefore get more throughput. Channels So, what happens if the storage tier goes down momentarily, as in the case of a network partition? What happens to the events if a Flume agent machine crashes? We still want to be able to serve our users on the application tier and retain our data somehow. In order to accomplish this, we need a buffering mechanism on each agent that allows it to store events in the case of downstream failures or slowdowns. In Flume, the channel is what persists events at each hop in the flow. Below is a diagram that illustrates where the channel sits in the architecture of a Flume agent. Memory Channel vs. File Channel An important decision to make when designing your Flume flow is what type of channel you want to use. At the time of this writing, the two recommended channels are the file channel and the memory channel. The file channel is a durable channel, as it persists all events that are stored in it to disk. So, even if the Java virtual machine is killed, or the operating system crashes or reboots, events that were not successfully transferred to the next agent in the pipeline will still be there when the Flume agent is restarted. The memory channel is a volatile channel, as it buffers events in memory only: if the Java process dies, any events stored in the memory channel are lost. Naturally, the memory channel also exhibits very low put/take latencies compared to the file channel, even for a batch size of 1. Since the number of events that can be stored is limited by available RAM, its ability to buffer events in the case of temporary downstream failure is quite limited. The file channel, on the other hand, has far superior buffering capability due to utilizing cheap, abundant hard disk space. Flume Event Batching As mentioned earlier, Flume can batch events. The batch size is the maximum number of events that a sink or client will attempt to take from a channel in a single transaction. Tuning the batch size trades throughput vs. latency and duplication under failure. With a small batch size, throughput decreases, but the risk of event duplication is reduced if a failure were to occur. With a large batch size, you get much better throughput, but increased latency, and in the case of a transaction failure, the number of possible duplicates increases. Transactions are a critical concept in Flume, because the delivery and durability guarantees made by channels only take effect at the end of each successful transaction. For example, when a source receives or generates an event, in order to store that event into a channel a transaction must first be opened on that channel. Within the transaction, the source puts up to the batch size number of events into the channel, and on success commits the transaction. A sink must go through the same process of operating within a transaction when taking events from a channel. Batch size is configured at the sink level. The larger the batch, the faster the channels operate (but there is a caveat). In the case of the file channel, this speed difference is because all buffers are flushed and then synced to disk when each transaction is committed. A disk sync is a time-consuming operation (it may take several milliseconds), but it is required to ensure the durability of the data. Likewise, with the memory channel, there is a memory synchronization step when each transaction is committed. Of course, memory synchronization is much faster than a disk sync. For more information on the inner workings of the file channel, please see Brock Noland’s�article about the Apache Flume File Channel. The downside of using a large batch size is that if there is some type of failure in the middle of a transaction, such as a downstream host or network failure, there is a possibility of duplicates being created. So, for example, if you set your batch size to 1000, and the machine you are writing to goes offline, duplicates may be generated in groups of up to 1000. This may occur in special cases, for example, if the events got written to the downstream machine but then the connection failed before it could acknowledge that it had received them. However, duplicate events will only appear in exceptional circumstances. Choosing a Batch Size To squeeze all the performance possible out of a Flume system, batch sizes should be tuned with care through experimentation. While I will get into this in more detail in the follow-up to this post, here are some rules of thumb for selecting batch sizes. Start off with batch sizes equal to the sum of the batch sizes of the input streams coming into that tier. For example, if you have a downstream Flume agent running an Avro source with 10 upstream agents sending events via Avro sinks using a batch size of 100 each, consider starting that downstream agent with a batch size of 1,000. Tune / experiment from there. If you find yourself setting the batch size very high (say, higher than 10,000) then consider adding another Sink instead, in order to increase the parallelism (each sink typically runs on its own thread). Say you were going to use one HDFS sink with a batch size of 20,000. Experiment with using 2 HDFS sinks with batch sizes of 5,000 or 10,000 to see if that helps more. Prefer the lowest batch size that gives you acceptable performance. Monitor the steady-state channel sizes to get more tuning insight (more on this in the next article). Different Batch Sizes in Different Situations Due to the performance / duplication tradeoff of the batch size parameter, I often see varying batch size settings depending on the use case. In the case of using Flume to write to HBase using the HBase Sink, incrementing counters, a smaller batch size on the order of 100 is often used to reduce the impact in case of hiccups in the system. On the other hand, with an HDFS sink, in order to get maximum throughput, I see people running with batch sizes of 1,000 or even 10,000, since typically they can easily run map-reduce jobs to de-duplicate the data at processing time. Note that when writing large events with large batch sizes to HDFS, often other parameters need to be increased as well. One such parameter is hdfs.callTimeout, which may be increased to 60 seconds or more to account for the long tail of occasional higher-latency calls to HDFS. At the other end of the spectrum, in cases where batching events at the application (Flume client) tier is not possible, the memory channel is often used at the collector-tier Flume agent (or a localhost agent on the app servers) to get acceptable performance with a batch size of 1, while using larger batch sizes and file channels in the downstream agents in order to get most of the benefits of durability there. For the best performance, however, all tiers including the client/application tier would perform some level of batching. (Please see above diagram for an illustration of the tiers referenced in this scenario.) Configuration Parameters and Gotchas The actual parameter used to set the batch size varies between sinks, but for most sinks it�s simply called batchSize or batch-size. For the HDFS sink, it�s actually called hdfs.batchSize for historical reasons; I recommend setting hdfs.txnEventMax to the same value as hdfs.batchSize for simplicity. Historically, in the HDFS sink, the number of events taken in a single transaction can be different from the number of events written to HDFS before a sync() operation; In practice, there is little reason these should not be set to the same value. One potentially confusing gotcha when tuning the batch size on a sink is that it must be less than or equal to the transactionCapacity set on the corresponding channel. The transactionCapacity should be set to the value of the largest batch size that will be used to store or remove events from that channel. tl;dr: Below is a �cheat sheet� batch size tuning summary for your convenience. Please note that these are just starting points for tuning. Sink Type Config parameter Typical value Avro batch-size 100 HDFS hdfs.batchSize, hdfs.txnEventMax 1000 HBaseSink batchSize 100 AsyncHBaseSink batchSize 100 That�s all I have space for in one blog post. Please leave feedback and questions in the comments. Look for another post in the future with tips on using Flume�s monitoring capabilities to take advantage of important information which can aid you in your quest for optimum performance. Mike Percy is a software engineer at Cloudera and an Apache Flume committer.</snippet></document><document id="305"><title>Apache Hadoop in 2013: The State of the Platform</title><url>http://blog.cloudera.com/blog/2013/01/apache-hadoop-in-2013-the-state-of-the-platform/</url><snippet>For several good reasons, 2013 is a Happy New Year for Apache Hadoop enthusiasts. In 2012, we saw continued progress on developing the next generation of the MapReduce processing framework (MRv2), work that will bear fruit this year. HDFS experienced major progress toward becoming a lights-out, fully enterprise-ready distributed filesystem with the addition of high availability features and increased performance. And a hint of the future of the Hadoop platform was provided with the Beta release of Cloudera Impala, a real-time query engine for analytics across HDFS and Apache HBase data. Let’s look at the highlights of the 2012 developments around projects supported by Cloudera. Apache Hadoop Releases The Hadoop 1 code line produced four bug fix releases and two minor releases (1.1.0 and 1.1.1). Among the new features was the addition of snappy compression/decompression. Hadoop 2 saw a renaming from 0.23 and four bug fix releases. Major elements of the releases: Support/integration for HBase, Pig, Oozie, and other Hadoop family members HDFS High Availability (HA) HDFS Federation Performance enhancements in both MapReduce and HDFS The 0.23 code line (Yahoo only) saw four minor updates. That code line does not include HDFS HA. Hadoop Family Releases Apache ZooKeeper�(distributed lock manager) produced five bug fix releases. Apache HBase (distributed key-value store on HDFS) offered two major releases and five minor releases: 0.92 – Security, coprocessors, a new and improved storage format, distributed log splitting 0.94 � Performance, MultiGet functionality for increments and appends, online automated table and region repair Apache Avro�(data serialization) saw six bug fix releases. Apache Hive (SQL-like interface to Hadoop) had one minor release and one major release: 0.9 – Access primitive binary types in HBase, BETWEEN, several useful UDFs Apache Mahout�(machine learning with Hadoop) produced two major releases: 0.6 – Implementations of AVF algorithm, Lucene filter for Collocations, Conjugate Gradient for solving large linear systems, Online Passive Aggressive learner, Random Walk with Restarts, and many more 0.7 – Outlier removal capability in K-Means, Fuzzy K, Canopy and Dirichlet Clustering, New Clustering implementation for K-Means, Implicit Alternating Least Squares SVD Apache Pig�(data flow language for Hadoop) had one minor and one major release: 0.10 – boolean datatype, nested cross/foreach, JRuby UDF, limit by expression, split default destination, tuple/bag/map syntax support and map-side aggregation Apache Hama (bulk synchronous parallel computing for e.g. matrix, graph, and network algorithms) graduated from the Apache Incubator and provided four major releases. Major additions: Streaming, K-Means, gradient descent BSP, Sparse Matrix-Vector multiplication, partitioned files Apache Whirr�(libraries for running Cloud services) had one major and one minor release: 0.8 – Support EC2 Cluster Compute Groups for Hadoop, CloudStack; Solr as a service Apache Flume�(streaming data into Hadoop) graduated from the Incubator and produced four minor releases of the rewritten high performance Flume NG Apache Bigtop (packaging, deployment, and test framework for Hadoop) graduated from the Incubator and had three major releases: 0.3 – Apache Hadoop 1.0 0.4 – bootable ISO, box grinder appliance, HDFS HA name nodes, Apache Giraph, Hue 0.5 – Apache Solr, Apache Crunch (incubating), DataFu Apache Giraph�(graph processing with Hadoop) graduated to a Top Level Project after releasing 0.1 as an Incubator project. Apache HCatalog�(extension of Hive MetaStore) released 0.4.0 as an Incubator project. Apache Oozie (workflow management for MapReduce, Hive, Pig, and other Hadoop jobs) graduated from the Incubator and provided one bug fix release and two minor releases: 3.2.0 – Hive, Sqoop, and Shell actions, Kerberos SPNEGO authentication 3.3.0 – Direct submission of MapReduce jobs, parameterization in workflow and coordinator jobs Apache Sqoop�(data transfer between Hadoop and relational databases) graduated from the Incubator and did three bug fix releases along with a first cut of the next-generation Sqoop 2 (client-server). Apache Crunch�(Java library for MapReduce pipelines) provided two minor releases as an Incubator project: 0.3 – Map-side joins 0.4 – Bloom filters, read from database, launch pipeline from a REPL Hue�(Web interface to Hadoop, Hive, Impala, Oozie, Pig) produced a major release 2.0 and a minor release 2.1 on Github: Redesigned full-page paradigm, LDAP authentication, per-application authorization, Oozie workflow/coordinator dashboard and designer, localization for eight languages Hadoop-related Conferences 2012 saw the first ever HBase conference – HBaseCon in San Francisco in May – with over 600 participants. Hadoop World merged with Strata East and had 2,500 attendees – almost twice the number of Strata East last year – and it sold out in advance. Hadoop Summit went from over 1,600 in 2011 to over 2,100 in 2012. Hadoop was featured at many other conferences during the year, including GigaOM Structure, CeBIT Big Data, DataWeek, Berlin BuzzWords, ApacheCon, Cloud Computing Expo, OSCON, Strange Loop and CloudCon. Other Hadoop News Hadoop won the “Duke’s Choice” award for “extreme innovation” in Java. 10gen announced support for Hadoop, as did VMware/SpringSource, Splunk, Revolution Analytics, SAS, TIBCO, QlikTech�and others. Cloudera announced the public Beta of�Impala, the first real-time SQL query interface to HDFS and HBase data. Other Hadoop Indicators The percentage of all job postings analyzed by�Indeed that mentioned Hadoop almost doubled again over 2011, as it did the year before. The Hadoop family mailing lists saw about 101k messages in 2011, 129k in 2012 (going by�markmail.org). The �Powered By� list at�http://wiki.apache.org/hadoop/PoweredBy went from 157 to 165 entries during the year. Ten new committers were added to core Hadoop in 2012, many more to the various Hadoop family projects. Rob runs the Platform Engineering team at Cloudera.�</snippet></document><document id="306"><title>Data Hacking Day with Cloudera (Feb. 25, Palo Alto)</title><url>http://blog.cloudera.com/blog/2013/01/data-hacking-day-with-cloudera-feb-25-palo-alto/</url><snippet>(Update 2/6/2013 – Sorry, this event is sold out!) With Strata Conference�2013 coming to town (Feb. 26-28, in Santa Clara, Calif.), we thought it would be a great opportunity to open our Palo Alto office’s doors for a pre-conference “Data Hacking Day” on Monday, Feb. 25! Participants will use�Cloudera Impala, the open-source, real-time query engine for Apache Hadoop, to hack on a rich public data set. After forming teams, you’ll compete to see whose project will earn enough votes to win the�data-hacking trophy�for the day. All members of the winning team will get free hard copies of Eric Sammer’s coveted O’Reilly book,�Hadoop Operations. Pizza/snacks, refreshments, VMs, install support, and cluster time will all be provided. Plus, you’ll get a free Impala T-shirt! 12-1pm -�Lunch &amp; Introduction 1-1:30pm -�Installation &amp; Configuration 1:30-5pm -�Data Hacking 5-6pm -�Presentations &amp; Voting This event is capped at 80 registrations, so jump in!�</snippet></document><document id="307"><title>Get a Free Hadoop Operations Ebook with Administrator Training</title><url>http://blog.cloudera.com/blog/2013/01/get-a-free-hadoop-operations-ebook-with-administrator-training/</url><snippet>Start the year off with bigger questions by taking advantage of Cloudera University�s special offer for aspiring Hadoop administrators. All participants who complete a Cloudera Administrator Training for Apache Hadoop public course by the end of March 2013 will receive a free digital copy of Hadoop Operations by Eric Sammer. If you�ve been asked to maintain large and complex Hadoop clusters, this book is a must. In addition to providing practical guidance from an expert, Hadoop Operations�is also a terrific companion reference to the full Cloudera Administrator course. Cloudera�s three-day course provides administrators a comprehensive understanding of all the steps necessary to operate and manage Hadoop clusters. From installation and configuration through load balancing and tuning your cluster, Cloudera�s administration course has you covered. This course is appropriate for system administrators and others who will be setting up or maintaining a Hadoop cluster. Basic Linux experience is a prerequisite, but prior knowledge of Hadoop is not required. Upon completion of the course, attendees also receive a voucher for a Cloudera Certified Administrator for Apache Hadoop (CCAH) exam. Certification is a great differentiator; it helps establish individuals as leaders in their field, providing customers with tangible evidence of skills and expertise. Register today!</snippet></document><document id="308"><title>Meet the Engineer: Marcel Kornacker</title><url>http://blog.cloudera.com/blog/2013/01/meet-the-engineer-marcel-kornacker/</url><snippet>In this installment of “Meet the Engineer”, meet Marcel Kornacker, the architect of the Cloudera Impala open-source real-time query engine for Apache Hadoop. What do you do at Cloudera? I�m a tech lead at Cloudera, working on the Cloudera Impala team. And although it�s not in my formal title, I�m also the architect of Impala. What that means in practice is that I have the very enviable but demanding job of not only creating Impala requirements, but also delivering the code that meets them! Why do you enjoy your job? Fundamentally, I just really enjoy building new things. Impala is a perfect example; I�ve had the opportunity to design, and build, an entirely new code base from the ground floor and up. But it gets even better, because at Cloudera I also have the opportunity to speak with customers and users about their needs and requests with respect to Impala, and then bring that feedback back into the roadmap. At most other software companies, engineers and even architects are usually completely isolated from that process. What is your favorite thing about Hadoop? Hadoop makes enterprise data storage, which is traditionally quite expensive, a commodity. If you�re a data analyst or application developer, you have to love that, because with Hadoop you can throw more stuff – questions or application logic – at more data than you ever could before, and do it relatively cheaply. Hadoop just allows you to solve bigger problems, or as Cloudera would put it, ask bigger questions. What inspired you to design Impala? Before coming to Cloudera, I worked at Google – my last role was as tech lead of the query engine for the Google F1 distributed RDBMS. This was simply a fascinating project that opened my eyes to new possibilities for querying massive amounts of data in a very scalable manner. At the same time, F1 was proprietary to Google, so the opportunity to bring those discoveries to the outside world was very limited. The motivation to help a much broader audience take advantage of these fairly advanced concepts is really what inspired my Impala work. At what age did you become interested and programming, and why? I think I was around 15 when I first started programming – initially in Pascal, and later on in C. The thing that appealed to me about it was that you could practically create something out of nothing. Unlike, say, an architect or mechanical engineer, you�re not constrained by the laws of physics or the presence of construction material. You�re just encoding ideas. Furthermore, I discovered I really enjoyed the design process – to break an idea down into its components, and to have to articulate those component ideas in a programming language, which is, compared to normal prose, a fairly rigid formalism. Join Marcel this Thursday, Jan. 10, at 11am PT for a technical webinar about Cloudera Impala. Marcel will present a technical overview of Impala from the user’s perspective, including Impala’s architecture and implementation. He will also provide a comparison of Impala with Apache Hive, commercial MapReduce alternatives, and traditional data warehouse infrastructure.</snippet></document><document id="309"><title>A Guide to Python Frameworks for Hadoop</title><url>http://blog.cloudera.com/blog/2013/01/a-guide-to-python-frameworks-for-hadoop/</url><snippet>I recently joined Cloudera after working in computational biology/genomics for close to a decade. My analytical work is primarily performed in Python, along with its fantastic scientific stack. It was quite jarring to find out that the Apache Hadoop ecosystem is primarily written in/for Java. So my first order of business was to investigate some of the options that exist for working with Hadoop from Python. In this post, I will provide an unscientific, ad hoc review of my experiences with some of the Python frameworks that exist for working with Hadoop, including: Hadoop Streaming mrjob dumbo hadoopy pydoop and others Ultimately, in my analysis, Hadoop Streaming is the fastest and most transparent option, and the best one for text processing. mrjob is best for rapidly working on Amazon EMR, but incurs a significant performance penalty. dumbo is convenient for more complex jobs (objects as keys; multistep MapReduce) without incurring as much overhead as mrjob, but it’s still slower than Streaming. Read on for implementation details, performance comparisons, and feature comparisons. Toy Problem Definition To test out the different frameworks, we will not be doing “word count”. Instead, we will be transforming the Google Books Ngram data. An n-gram is a synonym for a tuple of n words. The n-gram data set provides counts for every single 1-, 2-, 3-, 4-, and 5-gram observed in the Google Books corpus grouped by year. Each row in the n-gram data set is composed of 3 fields: the n-gram, the year, and the number of observations. (You can explore the data interactively here.) We would like to aggregate the data to count the number of times any pair of words are observed near each other, grouped by year. This would allow us to determine if any pair of words are statistically near each other more often than we would expect by chance. Two words are “near” if they are observed within 4 words of each other. Or equivalently, two words are near each other if they appear together in any 2-, 3-, 4-, or 5-gram. So a row in the resulting data set would be comprised of a 2-gram, a year, and a count. There is one subtlety that must be addressed. The n-gram data set for each value of n is computed across the whole Google Books corpus. In principle, given the 5-gram data set, I could compute the 4-, 3-, and 2-gram data sets simply by aggregating over the correct n-grams. For example, if the 5-gram data set contains (the, cat, in, the, hat)       1999     20
(the, cat, is, on, youtube)    1999     13
(how, are, you, doing, today)  1986   5000
   then we could aggregate this into 2-grams which would result in records like (the, cat)  1999    33      // i.e., 20 + 13
   However, in practice, Google only includes an n-gram if it is observed more than 40 times across the entire corpus. So while a particular 5-gram may be too rare to meet the 40-occurrence threshold, the 2-grams it is composed of may be common enough to break the threshold in the Google-supplied 2-gram data. For this reason, we use the 2-gram data for words that are next to each other, the 3-gram data for pairs of words that are separated by one word, the 4-gram data for pairs of words that are separated by 2 words, etc. In other words, given the 2-gram data, the only additional information the 3-gram data provide are the outermost words of the 3-gram. In addition to being more sensitive to potentially rare n-grams, using only the outermost words of the n-grams helps ensure we avoid double counting. In total, we will be running our computation on the combination of 2-, 3-, 4-, and 5-gram data sets. The MapReduce pseudocode to implement this solution would look like so: def map(record):
    (ngram, year, count) = unpack(record)
    // ensure word1 has the lexicographically first word:
    (word1, word2) = sorted(ngram[first], ngram[last])
    key = (word1, word2, year)
    emit(key, count)

def reduce(key, values):
    emit(key, sum(values))
   Hardware These MapReduce jobs are executed on a ~20 GB random subset of the data. The full data set is split across 1500 files; we select a random subset of the files using this script. The filenames remain intact, which is important because the filename identifies the value of n in the n-grams for that chunk of data. The Hadoop cluster comprises five virtual nodes running CentOS 6.2 x64, each with 4 CPUs, 10 GB RAM, 100 GB disk, running CDH4. The cluster can execute 20 maps at a time, and each job is set to run with 10 reducers. The software versions I worked with on the cluster were as follows: Hadoop: 2.0.0-cdh4.1.2 Python: 2.6.6 mrjob: 0.4-dev dumbo: 0.21.36 hadoopy: 0.6.0 pydoop: 0.7 (PyPI) and the latest version on git repository Java: 1.6 Implementations Most of the Python frameworks wrap Hadoop Streaming, while others wrap Hadoop Pipes�or implement their own alternatives. Below, I will discuss my experience with a number of tools for using Python to write Hadoop jobs, along with a final comparison of performance and features. One of the features I am interested in is the ease of getting up and running, so I did not attempt to optimize the performance of the individual packages. As with every large data set, there are bad records. We check for a few kinds of errors in each record including missing fields and wrong n-gram size. For the latter case, we must know the name of the file that is being processed in order to determine the expected n-gram size. All the code is available in this GitHub repo. Hadoop Streaming Hadoop Streaming is the canonical way of supplying any executable to Hadoop as a mapper or reducer, including standard Unix tools or Python scripts. The executable must read from stdin and write to stdout using agreed-upon semantics. One of the disadvantages of using Streaming directly is that while the inputs to the reducer are grouped by key, they are still iterated over line-by-line, and the boundaries between keys must be detected by the user. Here is the code for the mapper: #! /usr/bin/env python

import os
import re
import sys

# determine value of n in the current block of ngrams by parsing the filename
input_file = os.environ['map_input_file']
expected_tokens = int(re.findall(r'([\d]+)gram', os.path.basename(input_file))[0])

for line in sys.stdin:
    data = line.split('\t')

    # perform some error checking
    if len(data) &lt; 3:
        continue

    # unpack data
    ngram = data[0].split()
    year = data[1]
    count = data[2]

    # more error checking
    if len(ngram) != expected_tokens:
        continue

    # build key and emit
    pair = sorted([ngram[0], ngram[expected_tokens - 1]])
    print &gt;&gt;sys.stdout, "%s\t%s\t%s\t%s" % (pair[0], pair[1], year, count)
   And here is the reducer: #! /usr/bin/env python

import sys

total = 0
prev_key = False
for line in sys.stdin:
    data = line.split('\t')
    curr_key = '\t'.join(data[:3])
    count = int(data[3])

    # found a boundary; emit current sum
    if prev_key and curr_key != prev_key:
        print &gt;&gt;sys.stdout, "%s\t%i" % (prev_key, total)
        prev_key = curr_key
        total = count
    # same key; accumulate sum
    else:
        prev_key = curr_key
        total += count

# emit last key
if prev_key:
    print &gt;&gt;sys.stdout, "%s\t%i" % (prev_key, total)
   Hadoop Streaming separates the key and value with a tab character by default. Because we also separate the fields of our key with tab characters, we must tell Hadoop that the first three fields are all part of the key by passing these options: -jobconf stream.num.map.output.key.fields=3
-jobconf stream.num.reduce.output.key.fields=3
   The command to execute the Hadoop job is hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.1.2.jar \
        -input /ngrams \
        -output /output-streaming \
        -mapper mapper.py \
        -combiner reducer.py \
        -reducer reducer.py \
        -jobconf stream.num.map.output.key.fields=3 \
        -jobconf stream.num.reduce.output.key.fields=3 \
        -jobconf mapred.reduce.tasks=10 \
        -file mapper.py \
        -file reducer.py
   Note that the files mapper.py and reducer.py must be specified twice on the command line: the first time points Hadoop at the executables, while the second time tells Hadoop to distribute the executables around to all the nodes in the cluster. Hadoop Streaming is clean and very obvious/precise about what is happening under the hood. In contrast, the Python frameworks all perform their own serialization/deserialization that can consume additional resources in a non-transparent way. Also, if there is a functioning Hadoop distribution, then Streaming should just work, without having to configure another framework on top of it. Finally, it’s trivial to send Unix commands and/or Java classes as mappers/reducers. The disadvantage of Streaming is that everything must be done manually. The user must decide how to encode objects as keys/values (e.g., as JSON objects). Also, support for binary data is not trivial. And as mentioned above, the reducer must keep track of key boundaries manually, which can be prone to errors. mrjob mrjob is an open-source Python framework that wraps Hadoop Streaming and is actively developed by Yelp. Since Yelp operates entirely inside Amazon Web Services, mrjob’s integration with EMR is incredibly smooth and easy (using the boto package). mrjob provides a pythonic API to work with Hadoop Streaming, and allows the user to work with any objects as keys and mappers. By default, these objects are serialized as JSON objects internally, but there is also support for pickled objects. There are no other binary I/O formats available out of the box, but there is a mechanism to implement a custom serializer. Significantly, mrjob appears to be very actively developed, and has great documentation. As with all the Python frameworks, the implementation looks like pseudocode: #! /usr/bin/env python

import os
import re

from mrjob.job import MRJob
from mrjob.protocol import RawProtocol, ReprProtocol

class NgramNeighbors(MRJob):

    # mrjob allows you to specify input/intermediate/output serialization
    # default output protocol is JSON; here we set it to text
    OUTPUT_PROTOCOL = RawProtocol

    def mapper_init(self):
        # determine value of n in the current block of ngrams by parsing filename
        input_file = os.environ['map_input_file']
        self.expected_tokens = int(re.findall(r'([\d]+)gram', os.path.basename(input_file))[0])

    def mapper(self, key, line):
        data = line.split('\t')

        # error checking
        if len(data) &lt; 3:
            return

        # unpack data
        ngram = data[0].split()
        year = data[1]
        count = int(data[2])

        # more error checking
        if len(ngram) != self.expected_tokens:
            return

        # generate key
        pair = sorted([ngram[0], ngram[self.expected_tokens - 1]])
        k = pair + [year]

        # note that the key is an object (a list in this case)
        # that mrjob will serialize as JSON text
        yield (k, count)

    def combiner(self, key, counts):
        # the combiner must be separate from the reducer because the input
        # and output must both be JSON
        yield (key, sum(counts))

    def reducer(self, key, counts):
        # the final output is encoded as text
        yield "%s\t%s\t%s" % tuple(key), str(sum(counts))

if __name__ == '__main__':
    # sets up a runner, based on command line options
    NgramNeighbors.run()
   mrjob is only required to be installed on the client node where the job is submitted. Here are the commands to run it: export HADOOP_HOME="/usr/lib/hadoop-0.20-mapreduce"
./ngrams.py -r hadoop --hadoop-bin /usr/bin/hadoop --jobconf mapred.reduce.tasks=10 -o hdfs:///output-mrjob hdfs:///ngrams
   Writing MapReduce jobs is incredibly intuitive and simple. However, there is a significant cost incurred by the internal serialization scheme. A binary scheme would most likely need to be implemented by the user (e.g., to support typedbytes). There are also some built-in utilities for log file parsing. Finally, mrjob allows the user to write multi-step MapReduce workflows, where intermediate output from one MapReduce job is automatically used as input into another MapReduce job. (Note: The rest of the implementations are all highly similar, aside from package-specific implementation details. They can all be found here.) dumbo dumbo is another Python framework that wraps Hadoop Streaming. It seems to enjoy relatively broad usage, but is not developed as actively as mrjob at this point. It is one of the earlier Python Hadoop APIs, and is very mature. However, its documentation is lacking, which makes it a bit harder to use. It performs serialization with typedbytes, which allows for more compact data transfer with Hadoop, and can natively read SequenceFiles or any other file type by specifying a Java InputFormat. In fact, dumbo enables the user to execute code from any Python egg or Java JAR file. In my experience, I had to manually install dumbo on each node of my cluster for it to work. It only worked if typedbytes and dumbo were built as Python eggs. Finally, it failed to run with a combiner, as it would terminate on MemoryErrors. The command to run the job with dumbo is dumbo start ngrams.py \
        -hadoop /usr \
        -hadooplib /usr/lib/hadoop-0.20-mapreduce/contrib/streaming \
        -numreducetasks 10 \
        -input hdfs:///ngrams \
        -output hdfs:///output-dumbo \
        -outputformat text \
        -inputformat text
   hadoopy hadoopy�is another Streaming wrapper that is compatible with dumbo. Similarly, it focuses on typedbytes serialization of data, and directly writes typedbytes to HDFS. It has a nice debugging feature, in which it can directly write messages to stdout/stderr without disrupting the Streaming process. It feels similar to dumbo, but the documentation is better. The documentation also mentions experimental Apache HBase integration. With hadoopy, there are two ways to launch jobs: launch requires Python/hadoopy to be installed on each node in the cluster, but has very little overhead after that. launch_frozen does not even require that Python is installed on the nodes, but it incurs a ~15 second penalty for PyInstaller to work. (It’s claimed that this can be somewhat mitigated by optimizations and caching tricks.) Jobs in hadoopy must be launched from within a Python program. There is no built-in command line utility. I launch hadoopy via the launch_frozen scheme using my own Python script: python launch_hadoopy.py
   After running it with launch_frozen, I installed hadoopy on all nodes and used the launchmethod instead. The performance was not significantly different. pydoop In contrast to the other frameworks, pydoop wraps Hadoop Pipes, which is a C++ API into Hadoop. The project claims that they can provide a richer interface with Hadoop and HDFS because of this, as well as better performance, but this is not clear to me. However, one advantage is the ability to implement a Python Partitioner, RecordReader, and RecordWriter. All input/output must be strings. Most importantly, I could not successfully build pydoop via pip or directly from source. Others happy is a framework for writing Hadoop jobs through Jython, but seems to be dead. Disco is a full-blown non-Hadoop reimplementation of MapReduce. Its core is written in Erlang, with the primary API in Python. It is developed at Nokia, but is much less used than Hadoop. octopy is a reimplementation of MapReduce purely in Python in a single source file. It is not intended for “serious” computation. Mortar is another option for working with Python that was just recently launched. Through a web app, the user can submit Apache Pig or Python jobs to manipulate data sitting in Amazon S3. There are several higher-level interfaces into the Hadoop ecosystem, such as Apache Hive and Pig. Pig provides the facility to write user-defined-functions with Python, but it appears to run them through Jython. Hive also has a Python wrapper called hipy. (Added Jan. 7 2013)�Luigi is a Python framework for managing multistep batch job pipelines/workflows. It is probably a bit similar to Apache Oozie but it has some built-in functionality for wrapping Hadoop Streaming jobs (though it appears to be a light wrapper). Luigi has a nice feature of extracting out a Python traceback if your Python code crashes a job, and also has nice command-line features. It has a great introductory README file but seems to lack comprehensive reference documentation. Luigi is actively developed and used at Spotify for running many jobs there. Native Java Finally, I implemented the MR job using the new Hadoop Java API. After building it, I ran it like so: hadoop jar /root/ngrams/native/target/NgramsComparison-0.0.1-SNAPSHOT.jar NgramsDriver hdfs:///ngrams hdfs:///output-native
   A Note About Counters In my initial implementations of these MR jobs, I used counters to keep track of the number of bad records. In Streaming, this requires writing messages to stderr. It turns out this incurs a significant overhead: the Streaming job took 3.4x longer than the native Java job. The frameworks were similarly penalized. Performance Comparison The MapReduce job was also implemented in Java as a baseline for performance. All values for the Python frameworks are ratios relative to the corresponding Java performance. Java is obviously the fastest, with Streaming taking 50% longer, and the Python frameworks taking substantially longer still. From a profile of the mrjob mapper, it appears a substantial amount of time is spent in serialization/deserialization. The binary formats in dumbo and hadoopy may ameliorate the problem. The dumbo implementation may have been faster if the combiner was allowed to run. Feature Comparison Mostly gleaned from the respective packages’ documentation or code repositories. Conclusions Streaming appears to be the fastest Python solution, without any magic under the hood. However, it requires care when implementing the reducer, and also when working with more complex objects. All the Python frameworks look like pseudocode, which is a huge plus. mrjob seems highly active, easy-to-use, and mature. It makes multistep MapReduce flows easy, and can easily work with complex objects. It also works seamlessly with EMR. But it appears to perform the slowest. The other Python frameworks appear to be somewhat less popular. Their main advantage appears to be built-in support for binary formats, but this is probably something that can be implemented by the user, if it matters. So for the time being: Prefer Hadoop Streaming if possible. It’s easy enough, as long as care is taken with the reducer. Prefer mrjob to rapidly get on Amazon EMR, at the cost of significant computational overhead. Prefer dumbo for more complex jobs that may include complex keys and multistep MapReduce workflows; it’s slower than Streaming but faster than mrjob. If you have your own observations based on practice, or for that matter any errors to point out, please do so in comments. Uri Laserson is a data scientist at Cloudera. &gt; Watch a video of Uri’s “Guide to Python Frameworks for Hadoop” presentation from QCon NY 2013.</snippet></document><document id="310"><title>Apache Bigtop 0.5.0 Has Been Released</title><url>http://blog.cloudera.com/blog/2013/01/apache-bigtop-0-5-0-has-been-released/</url><snippet>The following post was originally published via apache.org. We bring it to you here in a slightly modified form. We hope you all had a wonderful and restful holiday season and wish you all the very best for 2013!�We are pleased to announce the release of Apache Bigtop 0.5.0!� Apache Bigtop, as many of you might already know, is a project for the development of packaging and tests for the Apache Hadoop ecosystem. You can learn more about it by reading one of our earlier blog posts on Apache Blogs. Bigtop 0.5.0 is our first release as an Apache top level project (TLP). It is based on Hadoop 2.0.2-alpha with a larger selection of fully integrated Hadoop ecosystem components (Solr, Crunch, etc.). We are pleased to offer the following as a part of our Bigtop 0.5.0 distribution: Apache Zookeeper 3.4.5 Apache Flume 1.3.0 Apache HBase 0.94.2 Apache Pig 0.10.0 Apache Hive 0.9.0 Apache Sqoop 1.4.2 Apache Oozie 3.3.0 Apache Whirr 0.8.1 Apache Mahout 0.7 Apache Solr (SolrCloud) 4.0.0 Apache Crunch (incubating) 0.4.0 LinkedIn DataFu 0.0.4 Cloudera Hue 2.1.0 The list of supported Linux platforms has expanded to include: CentOS/RHEL 5 and 6 Fedora 16 and 17 SuSE Linux Enterprise 11 OpenSUSE 12.2 Ubuntu LTS Lucid (10.04) and Precise (12.04) Ubuntu Quantal (12.10) There are a lot of newly introduced features in this release: SolrCloud, a brand new Hue UI and the latest and greatest Flume, to name a few. We would like to invite everybody to give the Bigtop 0.5.0 binary distribution a try. All you have to do is to pick your favorite Linux distribution, follow our wiki instructions�and you will have your first pseudo-distributed cluster computing Pi in no time. If you’re thinking about deploying Bigtop to a fully-distributed cluster you might find our Puppet code to be useful — after all we use it all the time ourselves to test Bigtop. There is brief documentation on how to run our Puppet recipes in a master-less puppet configuration, but a typical Puppet master setup should work as well. Bigtop plays an important role in CDH which leverages all its packaging code from Bigtop. Cloudera takes pride in leveraging open source packaging code and contributing the same back to the community. Whatever you do, don�t forget to check us out at Apache and consider getting involved. Bigtop is a community-driven effort and we need your help. Of course, above all we need you to use Bigtop and give us your the feedback. Happy Big Data discoveries, Your faithful and tireless Bigtop development team!  </snippet></document><document id="311"><title>The Dynamic Workflow Builder in Hue</title><url>http://blog.cloudera.com/blog/2013/01/dynamic-workflow-builder-in-hue/</url><snippet>Hue is a web interface for Apache Hadoop that makes common Hadoop tasks such as running MapReduce jobs, browsing HDFS, and creating Apache Oozie workflows, easier. In this post, we�re going to focus on the dynamic workflow builder that Hue provides for Oozie that will be released in Hue 2.2.0 (For a high-level description of Oozie integration in Hue, see this blog post). The dynamic workflow editor is feature packed and emphasizes usability. The heavy lifting is delegated to the client via Javascript to provide a more �dynamic� experience for the user. This is achieved by using the MVVM (Model View View-Model) design pattern with KnockoutJS and event handling with JQuery. In effect, moving, creating, modifying, and deleting actions will be much easier. Also, the workflow editor will support Oozie�s Decision node. Basic Operations on Actions The experience of performing basic operations on actions has been simplified (IE: Creating, updating, and deleting a node). Nodes can be added seamlessly by clicking on the desired node type and filling out a few parameters. The parameters shown will vary depending on the type of node being added. The node�s attributes can be modified in the future by clicking on the name of the node. Whenever a node is added or updated, the node will be validated before the popup will be closed. Also, a node can be removed by clicking on the �x� symbol within the node: Drag and Drop Actions in the Workflow The dynamic workflow editor has three main movement functions: action placement, forking, and splitting by decision. Here�s a quick demo of each of the main movement functions in the new workflow editor: Basic Node Placement Actions can be placed almost anywhere in the workflow by simply dragging the action to the desired location. This can be in between two other actions, at the top of the workflow, or at the very bottom of the workflow. Forking Fork nodes split the path of execution into multiple paths of execution. These paths of execution run concurrently. Fork nodes can be created by dragging an action onto another action. More paths of execution can be added by dropping an action to the fork node itself. Fork nodes are immobile, but can be removed by moving all actions out of the split paths of execution. Decision Node Support Decision nodes choose a path of execution based on conditions specified in the node itself. To create a decision node, users will need to convert existing Fork nodes by clicking on a Fork node�s name. The conditions can be added, or modified, by editing the node itself and providing expressions in the form ${ } as shown below. Summary The soon to be released workflow editor is much more usable and flexible. It allows users to drag actions to any location in the workflow, trivially create forks, and perform basic operations on actions. It also introduces the decision node, which can be used to conditionally split the paths of execution. Hue will be seeing a slew of updates in the near future. The workflow editor itself will support more actions and provide a better experience when managing coordinators. Have any suggestions? Feel free to tell us what you think through hue-user.</snippet></document><document id="312"><title>How-to: Use a SerDe in Apache Hive</title><url>http://blog.cloudera.com/blog/2012/12/how-to-use-a-serde-in-apache-hive/</url><snippet>Apache Hive is a fantastic tool for performing SQL-style queries across data that is often not appropriate for a relational database. For example, semistructured and unstructured data can be queried gracefully via Hive, due to two core features: The first is Hive�s support of complex data types, such as structs, arrays, and unions, in addition to many of the common data types found in most relational databases. The second feature is the SerDe. What is a SerDe? The SerDe interface allows you to instruct Hive as to how a record should be processed. A SerDe is a combination of a Serializer and a Deserializer (hence, Ser-De). The Deserializer interface takes a string or binary representation of a record, and translates it into a Java object that Hive can manipulate. The Serializer, however, will take a Java object that Hive has been working with, and turn it into something that Hive can write to HDFS or another supported system. Commonly, Deserializers are used at query time to execute SELECT statements, and Serializers are used when writing data, such as through an INSERT-SELECT statement. In this article, we will examine a SerDe for processing JSON data, which can be used to transform a JSON record into something that Hive can process. Developing a SerDe To start, we can write a basic template for a SerDe, which utilizes the Hive serde2 API (org.apache.hadoop.hive.serde2). This API should be used in favor of the older serde API, which has been deprecated: package com.cloudera.hive.serde; import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Properties;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hive.serde.Constants;
import org.apache.hadoop.hive.serde2.SerDe;
import org.apache.hadoop.hive.serde2.SerDeException;
import org.apache.hadoop.hive.serde2.SerDeStats;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;

/**
* A template for a custom Hive SerDe
*/
public class BoilerplateSerDe implements SerDe {
�
�private StructTypeInfo rowTypeInfo;
�private ObjectInspector rowOI;
�private List&lt;String&gt; colNames;
�private List&lt;Object&gt; row = new ArrayList&lt;Object&gt;();

�/**
� * An initialization function used to gather information about the table.
� * Typically, a SerDe implementation will be interested in the list of
� * column names and their types. That information will be used to help�
� * perform actual serialization and deserialization of data.
� */
�@Override
�public void initialize(Configuration conf, Properties tbl)
���� throws SerDeException {
�� // Get a list of the table's column names.
�� String colNamesStr = tbl.getProperty(Constants.LIST_COLUMNS);
�� colNames = Arrays.asList(colNamesStr.split(","));
��
�� // Get a list of TypeInfos for the columns. This list lines up with
�� // the list of column names.
�� String colTypesStr = tbl.getProperty(Constants.LIST_COLUMN_TYPES);
�� List&lt;TypeInfo&gt; colTypes =
������ TypeInfoUtils.getTypeInfosFromTypeString(colTypesStr);
��
�� rowTypeInfo =
������ (StructTypeInfo) TypeInfoFactory.getStructTypeInfo(colNames, colTypes);
�� rowOI =
������ TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(rowTypeInfo);
�}

�/**
� * This method does the work of deserializing a record into Java objects
� * that Hive can work with via the ObjectInspector interface.
� */
�@Override
�public Object deserialize(Writable blob) throws SerDeException {
�� row.clear();
�� // Do work to turn the fields in the blob into a set of row fields
�� return row;
�}

�/**
� * Return an ObjectInspector for the row of data
� */
�@Override
�public ObjectInspector getObjectInspector() throws SerDeException {
�� return rowOI;
�}

�/**
� * Unimplemented
� */
�@Override
�public SerDeStats getSerDeStats() {
�� return null;
�}

�/**
� * Return the class that stores the serialized data representation.
� */
�@Override
�public Class&lt;? extends Writable&gt; getSerializedClass() {
�� return Text.class;
�}

�/**
� * This method takes an object representing a row of data from Hive, and
� * uses the ObjectInspector to get the data for each column and serialize
� * it.
� */
�@Override
�public Writable serialize(Object obj, ObjectInspector oi)
���� throws SerDeException {
�� // Take the object and transform it into a serialized representation
�� return new Text();
�}
}
 Breaking this down a bit, the initialize() method is called only once and gathers some commonly-used pieces of information from the table properties, such as the column names and types. Using the type info of the row, you can instantiate an ObjectInspector for the row (ObjectInspectors are Hive objects that are used to describe and examine complex type hierarchies.) The two other important methods are serialize() and deserialize(), which do the namesake work of the SerDe. In a SerDe, the serialize() method takes a Java object representing a row of data, and converts that object into a serialized representation of the row. The serialized class is determined by the return type of getSerializedClass(). In the JSONSerDe, the serialize() method converts the object into a JSON string represented by a Text object. To do the serialization from Java into JSON, I�ve opted to use the Jackson JSON library, which allows me to convert a Java object to a JSON string with just a small amount of code: ObjectMapper mapper = new ObjectMapper();
// Let Jackson do the work of serializing the object
return new Text(mapper.writeValueAsString(deparsedObj));
 Jackson understands how to convert basic Java objects like Maps, Lists, and primitives into JSON strings. However, the Java object that is passed into the serialize() method is an internal Hive representation of a row, which Jackson can�t work with. The goal here is to use the ObjectInspector to interpret the Hive object, and convert it into a more basic Java representation. In the JSONSerDe code, this process is broken up into a number of methods. The control flow is fairly simple, so let�s just examine some of the interesting pieces: private Object deparseObject(Object obj, ObjectInspector oi) {
�switch (oi.getCategory()) {
�case LIST:
�� return deparseList(obj, (ListObjectInspector)oi);
�case MAP:
�� return deparseMap(obj, (MapObjectInspector)oi);
�case PRIMITIVE:
�� return deparsePrimitive(obj, (PrimitiveObjectInspector)oi);
�case STRUCT:
�� return deparseStruct(obj, (StructObjectInspector)oi, false);
�case UNION:
�� // Unsupported by JSON
�default:
�� return null;
�}
}
 The deparseObject()method is nothing more than a fork in the road. ObjectInspectors have a category, which will identify the underlying subtype of the inspector. private Object deparseList(Object obj, ListObjectInspector listOI) {
�List&lt;Object&gt; list = new ArrayList&lt;Object&gt;();
�List&lt;?&gt; field = listOI.getList(obj);
�ObjectInspector elemOI = listOI.getListElementObjectInspector();
�for (Object elem : field) {
�� list.add(deparseObject(elem, elemOI));
�}
�return list;
}
 In the deparseList() method, your goal is to translate a Hive list field into a Java array. In order to do this properly, you need to also deparse each of the list elements. Fortunately, you can obtain an ObjectInspector specifically for the list elements from a ListObjectInspector. You can follow this same pattern with all the other Hive data types to fully translate the object, and then let Jackson do the work of writing out a JSON object. The opposite of serialize() is deserialize(). The deserialize() method takes a JSON string, and converts it into a Java object that Hive can process. Again, you can use Jackson to do most of the heavy lifting. Jackson will convert a JSON record into a Java Map with just a couple lines of code: ObjectMapper mapper = new ObjectMapper();
  // This is really a Map&lt;String, Object&gt;. For more information about how
  // Jackson parses JSON in this example, see
  // http://wiki.fasterxml.com/JacksonDataBinding
  Map&lt;?,?&gt; root = mapper.readValue(blob.toString(), Map.class);
  When deserializing, you need information from Hive about what type of data each field contains. You can use the TypeInfo API similarly to how we used ObjectInspectors while serializing. Looking again at handling a list type: private Object parseList(Object field, ListTypeInfo fieldTypeInfo) {
  �ArrayList&lt;Object&gt; list = (ArrayList&lt;Object&gt;) field;
  �TypeInfo elemTypeInfo = fieldTypeInfo.getListElementTypeInfo();
  �
  �for (int i = 0; i &lt; list.size(); i++) {
  �� list.set(i, parseField(list.get(i), elemTypeInfo));
  �}

  �return list.toArray();
  }
 Also like ObjectInspectors, TypeInfos have subtypes. For a Hive list field, the TypeInfo is actually a ListTypeInfo, which we can use to also determine the type of the list elements. You can parse each list element one-by-one, and return the necessary array. Using the SerDe Tables can be configured to process data using a SerDe by specifying the SerDe to use at table creation time, or through the use of an ALTER TABLE statement. For example: ADD JAR /tmp/hive-serdes-1.0-SNAPSHOT.jar

CREATE EXTERNAL TABLE tweets (
  �...
  � retweeted_status STRUCT&lt;
  ��� text:STRING,
  ��� user:STRUCT&lt;screen_name:STRING,name:STRING&gt;&gt;,
  � entities STRUCT&lt;
  ��� urls:ARRAY&lt;STRUCT&lt;expanded_url:STRING&gt;&gt;,
  ��� user_mentions:ARRAY&lt;STRUCT&lt;screen_name:STRING,name:STRING&gt;&gt;,
  ��� hashtags:ARRAY&lt;STRUCT&lt;text:STRING&gt;&gt;&gt;,
  � text STRING,
  �...
  )
  PARTITIONED BY (datehour INT)
  ROW FORMAT SERDE 'com.cloudera.hive.serde.JSONSerDe'
  LOCATION '/user/flume/tweets';
 The bolded section of the above CREATE TABLE statement shows how a table is configured to use a SerDe. If the SerDe is not on the Hive classpath, it must be added at runtime using the ADD JARcommand. It should be noted that one limitation of the JSONSerDe is that the field names must match the JSON field names. JSON fields that are not present in the table will be ignored, and records that don�t have certain fields will return NULLs for any missing fields. As an example, the raw data that the above fields refer to looks like this: {
  �� "retweeted_status": {
  ����� "contributors": null,
  ����� "text": "#Crowdsourcing � drivers already generate traffic data for your smartphone to suggest alternative routes when a road is clogged. #bigdata",
  ����� "geo": null,
  ����� "retweeted": false,
  ����� "in_reply_to_screen_name": null,
  ����� "truncated": false,
  ����� "entities": {
  �������� "urls": [],
  ����� ���"hashtags": [
  ����������� {
  �������������� "text": "Crowdsourcing",
  �������������� "indices": [
  ����������������� 0,
  ����������������� 14
  �������������� ]
  ����������� },
  ����������� {
  �������������� "text": "bigdata",
  �������������� "indices": [
  ������ �����������129,
  ����������������� 137
  �������������� ]
  ����������� }
  �������� ],
  �������� "user_mentions": []
  ����� },
  ����� "in_reply_to_status_id_str": null,
  ����� "id": 245255511388336128,
  ����� "in_reply_to_user_id_str": null,
  ����� "source": "SocialOomph",
  ����� "favorited": false,
  ����� "in_reply_to_status_id": null,
  ����� "in_reply_to_user_id": null,
  ����� "retweet_count": 0,
  ����� "created_at": "Mon Sep 10 20:20:45 +0000 2012",
  ����� "id_str": "245255511388336128",
  ����� "place": null,
  ����� "user": {
  �������� "location": "Oregon, ",
  �������� "default_profile": false,
  �������� "statuses_count": 5289,
  �������� "profile_background_tile": false,
  �������� "lang": "en",
  �������� "profile_link_color": "627E91",
  �������� "id": 347471575,
  �������� "following": null,
  �������� "protected": false,
  �������� "favourites_count": 17,
  �������� "profile_text_color": "D4B020",
  �������� "verified": false,
  �������� "description": "Dad, Innovator, Sales Professional. Project Management Professional (PMP).� Soccer Coach,� Little League Coach� #Agile #PMOT - views are my own -",
  �������� "contributors_enabled": false,
  �������� "name": "Scott Ostby",
  �������� "profile_sidebar_border_color": "404040",
  �������� "profile_background_color": "0F0F0F",
  �������� "created_at": "Tue Aug 02 21:10:39 +0000 2011",
  �������� "default_profile_image": false,
  �������� "followers_count": 19005,
  �������� "profile_image_url_https": "https://si0.twimg.com/profile_images/1928022765/scott_normal.jpg",
  �������� "geo_enabled": true,
  �������� "profile_background_image_url": "http://a0.twimg.com/profile_background_images/327807929/xce5b8c5dfff3dc3bbfbdef5ca2a62b4.jpg",
  �������� "profile_background_image_url_https": "https://si0.twimg.com/profile_background_images/327807929/xce5b8c5dfff3dc3bbfbdef5ca2a62b4.jpg",
  �������� "follow_request_sent": null,
  �������� "url": "http://facebook.com/ostby",
  �������� "utc_offset": -28800,
  �������� "time_zone": "Pacific Time (US &amp; Canada)",
  �������� "notifications": null,
  �������� "friends_count": 13172,
  �������� "profile_use_background_image": true,
  �������� "profile_sidebar_fill_color": "1C1C1C",
  �������� "screen_name": "ScottOstby",
  �������� "id_str": "347471575",
  �������� "profile_image_url": "http://a0.twimg.com/profile_images/1928022765/scott_normal.jpg",
  �������� "show_all_inline_media": true,
  �������� "is_translator": false,
  �������� "listed_count": 45
  ����� },
  ����� "coordinates": null
  �� },
  �� "contributors": null,
  �� "text": "RT @ScottOstby: #Crowdsourcing � drivers already generate traffic data for your smartphone to suggest alternative routes when a road is� ...",
  �� "geo": null,
  �� "retweeted": false,
  �� "in_reply_to_screen_name": null,
  �� "truncated": false,
  �� "entities": {
  ����� "urls": [],
  ����� "hashtags": [
  �������� {
  ����������� "text": "Crowdsourcing",
  ����������� "indices": [
  �������������� 16,
  �������������� 30
  ����������� ]
  �������� }
  ����� ],
  ����� "user_mentions": [
  �������� {
  ����������� "id": 347471575,
  ����������� "name": "Scott Ostby",
  ����������� "indices": [
  �������������� 3,
  �������������� 14
  ����������� ],
  ����������� "screen_name": "ScottOstby",
  ����������� "id_str": "347471575"
  �������� }
  ����� ]
  �� },
  �� "in_reply_to_status_id_str": null,
  �� "id": 245270269525123072,
  �� "in_reply_to_user_id_str": null,
  �� "source": "web",
  �� "favorited": false,
  �� "in_reply_to_status_id": null,
  �� "in_reply_to_user_id": null,
  �� "retweet_count": 0,
  �� "created_at": "Mon Sep 10 21:19:23 +0000 2012",
  �� "id_str": "245270269525123072",
  �� "place": null,
  �� "user": {
  ����� "location": "",
  ����� "default_profile": true,
  ����� "statuses_count": 1294,
  ����� "profile_background_tile": false,
  ����� "lang": "en",
  ����� "profile_link_color": "0084B4",
  ����� "id": 21804678,
  ����� "following": null,
  ����� "protected": false,
  ����� "favourites_count": 11,
  ����� "profile_text_color": "333333",
  ����� "verified": false,
  ����� "description": "",
  ����� "contributors_enabled": false,
  ����� "name": "Parvez Jugon",
  ����� "profile_sidebar_border_color": "C0DEED",
  ����� "profile_background_color": "C0DEED",
  ����� "created_at": "Tue Feb 24 22:10:43 +0000 2009",
  ����� "default_profile_image": false,
  ����� "followers_count": 70,
  ����� "profile_image_url_https": "https://si0.twimg.com/profile_images/2280737846/ni91dkogtgwp1or5rwp4_normal.gif",
  ����� "geo_enabled": false,
  ����� "profile_background_image_url": "http://a0.twimg.com/images/themes/theme1/bg.png",
  ����� "profile_background_image_url_https": "https://si0.twimg.com/images/themes/theme1/bg.png",
  ����� "follow_request_sent": null,
  ����� "url": null,
  ����� "utc_offset": null,
  ����� "time_zone": null,
  ����� "notifications": null,
  ����� "friends_count": 299,
  ����� "profile_use_background_image": true,
  ����� "profile_sidebar_fill_color": "DDEEF6",
  ����� "screen_name": "ParvezJugon",
  ����� "id_str": "21804678",
  ����� "profile_image_url": "http://a0.twimg.com/profile_images/2280737846/ni91dkogtgwp1or5rwp4_normal.gif",
  ����� "show_all_inline_media": false,
  ����� "is_translator": false,
  ����� "listed_count": 7
  �� },
  �� "coordinates": null
  }
 Once the table is set up, querying complex data is as simple as SQL: hive&gt; SELECT * FROM tweets;
OK
id	created_at	source	favorited	retweet_count	retweeted_status	entities	text	user	in_reply_to_screen_name
245270269525123072	Mon Sep 10 21:19:23 +0000 2012	web	false	0	{"text":"#Crowdsourcing � drivers already generate traffic data for yoursmartphone to suggest alternative routes when a road is clogged. #bigdata","user":{"screen_name":"ScottOstby","name":"Scott Ostby"}}	{"urls":[],"user_mentions":[{"screen_name":"ScottOstby","name":"Scott Ostby"}],"hashtags":[{"text":"Crowdsourcing"}]}	RT @ScottOstby: #Crowdsourcing � drivers already generate trafficdata for your smartphone to suggest alternative routes when a road is  ...	{"screen_name":"ParvezJugon","name":"Parvez Jugon","friends_count":299,"followers_count":70,"statuses_count":1294,"verified":false,"utc_offset":null,"time_zone":null}	NULL
Time taken: 3.07 seconds
 If you�re interested in getting the code to use the JSONSerDe in a real environment, see the CDH Twitter Example on the Cloudera Github. Conclusion The SerDe interface is extremely powerful for dealing with data with a complex schema. By utilizing SerDes, any dataset can be made queryable through Hive. For a full use case involving the JSONSerDe discussed in this article, see Analyzing Twitter with Apache Hadoop. Jon Natkins is a Software Engineer at Cloudera, working on the Enterprise team.</snippet></document><document id="313"><title>Meet the Engineer: Nong Li</title><url>http://blog.cloudera.com/blog/2012/12/meet-the-engineer-nong-li/</url><snippet>In this installment of “Meet the Engineer”, meet Nong Li, a software engineer working on the open-source Cloudera Impala real-time query engine. What do you do at Cloudera? I’ve been working at Cloudera for a little over a year now and for the whole time, I’ve been working on Cloudera Impala. On Impala, I spend most of my time focusing on improving the performance of the query execution engine, working on the IO subsystem, JIT-compiling portions of the query execution, and working on expression evaluation and other performance-centric components. Why do you enjoy your job? Working on Impala is pretty awesome. From a engineering project perspective, it’s a new project with a new code base that tries to incorporate the latest technologies to solve a hard problem. Hard to beat that! I’ve always enjoyed working on performance engineering for as long as I’ve been programming, and it’s incredible that I get to do that on a daily basis. Working on performance really forces you to understand the entire system: the hardware, the OS, the components, and the code you are working on. I like understanding exactly what a line of code is doing when it’s run. The people we have at Cloudera are also really great. If you enjoy hanging out with your colleagues at and outside of work, it’s a great place to be. What is your favorite thing about Apache Hadoop? I like that Hadoop lets people tackle problems that were previously inaccessible. I also like that these problems are “real-world” problems, not just computer-science ones. The ecosystem is still fairly young, with new components constantly being built. Hadoop has already proven itself to be incredibly useful and I only see this getting more true in the future. What is your advice for someone who is interested in participating in any open-source project for the first time? Learn to take feedback. While this is true for all jobs, open-source projects make this more important. There are more parties involved with different priorities. Everyone wants to do the right thing and you just need to provide a sufficiently compelling reason for any proposal you make. At what age did you become interested and programming, and why? I first took a programming course junior year in high school, and although I enjoyed it, I didn’t think it was for me. I didn’t do any programming until I got to college. During my first semester, I took a bunch of courses in different subjects, and one of them was an introduction to computer science. It was a pretty big contrast to the other courses I was taking. Instead of spending most my time trying to remember stuff from lectures or books, I was trying to figuring new things out by doing stuff on my own. To this day, I enjoy it for the same reason. I’m always working on a problem I haven’t seen before.</snippet></document><document id="314"><title>How-to: Use the ShareLib in Apache Oozie</title><url>http://blog.cloudera.com/blog/2012/12/how-to-use-the-sharelib-in-apache-oozie/</url><snippet>As Apache Oozie, the workflow engine for Apache Hadoop, continues to receive wider adoption from our customers and the community, we�re seeing patterns with respect to the biggest challenges for users.�One such point of difficulty is setting up and using Oozie’s ShareLib for allowing JARs to be shared by different workflows. This blog post is intended to help you with those tasks.� Errors A missing or improperly installed ShareLib will cause some action types (DistCp, Streaming, Pig, Sqoop, and Hive) to fail.�In this case, you’ll typically see any of the following exceptions in the Oozie and JobTracker logs:   java.lang.ClassNotFoundException: org.apache.hadoop.tools.DistCp
  java.lang.NoClassDefFoundError: org/apache/pig/Main
  java.lang.ClassNotFoundException: org.apache.pig.Main
  java.lang.NoClassDefFoundError: org/apache/sqoop/Sqoop
  java.lang.ClassNotFoundException: org.apache.sqoop.Sqoop
  java.lang.NoClassDefFoundError: org/apache/hadoop/hive/cli/CliDriver
  java.lang.ClassNotFoundException: org.apache.hadoop.hive.cli.CliDriver
  java.lang.ClassNotFoundException: Class org.apache.hadoop.streaming.PipeMapRunner not found
  Before exploring these errors, let’s first discuss what the ShareLib is and how it works. Why Use the ShareLib? Suppose you have an Oozie workflow that runs a MapReduce action. You want to specify your own Mapper and Reducer classes, but how does Oozie know where to find those two classes?� There are two ways to let Oozie know about Mapper and Reducer classes or any other additional JARs required by your workflow. The first approach is based on the fact that a workflow typically consists of a job.properties file, a workflow.xml file, and an optional lib folder (and perhaps other files such as Pig scripts).�Usually, you�d place them in a folder like this:    �$ sudo -u oozie hadoop fs -ls examples/apps/demo/
  � Found 4 items
  � -rw-r--r--�� 1 oozie supergroup 930� 2012-12-14 13:23 examples/apps/demo/id.pig
  � -rw-r--r--�� 1 oozie supergroup 1020 2012-12-14 13:23 examples/apps/demo/job.properties
  � drwxr-xr-x�� - oozie supergroup 0��� 2012-12-14 13:23 examples/apps/demo/lib
  � -rw-r--r--�� 1 oozie supergroup 6136 2012-12-14 13:23 examples/apps/demo/workflow.xml Oozie will take any of the JARs that you put in that lib folder and automatically add them to your workflow�s classpath when it�s executed.� This is the simplest approach.� Alternatively, you can use the oozie.libpath property in your job.properties file to specify additional HDFS directories (multiple directories can be separated by a comma) that contain JARs.�The advantage of using this property over the lib folder discussed above is in cases where you have many workflows all using the same set of JARs.� For example, suppose you have 20 workflows that each need the same three additional JARs. Instead of storing and maintaining 20 copies of each of the three JARs (that�s 20 x 3 – 3 = 57 redundant JARs), you can simply keep the three JARs in one location and have all your workflows refer to that one copy. (Technically, there would be even more copies because of HDFS replication.) How Does the ShareLib Work? Some of the actions – specifically DistCp, Streaming, Pig, Sqoop, and Hive – require external JAR files in order to run successfully.�Instead of having to keep these JAR files in each workflow�s lib folder, or forcing the user to manually manage them via the oozie.libpath property on every workflow using one of these actions, Oozie provides the ShareLib.�The ShareLib behaves very similarly to oozie.libpath, except that it’s specific to the aforementioned actions and their required JARs.�Here�s what the (MRv1) ShareLib looks like in CDH 4.1.2:   drwxr-xr-x share�
  drwxr-xr-x share/lib
  drwxr-xr-x share/lib/distcp
  -rw-r--r-- share/lib/distcp/hadoop-tools-2.0.0-mr1-cdh4.1.2.jar
  drwxr-xr-x share/lib/hive
  -rw-r--r-- share/lib/hive/JavaEWAH-0.3.2.jar
  -rw-r--r-- share/lib/hive/antlr-2.7.7.jar
  -rw-r--r-- share/lib/hive/antlr-3.0.1.jar
  -rw-r--r-- share/lib/hive/antlr-runtime-3.0.1.jar
  -rw-r--r-- share/lib/hive/avro-ipc-1.7.1.cloudera.2.jar
  -rw-r--r-- share/lib/hive/avro-mapred-1.7.1.cloudera.2.jar
  -rw-r--r-- share/lib/hive/commons-beanutils-1.7.0.jar
  -rw-r--r-- share/lib/hive/commons-beanutils-core-1.8.0.jar
  -rw-r--r-- share/lib/hive/commons-collections-3.2.1.jar
  -rw-r--r-- share/lib/hive/commons-compress-1.4.1.jar
  -rw-r--r-- share/lib/hive/commons-configuration-1.6.jar
  -rw-r--r-- share/lib/hive/commons-dbcp-1.4.jar
  -rw-r--r-- share/lib/hive/commons-digester-1.8.jar
  -rw-r--r-- share/lib/hive/commons-pool-1.5.4.jar
  -rw-r--r-- share/lib/hive/datanucleus-connectionpool-2.0.3.jar
  -rw-r--r-- share/lib/hive/datanucleus-core-2.0.3.jar
  -rw-r--r-- share/lib/hive/datanucleus-enhancer-2.0.3.jar
  -rw-r--r-- share/lib/hive/datanucleus-rdbms-2.0.3.jar
  -rw-r--r-- share/lib/hive/derby-10.6.1.0.jar
  -rw-r--r-- share/lib/hive/guava-11.0.2.jar
  -rw-r--r-- share/lib/hive/haivvreo-1.0.7-cdh-4.jar
  -rw-r--r-- share/lib/hive/hive-builtins-0.9.0-cdh4.1.2.jar
  -rw-r--r-- share/lib/hive/hive-cli-0.9.0-cdh4.1.2.jar
  -rw-r--r-- share/lib/hive/hive-common-0.9.0-cdh4.1.2.jar
  -rw-r--r-- share/lib/hive/hive-contrib-0.9.0-cdh4.1.2.jar
  -rw-r--r-- share/lib/hive/hive-exec-0.9.0-cdh4.1.2.jar
  -rw-r--r-- share/lib/hive/hive-metastore-0.9.0-cdh4.1.2.jar
  -rw-r--r-- share/lib/hive/hive-serde-0.9.0-cdh4.1.2.jar
  -rw-r--r-- share/lib/hive/hive-service-0.9.0-cdh4.1.2.jar
  -rw-r--r-- share/lib/hive/hive-shims-0.9.0-cdh4.1.2.jar
  -rw-r--r-- share/lib/hive/httpclient-4.0.1.jar
  -rw-r--r-- share/lib/hive/httpcore-4.0.1.jar
  -rw-r--r-- share/lib/hive/jackson-core-asl-1.8.8.jar
  -rw-r--r-- share/lib/hive/jackson-mapper-asl-1.8.8.jar
  -rw-r--r-- share/lib/hive/jdo2-api-2.3-ec.jar
  -rw-r--r-- share/lib/hive/jetty-util-6.1.26.cloudera.2.jar
  -rw-r--r-- share/lib/hive/jline-0.9.94.jar
  -rw-r--r-- share/lib/hive/json-20090211.jar
  -rw-r--r-- share/lib/hive/jsr305-1.3.9.jar
  -rw-r--r-- share/lib/hive/jta-1.1.jar
  -rw-r--r-- share/lib/hive/libfb303-0.7.0.jar
  -rw-r--r-- share/lib/hive/libthrift-0.7.0.jar
  -rw-r--r-- share/lib/hive/netty-3.4.0.Final.jar
  -rw-r--r-- share/lib/hive/servlet-api-2.5-20081211.jar
  -rw-r--r-- share/lib/hive/stringtemplate-3.1-b1.jar
  -rw-r--r-- share/lib/hive/xz-1.0.jar
  drwxr-xr-x share/lib/mapreduce-streaming
  -rw-r--r-- share/lib/mapreduce-streaming/commons-cli-1.2.jar
  -rw-r--r-- share/lib/mapreduce-streaming/commons-codec-1.4.jar
  -rw-r--r-- share/lib/mapreduce-streaming/commons-el-1.0.jar
  -rw-r--r-- share/lib/mapreduce-streaming/commons-httpclient-3.1.jar
  -rw-r--r-- share/lib/mapreduce-streaming/commons-logging-1.1.jar
  -rw-r--r-- share/lib/mapreduce-streaming/commons-net-3.1.jar
  -rw-r--r-- share/lib/mapreduce-streaming/core-3.1.1.jar
  -rw-r--r-- share/lib/mapreduce-streaming/hadoop-core-2.0.0-mr1-cdh4.1.2.jar
  -rw-r--r-- share/lib/mapreduce-streaming/hadoop-streaming-2.0.0-mr1-cdh4.1.2.jar
  -rw-r--r-- share/lib/mapreduce-streaming/hsqldb-1.8.0.7.jar
  -rw-r--r-- share/lib/mapreduce-streaming/jackson-core-asl-1.8.8.jar
  -rw-r--r-- share/lib/mapreduce-streaming/jackson-mapper-asl-1.8.8.jar
  -rw-r--r-- share/lib/mapreduce-streaming/jasper-compiler-5.5.23.jar
  -rw-r--r-- share/lib/mapreduce-streaming/jasper-runtime-5.5.23.jar
  -rw-r--r-- share/lib/mapreduce-streaming/jets3t-0.6.1.jar
  -rw-r--r-- share/lib/mapreduce-streaming/jetty-6.1.14.jar
  -rw-r--r-- share/lib/mapreduce-streaming/jetty-util-6.1.26.cloudera.2.jar
  -rw-r--r-- share/lib/mapreduce-streaming/jsp-api-2.1.jar
  -rw-r--r-- share/lib/mapreduce-streaming/log4j-1.2.16.jar
  -rw-r--r-- share/lib/mapreduce-streaming/oro-2.0.8.jar
  -rw-r--r-- share/lib/mapreduce-streaming/servlet-api-2.5-6.1.14.jar
  -rw-r--r-- share/lib/mapreduce-streaming/servlet-api-2.5.jar
  -rw-r--r-- share/lib/mapreduce-streaming/xmlenc-0.52.jar
  drwxr-xr-x share/lib/oozie
  -rw-r--r-- share/lib/oozie/json-simple-1.1.jar
  drwxr-xr-x share/lib/pig
  -rw-r--r-- share/lib/pig/activation-1.1.jar
  -rw-r--r-- share/lib/pig/antlr-2.7.7.jar
  -rw-r--r-- share/lib/pig/antlr-runtime-3.4.jar
  -rw-r--r-- share/lib/pig/commons-beanutils-1.7.0.jar
  -rw-r--r-- share/lib/pig/commons-beanutils-core-1.8.0.jar
  -rw-r--r-- share/lib/pig/commons-collections-3.2.1.jar
  -rw-r--r-- share/lib/pig/commons-configuration-1.6.jar
  -rw-r--r-- share/lib/pig/commons-digester-1.8.jar
  -rw-r--r-- share/lib/pig/commons-io-2.1.jar
  -rw-r--r-- share/lib/pig/guava-11.0.2.jar
  -rw-r--r-- share/lib/pig/hbase-0.92.1-cdh4.1.2.jar
  -rw-r--r-- share/lib/pig/high-scale-lib-1.1.1.jar
  -rw-r--r-- share/lib/pig/hsqldb-1.8.0.7.jar
  -rw-r--r-- share/lib/pig/httpclient-4.0.1.jar
  -rw-r--r-- share/lib/pig/httpcore-4.0.1.jar
  -rw-r--r-- share/lib/pig/jaxb-api-2.2.2.jar
  -rw-r--r-- share/lib/pig/jline-0.9.94.jar
  -rw-r--r-- share/lib/pig/joda-time-1.6.jar
  -rw-r--r-- share/lib/pig/jruby-complete-1.6.5.jar
  -rw-r--r-- share/lib/pig/jsch-0.1.42.jar
  -rw-r--r-- share/lib/pig/jsr305-1.3.9.jar
  -rw-r--r-- share/lib/pig/jython-2.5.0.jar
  -rw-r--r-- share/lib/pig/libthrift-0.7.0.jar
  -rw-r--r-- share/lib/pig/metrics-core-2.1.2.jar
  -rw-r--r-- share/lib/pig/pig-0.10.0-cdh4.1.2.jar
  -rw-r--r-- share/lib/pig/protobuf-java-2.4.0a.jar
  -rw-r--r-- share/lib/pig/stax-api-1.0.1.jar
  -rw-r--r-- share/lib/pig/stringtemplate-3.2.1.jar
  -rw-r--r-- share/lib/sharelib.properties
  drwxr-xr-x share/lib/sqoop
  -rw-r--r-- share/lib/sqoop/avro-ipc-1.7.1.cloudera.2.jar
  -rw-r--r-- share/lib/sqoop/avro-mapred-1.7.1.cloudera.2.jar
  -rw-r--r-- share/lib/sqoop/commons-beanutils-1.7.0.jar
  -rw-r--r-- share/lib/sqoop/commons-beanutils-core-1.8.0.jar
  -rw-r--r-- share/lib/sqoop/commons-configuration-1.6.jar
  -rw-r--r-- share/lib/sqoop/commons-digester-1.8.jar
  -rw-r--r-- share/lib/sqoop/commons-io-2.1.jar
  -rw-r--r-- share/lib/sqoop/guava-11.0.2.jar
  -rw-r--r-- share/lib/sqoop/hbase-0.92.1-cdh4.1.2.jar
  -rw-r--r-- share/lib/sqoop/high-scale-lib-1.1.1.jar
  -rw-r--r-- share/lib/sqoop/hsqldb-1.8.0.7.jar
  -rw-r--r-- share/lib/sqoop/httpclient-4.0.1.jar
  -rw-r--r-- share/lib/sqoop/httpcore-4.0.1.jar
  -rw-r--r-- share/lib/sqoop/jsr305-1.3.9.jar
  -rw-r--r-- share/lib/sqoop/libthrift-0.7.0.jar
  -rw-r--r-- share/lib/sqoop/metrics-core-2.1.2.jar
  -rw-r--r-- share/lib/sqoop/netty-3.4.0.Final.jar
  -rw-r--r-- share/lib/sqoop/servlet-api-2.5-20081211.jar
  -rw-r--r-- share/lib/sqoop/sqoop-1.4.1-cdh4.1.2.jar
  As you can see, the above actions each depend on many JARs that you now don�t have to worry about after deploying the ShareLib.�Each of these actions has its own folder with its own JARs; this allows Oozie to use only the JARs required for that action instead of including every JAR.�In fact, this is necessary because not all of these actions use the same, or even compatible, versions of the JARs.�For example, the Hive action uses antlr-runtime-3.0.1.jar and will fail if used with antlr-runtime-3.4.jar, which is what the Pig action uses. How to Install and Use the ShareLib By default, the ShareLib should be placed in the home folder in HDFS of the user who started the Oozie web server; this is not necessarily the same user as the one submitting a job.�In CDH3 and CDH4, this user is named �oozie�.�The property in oozie-site.xml for setting the location of the ShareLib is called oozie.service.WorkflowAppService.system.libpath and its default value is /user/${user.name}/share/lib, where ${user.name} gets resolved to the user who started the Oozie server.�Hence, the default location to install the ShareLib is /user/oozie/share/lib.�More detailed instructions for installing the ShareLib can be found in the CDH4 Oozie documentation here.� (A future release of Cloudera Manager will be able to install the ShareLib automatically.)� One caveat: Because CDH4 supports MRv1 and YARN, the CDH4 Oozie installation provides separate ShareLib archives for MRv1 (oozie-sharelib.tar.gz) and YARN (oozie-sharelib-yarn.tar.gz).�It is important that the correct one is installed based on which version of Hadoop is being used.� To enable a workflow to use the ShareLib, you would simply specify oozie.use.system.libpath=true in the job.properties file and Oozie will know to include the jars in the ShareLib with the necessary actions in your job.� Overriding the ShareLib In CDH 4.1.0 and later (or Oozie 3.3.0 and later), you can override the ShareLib location at the action, job, and server levels.�This allows users or admins to support multiple versions or a patched version of an action at the same time.�The property is called oozie.action.sharelib.for.actiontype, where actiontype is the name of the action type (e.g. Pig, Sqoop); you would set its value to the name of a subfolder in the ShareLib.�To set it at the action level you would put the property in that action�s &lt;configuration&gt;; to set it at the job level, you would put the property in that job�s job.properties; and to set it at the server level, you would put the property in oozie-site.xml.� For example, Oozie currently ships ready for Pig 0.10.x, but suppose you also want to be able to use Pig 0.9.x in the same workflow.�The share/lib/pig folder is for Pig 0.10.x, but if you add a new folder with the Pig 0.9.x JARs, say share/lib/pig-9, you can put the following in the &lt;configuration&gt; element for the Pig 0.9.x action: &lt;property&gt;
   &lt;name&gt;oozie.action.sharelib.for.pig&lt;/name&gt;
    &lt;value&gt;pig-9&lt;value&gt;
 &lt;/property&gt;
 Oozie will continue to use share/lib/pig for the Pig 0.10.x action but will use share/lib/pig-9 for the Pig 0.9.x action.� Conclusion Now that you understand the purpose of the ShareLib, how it works, and how to use it, you can better leverage it in your Oozie workflows.�As Oozie continues to grow and mature, features such as the ShareLib make it easier to use.�In the future, OOZIE-1054 will make it even easier for users by providing a script that installs the ShareLib. Have any suggestions?�Feel free to tell us what you think through user@oozie.apache.org or cdh-user@cloudera.org. Robert Kanter is a Software Engineer at Cloudera, working on the Platform team.</snippet></document><document id="315"><title>How Rapleaf Works Smarter with Cloudera</title><url>http://blog.cloudera.com/blog/2012/12/how-rapleaf-works-smarter-with-cloudera/</url><snippet>Because raising the visibility of Apache Hadoop use cases is so important, in this post we bring you a re-posted story about how and why Rapleaf, a marketing data company based in San Francisco, uses Cloudera Enterprise (CDH and Cloudera Manager). Founded in 2006, Rapleaf�s mission is to make it incredibly easy for marketers to access the data they need so they can personalize content for their customers. Rapleaf helps clients �fill in the blanks� about their customers by taking contact lists and, in real time, providing supplemental data points, statistics and aggregate charts and graphs that are guaranteed to have greater than 90% accuracy. Rapleaf is powered by Cloudera. Business Challenges Before Cloudera Rapleaf established itself as a data driven business early on, collecting feeds from numerous sources to create a single, accurate view of each customer. By 2008, �we were processing data in a complex pipeline that involved an organic structure of many MySQL instances and queues,� explained Rapleaf�s co-founder and vice president of engineering, Jeremy Lizt. �As data volumes increased, that structure became unmanageable and expensive. It started getting difficult to perform the kinds of operations that we wanted to be able to do. It was no secret that this wasn�t going to scale.� As part of its data synthesis, Rapleaf performs numerous processes and analytics to get the data it collects into a refined state. If they want to change an algorithm, that requires re-processing of all their data. Because of this, Rapleaf didn�t even consider migrating to a relational data warehouse. �If we had a stable set of data and had to do incremental updates, a traditional RDBMS might have been appropriate,� commented Lizt, who had read Google�s papers on MapReduce and learned about Hadoop as an open source implementation of the Google paradigm. �It was pretty straightforward: Hadoop was the clear opportunity and seemed to be the only option for us.� The company made the move to Apache Hadoop and was running in production after 9-12 months. They managed the environment independently for a year before learning about Cloudera. �We were already invested in the promise of Hadoop and our feeling was that Cloudera would be good for the community and good for Hadoop,� said Lizt. Rapleaf decided to migrate to Cloudera�s open-source Hadoop distribution (CDH) due to its quality and stability, and soon signed on as one of the first Cloudera Enterprise customers. Use Case Rapleaf processes the many feeds of data that it collects and synthesizes all of that data into a single, accurate view using Hadoop. Log messages are sent through Scribe and loaded into the Hadoop Distributed File System (HDFS). Log data is loaded into Hadoop every ten minutes, amounting to 1-2 TB each day. Other data sources load hourly or daily. Rapleaf has other jobs that run periodically on the logs to compute stats and make sure everything is running correctly. After processing their data in CDH, Rapleaf puts it into a distributed hash table � an open source key value store built by Rapleaf called Hank � that can be queried with very predictable, fast response times. The company has 250 TB on 280 CDH nodes today, with capacity for up to 400 TB of raw, unreplicated data. MySQL is also still in use at Rapleaf, mostly to store Hadoop�s output. Impact: Business-critical Reliability at Scale Hadoop provides the foundation for Rapleaf�s business. “If something is critical to your infrastructure, it�s hard to articulate value. It�s like asking what your heart or your liver is worth,� commented Lizt. With that being said, the reliability and stability of Rapleaf�s Hadoop platform is imperative. This is the biggest benefit that Cloudera Enterprise offers to Rapleaf. As an early Hadoop adopter, Rapleaf doesn�t rely heavily on the support provided by Cloudera but finds peace of mind in knowing it�s there. �Every time we�ve had an issue, we�ve had very fast support,� said software engineer Andre Rodriguez. Lizt added, �Cloudera�s engineers are a talented bunch of people � they�re really intelligent, and we have confidence in their abilities. Whether it�s a glitch or just a question, it�s really helpful for us to be able to get a quick answer from someone at Cloudera who knows what they�re talking about.� Impact: Operational Efficiency Hadoop delivers a massively scalable data processing and storage platform that costs, on average, 10x less than traditional relational systems. But deploying Hadoop and keeping the cluster running at peak performance is no easy task. Rapleaf has found value in Cloudera�s ability to simplify the deployment, management and monitoring of Hadoop through support, services and the Cloudera Manager tool. Rodriguez reported three main advantages offered by Cloudera Manager: Job-level statistics � �A lot of the things that I used to do manually before, I can just do through Cloudera Manager now,� said Rodriguez. �We can go back and see specifically what happened with jobs, get statistics from each job, and � perhaps most importantly � it keeps that information in the database. We actually use that data to compute other metrics so we can decide where to spend our engineering time.� Configuration management � Cloudera Manager provides explanations for what every configuration parameter means. �We could get that information before, but it wasn�t easy,� noted Rodriguez. Visibility into long-term trends � While Rapleaf had a very mature configuration management system before implementing Cloudera Manager, they had less visibility into long-term trends such as how things were performing over time. This is another key benefit that helps Rapleaf identify focus areas for their engineering efforts. In summary, Rapleaf has built its business on Hadoop. Because it is a mission-critical component of Rapleaf’s infrastructure, the company relies on Cloudera Enterprise to ensure a stable, reliable and optimally performing Hadoop platform 24×7.�</snippet></document><document id="316"><title>What’s Next for Cloudera Impala?</title><url>http://blog.cloudera.com/blog/2012/12/whats-next-for-cloudera-impala/</url><snippet>It�s been an exciting month and a half since the launch of the Cloudera Impala (the new open source distributed query engine for Apache Hadoop) beta, and we thought it�d be a great time to provide an update about what�s next for the project – including our product roadmap, release schedule and open-source plan. First of all, we�d like to thank you for your enthusiasm and valuable beta feedback. We�re actively listening and have already fixed many of the bugs reported, captured feature requests for the roadmap, and updated the Cloudera Impala FAQ based on user input. GA Roadmap Our primary focus between now and general availability (GA) is making Impala enterprise-ready for your production Hadoop clusters. This means continued investments in product stability as well as product functionality, including: Additional file formats – specifically the Avro file format and LZO-compressed TextFiles Additional OS support – for the same supported 64-bit OS platforms as CDH4 including RHEL/CentOS 5.7, Ubuntu, Debian, SLES, and Oracle Linux Straggler handling – enables Impala to give more work to faster machines and less to slower machines for the fastest response times. In large clusters you often see a large variance of performance across nodes due to things like slow and faulty disks. JDBC driver – enables Java apps to interface with Impala. We�ll leverage the JDBC driver from Apache Hive to provide a common SQL interface for Java apps for both Impala and Hive. Data Definition Language (DDL) – enables users to create tables in the shared Hive metastore from Impala as well as Hive. As of Impala beta version 0.3, you can query from Impala but need to create your tables through Hive first. Faster, bigger, and more memory efficient joins – through a partitioned hash join, Impala will be able to partition the second table in a join so only one copy of the table is partitioned across all the nodes in the cluster. Currently Impala stores the second table in a join in each node�s memory. Impala will use table statistics to determine which strategy is most performant for each query. Faster, bigger, and more memory efficient aggregations – enables pre-aggregation to occur distributed local to the data to offload work, and thus memory consumption from the coordinator node that returns the final results. Broader SQL performance optimizations – enables more of Impala�s SQL features and built-ins to return with lowest latency by expanding our usage of LLVM code generation. Automatic metadata refresh – enables new tables and data to seamlessly be available for Impala queries as they are added without having to issue a manual refresh command to Impala. New Parquet columnar file format – enables even faster performance through an optional columnar format like Google Dremel�s ColumnIO and those of other analytical query engines. For a Hadoop user, Parquet will be another file format so any processing framework can access data stored in Parquet format like they do today with Avro and SequenceFiles. Post-GA Top Asks We have a good list of additional enhancements that are important to us and our users that are on our post-GA roadmap. The most notable and frequently asked for items include: UDFs and extensibility – enables users to add their own custom functionality. This is a frequent request and will take a more time than GA to build the right model considering performance and isolation requirements. Cost-based join order optimization – avoids users having to correctly order the joins based on size and selectivity of the tables. External joins using disk – enables joins between tables to spill to disk for arbitrarily large joins. Nested data – enables queries on complex nested structures including maps, structs, and arrays. Release Plan We are tentatively planning for the Impala 1.0 GA at the end of the first quarter of 2013. During the beta period we will continue to ship Impala beta updates every 2-4 weeks. These updates will include stability fixes as well as features from our roadmap listed above as soon as they are ready. For example, two of our top asks, additional OS platforms and a JDBC driver, will be coming soon after the New Year. Open-Source Process For those of you involved in the Apache Hadoop community, we appreciate your patience as we provide more transparency into our open-source development.�Our internal test code and issue tracking has some confidential information from our early private beta customers.�We need to separate this out before we can push more of our infrastructure to public systems. Earlier this week we provided the second update to the Impala code base. Going forward, the plan is to provide: Up-to-date source repositories – we�ll keep the repo more up-to-date going forward. Transparent issue tracking – we�ll be moving bug and feature request tracking over to the public Jira we have set up for Impala. We are eagerly listening to feedback and continuously adjusting our roadmap to best meet the needs of our user base. As such, please note that as this is a beta product, so the roadmap and timelines above may change. Justin Erickson is the product manager for Cloudera Impala.</snippet></document><document id="317"><title>How-To: Run a MapReduce Job in CDH4</title><url>http://blog.cloudera.com/blog/2012/12/how-to-run-a-mapreduce-job-in-cdh4/</url><snippet>This is the first post in series that will get you going on how to write, compile, and run a simple MapReduce job on Apache Hadoop.�The full code, along with tests, is available at http://github.com/cloudera/mapreduce-tutorial.�The program will run on either MR1 or MR2. We�ll assume that you have a running Hadoop installation, either locally or on a cluster, and your environment is set up correctly so that typing �hadoop� into your command line gives you some notes on usage.�Detailed instructions for installing CDH, Cloudera’s open-source, enterprise-ready distro of Hadoop and related projects, are available here: https://ccp.cloudera.com/display/CDH4DOC/CDH4+Installation. We�ll also assume you have Maven installed on your system, as this will make compiling your code easier.�Note that Maven is not a strict dependency; we could also compile using Java on the command line or with an IDE like Eclipse. The Use Case There�s been a lot of brawling on our pirate ship recently. Not so rarely, one of the mates will punch another one in the mouth, knocking a tooth out onto the deck. Our poor sailors will wake up the next day with an empty bottle of rum, wondering who�s responsible for the gap between their teeth. All this violence has gotten out of hand, so as a deterrent, we�d like to provide everyone with a list of everyone that�s ever left them with a gap. Luckily, we�ve been able to set up a Flume source so that every time someone punches someone else, it gets written out as a line in a big log file in Hadoop.�To turn this data into these lists, we need a MapReduce job that can 1) invert the mapping from attacker to their victim, 2) group by victims, and 3) eliminate duplicates. The Input Data A file or set of text files, in which each line represents a specific instance of punching, containing a pirate, a tab character as a delimiter, and then a victim. An example input file is located in the github repo at https://github.com/cloudera/mapreduce-tutorial/blob/master/samples/gaplog.txt. Writing Our GapDeduce Program Our map function simply inverts the mapping of punchers to their targets. public class Gapper extends MapReduceBase implements Mapper&lt;Text, Text, Text, Text&gt; {
����� public void map(Text attacker, Text victim, OutputCollector&lt;Text, Text&gt; output, Reporter reporter) throws IOException {
������� output.collect(victim, attacker);
����� }
}
 By using the victim as our map output/reduce input key, the shuffle groups by victim. Our reduce function is a little more complicated but not by much. It uses a TreeSet to eliminate duplicates and order by name, and then outputs the list of pirates as a single string. public class Deducer extends MapReduceBase implements Reducer&lt;Text, Text, Text, Text&gt; {
  public void reduce(Text key, Iterator&lt;Text&gt; values,
      OutputCollector&lt;Text, Text&gt; output, Reporter reporter) throws IOException {
    Set&lt;String&gt; attackers = new TreeSet&lt;String&gt;();
    while (values.hasNext()) {
      String valStr = values.next().toString();
      attackers.add(valStr);
    }
    output.collect(key, new Text(attackers.toString()));
  }
} Finally, we write a driver that will run the job. public static void main(String[] args) throws Exception {
����� JobConf conf = new JobConf();
����� conf.setJobName("gapdeduce");
�����
����� // This line specifies the jar Hadoop should use to run the mapper and
����� // reducer by telling it a class that�s inside it
����� conf.setJarByClass(GapDeduceRunner.class);

����� conf.setMapOutputKeyClass(Text.class);
����� conf.setMapOutputValueClass(Text.class);

����� conf.setOutputKeyClass(Text.class);
����� conf.setOutputValueClass(Text.class);

����� conf.setMapperClass(Gapper.class);
����� conf.setReducerClass(Deducer.class);

����� // KeyValueTextInputFormat treats each line as an input record,
����� // and splits the line by the tab character to separate it into key and value
����� conf.setInputFormat(KeyValueTextInputFormat.class);
����� conf.setOutputFormat(TextOutputFormat.class);

����� FileInputFormat.setInputPaths(conf, new Path(args[0]));
����� FileOutputFormat.setOutputPath(conf, new Path(args[1]));

����� JobClient.runJob(conf);
��� }
 Compiling and Packaging To run our program, we need to compile it and package it into a jar that can be sent to the machines in our cluster. To do this with Maven, we set up a project whose POM contains the hadoop-client dependency. &lt;dependency&gt;
  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
  &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
  &lt;version&gt;2.0.0-cdh4.1.0&lt;/version&gt;
&lt;/dependency&gt;
 Furthermore, we include the following repositories so Maven knows where to get the bits. &lt;repository&gt;
  &lt;id&gt;maven-hadoop&lt;/id&gt;
  &lt;name&gt;Hadoop Releases&lt;/name&gt;
  &lt;url&gt;https://repository.cloudera.com/content/repositories/releases/&lt;/url&gt;
&lt;/repository&gt;
  &lt;repository&gt;
  &lt;id&gt;cloudera-repos&lt;/id&gt;
  &lt;name&gt;Cloudera Repos&lt;/name&gt;
  &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;
&lt;/repository&gt;
 The full pom.xml is included in the github repository at https://github.com/cloudera/mapreduce-tutorial/blob/master/pom.xml. Then, in the project�s root directory, we run �mvn install. This command both compiles our code and generates a jar, which should show up in the target/ directory under the root project directory. Running Our MapReduce Program The github repo contains a sample input file and expected output file. We can put the sample input file into a directory on hdfs called �gaps� using hadoop fs -mkdir /gaps
hadoop fs -put samples/gaplog.txt /gaps
 We submit our MapReduce job using �hadoop jar�, which is similar to �java jar�, but includes all the necessary environment for running our jar on Hadoop. hadoop jar target/gapdeduce-1.0-SNAPSHOT.jar GapDeduceRunner /gaps/gaplog.txt /gaps/output
 To inspect the output file of our job, we can run hadoop fs -cat /gaps/output/part-00000
 At last, we can provide our mates with a full accounting of their gaps.� In the next post in this series, we�ll cover some advanced MapReduce features like the distributed cache, as well as testing MapReduce code with MRUnit. Sandy Ryza is a Software Engineer at Cloudera, working on the Platform team.</snippet></document><document id="318"><title>Cloudera Speakers at ApacheCon NA 2013</title><url>http://blog.cloudera.com/blog/2012/12/cloudera-speakers-at-apachecon-na-2013/</url><snippet>Our hearty congratulations to the Cloudera engineers who have been accepted as ApacheCon NA 2013 (Feb. 26-28 in Portland, OR) speakers for these talks: Getting Hadoop, Hive and HBase Up and Running in Less Than 15 Minutes�(Mark Grover, Apache Hive and Apache BigTop Contributor) – Tues., Feb. 26 Seven Deadly Hadoop Misconfigurations�(Kate Ting, Apache Sqoop PMC Member) – Weds., Feb. 27 Planning and Deploying Apache Flume�(Arvind Prabhakar, PMC Member for Sqoop and Apache Flume) – Weds., Feb. 27 Mastering Sqoop for Data Transfer for Big Data (Kate Ting &amp; Arvind Prabhakar) – Weds., Feb. 27 Lessons in Apache Software Integration (Roman Shaposhnik, PMC Member for Sqoop and VP for Apache BigTop) – Thurs., Feb. 28 Congratulations to all the other speakers, too! Early registration ends on Dec. 31, BTW.</snippet></document><document id="319"><title>Secrets of Cloudera Support: The Champagne Strategy</title><url>http://blog.cloudera.com/blog/2012/12/secrets-of-cloudera-support-the-champagne-strategy/</url><snippet>At Cloudera, we put great pride into drinking our own champagne. That pride extends to our support team, in particular. Cloudera Manager, our end-to-end management platform for CDH (Cloudera’s open-source, enterprise-ready distribution of Apache Hadoop and related projects),�has a feature that allows subscription customers to send a snapshot of their cluster to us. When these cluster snapshots come to us from customers, they end up in a CDH cluster at Cloudera where various forms of data processing and aggregation can be performed.� Today, the system provides real-time support via an application we call Cloudera Support Interface (CSI). When a support employee looks at a ticket, they can use CSI to examine the customer�s latest snapshot and see cluster stats such as version information, number of nodes in service, which services are used, and so on. CSI also visualizes different aggregations and groupings, such as versions, which allows us to detect misconfigured clusters, or issues caused during upgrade or installation. The system collects a lot of information, including: Configuration data and historical information from Cloudera Manager Individual node configurations from nodes Log information from each node, with configurable log levels to control the size Per-node outputs of various command-line�utilities The information is parsed, aggregated, and stored in Apache HBase, which provides real-time information to the CSI application. Custom analytic queries are available to support employees upon request via JIRA. In the near future, however, this data will move to Cloudera Impala�(the new real-time query engine for Hadoop, in beta at the time of this writing) over HDFS for analytic use cases, at which point the support organization will use a BI tool to do self-service queries. For example, we could explore questions like: What is the distribution of workloads across Impala, Apache Hive, and HBase? Which OS versions are most commonly used? What are the mean and variance of hardware configurations? How many types of hardware configuration are there at a single customer site? Does anyone use that weirdo parameter that we want to deprecate? What are the most commonly encountered errors? The end result? Better support, better products, and happier customers. Now, that�s what I call a good vintage.</snippet></document><document id="320"><title>How-to: Manage Permissions in Hue</title><url>http://blog.cloudera.com/blog/2012/12/managing-permissions-in-hue/</url><snippet>Hue is a web interface for Apache Hadoop that makes common Hadoop tasks such as running MapReduce jobs, browsing HDFS, and creating Apache Oozie workflows, easier. (To learn more about the integration of Oozie and Hue, see this blog post.) In this post, we�re going to focus on how one of the fundamental components in Hue, Useradmin, has matured. New User and Permission Features User and permission management in Hue has changed drastically over the past year. Oozie workflows, Apache Hive queries, and MapReduce jobs can be shared with other users or kept private. Permissions exist at the app level. Access to particular apps can be restricted, as well as certain sections of the apps. For instance, access to the shell app can be restricted, as well as access to the Apache HBase, Apache Pig, and Apache Flume shells themselves. Access privileges are defined for groups and users can be members of one or more groups. Changes to Users, Groups, and Permissions Hue now supports authentication against PAM, Spnego, and an LDAP server. Users and groups can be imported from LDAP and be treated like their non-external counterparts. The import is manual and is on a per user/group basis. Users can authenticate using different backends such as LDAP. Using the LDAP authentication backend will allow users to login using their LDAP password. This can be configured in /etc/hue/hue.ini by changing the �desktop.auth.backend� setting to �desktop.auth.backend.LdapBackend�. The LDAP server to authenticate against can be configured through the settings under �desktop.ldap�. Here’s an example: A company would like to use the following LDAP users and groups in Hue: John Smith belonging to team A Helen Taylor belonging to team B Assuming the following access requirements: Team A should be able to use Beeswax, but nothing else. Team B should only be able to see the Oozie dashboard with readonly permissions. In Hue 1 the scenarios cannot be realistically addressed given the lack of groups. In Hue 2 the scenarios can be addressed more appropriately. Users can be imported from LDAP by clicking �Add/Sync LDAP user� in Useradmin &gt; Users: Similarly, groups can be imported from LDAP by clicking �Add/Sync LDAP group� in Useradmin &gt; Groups. If a previously imported user�s information was updated recently, the information in Hue will need to be resynchronized. This can be achieved through the LDAP sync feature: Part A of the example can be addressed by explicitly allowing access Beeswax for Team A. This is managed in the �Groups� tab of the Useradmin app: The Team A group can be edited by clicking on its name, where access privileges for the group are selectable. Here, the �beeswax.access� permission would be selected and the others would be unselected: Part B of the example can be handled by explicitly defining access for Team B. This can be accomplished by following the same steps in part A, except for Team B. Every permission would be unselected except �oozie.dashboard_jobs_access�: By explicitly setting the app level permissions, the apps that these users will be able to see will change. For instance, Helen, who is a member of Team B, will only see the Oozie app available: Summary User management has been revamped, groups were added, and various backends are exposed. One such backend, LDAP, facilitates synchronization of users and groups. App-level permissions allow administrators to control who can access certain apps and what documents can be shared. Hue is maturing quickly and many more features are on their way. Hue will soon have document-level permissions (workflows, queries, and so on), trash functionality, and improvements to the existing editors. Have any suggestions? Feel free to tell us what you think through hue-user.</snippet></document><document id="321"><title>Introducing Cloudera CDH4 Certification</title><url>http://blog.cloudera.com/blog/2012/12/introducing-cloudera-cdh4-certification/</url><snippet>We are very pleased to introduce new, CDH4.1-aligned versions of the Cloudera Certified Developer for Apache Hadoop and Cloudera Certified Administrator for Apache Hadoop�exams. To celebrate, we�re offering a steep 40% discount on the new exams until the end of the year! Just use the promotion code CDH4 when you register to take the CCD-410 or CCA-410 exam through Pearson VUE before Dec. 31, 2012. Promotion Code: CDH4 Expiration Date: 12/31/2012 Cloudera Certification establishes you as a trusted and valuable resource for those looking to work with an Apache Hadoop expert. Whether your company is just looking into the technology or your customers are asking for help with Hadoop, Cloudera Certification gives you a clear and tangible advantage, establishing you as a leader in today’s growing Big Data and open source space. If you are already Cloudera Certified as an Apache Hadoop Developer or Administrator on CDH3, your certification will not expire. However, the Hadoop ecosystem is dynamic, and we recommend that individuals keep their certifications current with CDH releases. Accordingly, we are now offering our first upgrade exams for CCDH and CCAH, which are shorter, more focused tests specifically for developers and administrators previously certified on CDH3. Please note the promotion code does not apply to upgrade exams, as they are already offered at a discounted rate. Cloudera is also offering a free retake on unsuccessful completion of any exam until the end of the year through the Second Shot program, which applies to both the new CDH4.1-aligned tests taken with a promo code and the pre-discounted CDH4 upgrade exams attempted before Dec. 31, 2012. Visit Cloudera University�to learn more about the new tests, peruse the test objectives, and read the FAQ.</snippet></document><document id="322"><title>New: Cloudera Manager Free Edition Demo VM</title><url>http://blog.cloudera.com/blog/2012/12/new-cloudera-manager-free-edition-demo-vm/</url><snippet>With the availability�of this new demo VM containing Cloudera Manager Free Edition and CDH4.1.2 on CentOS 6.2, getting quick hands-on experience with a freeze-dried single-node Apache Hadoop cluster is just a few minutes away after the download process.� This new addition to our growing�Demo VM menagerie�is available, as usual, in VMware, VirtualBox, and KVM flavors.�A 64-bit host OS is required. A few quick notes from the doc: Once you download and launch the VM, you are automatically logged in as the cloudera user. �Account details: username: cloudera password: cloudera (The cloudera account has sudo�privileges in the VM.) When the VM boots, it will automatically launch Cloudera Manager and configure it to run HDFS, MapReduce, ZooKeeper, HBase, Hue, and Oozie on the machine. You can then open a web browser and open the Cloudera Manager bookmark. Log into Cloudera Manager using these default credentials: Username: admin Password: admin You can also open the bookmarks in the web browser for the individual CDH4 services. Or, you can use the command-line tools. Consider exploring these resources as well: - Cloudera Manager Free Edition documentation -�Hadoop Tutorial - Cloudera Glossary� Have at it, and let us know if you have any problems or issues via the scm-users�or cdh-users lists.</snippet></document><document id="323"><title>Cloudera Impala Beta (version 0.3) and Cloudera Manager 4.1.2 Now Available</title><url>http://blog.cloudera.com/blog/2012/12/cloudera-impala-beta-version-0-3-and-cloudera-manager-4-1-2-now-available/</url><snippet>I am pleased to announce the release of Cloudera Impala Beta (version 0.3) and Cloudera Manager 4.1.2. Key enhancements in each release are: Cloudera Impala Beta (version 0.3) Stability and performance enhancements to address feedback from the beta program Impala Beta will be regularly updated with features, bug fixes, and performance enhancements. We will typically release such updates every 2 – 4 weeks. Please check the release notes to find out what’s new with each update. For more questions, visit the Impala FAQ page or email us at impala-user@cloudera.org. Cloudera Manager 4.1.2 Cloudera Manager now supports specifying an external program of your own choosing for user authentication. See�Configuring External Authentication for details. There�s now a Cloudera Manager Free Edition Demo VM available. Bug fixes Detailed release notes Download CM 4.1.2 or Upgrade to CM 4.1.2 For more questions, visit the CM FAQ page or email us at scm-users@cloudera.org. As a reminder, here is how you can get started with Impala: Use Cloudera Manager 4.1.2 to deploy Impala. Documentation can be found here. To manually install Impala, access the download of Impala, install Impala and try it out. Please note that you need to have CDH 4.1.x installed on RHEL/CentOS 6.2. Access the demo VM of Impala. The VM includes instructions that show you the power of Impala. E-learning course on “An Introduction To Impala“. Access the Impala source code at: https://github.com/cloudera/impala. Download and review the Impala documentation. Once you get started, we encourage you to provide feedback. We have the following mechanisms set up to do this: An Impala user group has been set up. Please use this to ask questions and provide feedback. An Impala Jira project has been set up. Feature requests and bug reports are welcome.</snippet></document><document id="324"><title>How to Contribute to Apache Hadoop Projects, in 24 Minutes</title><url>http://blog.cloudera.com/blog/2012/12/how-to-contribute-to-apache-hadoop-projects-in-24-minutes/</url><snippet>So, you want to report a bug, propose a new feature, or contribute code or doc to Apache Hadoop (or a related project), but you don’t know what to do and where to start? Don’t worry, you’re not alone. Let us help: in this 24-minute screencast, Clouderan Jeff Bean (@jwfbean) offers a step-by-step tutorial that explains why and how to contribute. Apache JIRA ninjas need not view, but anyone else with slight (or less) familiarity with that curious beast will find this information very helpful.� Among other things, you’ll learn: What “contribution” means The benefits of contribution – with Cloudera serving as an example What JIRA is, and how to get started/work with Apache’s JIRA (demos included) How to collaborate with others in the community How to become a committer and committer etiquette Really, it’s a great resource.�Now, I’ll leave you with your screencast: For additional resources about open-source contribution, see the “Contribute!” section on our “New to Hadoop” page.</snippet></document><document id="325"><title>Meet the Engineer-Turned-Product Manager: Eva Andreasson</title><url>http://blog.cloudera.com/blog/2012/11/meet-the-engineer-turned-product-manager-eva-andreasson/</url><snippet>In this installment of “Meet the Engineer”, meet Eva Andreasson (@EvaAndreasson), a former Java programmer currently working with Cloudera engineers as the product manager for CDH. What do you do at Cloudera? Although I have an engineering background�and a past as a�Java Virtual Machine (JVM) coder, I actually do not work as an engineer presently. Rather, I work very closely with engineering every day, as product manager for many of the components in CDH (Cloudera’s open-source distribution of Apache Hadoop and related projects).� One of my main responsibilities in this role is to understand customer needs and expectations, gather valuable use cases and in various ways translate them into clear, prioritized feature requirements. I�often discuss technical problems with engineering teams, support, and�field teams,�and weigh in on issues that need to be solved to ensure successful projects. In addition, I spend a lot of time with customers, partners and other teams within Cloudera to communicate goals, direction and intent. I also collect feedback and ideas to ensure the best possible product feature value, usability, and impact. I sometimes jokingly tell others that my job is to predict the future, and in a way I do that – by helping ensure that CDH will contain the features and functionality our customers and users will ask for next. Why do you enjoy your job? I really enjoy my work at Cloudera for many reasons. First and foremost, I appreciate the people here. They are bright, honest and almost everyone possesses a can-do attitude. Every level of the company has a sane approach to problem solving and has a great sense of achievable clear goals, which is key to any organization’s success, in my opinion. In addition, it makes me happy to see great people achieve fantastic results. On a personal level I also get to learn a lot of new things through the opportunity to work with a quickly emerging technology, which directly impacts how enterprises do business and make decisions tomorrow. In particular I like to dive into how people are using and find value from Hadoop. Interaction is key if people and businesses will be successful in their mission of getting more value out of their data. I have a special passion for the end user and making sure Hadoop is easy to understand and consume, as I truly believe technologies like this will help change the world.� Just think about how much medical research can be done quicker by processing more data across much larger data input sets? And think about how we can find new ways to optimize flight and traffic routs to spare the environment as well as fuel costs? Or how we can help improve education or military physical training by detecting patterns across student data or exersize injury data? All these big questions is what makes me feel my work is important and how Cloudera will play an important role in a bigger than Cloudera mission. At what age did you become interested and programming, and why? I started coding when I was 19, and if I remember correctly, the reason was that I didn’t like the computer games available at the time. I thought I would develop the first game in the world that adapted difficulty level based on your choices and interactions – dynamically. A game that would learn as you learned the game. I might not be there yet, but the last 10+ years have definitely given me so much fun coding (especially when I used to code garbage collection), experimenting with SOA and Cloud, working with distributed caches, web servers, and profiling tools (and even prototyping one or two mobile apps), that I feel the first goal can still wait. What do you like best about working with engineers? Working with engineers is a treat. The creativity and generosity with knowledge�are key parts of the joy to stay close to engineering. I learn new things every day, and I get to be part of difficult problem solving. At the same time I�can trust that these brilliant brains will design, code, and�deliver�good�ideas�into great products and features – all�with the bigger goal of helping real customer use cases.</snippet></document><document id="326"><title>Apache HBase AssignmentManager Improvements</title><url>http://blog.cloudera.com/blog/2012/11/apache-hbase-assignmentmanager-improvements/</url><snippet>AssignmentManager is a module in the Apache HBase Master that manages regions to RegionServers assignment. (See HBase architecture for more information.) It ensures that all regions are assigned and each region is assigned to just one RegionServer. Although the AssignmentManager generally does a good job, the existing implementation does not handle assignments as well as it could.�For example, if a region was assigned to two or more RegionServers, some regions were stuck in transition and never got assigned, or unknown region exceptions were thrown in moving a region from one RegionServer to another. In the past we tried to fix these bugs without changing the underlying design. Consequently, the AssignmentManager ended up having many band-aids, and the code base became hard to understand/maintain. Furthermore, the underlying issues had not been completely fixed. After looking at many of the cases, we decided that the best path forward would be to improve the underlying design to be more robust, and simplify the code base to be easier to maintain. The goal here is not to make a better AssignmentManager – it�s to make a correct one. In this post, I will explain some improvements that the Cloudera engineering team has done recently. They are primarily focused on correctness and reliability instead of performance; with the exception of HBASE-6977 (Multithreaded processing ZooKeeper assignment events), they improve performance only marginally.�As to performance, Nicolas Liochon and Chunhui Shen have done some great work in HBASE-6109 (Improve region-in-transition performances during assignment on large clusters) and HBASE-5970 (Improve AssignmentManager#updateTimer and speed up handling opened events). Region States Overview Almost all of the issues discussed previously are caused by inconsistent region states. For example, a region is assigned to one RegionServer, but the Master thinks it is assigned to a different one, or not assigned at all and assigns it to somewhere else again. So before we move to each patch, let�s talk a little bit about the region states. A region can be in one of these states: OFFLINE: Region is in an offline state PENDING_OPEN: Sent RPC to a RegionServer to open but has not begun OPENING: RegionServer has begun to open but not yet done OPEN: RegionServer opened region and updated META PENDING_CLOSE: Sent RPC to RegionServer to close but has not begun CLOSING: RegionServer has begun to close but not yet done CLOSED: RegionServer closed region and updated meta SPLITTING: RegionServer started split of a region SPLIT: RegionServer completed split of a region (The two states SPLITTING and SPLIT relate to region splitting, which is beyond the scope of this post.) Region state is used to track the transition of a region from unassigned (a.k.a offline) to assigned (a.k.a open), and from assigned to unassigned. The transition path (state machine) looks like this: open:� OFFLINE -&gt; PENDING_OPEN -&gt; OPENING -&gt; OPEN close:� OPEN -&gt; PENDING_CLOSE -&gt; CLOSING -&gt; CLOSED -&gt; OFFLINE The state machine is tracked in three different places: META table, Master server memory, and Apache ZooKeeper unassign znodes. As it moves through the state machine, we update the different storage locations at different times. The AssignmentManager�s job is to try to keep the three pieces of storage synchronized. The information in ZooKeeper is transient. It is some temporary information about a region state transition, which is used to track the states while the region moves through from unassigned to assigned, or the reverse way.� If the corresponding RegionServer or the Master crashes in the middle, that information can be used to recover the transition. Final region assignment information is persisted in the META table. However, it doesn�t have the latest information of regions in transition. It only has the most recent RegionServer each region is assigned to. If a region is not online, e.g. is in transition, the META table knows the previous RegionServer the region used to be assigned to, but it doesn�t know what�s going on with the region and where it is opening now. The Master holds all region states in memory.�This information is used by the AssignmentManager to track where each region is, and its current state.�Although region assignments are initiated by the Master and it knows a region is opening on a RegionServer, the Master depends on the ZooKeeper event update to find out if the opening is succeeded.�When a region is opened on a RegionServer, this information is already in the META table.�But it takes a very short time for the Master to find that out based on the region transition update from ZooKeeper. The region state in the Master memory is not always consistent with the information in the META table, or in ZooKeeper.�The AssignmentManager is responsible for keeping track of the current status of each region, and make sure they are eventually consistent. HBASE-6272 (In-memory region state should be consistent) The first improvement we did is HBASE-6272, to make sure the in-memory region state is consistent.�In the Master memory, we have the state of each region.�We also have the mapping of regions to the RegionServers they are on when they are open.�We need to make sure they are consistent. For example, if a region is in OPEN state, there should be a mapping of this region to an online RegionServer. In HBASE-6272, we separated this information from AssignmentManager into its own class, and synchronized the access, so that at least the Master has a consistent view to itself internally.�Otherwise, how can the Master tell what�s going on with a region for sure? Another thing is that we keep all the regions known to the Master so far always in memory although it could be offline at some time.�We used to not keep such information, and ran into UnknownRegionException in moving a region that might not be in memory temporarily. This improvement is the foundation for the following improvements. HBASE-6381 (AssignmentManager should use the same logic for clean startup and failover) The second improvement is HBASE-6381, to fix the region states recovery at startup time. When a Master starts up, it needs to recover its in-memory region states.�If it is a clean startup – i.e., all RegionServers, together with the Master, are restarted or started for the first time – things are quite simple:� all regions should be offline and need to be assigned. Let�s look at the case where some RegionServers are still up and serving some regions, and the Master and any dead RegionServers restart. The Master then needs to figure out what regions are not in service, and if any regions are still in transition. If so, are they transitioning to/from dead/live RegionServers?�There could be tables in the middle of enabling or disabling at this moment too.�Things can get very complex. If just one RegionServer and the Master, or just the Master dies, we don�t want to bring down other RegionServers, as it may take quite some time to re-open all regions. This scenario is called “Master failover.” In failover mode, the Master has its own dead server recovery logic, which is different but similar in function to the dead server handling logic used to recover regions on a RegionServer that dies while the Master is online.�One change we did is to reuse the dead server handling logic, as we don�t need to maintain similar logic in the AssignmentManager.�So the Master now submits all of the dead RegionServers to the dead server handler to process. The other change we did is to suspend the dead server handler until the region states are fully recovered. The goal is to prevent the handler to race with the region state recovering. If the handler tries to reassign a region, the region transition ZooKeeper event comes to the AssignmentManager. If the region states are not fully recovered yet, we used to have some special logic to handle this scenario since we need to find out this region�s state if we don�t know it already.�With this change, we removed the special logic and the code is now cleaner. One more change involves the ZooKeeper event. When the Master restarts and while it is recovering the region states, there could be some region that is in the middle of transition – so the ZooKeeper region transition data could be updated.�The change we did is to not watch the assignment znodes until the region states are recovered, for the same reason.�We don�t want to handle region state transition before we know all regions� states – i.e. the region states have been recovered. In this improvement, we also cleaned up and consolidated the bulk assigner. Bulk assignment is a good idea to reduce the number of RPC calls to a RegionServer. However, it is not as reliable as the individual assignment.�We would like to fix that eventually, via HBASE-6611.�As the first step, however, we did some cleanup:�We moved the bulk assigner out of AssignmentManager to a standalone class. Some useful logic in a bulk assignment method used for RegionServer recovery is folded into the standalone bulk assigner. The new bulk assigner is used for all bulk assigning, and redundant bulk assignment logic is removed. HBASE-6611 (Forcing region state offline cause double assignment) HBASE-6611 makes sure the Master closes a region before it changes the region�s state to offline/closed from a state like open, opening or pending open.�This is very important to prevent double assignments. When we try to assign a region, if its state is not offline, we used to forcibly change it to offline – but this does not actually close the region. Assuming the region state is right, it means the region could be already open�or opening in a RegionServer. If we try to assign the region again to a different RegionServer without closing it at first, it very likely leads to a double assignment. What we did is to send a close-region request to the corresponding RegionServer to close the region if it is open, or abort the region opening if it is still opening. We also retry the close-region if the region is in transition to make sure no timing/racing issue catches us at this moment. Another change in this patch is about the region state transition. Originally, we transition the state upon processing an assignment ZooKeeper event, as long as the current state matches. For example, if we get a region_opened event and the current state is opening or pending open, then we transition the state to open.�We added a check on the RegionServer name. We need to make sure the event is indeed from the RegionServer where the transition should be happening. This is to prevent updating the region state based on some old events since the region could be opening on another RegionServer and it is aborted. The server name in a region transition data used to be the source of the event, which could be the Master server since all assignments start from the Master.�We changed it to the name of the RegionServer where the transition is happening.�The point is to make sure all region transition data has the RegionServer name where the transition is happening, so that the RegionServer name checking discussed above is possible. Another point for the change is that if we use the source of the event, during the failover mode we could lose track of the RegionServer where the transition is going on, because the source of the event (Master server in this case) does not point to the expected RegionServer. We could keep the source of the event and add the target RegionServer name, but we chose not to do so since we don�t use the source of the event for now. With the additional RegionServer name checking, we are sure we get the right ZooKeeper event and do the right transition.�Since there could be a forced reassignment initiated by a timeout monitor or the client, the same region could still be opening somewhere else when we try to open it on a new RegionServer.�If the previous opening is not aborted fast enough, it could trigger some ZooKeeper events which could still be in the queue for processing. Without the RegionServer name checking,�some old ZooKeeper event could take the region state in some wrong direction. For example, the following diagram shows a scenario to demonstrate the problem. � An old ZooKeeper event leads to the wrong region state, if not checking RegionServer name. The Master tries to assign a region to RegionServer 1 The region is opened and RegionServer 1 transitions the region to opened in ZK The Master doesn�t get the ZooKeeper event fast enough and times out the assignment, so it closes the region The Master tries to open the region again on RegionServer 2 The previous ZooKeeper open event finally gets to the Master If not checking RegionServer name, the Master will think the region is opened on RegionServer 1, and deletes the unassign znode Region server 2 opens the region and can�t update the znode since it is already removed, and the region opening fails The Master gets the znode deletion event and moves the region out of transition. Now the Master thinks the region is online and assigned to RegionServer 1.� The region is actually not assigned anywhere. In this improvement we also changed the bulk assigner to lock the regions to prevent them from assigning again, and use the ZooKeeper offline znode version in requesting the RegionServer to open those regions.�That�s what we do in assigning an individual region.�Bulk assigner should do the same to be as reliable. Another locking we added is for the node-deleted ZooKeeper event callback. So, we have proper locking everywhere a region state can change and an existing assignment can be retried forcefully.�This will make sure the in-memory region state is trustworthy and reliable. With this improvement (and previous improvements), we think a bunch of holes have been closed, and now we can trust AssignmentManager much more than before. Based on our testing, we have never seen double assignments, or region stuck in transition forever, with minimal performance impact. On our testing cluster, without this patch, it took around 290 seconds to bulk-assign 10,339 regions to 4 RegionServers. With this patch, it took around 300 seconds. The little overhead is due to the locking in bulk assigning, which is reasonable. HBASE-6977 (Multithread processing ZooKeeper assignment events) The last improvement is HBASE-6977.� In this small patch, we removed the extra ZooKeeper watcher introduced in HBASE-6611 to prevent deadlocks. We delegated the ZooKeeper events to several workers to process so that the ZooKeeper event thread is freed up to do other things. With this patch, it takes around 250 seconds to bulk assign the same 10,339 regions to 4 RegionServers. Each ZooKeeper watcher has just one event thread to process all event notifications for this watcher.� If it gets stuck in one event due to locking, it blocks the whole ZooKeeper event notification processing, which could cause some deadlock if the locker holder is also waiting for the ZooKeeper event notification to meet some criteria to move on.� That�s why we introduced an additional ZooKeeper watcher in HBASE-6611.�The additional ZooKeeper watcher is for the async ZooKeeper offline assignment znodes callback. In this improvement, we created a pool of a configurable number of single-threaded workers to process ZooKeeper assignment events so the ZooKeeper event thread will not block. With multiple workers, we need to make sure the events are processed in the right order. In this patch, we choose which worker to use based on the hash code of the region�s unassignment znode path, so that we can guarantee all the events for a given region are always processed by the same worker.�The worker is a single threaded executor, and the events for the same region are always processed in order. Events for different regions don�t have to be processed in order. The hash code of the region�s unassignment znode path should be random enough so that all workers have a fair share of work. Summary With these patches, the AssignmentManager is much more stable and reliable now. There are no more double assignments or regions stuck in transition as far as our experiments show. Based on our testing, we can assign 10+K regions in around 4 minutes.�With these improvements, we are confident to reset the default region assignment timeout to 10 minutes (HBASE-5119 (Set the TimeoutMonitor’s timeout back down)) in the trunk (0.96) branch, with the AssignmentManager being stable and reliable. Of course, the region assignment time also depends on how long it actually takes to open the region on the RegionServer. Acknowledgements Thanks to the community and everyone who reported the issues, reviewed the patches, and discussed about the fixes. Jimmy Xiang is a Software Engineer for Cloudera, working on the Platform team.</snippet></document><document id="327"><title>External Hands-on Experiences with Cloudera Impala</title><url>http://blog.cloudera.com/blog/2012/11/external-observations-about-cloudera-impala/</url><snippet>The beta release of Cloudera Impala, the first (and open source) real-time query engine for Apache Hadoop, has been�out in the wild�(in binary as well as VM forms) for over a month now, and users have had time to get up-close and hands-on. Consequently, we’re beginning to see some fascinating self-published observations and guides. � Here are just a few examples; you may know of more that we’ve missed: How I Came to Love Big Data (or at least acknowledge its existence), by 37signals data analyst Noah Lorang Highlight: “We set up a couple of machines in a cluster, pulled together a few sample datasets, and ran a few benchmarks comparing Impala, Hive, and MySQL, and the results were encouraging for Impala.” Cloudera Impala � Closing the Near Real Time Gap Working with Big Data, by Six3 Systems developer�Wayne Wheeles Highlight: “This is as advertised; easy to use, easy to implement on, very fast, very flexible and more than capable of running real time analytics.”� BigData: Cloudera Impala and ArcPy, by ESRI architect Mansour Raad Highlight: “(Impala) was pretty fast and cool…This combination of Big Data in HDFS converted instantly into ‘SmallData’ for rendering and further processing in ArcMap is a great marriage.” Cloudera’s Impala, by IBM researcher Sandeep Tata Highlight: “I’m excited that Impala ups the game for structured data processing on Hadoop…” Cloudera Impala – Fast, Interactive Queries with Hadoop, by Vodafone UK architect Istvan Szegedi Highlight: �”Cloudera Impala is certainly an exciting solution that is utilizing the same concept as Google BigQuery but promises to support a wider range of input formats, and by making it available as an open source technology it can attract external developers to improve the software and take it to the next stage.” From Zero to Impala in Minutes, by AMPLab developer Matt Massie Highlight: “Use Apache Whirr to bring up a Cloudera Impala multi-node cluster on EC2 in minutes. When the installation script finishes, you�ll be able to immediately query the sample data in Impala without any more setup needed.” � (Added Nov. 29, 2012)�Nail Hadoop With Impala, by Will Crandle,�JobsTheWord co-founder Highlight:�”Impala looks like the real deal. As soon as we heard the news, one of our developers tried out the Beta release and reported, with wide-eyed concern surrounding his loss of magical powers, ‘it�s really fast; really, really fast.’ “ Many thanks to these users for their feedback and interest. If you know of other examples for this list, let us know in comments.�</snippet></document><document id="328"><title>Streaming Data into Apache HBase using Apache Flume</title><url>http://blog.cloudera.com/blog/2012/11/streaming-data-into-apache-hbase-using-apache-flume/</url><snippet>The following post was originally published via blog.apache.org; we are re-publishing it here. Apache Flume was conceived as a fault-tolerant ingest system for the Apache Hadoop ecosystem. Flume comes packaged with an HDFS Sink which can be used to write events into HDFS, and two different implementations of HBase sinks to write events into Apache HBase. You can read about the basic architecture of Apache Flume 1.x in this blog post. You can also read about how Flume�s File Channel persists events and still provides extremely high performance in an earlier blog post. In this article, we will explore how to configure Flume to write events into HBase, and write custom serializers to write events into HBase in a format of the user�s choice. Data is stored in HBase as tables. Each table has one or more column families, and each column family has one or more columns. HBase stores all columns in a column family in physical proximity. Each row is identified by a key known as the row key. To insert data into HBase, the table name, column family, column name and row key have to be specified. More details on the HBase data model can be found in the HBase documentation. Flume has two HBase Sinks, the HBaseSink(org.apache.flume.sink.hbase.HBaseSink) and AsyncHBaseSink(org.apache.flume.sink.hbase.AsyncHBaseSink). These two sinks will eventually converge to similar functionality, but currently each has some advantages over the other: The AsyncHBaseSink currently gives better performance than the HBaseSink, primarily because it makes non-blocking calls to HBase. The HBaseSink will soon support secure HBase clusters (FLUME-1626) and the new HBase IPC which was introduced in HBase 0.96. The configuration for both these sinks are very similar. A sample configuration is shown below:   #Use the AsyncHBaseSink
  host1.sinks.sink1.type = org.apache.flume.sink.hbase.AsyncHBaseSink
  #Use the HBaseSink
  #host1.sinks.sink1.type = org.apache.flume.sink.hbase.HBaseSink
  host1.sinks.sink1.channel = ch1
  host1.sinks.sink1.table = transactions
  host1.sinks.sink1.columnFamily = clients
  host1.sinks.sink1.column = charges
  host1.sinks.sink1.batchSize = 5000
  #Use the SimpleAsyncHbaseEventSerializer that comes with Flume
  host1.sinks.sink1.serializer = org.apache.flume.sink.hbase.SimpleAsyncHbaseEventSerializer
  #Use the SimpleHbaseEventSerializer that comes with Flume
  #host1.sinks.sink1.serializer = org.apache.flume.sink.hbase.SimpleHbaseEventSerializer
  host1.sinks.sink1.serializer.incrementColumn = icol
  host1.channels.ch1.type=memory
  In the above config, the �table� parameter specifies the table in HBase that the sink has to write to – in this case, �transactions�; the �columnFamily� parameter specifies the column family in that table to insert the data into, in this case, �clients�; and the �column� parameter specifies the column in the column family to write to, in this case �charges�. Apart from this the sink requires the channel to be configured, like all other Flume Sinks. The other interesting configuration parameters are the �serializer� and the �serializer.*� parameters. The two sinks use different interfaces for the serializer. In both cases, the serializer is a class that converts the Flume Event into an HBase-friendly format. This piece of code that �translates� the events is usually specific to the schema used by the user�s HBase cluster and is usually implemented by the user. All configuration parameters passed in as �serializer.*� are passed to the serializer. This configuration can be used to set up any internal state the serializer needs. In case of the HBaseSink, the serializer converts a Flume Event into one or more HBase Puts and/or Increments. The serializer must implement the HbaseEventSerializer. The serializer is instantiated when the sink is started by the Flume configuration framework. For each event processed by the sink, the sink calls the initialize method in the serializer. The serializer must �translate� the Flume Event into HBase puts and increments which should be returned by getActions and getIncrements methods.� These puts and increments are then sent over the wire to the HBase cluster. When the sink stops, this instance of the serializer is closed by the HBaseSink. The AsyncHBaseSink�s serializer must implement AsyncHbaseEventSerializer. In this case, the initialize method is called once by the sink, when it starts up. For every event, the sink calls the setEvent method and then calls the getActions and getIncrements methods – similar to the HBaseSink. When the sink is stopped, the serializer�s cleanUp method is called. Notice that the methods do not return the standard HBase Puts and Increments, but PutRequest and AtomicIncrementRequest from the asynchbase API. These are roughly equivalent to the HBase Puts and Increments respectively, with some differences. � An example of such a serializer is below. /**
  �* A serializer for the AsyncHBaseSink, which splits the event body into
  �* multiple columns and inserts them into a row whose key is available in
  �* the headers
  �*/
  public class SplittingSerializer implements AsyncHbaseEventSerializer {
  � private byte[] table;
  � private byte[] colFam;
  � private Event currentEvent;
  � private byte[][] columnNames;
  � private final List&lt;PutRequest&gt; puts = new ArrayList&lt;PutRequest&gt;();
  � private final List&lt;AtomicIncrementRequest&gt; incs = new ArrayList&lt;AtomicIncrementRequest&gt;();
  � private byte[] currentRowKey;
  � private final byte[] eventCountCol = "eventCount".getBytes();

  � @Override
  � public void initialize(byte[] table, byte[] cf) {
  ��� this.table = table;
  ��� this.colFam = cf;
  � }

  � @Override
  � public void setEvent(Event event) {
  ��� // Set the event and verify that the rowKey is not present
  ��� this.currentEvent = event;
  ��� String rowKeyStr = currentEvent.getHeaders().get("rowKey");
  ��� if (rowKeyStr == null) {
  ����� throw new FlumeException("No row key found in headers!");
  ��� }
  ��� currentRowKey = rowKeyStr.getBytes();
  � }

  � @Override
  � public List&lt;PutRequest&gt; getActions() {
  ��� // Split the event body and get the values for the columns
  ��� String eventStr = new String(currentEvent.getBody());
  ��� String[] cols = eventStr.split(",");
  ��� puts.clear();
  ��� for (int i = 0; i &lt; cols.length; i++) {
  ����� //Generate a PutRequest for each column.
  ����� PutRequest req = new PutRequest(table, currentRowKey, colFam,
  ������������� columnNames[i], cols[i].getBytes());
  ����� puts.add(req);
  ��� }
  ��� return puts;
  � }

  � @Override
  � public List&lt;AtomicIncrementRequest&gt; getIncrements() {
  ��� incs.clear();
  ��� //Increment the number of events received
  ��� incs.add(new AtomicIncrementRequest(table, "totalEvents".getBytes(), colFam, eventCountCol));
  ��� return incs;
  � }

  � @Override
  � public void cleanUp() {
  ��� table = null;
  ��� colFam = null;
  ��� currentEvent = null;
  ��� columnNames = null;
  ��� currentRowKey = null;
  � }

  � @Override
  � public void configure(Context context) {
  ��� //Get the column names from the configuration
  ��� String cols = new String(context.getString("columns"));
  ��� String[] names = cols.split(",");
  ��� byte[][] columnNames = new byte[names.length][];
  ��� int i = 0;
  ��� for(String name : names) {
  ����� columnNames[i++] = name.getBytes();
  ��� }
  � }

  � @Override
  � public void configure(ComponentConfiguration conf) {
  � }
  }
  This serializer splits the event body based on a delimiter and inserts each split into a different column. The row is defined in the event header. When each event is received, a counter is incremented to keep track of the number of events received as well. This serializer can be configured by the following configuration:   host1.sinks.sink1.type = org.apache.flume.sink.hbase.AsyncHBaseSink
  host1.sinks.sink1.channel = ch1
  host1.sinks.sink1.table = transactions
  host1.sinks.sink1.columnFamily = clients
  host1.sinks.sink1.batchSize = 5000
  #The serializer to use
  host1.sinks.sink1.serializer = org.apache.flume.sink.hbase.SplittingSerializer
  #List of columns each event writes to.
  host1.sinks.sink1.serializer.columns = charges,date,priority
  Internals of the HBaseSink and AsyncHBaseSink The HBaseSink uses the HBase HTable API to write events out to HBase. HTable supports batching of Puts, but only HBase 0.92+ supports batching of Increments. Currently, the HBase Sink is single-threaded and will call the serializer to get the Puts and Increments once per event it processes. HBase Put and Increments are sent to HBase via blocking calls, which means the next event is read and passed to the serializer only once the current event is successfully written to HBase. Each transaction consists of at most the number of events specified by the batchSize property in the configuration. Like all other Flume sinks, if one of these events fails to get written successfully, the sink will retry the entire transaction again. On the other hand, the AsyncHBaseSink uses the asynchbase API, and sends out events asynchronously to HBase. The AsyncHBaseSink, in the same way as the HBase sink, generates Puts and Increments for each event. Once the Puts and Increments are generated, the sink sends them out immediately to HBase and moves on to process the next event. Success or failure is handled through callbacks. Again, each transaction consists of at most the number of events specified by the batchSize configuration parameter. The sink waits until either success callbacks are received for all the events sent, or at least one error callback is received. If an error callback is received, the entire transaction is retried, in true Flume style. A Word of Caution As you can see, if HBase reports failure to write even one Put or Increment, the entire transaction is retried – this is how Flume�s at-least-once semantics work, and most Flume sinks operate in the same way. In case of HBase Increments, this means it is possible that the same event would cause a counter to be incremented more than once. This is something to keep in mind while using Flume to perform Increments. Also, if the serializer is not idempotent, then this means that it is possible that the same event can cause multiple different Puts to be written to HBase. Imagine a case where we are talking about credit card transactions represented by the event. If the same event can generate different Puts each time, it is possible that HBase would have multiple records of the same transactions, which is probably not desired. The AsyncHBaseSink is known to give better performance than the HBaseSink primarily because of the non-blocking nature of the underlying API it uses. The HBase community is working on improving the HBase client API to improve its performance, which would vastly improve the HBaseSink performance. Conclusion Flume is an excellent tool to write events out to the different storage systems in the Hadoop ecosystem including HBase. The HBase sinks provide the functionality to write data to HBase in your own schema and allows the user to �map� the Flume event to HBase data. Hari Shreedharan is a Software Engineer at Cloudera, and an Apache Flume committer.</snippet></document><document id="329"><title>This Month in Data Science</title><url>http://blog.cloudera.com/blog/2012/11/this-month-in-data-science/</url><snippet>Data science has been a ubiquitous topic of conversation in the IT and business worlds across the month of November. In this brief post, I’ll bring you just a small cross-section of the data science meme on the Interwebs in the past 4 weeks: As part of its annual “Best Jobs 2012″ feature, CNNMoney called data science one of the “best new jobs in America” – right up there with “video game designer” and “solar sales consultant”. How could you go wrong with that career choice? � The National Cancer Institute-funded Frederick National Laboratory was named the winner of the 2012 Government Big Data Solutions Award, for “pioneering ways to support researchers working on complex challenges around the relationship between genes and cancers.”�This application highlights the unique power of data science to correlate relationships across vast data volumes. � John Foreman (@John4Man), a practicing data scientist himself, published a fascinating blog post in which he explains the “5 things” that data scientists should know about their role. His main message to other practitioners? Get over yourself, and “work with the rest of your organization to do better, not to do data science for its own sake.” � RedMonk’s Donnie Berkholz (@dberkholz) wondered aloud whether data science will/should become social and collaborative, via agile-esque workflow. Interesting thought. � Cloudera held its first “Introduction to Data Science” training class and announced a new�round of 2012 cities and dates�- New York, Redwood City, and Washington, D.C., included thus far. (Private training is also available in other cities for groups of 20 or more.) � Cloudera released a “New to Data Science” page at cloudera.com/developer. Have other resources to suggest for this list? Let us know in comments. � And finally, SINAInnovations 2012, a medical technology conference associated with Mount Sinai School of Medicine of New York, convened and completed its 2012 program, which included a keynote (“Innovation and Data”) from Cloudera’s Chief Scientist, Jeff Hammerbacher (@hackingdata). See below! �</snippet></document><document id="330"><title>Introducing Hannibal: A Tool for Apache HBase Region Monitoring</title><url>http://blog.cloudera.com/blog/2012/11/introducing-hannibal-a-tool-for-hbase-region-monitoring/</url><snippet>The following is a guest post from Nils K�bler, the creator of the Hannibal project. He is software engineer at Sentric, a Swiss big data specialist, providing consultancy, development and training. Hannibal aims to help Apache HBase administrators monitor the cluster in terms of region distribution and is basically a decision-making aid for manual splitting. It widens the monitoring capabilities of HBase by providing different views with interactive graphs of the cluster. Hannibal is also a Web-based tool that fits smoothly into your existing Hadoop/HBase ecosystem. Hannibal is open source (MIT License) and implemented in Scala. In its current version it supports HBase 0.90. Support for versions &gt; 0.90 is planned and will be added soon. The Joy of Splitting A “region” is the basic unit of data distribution and balancing in HBase. The proper region size (and quantity) has a direct impact on the overall system performance. It is therefore vital for any production cluster to monitor the region growth and distribution over time respectively. Manually splitting Apache HBase regions has some advantages over managed splitting and is a widely used practice in the industry. Possible advantages include: Much easier debugging and profiling of region log files Shifting of the region split to off-peak hours Prevention of region hot-spotting, at least if the row-key design allows it Prevention of a compaction storm (large disk I/O and network traffic) when having roughly uniform data distribution and growth Now, let’s take a closer look at the Hannibal UI. Region Distribution Hannibal�s main page shows a graph with the distribution of the regions over the cluster. It�s a bar-chart showing how much space is assigned on each RegionServer. Each bar is also separated into multiple colors for the tables. Hovering over those parts reveals more information such as the number of regions on that server. � This view can give first hints whether the distribution of the tables is ideal or not. Region Splits per Table Hannibal�s table view shows a graph for all regions of a table, ordered by the size. On an optimal table with evenly distributed regions, every bar should be about the same size. There is also a red line which shows the configured hbase.hregion.max.filesize, which, depending on your configuration, may help you to decide when a region should be split or not. This graph can show you which regions you should split or merge next. Region History Hannibal also allows you to get deeper information for each region. Therefore Hannibal records different metrics. Right now the recorded metrics are: Number of storefiles Size of the memstore Size of the storefiles Compactions This information can also help you make decisions like whether the region should be split. The Graph reveals details and problems on your region. More Information Please have a look at the readme on GitHub. You can find links there to the source code, documentation and installation information. There is also a video tutorial available. We encourage HBase developers and administrators to try Hannibal out. Let us know what you think, what you like, what you don�t, or what additional features you would like to see.</snippet></document><document id="331"><title>The Winner of the 2012 Government Big Data Solutions Award is the National Cancer Institute</title><url>http://blog.cloudera.com/blog/2012/11/the-winner-of-the-2012-government-big-data-solutions-award-is-the-national-cancer-institute/</url><snippet>The following is a re-post from CTOVision.com. The Government Big Data Solutions Award was established to highlight innovative solutions and facilitate the exchange of best practices, lessons learned and creative ideas for addressing�Big Data challenges. The�Top Five Nominees of 2012�were chosen for criteria that included: Focus on current solutions: The ability to make a difference in government missions in the very near term was the most important evaluation factor. Focus on government teams: Industry supporting government also considered, but this is about government missions. Consideration of new approaches: New business processes, techniques, tools, models for enhancing analysis are key. The top five honorees for the 2012 award include: Veterans Health Administration: New Big Data approaches and frameworks provide data and tools for 20,000 clinicians to track medical trends, better anticipate outcomes. The scope of the data set is over 80 billion data files. Focused on service to 25 million veterans. Judges selected Veterans Health Administration because of the impact and best practices in Big Data solutions. NASA: Multiple and extensive activities. One of many exemplars was the NASA�Center for Climate Simulation (NCCS). Their work includes scalable Apache Hadoop clusters for large scale climate simulations. Bureau of Engraving and Printing: This government agency is the largest producer of security documents in country. They have fielded an Oracle based Big Data solution enhanced quality and mission support, reduced waste. Judges characterized this as a good match of right business processes to and a modern technical approach. AMSAA: Army Materiel Systems Analysis Activity. Vehicle data analysis program instruments vehicles in theater to collect operational and environmental parameter historical data. Massive data pattern screening and analysis toolsets put in place. Result: rapid identification of issues before mission impact. National Cancer Institute Funded Frederick National Laboratory: Extensive research and working prototypes of cutting edge systems based on Cloudera Distribution of Apache Hadoop (CDH) and the Oracle Big Data Appliance. Judges noted the significant potential impact of this solution as well as the strength of the technical approach. The winner of the 2012 Government Big Data Solutions Award is the�National Cancer Institute Funded Frederick National Laboratory. The National Cancer Institute (NCI) has spent years focusing thought and technical research on issues of concern to all of humanity. The are the part of the National Institutes of Health (NIH) responsible for coordinating the US National Cancer Program. They conduct �and support research, training, health information dissemination and other related�activities�related�to the causes, prevention, diagnosis and treatment of cancer; the supportive care of cancer�patients�and their families, and cancer survivorship. NCI funds the�Frederick National Laboratory. The�NCI Funded Frederick National Laboratory�has been using Big Data solutions in pioneering ways to support researchers working on complex challenges around the relationship between genes and cancers. In a �recent example, they have built infrastructure capable of cross-referencing the�relationships between 17000 genes and five major cancer subtypes across 20 million biomedical publication abstracts. �By cross referencing TCGA gene expression data from simulated 60 million patients and miRNA expression for a�simulated�900 million patients. The result: understanding additional layers of the pathways these genes operate in and the drugs that target them. This will help researchers accelerate their work in areas of importance for all humanity. �This solution, based on the Oracle Big Data Appliance with the Cloudera Distribution of Apache Hadoop (CDH), leverages capabilities available from the Big Data community today in pioneering ways that can serve a broad range of researchers. The promising approach of this solution is repeatable across many other Big Data challenges for bioinfomatics, making this approach worthy of its selection as the 2012 Government Big Data Solution Award. On behalf of our judges and all those who may one day benefit from research into the prevention and treatment of cancer, we congratulate the�Frederick National Laboratory�for this pioneering work, and add our appreciation to the teams of industry technologists developing well engineered solutions that make this work possible (with a special thanks to�Oracle�and�Cloudera).</snippet></document><document id="332"><title>The "Ask Bigger Questions" Contest!</title><url>http://blog.cloudera.com/blog/2012/11/the-ask-bigger-questions-contest/</url><snippet>Have you helped your company ask bigger questions? Our mission at Cloudera University is to equip Hadoop professionals with the skills to manage, process, analyze, and monetize more data than they ever thought possible. Over the past three years, we’ve heard many great stories from our training participants about faster cluster deployments, complex data workflows made simple, and superhero troubleshooting moments. And we’ve heard from executives in all types of businesses that staffing Cloudera Certified professionals gives them confidence that their Hadoop teams have the skills to turn data into breakthrough insights. Now, it’s your turn to tell us your bigger questions story! Cloudera University is seeking tales of Apache Hadoop success originating with training and certification. How has an investment in your education paid dividends for your company, team, customer, or career? The most compelling stories chosen from all entrants will receive prizes like Amazon gift cards, discounted Cloudera University training, autographed copies of Hadoop books from O’Reilly Media, and Cloudera swag. We may even turn your story into a case study! Sign up to participate�here. Submissions must be received by Friday, Feb. 1, 2013 to qualify for a prize.</snippet></document><document id="333"><title>Apache ZooKeeper 3.4.5 Has Been Released</title><url>http://blog.cloudera.com/blog/2012/11/apache-zookeeper-3-4-5-has-been-released/</url><snippet>Apache ZooKeeper release 3.4.5 is now available. This is a bug fix release covering 3 issues, one of which was considered critical. These issues were: ZOOKEEPER-1550 ZooKeeperSaslClient does not finish anonymous login on OpenJDK ZOOKEEPER-1560 Zookeeper client hangs on creation of large nodes ZOOKEEPER-1376 zkServer.sh does not correctly check for $SERVER_JVMFLAGS Stability, Compatibility and Testing 3.4.5 is a stable release that�s fully backward compatible with 3.4.4. Only bug fixes relative to 3.4.4 have been applied. Issues running ZooKeeper using Java 7 and OpenJDK have been resolved. Getting Involved The Apache ZooKeeper project is working on a number of new features. Our How To Contribute page is a great place to start if you�re interested in getting involved as a developer, or dive right into an open issue. Acknowledgements A special thanks to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc), especially Mahadev Konar for acting as release manager.</snippet></document><document id="334"><title>Dive Into Cloudera Impala at a Meetup Near You</title><url>http://blog.cloudera.com/blog/2012/11/dive-into-cloudera-impala-at-a-meetup-near-you/</url><snippet>Since the Cloudera Impala announcement of a few weeks ago, we’ve been busy partnering-up with Hadoop meetups around the country (and beyond) to bring Impala tech talks directly to the community. Here’s the list for the remainder of 2012, thus far: Nov. 1, 2012 – Twin Cities Hadoop Users Group (Minneapolis) – lapsed Nov. 14, 2012 – Orange County (CA) Hadoop Users Group Nov. 14, 2012 – SF Hadoop Users (open agenda, but we’ll be pitching an Impala talk!) Nov. 15, 2012 �- Los Angeles Hadoop Users Group Nov. 26, 2012 – Swiss Big Data Users Group (Zurich) Dec. 3, 2012 – Chicago-area Hadoop Users Group/Chicago Big Data Meetup Dec. 5, 2012 – Seattle Scalability Meetup …and we’re working on additional (2013) dates in Boston, Raleigh-Durham, Washington, D.C. , (added 11/15/2012) London, France, and Belgium – and those are just the groups we’re currently in touch with. If you’re a HUG, Big Data, BI, or Data Science meetup organizer and are interested in an Impala speaker too, just let us know in comments below.</snippet></document><document id="335"><title>Announcing the Kiji Project: An Open Source Framework for Building Big Data Applications with Apache HBase</title><url>http://blog.cloudera.com/blog/2012/11/announcing-the-kiji-project-an-open-source-framework-for-building-big-data-applications-with-apache-hbase/</url><snippet>The following is a guest post from Aaron Kimball, who was Cloudera�s first engineer and the creator of the Apache Sqoop project. He is the Founder and CTO at WibiData, a San Francisco-based company building big data applications. Our team at WibiData has been developing applications on Hadoop since 2010 and we�ve helped many organizations transform how they use data by deploying Hadoop. HBase in particular has allowed companies of all types to drive their business using scalable, high performance storage. Organizations have started to leverage these capabilities for various big data applications, including targeted content, personalized recommendations, enhanced customer experience and social network analysis. While building many of these applications, we have seen emerging tools, design patterns and best practices repeated across projects. One of the clear lessons learned is that Hadoop and HBase provide very low-level interfaces. Each large-scale application we have built on top of Hadoop has required a great deal of scaffolding and data management code. This repetitive programming is tedious, error-prone, and makes application interoperability more challenging in the long run. Today, we are proud to announce the launch of the Kiji project (www.kiji.org), as well as the first Kiji component: KijiSchema. The Kiji project was developed to host a suite of open source components built on top of Apache HBase and Apache Hadoop, that makes it easier for developers to: Use HBase as a real-time data storage and serving layer for applications Maximize HBase performance using data management best practices Get started building data applications quickly with easy startup and configuration Kiji is open source and licensed under the Apache 2.0 license. The Kiji project is modularized into separate components to simplify adoption and encourage clean separation of functionality. Our approach emphasizes interoperability with other systems, leveraging the open source HBase, Avro and MapReduce projects, enabling you to easily fit Kiji into your development process and applications.� KijiSchema: Schema Management for HBase The first component within the Kiji project is KijiSchema, which provides layout and schema management on top of HBase. KijiSchema gives developers the ability to easily store both structured and unstructured data within HBase using Avro serialization. It supports a variety of rich schema features, including complex, compound data types, HBase column key and time-series indexing, as well cell-level evolving schemas that dynamically encode version information. KijiSchema promotes the use of entity-centric data modeling, where all information about a given entity (user, mobile device, ad, product, etc.), including dimensional and transaction data, is encoded within the same row. This approach is particularly valuable for user-based analytics such as targeting, recommendations, and personalization. BentoBox: Get Started Developing with Kiji and Hadoop Fast To aid developers new to HBase and Hadoop, we are also providing the quickest, easiest deployment of HBase as part of a Kiji BentoBox, which will install and run a fully-functional HBase mini-cluster with KijiSchema on your machine in under 15 minutes. You do not need to have Hadoop or HBase installed to run the BentoBox. You can get a Kiji BentoBox and KijiSchema here. We encourage current HBase developers to check out KijiSchema by downloading the source code from GitHub. Over the next several months, we will be releasing additional Kiji components focused on improving the usability and performance of HBase and Hadoop for application development. We also welcome outside contributors who would like to help support and develop Kiji. You can join our mailing lists and learn more about how to contribute at kiji.org.</snippet></document><document id="336"><title>Top Five Nominees for the 2012 Government Big Data Solutions Award</title><url>http://blog.cloudera.com/blog/2012/11/top-five-nominees-for-the-2012-government-big-data-solutions-award/</url><snippet>The following is a re-post from Bob Gourley of�CTOVision.com. The amount of data being created in governments is growing faster than humans can analyze. But analysis can solve tough challenges. Those two facts are driving the continual pursuit of new Big�Data solutions. Big Data solutions are of particular importance in government. The government has special abilities to focus research in areas like Health Sciences, Economics,�Law Enforcement, Defense, Geographic Studies,�Environmental Studies, Bioinformatics, and Computer Security. Each of those area can be well served by Big Data approaches, and each has exemplars of solutions worthy of highlighting to the community. The Government Big Data Solutions Award was established to help highlight some of the best innovation in the federal space. The 2012 award process solicited nominations from across federal, state and local governments. Nominations were evaluated based on how well submissions addressed three key factors: Focus on current solutions: The ability to make a difference in government missions in the very near term was the most important evaluation factor. Focus on government teams: Industry supporting government also considered, but this is about government missions. Consideration of new approaches: New business processes, techniques, tools, models for enhancing analysis are key. Nominees were evaluated by a panel of judges that included:�Doug Cutting of�Cloudera�(creator of Hadoop), Alan Wade (former government CIO), Chris Dorobek (leading government journalist and publisher of the�DorobekInsider.com, and Ed Granstedt (industry leading federal sector technologist). The judges selected five exemplars for recognition as the top five nominees for the 2012 award. They include: Veterans Health Administration: New Big Data approaches and frameworks provide data and tools for 20,000 clinicians to track medical trends, better anticipate outcomes. The scope of the data set is over 80 billion data files. Focused on service to 25 million veterans. Judges selected Veterans Health Administration because of the impact and best practices in Big Data solutions. NASA: Multiple and extensive activities. One of many exemplars was the NASA�Center for Climate Simulation (NCCS). Their work includes scalable Hadoop clusters for large scale climate simulations. Bureau of Engraving and Printing: This government agency is the largest producer of security documents in country. They have fielded an Oracle based Big Data solution enhanced quality and mission support, reduced waste. Judges characterized this as a good match of right business processes to and a modern technical approach. AMSAA: Army Materiel Systems Analysis Activity. Vehicle data analysis program instruments vehicles in theater to collect operational and environmental parameter historical data. Massive data pattern screening and analysis toolsets put in place. Result: rapid identification of issues before mission impact. National Cancer Institute: Extensive research and working prototypes of cutting edge systems based on Cloudera Distribution of Apache Hadoop (CDH) and the Oracle Big Data Appliance. Judges noted the significant potential impact of this solution as well as the strength of the technical approach. Details on the judges’ top pick will be announced on 20 Nov 2012.</snippet></document><document id="337"><title>Cloudera Impala Beta (version 0.2) and Cloudera Manager 4.1.1 Now Available</title><url>http://blog.cloudera.com/blog/2012/11/cloudera-impala-beta-version-0-2-and-cloudera-manager-4-1-1-now-available/</url><snippet>I am pleased to announce the release of Cloudera Impala Beta (version 0.2) and Cloudera Manager 4.1.1. These are both enhancement releases to make bug fixes available quickly. Key enhancements in each release are: Cloudera Impala Beta (version 0.2) Bug fixes Impala Beta will be regularly updated with features, bug fixes, and performance enhancements. We will typically release such updates every 2 weeks. Please check the release notes to find out what’s new with each update. Cloudera Manager 4.1.1 Tarballs for installing Cloudera Manager are now available Bug fixes Detailed release notes Download CM 4.1.1 or Upgrade to CM 4.1.1 As a reminder, here is how you can get started with Impala: Use Cloudera Manager 4.1.x to deploy Impala. Documentation can be found here. To manually install Impala, access the download of Impala, install Impala and try it out. Please note that you need to have CDH 4.1.x installed on RHEL/CentOS 6.2. Access the demo VM of Impala. The VM includes instructions that show you the power of Impala. E-learning course on “An Introduction To Impala“. Access the Impala source code at: https://github.com/cloudera/impala. Download and review the Impala documentation. Once you get started, we encourage you to provide feedback. We have the following mechanisms set up to do this: An Impala user group has been set up. Please use this to ask questions and provide feedback. An Impala Jira project has been set up. Feature requests and bug reports are welcome.</snippet></document><document id="338"><title>Analyzing Twitter Data with Apache Hadoop, Part 3: Querying Semi-structured Data with Apache Hive</title><url>http://blog.cloudera.com/blog/2012/11/analyzing-twitter-data-with-hadoop-part-3-querying-semi-structured-data-with-hive/</url><snippet>This is the third article in a series about analyzing Twitter data using some of the components of the Apache Hadoop ecosystem that are available in CDH (Cloudera’s open-source distribution of Apache Hadoop and related projects). If you’re looking for an introduction to the application and a high-level view, check out the first article in the series. In the previous article in this series, we saw how Flume can be utilized to ingest data into Hadoop. However, that data is useless without some way to analyze the data. Personally, I come from the relational world, and SQL is a language that I speak fluently. Apache Hive�provides an interface that allows users to easily access data in Hadoop via SQL. Hive compiles SQL statements into MapReduce jobs, and then executes them across a Hadoop cluster. In this article, we�ll learn more about Hive, its strengths and weaknesses, and why Hive is the right choice for analyzing tweets in this application. Characterizing Data One of the first questions to ask when deciding on the right tool for the job is: �what does my data look like?� If your data has a very strict schema, and it doesn�t deviate from that schema, maybe you should just be using a relational database. MySQL�is just as free as Hive, and very effective for dealing with well-structured data. However, as you start to try to analyze data with less structure or with extremely high volume, systems like MySQL become less useful, and it may become necessary to move out of the relational world. Unstructured, semi-structured, and poly-structured are all terms for data that doesn�t fit well into the relational model. This is data like JSON, XML, RDF, or other sorts of data with a schema that may vary from record to record. What do we do with this data? Here�s where Hive shines. Hive is extremely effective for dealing with data that doesn�t quite fit into the relational bucket, because it can process complex, nested types natively. Hive avoids the need for complicated transformations that might be otherwise necessary to handle this sort of data in a traditional relational system. Hive can also gracefully handle records that don�t strictly conform to a table�s schema. For example, if some columns are missing from a particular record, Hive can deal with the record by treating missing columns as NULLs. In the Twitter analysis example, we loaded raw tweets into HDFS. Using the Twitter Streaming API, tweets are represented as JSON blobs. {
   "retweeted_status": {
      "contributors": null,
      "text": "#Crowdsourcing � drivers already generate traffic data for your smartphone to suggest alternative routes when a road is clogged. #bigdata",
      "geo": null,
      "retweeted": false,
      "in_reply_to_screen_name": null,
      "truncated": false,
      "entities": {
         "urls": [],
         "hashtags": [
            {
               "text": "Crowdsourcing",
               "indices": [
                  0,
                  14
               ]
            },
            {
               "text": "bigdata",
               "indices": [
                  129,
                  137
               ]
            }
         ],
         "user_mentions": []
      },
      "in_reply_to_status_id_str": null,
      "id": 245255511388336128,
      "in_reply_to_user_id_str": null,
      "source": "&lt;a href=\"http://www.socialoomph.com\" rel=\"nofollow\"&gt;SocialOomph&lt;\/a&gt;",
      "favorited": false,
      "in_reply_to_status_id": null,
      "in_reply_to_user_id": null,
      "retweet_count": 0,
      "created_at": "Mon Sep 10 20:20:45 +0000 2012",
      "id_str": "245255511388336128",
      "place": null,
      "user": {
         "location": "Oregon, ",
         "default_profile": false,
         "statuses_count": 5289,
         "profile_background_tile": false,
         "lang": "en",
         "profile_link_color": "627E91",
         "id": 347471575,
         "following": null,
         "protected": false,
         "favourites_count": 17,
         "profile_text_color": "D4B020",
         "verified": false,
         "description": "Dad, Innovator, Sales Professional. Project Management Professional (PMP).  Soccer Coach,  Little League Coach  #Agile #PMOT - views are my own -",
         "contributors_enabled": false,
         "name": "Scott Ostby",
         "profile_sidebar_border_color": "404040",
         "profile_background_color": "0F0F0F",
         "created_at": "Tue Aug 02 21:10:39 +0000 2011",
         "default_profile_image": false,
         "followers_count": 19005,
         "profile_image_url_https": "https://si0.twimg.com/profile_images/1928022765/scott_normal.jpg",
         "geo_enabled": true,
         "profile_background_image_url": "http://a0.twimg.com/profile_background_images/327807929/xce5b8c5dfff3dc3bbfbdef5ca2a62b4.jpg",
         "profile_background_image_url_https": "https://si0.twimg.com/profile_background_images/327807929/xce5b8c5dfff3dc3bbfbdef5ca2a62b4.jpg",
         "follow_request_sent": null,
         "url": "http://facebook.com/ostby",
         "utc_offset": -28800,
         "time_zone": "Pacific Time (US &amp; Canada)",
         "notifications": null,
         "friends_count": 13172,
         "profile_use_background_image": true,
         "profile_sidebar_fill_color": "1C1C1C",
         "screen_name": "ScottOstby",
         "id_str": "347471575",
         "profile_image_url": "http://a0.twimg.com/profile_images/1928022765/scott_normal.jpg",
         "show_all_inline_media": true,
         "is_translator": false,
         "listed_count": 45
      },
      "coordinates": null
   },
   "contributors": null,
   "text": "RT @ScottOstby: #Crowdsourcing � drivers already generate traffic data for your smartphone to suggest alternative routes when a road is  ...",
   "geo": null,
   "retweeted": false,
   "in_reply_to_screen_name": null,
   "truncated": false,
   "entities": {
      "urls": [],
      "hashtags": [
         {
            "text": "Crowdsourcing",
            "indices": [
               16,
               30
            ]
         }
      ],
      "user_mentions": [
         {
            "id": 347471575,
            "name": "Scott Ostby",
            "indices": [
               3,
               14
            ],
            "screen_name": "ScottOstby",
            "id_str": "347471575"
         }
      ]
   },
   "in_reply_to_status_id_str": null,
   "id": 245270269525123072,
   "in_reply_to_user_id_str": null,
   "source": "web",
   "favorited": false,
   "in_reply_to_status_id": null,
   "in_reply_to_user_id": null,
   "retweet_count": 0,
   "created_at": "Mon Sep 10 21:19:23 +0000 2012",
   "id_str": "245270269525123072",
   "place": null,
   "user": {
      "location": "",
      "default_profile": true,
      "statuses_count": 1294,
      "profile_background_tile": false,
      "lang": "en",
      "profile_link_color": "0084B4",
      "id": 21804678,
      "following": null,
      "protected": false,
      "favourites_count": 11,
      "profile_text_color": "333333",
      "verified": false,
      "description": "",
      "contributors_enabled": false,
      "name": "Parvez Jugon",
      "profile_sidebar_border_color": "C0DEED",
      "profile_background_color": "C0DEED",
      "created_at": "Tue Feb 24 22:10:43 +0000 2009",
      "default_profile_image": false,
      "followers_count": 70,
      "profile_image_url_https": "https://si0.twimg.com/profile_images/2280737846/ni91dkogtgwp1or5rwp4_normal.gif",
      "geo_enabled": false,
      "profile_background_image_url": "http://a0.twimg.com/images/themes/theme1/bg.png",
      "profile_background_image_url_https": "https://si0.twimg.com/images/themes/theme1/bg.png",
      "follow_request_sent": null,
      "url": null,
      "utc_offset": null,
      "time_zone": null,
      "notifications": null,
      "friends_count": 299,
      "profile_use_background_image": true,
      "profile_sidebar_fill_color": "DDEEF6",
      "screen_name": "ParvezJugon",
      "id_str": "21804678",
      "profile_image_url": "http://a0.twimg.com/profile_images/2280737846/ni91dkogtgwp1or5rwp4_normal.gif",
      "show_all_inline_media": false,
      "is_translator": false,
      "listed_count": 7
   },
   "coordinates": null
}
 It�s fairly easy to see that there is a good bit of complexity to this data structure. Since JSON can contain nested data structures, it becomes very hard to force JSON data into a standard relational schema. Processing JSON data in a relational database would likely require significant transformation, making the job much more cumbersome. Looking at this particular bit of JSON, there are some very interesting fields: At the very top, there is a retweeted_status object, whose existence indicates that this tweet was retweeted by another user. If the tweet was not a retweet, you would not have a retweeted_status object at all. Tweets also contain an entities element, which is a nested structure. It contains three arrays, the elements of which are all nested structures in their own right, as can be seen in the hashtags array, which has two entries. How do you deal with a record like this in Hive? Complex Data Structures Hive has native support for a set of data structures that normally would either not exist in a relational database, or would require definition of custom types. There are all the usual players: integers, strings, floats, and the like, but the interesting ones are the more exotic maps, arrays, and structs. Maps and arrays work in a fairly intuitive way, similar to how they work in many scripting languages: SELECT array_column[0] FROM foo;
SELECT map_column[�map_key�] FROM foo;
 Structs are a little more complicated, since they are arbitrary structures, and a struct field can be queried much like an instance variable in a Java class: SELECT struct_column.struct_field FROM foo;
 To store the data for a tweet, arrays and structs will be crucial. A Table for Tweets Here is the table that was designed to store tweets, with some columns omitted: CREATE EXTERNAL TABLE tweets (
�...
�retweeted_status STRUCT&lt;
���text:STRING,
���user:STRUCT&gt;,
�entities STRUCT&lt;
���urls:ARRAY&gt;,
���user_mentions:ARRAY&gt;,
���hashtags:ARRAY&gt;&gt;,
�text STRING,
�...
)
PARTITIONED BY (datehour INT)
ROW FORMAT SERDE 'com.cloudera.hive.serde.JSONSerDe'
LOCATION '/user/flume/tweets';
 By comparing the JSON objects from the tweet with the columns in the table, we can see how the JSON objects are mapped to Hive columns. Looking at the entities column, we can see what a particularly complex column might look line: �entities STRUCT&lt;
���urls:ARRAY&gt;,
���user_mentions:ARRAY&gt;,
���hashtags:ARRAY&gt;&gt;
 entities is a struct which contains three arrays, and each individual array stores elements which are also structs. If we wanted to query the screen names of the first mentioned user from each tweet, we could write a query like this: SELECT entities.user_mentions[0].screen_name FROM tweets;
 If the user_mentions array is empty, Hive will just return NULL for that record. The PARTITIONED BY clause utilizes a feature of Hive called partitioning, which allows tables to be split up in different directories. By building queries that involve the partitioning column, Hive can determine that certain directories cannot possibly contain results for a query. Partitioning allows Hive to skip the processing of entire directories at query time, which can improve query performance dramatically. The LOCATION clause is a requirement when using EXTERNAL tables. By default, data for tables is stored in a directory located at /user/hive/warehouse/ . However, EXTERNAL tables can specify an alternate location where the table data resides, which works nicely if Flume is being used to place data in a predetermined location. EXTERNAL tables also differ from regular Hive tables, in that the table data will not be removed if the EXTERNAL table is dropped. The ROW FORMAT clause is the most important one for this table. In simple datasets, the format will likely be DELIMITED, and we can specify the characters that terminate fields and records, if the defaults are not appropriate. However, for the tweets table, we�ve specified a SERDE. Serializers and Deserializers In Hive, SerDe is an abbreviation for Serializer and Deserializer, and is an interface used by Hive to determine how to process a record. Serializers and Deserializers operate in opposite ways. The Deserializer interface takes a string or binary representation of a record, and translates it into a Java object that Hive can manipulate. The Serializer, on the other hand, will take a Java object that Hive has been working with, and turn it into something that Hive can write to HDFS. Commonly, Deserializers are used at query time to execute SELECT statements, and Serializers are used when writing data, such as through an INSERT-SELECT statement. In the Twitter analysis example, we wrote a JSONSerDe, which can be used to transform a JSON record into something that Hive can process. Putting It All Together By utilizing the SerDe interface, we can instruct Hive to interpret data according to its inherent structure (or lack thereof). Since a SerDe is just a property of a Hive table, rather than the data, itself, we can also swap out SerDes as our data evolves. That flexibility allows us to choose the right tools for the job at hand, and to interpret data in different ways. It makes Hive a spectacular choice for getting quick access to data of all types. In the first post in this series, we saw how we could use Hive to find influential users. Let�s look at some other queries we might want to write. Geographic distributions of users can be interesting to look at. Unfortunately, the data I got from Twitter does not contain much of the geographic information necessary to plot really precise locations for users, but we can use time zones to get a sense of where in the world the users are. We can ask a question like, �Which time zones are the most active per day?�: SELECT
�user.time_zone,
�SUBSTR(created_at, 0, 3),
�COUNT(*) AS total_count
FROM tweets
WHERE user.time_zone IS NOT NULL
GROUP BY
�user.time_zone,
�SUBSTR(created_at, 0, 3)
ORDER BY total_count DESC
LIMIT 15;

Results in:
Eastern Time (US &amp; Canada) ���Tue ���2737
Eastern Time (US &amp; Canada) ���Wed ���2663
Pacific Time (US &amp; Canada) ���Wed ���1725
Pacific Time (US &amp; Canada) ���Tue ���1643
Eastern Time (US &amp; Canada) ���Thu ���1421
Central Time (US &amp; Canada) ���Wed ���1376
Central Time (US &amp; Canada) ���Tue ���1285
London ���Wed ���1066
London ���Tue ���1065
Brussels ���Wed ���920
Brussels ���Tue ���894
Pacific Time (US &amp; Canada) ���Thu ���892
London ���Thu ���824
Brussels ���Thu ���713
Central Time (US &amp; Canada) ���Thu ���655
 Interestingly, more users are tweeting about the selected terms on the east coast, than the west coast. Europe also seems to be pretty interested in big data. We can also formulate more complex queries to ask questions like �Which were the most common hashtags?�: SELECT
�LOWER(hashtags.text),
�COUNT(*) AS total_count
FROM tweets
LATERAL VIEW EXPLODE(entities.hashtags) t1 AS hashtags
GROUP BY LOWER(hashtags.text)
ORDER BY total_count DESC
LIMIT 15;

Results in:
bigdata ���8890
analytics ���2965
cloud ���1390
job ���1090
jobs ���876
cloudcomputing ���675
cloudexpo ���625
seo ���522
hadoop ���516
data ���470
bi ���463
nosql ���426
marketing ���349
ibm ���349
tech ���272
 Not surprisingly, several of the terms that I searched for when I was collecting data show up here. The first term that shows up, which I didn�t search for, is job, followed by jobs. Cloudera�s hiring, by the way. You may also notice the use of some non-standard SQL constructs, like LATERAL VIEW and EXPLODE. Lateral views are used when using functions like EXPLODE, which may generate more than one row of output for each row of input. One Thing to Watch Out For… If it looks like a duck, and it sounds like a duck, then it must be a duck, right? For users who are new to Hive, do not mistake Hive for a relational database. Hive looks a lot like a database, and you can interact with it very much like a database, but it should not be treated as such. Any query run in Hive is actually executed as a sequence of MapReduce jobs, which brings with it all of the performance implications of having to start up multiple JVMs. This means that all queries will have to pay a fixed setup cost, which will result in poor performance when running lightweight queries. This fact makes Hive particularly nice for executing batch workloads. Like MapReduce, Hive shines brightest when working with massive data sets. However, it is important to realize that Hive queries may not have a response time that can be considered interactive, and Hive will likely not serve as a replacement for a traditional analytical database. Conclusion In this article we�ve discussed some of the benefits and trade-offs of using Hive, and seen how to build a SerDe to process JSON data, without any preparation of the data. By using the powerful SerDe interface with Hive, we can process data that has a looser structure than would be possible in a relational database. This enables us to query and analyze traditional structured data, as well as semi- and even unstructured data. Jon Natkins (@nattybnatkins) is a Software Engineer at Cloudera, where he has worked on Cloudera Manager and Hue, and has contributed to a variety of projects in the Apache Hadoop ecosystem. Prior to Cloudera, Jon wrangled databases at Vertica. He holds an Sc.B in Computer Science from Brown University.</snippet></document><document id="339"><title>What’s New in Apache Sqoop 1.4.2</title><url>http://blog.cloudera.com/blog/2012/11/whats-new-in-apache-sqoop-1-4-2/</url><snippet>(The following is a re-post from apache.org) Apache Sqoop 1.4.2 was released in August 2012. As this was an extremely important release for the Sqoop community – our first release as an Apache Top Level project – I would like to highlight the key features and fixes of this release. The entire change log can be viewed on our JIRA and actual bits can be downloaded from the usual place. Apache Hadoop 2.0.0 Support One of the main the goals of the previous 1.4.1-incubating release was to make sure that Sqoop works on various Apache Hadoop versions (that it’s not locked to one specific version). We’ve made sure that Sqoop works on Hadoop 0.20, 1.0 and 0.23. We’re excited to announce that in Sqoop 1.4.2 we’ve added support for Hadoop 2.0.x (SQOOP-574) and so now Sqoop will work on four different major Hadoop releases! Even though we made sure that the same codebase will work across all different Hadoop versions, we couldn’t ensure the same for the generated binary artifacts. Different Hadoop versions are not binary compatible with each other and there is nothing that Sqoop can do about that. You need to make sure that the Sqoop version that you’re using has been compiled against the appropriate Hadoop version that your cluster is using. To help users pick the correct Sqoop binary, all artifacts in the release have a suffix that encodes the supported Hadoop version. Compatibility with Old Connectors One of our goals during incubation was to move the entire code base from the com.cloudera.sqoop namespace to the org.apache.sqoop namespace. The change would have broken backward compatibility for connectors compiled against Sqoop 1.3.x. Incompatibility was a showstopper for us and thus we developed this solution. Unfortunately during the code migration we overlooked one issue that affected seamless usage of Microsoft, Oracle and other special connectors. With that resolved, there are no outstanding compatibility issues and all special connectors that were built against Sqoop 1.3.x should work on 1.4.2 without any issues or required modifications. Incremental Imports of Free Form Queries One of many interesting and very useful features of Sqoop is incremental import: only deltas of new data are imported on an incremental basis to Hadoop (HDFS, Apache Hive, Apache HBase). We’ve extended this feature to support free form queries (SQOOP-444) as well. Implicit and Explicit Connector Pickup Improvements Logic that leads to connector selection in Sqoop was fragile and very hard to predict. Sqoop supports the parameters --driver and --connection-manager that are supposed to provide a reliable way to specify what connector and JDBC driver will be picked up. Unfortunately, by specifying the --driver option, the built-in connection manager selection defaults to the generic JDBC connector, ignoring the --connection-manager option and any installed third party connectors. To address those nasty issues we’ve improved the connector selection (SQOOP-529) process, so that the entire logic is now far more predictable and reliable. Exporting Only a Subset of Columns The Export feature allows you to move large data volumes from the Hadoop ecosystem (HDFS, Hive) to relational database systems and data warehouses for additional processing or reporting. Until this release, the table that Sqoop was performing export into had to contain exactly the same number of columns as the exported data. This is highly undesirable in some use cases, so we’ve made sure (SQOOP-503) that the parameter --columns that was working in import will also work for export. However exporting data to only a subset of columns brings additional requirements to the target table: other columns must have either a default value or be nullable. The export job will fail otherwise. Verbose Logging Verbose mode wasn’t correctly working in Sqoop 1.4.x due to the aforementioned namespace migration. We’ve made sure to fix that (SQOOP-460, �SQOOP-488). In addition we’ve extended the reach of the --verbose parameter to generated mapreduce jobs (SQOOP-436), so we now provide much better logging when needed during the entire import or export process! Hive Imports The Sqoop team has improved Hive imports in several ways. The first improvement was that the user no longer needs to remove the empty temporary directory after Sqoop moves data to Hive (SQOOP-443). The main benefit is that the user now can perform multiple Hive imports with the same temporary directory without the need for extra manual steps. Until this release, each Hive import was using a temporary directory with the same name as the importing table stored in the user’s home directory. The second notable improvement was to support arbitrary temporary directories when doing a Hive import (SQOOP-483). I believe that 1.4.2 is a great release with many improvements and interesting new features. All users are highly encouraged to upgrade to this latest release. Jarek Jarcec Cecho is Software Engineer at Cloudera, and a Sqoop committer and PMC member.</snippet></document><document id="340"><title>What’s New in CDH4.1 Mahout</title><url>http://blog.cloudera.com/blog/2012/11/whats-new-in-cdh4-1-mahout/</url><snippet>Cloudera recently announced the general availability of CDH4.1, an update to our open-source, enterprise-ready distribution of Apache Hadoop and related projects. Among various components, Apache Mahout is a relatively recent addition to CDH (first added to CDH3u2 in 2011), but is already attracting increasing interest out in the field.� Mahout started as a sub-project of Apache Lucene to provide machine-learning libraries in the area of clustering and classification.�It later evolved into a top-level Apache project with much broader coverage of machine-learning techniques (clustering, classification, recommendation, frequent itemset mining etc.).� In CDH4.1, Mahout is upgraded to upstream version 0.7.�Several new changes are included in this release, and this post will briefly go over some of the interesting ones. Outlier Removal Capability A new design of cluster classification (MAHOUT-930) is introduced to enable consistency and extensibility across a number of clustering implementations.�Classifying data into clusters is now factored out as a separate step after building clusters from data.�This separation provides a nice foundation for plugging in the outlier removal capability (MAHOUT-929).�This new feature helps prune out those data that are far different from others in the same cluster.�To use this capability, an outlier threshold (between 0.0 and 1.0) needs to be provided.�With an outlier threshold specified, data will not be classified into the cluster if their probability distribution function values are less than the threshold value. New Clustering Implementations The implementations of the K-Means (MAHOUT-981), Fuzzy K-Means (MAHOUT-984), Canopy (MAHOUT-982), and Dirichlet (MAHOUT-983) algorithms are now based on the following cluster classification interfaces: ClusterClassifier, ClusteringPolicy, ClusterIterator, etc. In addition, as a result of the enhancements, these clustering implementations are equipped with the new outlier removal capability.�From a user perspective, existing drivers, such as KMeansDriver, FuzzyKMeansDriver, CanopyDriver and DirichletDriver, can still be used as the entry points for clustering data. Bayes Classifier Cleanup Prior to the current release, there were two different implementations of Naive Bayes classifier.�The commands mahout trainclassifier and mahout testclassifier utilized an implementation that took text-based data.�However, there were occasional out-of-memory problems with the original implementation. A new implementation (MAHOUT-287) taking vector-based data was subsequently introduced with the commands mahout trainnb and mahout testnb.�This new implementation proved to work better than the old one in the past few releases, and therefore the old Naive Bayes implementation has been removed as part of this release (MAHOUT-1010). Known Issue under Oozie There is a known issue of null pointer exceptions when Mahout is invoked as a shell action of Oozie and the distributed (map reduce) version of an algorithm is used in Mahout.�This issue results from the effort (MAHOUT-848) to propagate Oozie action configuration down to Mahout job execution, but checking for null on configuration was missing there. The issue was later discovered, but the fix (MAHOUT-1033) didn�t catch the release vehicle of version 0.7.�One workaround is to use Mahout as a Java action of Oozie instead of a shell action to bypass the configuration propagation and thus the issue itself. Summary With CDH4.1, Mahout provides machine learning libraries in four main areas. Recommendation finds items that users might like. Clustering groups together items that are related. Classification assigns unlabeled items to categories.� Frequent itemset mining find items that appear together. Mahout in CDH4.1 is upgraded to upstream version 0.7 with several new features and improvements. You can refer to the release notes for details about these changes.�You are also encouraged to check out the CDH4.1 documentation and the Mahout website for more information.</snippet></document><document id="341"><title>See You at Data Science Day (Nov. 29, New York)!</title><url>http://blog.cloudera.com/blog/2012/11/see-you-at-data-science-day-nov-29-new-york/</url><snippet>[Updated Nov. 26, 2012: Sorry, this event has reached capacity and is now closed.] Please join us�in New York on Nov. 29, 2012, for a unique opportunity to hear from industry icons Jeff Hammerbacher (@hackingdata), Amr Awadallah (@awadallah) and Josh Wills (@josh_wills) as they discuss their approach to Data Science and how it transformed business for companies like Facebook, Yahoo! and Google. You will also hear more about Cloudera Enterprise: The Platform for Big Data powered by Cloudera Impala, which takes Hadoop �beyond batch� and into the world of real-time interactivity. All are welcome – however, quantitative analysts, Hadoop users/developers, business management or those involved in business intelligence and enterprise data warehousing projects would benefit greatly from attending. The first 25 registered individuals to arrive will receive a copy of Beautiful Data: The Stories Behind Elegant Data Solutions autographed by Jeff Hammerbacher, contributing author and editor. Register today, space is limited! All attendees will be entered into a drawing to win one of two $250 American Express Gift Cards.</snippet></document><document id="342"><title>How to Get Rich on Big Data</title><url>http://blog.cloudera.com/blog/2012/11/how-to-get-rich-on-big-data/</url><snippet>The 2012 Strata + Hadoop World conference was week before last in New York City. Cloudera co-presented the conference with O’Reilly Media this year, and we were really pleased with how the event turned out. Of course we launched Cloudera Impala, but there was a ton of news from companies across the Apache Hadoop ecosystem. Andrew Brust over at ZDNet wins the prize for comprehensive coverage of all the announcements. I also liked Tony Baer’s excellent roll-up of all the SQL news on the OnStrategies blog. One piece of coverage crossed my inbox this past week that is not generally available. Peter Goldmacher is a Managing Director and Senior Research Analyst for The Cowen Group, a financial services company headquartered in Manhattan. Cowen helps its clients invest wisely, and Peter’s job is to research and report on industry trends that could shape that investment. Peter and his colleague Joe del Callar wrote up an excellent analysis of the Big Data market after attending Strata + Hadoop World. Because their report is published primarily for Cowen’s clients, it’s not easy to link to. Peter has, however, graciously given me permission to excerpt it here. Thank you, Peter! The title of the report is Quick Take: Hadoop World Observations: One Step Closer to Mainstream Adoption. It makes the argument that Hadoop isn’t yet mainstream, but that customer adoption and vendor investment have accelerated in the last twelve months, and that an inflection point is looming. Peter and Joe say: In our mind, the signs that Hadoop is about to inflect will include: 1. Meaningful and repeatable projects: Early adopters will pioneer use cases and fast followers will replicate those use cases. Once more mainstream companies see these use cases and are comfortable that the vanguard has derisked the implementation, the use case goes big time. … 2. M&amp;A: The market won�t support 50+ Big Data infrastructure companies. … Every successful software company wants to leverage its installed base with more products and when we start to see the separation of winners from losers, we�ll see the venture community doubling down on winners and cutting losses on their losers. This will come in the form of M&amp;A. 3. Apps companies built on Big Data Infrastructure: … The reason Oracle emerged as the dominant database vendor was because of its partnerships with VARs and ISVs that helped abstract the complexity of working directly with the database and created a new market opportunity of large and repeatable use cases like financials and manufacturing. … We expect to see a lot more Big Data Apps companies getting started in the coming years. From our perspective at Cloudera, all three points are exactly right. Our customers don’t care nearly as much about Hadoop as they care about engaging better with their customers or preventing fraud in their transaction flows. People choose the platform because it solves problems that they care about. As a company, we talk much less about technology, and much more about use cases that matter to lots of organizations. M&amp;A activity has been modest so far, but we expect it to accelerate. I gave a talk at the beginning of 2012 laying out twelve predictions for Hadoop for the year, and number zero (the bonus prediction) was that we’d see some acquisitions before end of year. VMware picked up analytics tool vendor Cetas not long after, but there’s been less activity than I had expected and there are just two months left to go in 2012. Nevertheless, we’re still betting on more acquisitions of smaller players in the near term, because the Cowen argument is sound: The market needs consolidation to concentrate investment. That trend will be balanced by new companies being created at a fast pace to commercialize innovative ideas for Big Data analytics out of industry and academia. The pace of new product releases and announcements by tools and applications vendors has been torrid this year. Established vendors like Microstrategy, Informatica, SAS and others have delivered Hadoop-capable products. Specialist BI and analytics companies like Tableau, Pentaho and Splunk have expanded their product lines to embrace Hadoop. Emerging companies with products purpose-built for Hadoop, like Datameer, Continuuity, Wibidata, Platfora and Trifacta have entered the fray. This activity is crucial. Since customers care about solving real problems, they need tools and apps that are easy to use and that exploit the strength of the platform while hiding its complexity. In the report, Peter and Joe go on to identify three categories of winners as Big Data proliferates. Big Data infrastructure providers (we count ourselves in this category at Cloudera) will produce a handful of dominant new companies in the industry. Big Data application providers will be much more numerous and will create a rich and valuable ecosystem for data processing and analysis. It is the third category, though, that caught my eye and that convinced me to sit down and write a blog post. I’ll let Peter and Joe speak again: 3. Big Data Practitioners: We believe the biggest winners in the Big Data world aren�t the Big Data technology vendors, but rather the companies that will leverage Big Data technology to create entirely new businesses or disrupt legacy businesses. Past as prologue, if we look at the history of ERP, there were over 200 companies created to capitalize on the automation of standard business processes. This means that investors in 1990 had less than a 0.5% chance of picking either SAP or ORCL as the ultimate winners in the space. However, if an investor had purchased stock in the 30 components of the Dow in 1990 that were all deploying ERP, that investor would have benefited from a 35% decline in General and Administrative costs as a percentage of revenues, a five-fold increase in revenues as automation enabled massive scale, and an almost eight-fold increase in market cap. Of course we at Cloudera believe we are creating a very valuable company for the long term — we wouldn’t be doing this otherwise. Nevertheless, I agree entirely with the conclusion in the Cowen report. What’s exciting about this platform isn’t the opportunity it’s giving to vendors like us. It’s the value that Hadooop is unlocking in Big Data for the industry and for society at large. Companies that embrace the platform and the tools to ask bigger questions are the ones that will profit most. This echoes the spread of a generation of open source platform technologies like Linux, MySQL and JBoss. Of course there were entrepreneurs and companies behind each of those projects, and some of them did pretty well. Think, though, about companies like Google and Amazon, that adopted those projects and built their businesses on top of them. Think of whole industries, like financial services, that embraced them to do more things faster, better and more cheaply. That’s where the real money was made. You want to get rich on Big Data? Use it!</snippet></document><document id="343"><title>The New "Hadoop in Practice" Book: A Chat with The Author</title><url>http://blog.cloudera.com/blog/2012/11/the-new-hadoop-in-practice-book-a-chat-with-the-author/</url><snippet>Today we bring you a brief interview with Alex Holmes, author of the new book, Hadoop in Practice (Manning). You can learn more about the book and download a free sample chapter here. There are a few good Hadoop books on the market right now. Why did you decide to write this book, and how is it complementary to them? When I started working with Hadoop I leaned heavily on Tom White’s excellent book, Hadoop: The Definitive Guide (O’Reilly Media), to learn about MapReduce and how the internals of Hadoop worked. As my experience grew and I started working with Hadoop in production environments I had to figure out how to solve problems such as moving data in and out of Hadoop, using compression without destroying data locality, performing advanced joining techniques and so on. These items didn’t have a lot of coverage in existing Hadoop books, and that’s really the idea behind Hadoop in Practice – it’s a collection of real-world recipes that I learned the hard way over the years. Hadoop in Practice covers more advanced aspects of working with Hadoop such as MapReduce and HDFS patterns, performance tuning and debugging. The book also looks at how Hadoop can be used as a platform for data science and for data warehousing by studying R integration techniques, and intermediary Pig and Hive recipes. Data mining is another important topic today, and a book on Hadoop isn’t complete without a look at how Mahout lets you run your favorite algorithms at scale. I believe this is the first Hadoop book which presents its contents in a problem/solution format. Accompanying each solution is the background behind it, as well as alternatives if the recommended solution doesn’t work in the reader’s particular situation. Another unique trait of my book is its heavy use of visual aids to help explain complex concepts, and the large number of working code examples which can be immediately leveraged by the reader. Who is your intended reader? I view my book as being useful to developers that have committed to using Hadoop, are familiar with Hadoop fundamentals, and are starting to ask questions such as “What data format should I use to store my data?”, “How do I run algorithms such as PageRank in MapReduce?” and “How do I use Bloom filters to optimize my joins?” The applications of traditional software engineering practices such as unit testing, debugging and performance tuning to Hadoop are also covered to help ease the adoption of Hadoop in engineering teams. In your research, what did you learn that you did not already know? Writing about these subjects in my book required me to pile through reams of Hadoop source code as well as Hadoop-related open-source projects so that I could better explain concepts to readers. This was often a humbling experience – I would start out thinking I understood a topic inside-out, only to discover that in reality my working knowledge wasn’t all it was cracked up to be! Using Bloom filters in MapReduce to optimize joins was one area which I hadn’t had any exposure to prior to writing the book, but it has ended up being one of the recipes that I use the most. In your view, what is the #1 most important thing needed for wide adoption of Hadoop? I believe making Hadoop easier to deploy, administer and interact with are key to its continued adoption. Organizations such as Cloudera are helping address the administrative challenges, and I believe that we will see technologies such as YARN and Cloudera Impala open Hadoop up to a wider audience. What recent additions to the Hadoop stack have you most excited? YARN, Impala, HA? These technologies are really exciting, and a testament to how Hadoop is evolving and maturing. CTO’s and folks in Operations will love Hadoop HA and how Hadoop is being transformed into a de facto enterprise technology. Impala is a great boon to data scientists and data analysts everywhere that have been crying out for a real-time analytics layer on top of their data in Hadoop. I’m a developer and I’m very interested in the opportunities that YARN opens up for me in terms of alternative computing models to MapReduce. It’s a great time to be working with Hadoop!</snippet></document><document id="344"><title>Mike Olson at FutureBI Meetup (Berkeley, Nov. 6)</title><url>http://blog.cloudera.com/blog/2012/11/mike-olson-at-futurebi-meetup-berkeley-nov-6/</url><snippet>The FutureBI meetup is excitedly preparing to host Cloudera CEO Mike Olson at its upcoming meetup on Nov. 6 at the Berkeley School of Information, where he�ll be joined by SiSenseVP of Marketing Bruno Aziza in a conversation titled �The Future of Big Data: We depend on you!�� The event will be open to the public as well as to Berkeley students and faculty.� Questions are encouraged during this informal and interactive session, which will cover topics ranging from the evolution of open source software, the changing entrepreneurial ecosystem in the Bay Area, and the likely future of information management.� Founders, geeks, and tech industry professionals are all welcome to attend and join the discussion! Please RSVP for the event before Nov. 6 by visiting the meetup page, and follow @Cloudera or @FutureBI for updates and more information.�</snippet></document><document id="345"><title>Training a New Generation of Data Scientists</title><url>http://blog.cloudera.com/blog/2012/10/data-science-training/</url><snippet>Last week at Strata + Hadoop World 2012, we announced a new data science training�and certification program. I am very excited to have been part of the team that put the program together, and I would like to answer some of the most frequently asked questions about the course and the certification that we will be offering. Why is Cloudera offering data science training? The primary bottleneck on the success of Hadoop is the number of people who are capable of using it effectively to solve business problems. Addressing that�bottleneck with training has always been a very large part of our mission here at Cloudera, and we are very fortunate to have one of the best training teams�anywhere. So far, we have trained over 15,000 Hadoop developers and administrators, and our courses and certification exams are available all over the world. Right now, one of the biggest barriers to the widespread adoption of Hadoop is the supply of data scientists, the peculiar blend of software engineer and statistician that�is capable of turning data into awesome. We’ve started to see data science courses develop at universities like Columbia, The University of Washington, and UC Berkeley (taught by Cloudera�co-founder Jeff Hammerbacher). While these courses provide excellent instruction to a new generation of data scientists, the instruction they provide is necessarily�limited to the students who are enrolled in those institutions, and the need for data science training is much broader and much more immediate. Earlier this year, Jeff and I started working with Cloudera’s training team to distill our experiences at Facebook and Google into a course�that would teach the fundamentals of data science: everything from the pragmatic application of machine learning and statistics to business problems�to the data ingest and preparation that is so critical in our work. We hope that by sharing our experience and showing how we take advantage of Hadoop to�solve problems, we can help address the shortage of data scientists. What are the goals of the class? What do you expect students to get out of it? First, we want the course to cover the lifecycle of a data science project, from data acquisition and preparation through model development,�production deployment, and evaluation. If you want to sample from a grab bag of methods in machine learning and statistics, there are lots of courses to�choose from; we wanted our course to teach students how to build data products. Second, we want our students to understand that data science isn’t nearly as difficult as it is made out to be. It does involve some new tools and a different way�of thinking about problems, but it doesn’t require any skills that can’t be taught to a motivated student and then improved upon with practice. Third, we want data scientists to understand that they are force multipliers within an organization, and that everything they do should be oriented towards�making everyone- decision makers, suppliers, and customers- more effective at using data to make decisions. Why is the focus on building recommender systems? Recommender systems are an ideal way to learn about data science with Hadoop, if only because of how simply and clearly a recommendation engine can�demonstrate the unreasonable effectiveness of data. But that isn’t the only reason we wanted to build the course around recommendation engines: They are applicable in lots of industries. Almost everyone is familiar with recommendations from retail services like Amazon and Netflix, but recommender systems can�also used in predicting disease, improving quality of service, and estimating carbon footprints. If you are in a business that relies on the effective use of data, you probably have a problem that could benefit from�using a recommendation engine. The mathematics of recommenders is simple to understand. We wanted the course to be approachable by people who hadn’t taken a math class in quite some time,�and other kinds of problems (e.g., building classifiers for predicting ad clicks or fraudulent transactions) require at least a little bit of calculus. We didn’t�want the course to get bogged down in the technical aspects of the modeling problem and lose focus on the practical techniques that data scientists need�to do in their day to day work. The skills required to build a recommender generalize well to other problems. We felt that the process of building a recommendation engine�perfectly illustrated all of the steps invovled in creating a data product. No matter what you do in your career as a data scientist- and you will do a little�bit of everything, from creating dashboards, to advancing the state of the art in machine learning, to reconciling what your customers say they do in surveys with�what they actually do in transaction logs- what you learn in this course will serve you well. What are the prerequisites for the course? The course is appropriate for software engineers, statisticians, and business analysts who are familiar with basic Hadoop commands, Hive, and a scripting language�like Python, Perl, or Ruby (the labs in the course use Python). There isn’t any Java programming in the course, but we do discuss and make use of Mahout’s commandline tools�to create recommendations. We will also show you how to use R�to visualize data and perform simple data analysis tasks. Data science is an interdisciplinary field, which means that there will be parts of the course that will be more or less familiar to you depending on your�background and experience. We also want to emphasize the importance of communication and teamwork in data science: there will be some labs where you guide other�students, and others where you may need help from students who have more experience. This is very much by design; no single person is an expert at every aspect�of data science, and learning how to work as part of a multidisciplinary team is crucial. How will the certification program work? Certifying data scientists is difficult, as the ability to create data products is the real mark of a practicing data scientist. Cloudera is going to do something new for our data science certification program: we will be combining a written exam that ensures students have a basic set of�skills and knowledge with a hands-on exam that is designed to measure both technical ability and the capacity to develop creative approaches to building data�products. You won’t be required to take the data science course in order to take the data science certification exam, but it will certainly help. We will be�announcing more details about the certification exam process in January, after we’ve had our first cohort of students go through the data science course. When can I take the course? I’m so glad you asked. I have the pleasure of teaching the first course in the Bay Area myself, Nov. 14-16, 2012, and our training team will offer the second course in New York on Dec. 12-14, 2012.�We will be teaching the course in additional locations based on demand; you can keep an eye on the schedule of public training courses here, and we’re always happy to do onsite training classes�that are optimized for the needs of your team. We look forward to seeing you in class.</snippet></document><document id="346"><title>Quorum-based Journaling in CDH4.1</title><url>http://blog.cloudera.com/blog/2012/10/quorum-based-journaling-in-cdh4-1/</url><snippet>A few weeks back, Cloudera announced CDH 4.1, the latest update release to Cloudera’s Distribution including Apache Hadoop. This is the first release to introduce truly standalone High Availability for the HDFS NameNode, with no dependence on special hardware or external software. This post explains the inner workings of this new feature from a developer’s standpoint. If, instead, you are seeking information on configuring and operating this feature, please refer to the CDH4 High Availability Guide. Background Since the beginning of the project, HDFS has been designed around a very simple architecture: a master daemon, called the NameNode, stores filesystem metadata, while slave daemons, called DataNodes, store the filesystem data. The NameNode is highly reliable and efficient, and the simple architecture is what has allowed HDFS to reliably store petabytes of production-critical data in thousands of clusters for many years; however, for quite some time, the NameNode was also a single point of failure (SPOF) for an HDFS cluster. Since the first beta release of CDH4 in February, this issue has been addressed by the introduction of a Standby NameNode, which provides automatic hot failover capability to a backup. For a detailed discussion of the design of the HA NameNode, please refer to the�earlier post by my colleague Aaron Myers. Limitations of NameNode HA in Previous Versions As described in the March blog post, NameNode High Availability relies on shared storage�- in particular, it requires some place in which to store the HDFS edit log which can be written by the Active NameNode, and simultaneously read by the Standby NameNode. In addition, the shared storage must itself be highly available — if it becomes inaccessible, the Active NameNode will no longer be able to continue taking namespace edits. In versions of HDFS prior to CDH4.1, we required that this shared storage be provided in the form of an NFS mount, typically on an enterprise-grade NAS device. For some organizations, this fit well with their existing operational practices, and indeed we have several customers running highly available NameNode setups in production environments. However, other customers and community members found a number of limitations with the NFS-based storage: Custom hardware – the hardware requirements of a NAS device can be expensive. Additionally, fencing configurations may require a remotely controllable Power Distribution Unit (PDU) or other specialized hardware. In addition to the financial expenses, there may be operational costs: many organizations choose not to deploy NAS devices or other custom hardware in their datacenter. Complex deployment – even after HDFS is installed, the administrator must take extra steps�to configure NFS mounts, custom fencing scripts, etc. This complicates HA deployment and may even�cause unavailability if misconfigured. Poor NFS client implementations – Many versions of Linux include NFS client implementations that are buggy and difficult to configure. For example, it is easy for an administrator to misconfigure mount�options in such a way that the NameNodes will freeze unrecoverably in some outage scenarios. External dependencies�- depending on a NAS device for storage requires that operators monitor and maintain one more piece of infrastructure. At the minimum, this involves configuring extra alerts and metrics, and in some organizations may also introduce an inter-team dependency: the operations team responsible for storage may be part of a different organizational unit as those responsible for Hadoop Operations. Removing These Limitations Given the above limitations and downsides, we evaluated many options and created a short list of requirements for a viable replacement: No requirement for special hardware�- like the rest of Hadoop, we should depend only on commodity hardware — in particular, on physical nodes that are part of existing clusters. No requirement for custom fencing configuration�- fencing methods such as STONITH�require custom hardware; instead, we should rely only on software methods. No SPOFs�- since the goal here is HA, we don’t want to simply push the HA requirement onto another component. Given the requirement to avoid SPOFs and custom hardware, we knew that any design we decided upon would involve storing multiple replicas of the metadata on multiple commodity nodes. Given this, we added the following additional requirements: Configurable for any number of failures�- rather than designing a system which only tolerates a single failure, we should give operators the flexibility to choose their desired level of resiliency, by adding extra replicas of the metadata. One slow replica should not affect latency�- because the metadata write path is a critical component for the performance of NameNode operations, we need to ensure that the latency remains low. If we have several replicas, we need to ensure that a failure or slow disk on one of the replicas does not impact the latency of the system. Adding journal replicas should not negatively impact latency - if we allow administrators to configure extra replicas to tolerate several simultaneous failures, this should not result in an adverse impact on performance. As a company focused on making Hadoop easier to deploy and operate, we also considered the following operational requirements: Consistency with other Hadoop components- any new components introduced by the design should operate similarly to existing components; for example, they should use XML-based configuration files, log4j logging, and the same metrics framework. Operations-focused metrics�- since the system is a critical part of NameNode operation, we put a high emphasis on exposing metrics. The new system needs to expose all important metrics so that it can be operated in a long-lived production cluster and give early warnings of any problems — long before�they cause unavailability. Security�- CDH offers comprehensive security, including encryption on the wire and strong authentication via Kerberos. Any new components introduced for the design must uphold the same standards as the rest of the stack: for customers requiring encryption, it is just as important to encrypt metadata as it is to encrypt data. QuorumJournalManager After discussion internally at Cloudera, with our customers, and with the community, we designed a system called QuorumJournalManager. This system is based around a simple idea: rather than store the HDFS edit logs in a single location (eg an NFS filer), instead store them on several locations, and use a distributed protocol to ensure that these several locations stay correctly synchronized. In our system, the remote storage is on a new type of HDFS daemon called the JournalNode. The NameNode acts as a client and writes edits to a set of JournalNodes, and considers the edits committed when they have been replicated successfully to a majority of these nodes. � Similarly, when the NameNode in Standby state needs to read the edits to maintain its hot backup of the namespace, it can read from any of the replicas stored on the JournalNodes. Distributed Commit Protocol The description above is simplified: the NameNode simply writes edits to each of the three nodes, and succeeds when it is replicated to the majority of them. However, this raises several interesting questions: What happens if a batch of edits is sent to one of the nodes but not the others, and then the NameNode crashes? What happens if there is a “split brain” scenario in which two NameNodes both try to assert their active (writer) status? How does the system recover from inconsistencies at startup, in the case that several nodes crashed while edits were in-flight? The fully detailed answer to these questions doesn’t fit nicely into a blog post, but short answer is that the system relies upon an implementation of the well-known Paxos�protocol. This protocol specifies a correct way of ensuring a consistent value for some piece of data across a cluster of nodes. In our system, we use an implementation of Multi-Paxos to commit each batch of edits, and additionally use Paxos for recovery�- the process by which the standby NameNode cleans up any pending batches of edits immediately after a failover. Those interested in the full details and algorithms should reference the HDFS-3077 design document. Fencing and Epoch Numbers One of the key requirements for the system, as described in the introduction of this post, is to avoid any special fencing hardware or software. Fencing is the mechanism by which, after a failover, the new Active NameNode ensures that the previous Active is no longer able to make any changes to the system metadata. In other words, fencing is the cure for “Split Brain Syndrome” — a potential scenario in which two nodes both think they are active and make conflicting modifications to the namespace.�So, how does the QuorumJournalManager implement fencing? The key to fencing in the QuorumJournalManager is the concept of epoch numbers. Whenever a NameNode becomes active, it first needs to generate an epoch number. These numbers are integers which strictly increase, and are guaranteed to be unique once assigned. The first active NameNode after the namespace is initialized starts with epoch number 1, and any failovers or restarts result in an increment of the epoch number. In essence, the epoch numbers act as a sequencer�between two NameNodes – if a NameNode has a higher epoch number, then it is said to be “newer” then any NameNodes with an earlier epoch number. NameNodes generate these epoch numbers using a simple algorithm which ensures that they are fully unique: a given epoch number will never be assigned twice. The details of this algorithm can also be found in the design document referenced above. Given two NameNodes, which both think they are active, each with their own unique epoch numbers, how can we avoid Split Brain Syndrome? The answer is suprisingly simple and elegant: when a NameNode sends any message (or remote procedure call) to a JournalNode, it includes its epoch number as part of the request. Whenever the JournalNode receives such a message, it compares the epoch number against a locally stored value called the promised epoch. If the request is coming from a newer epoch, then it records that new epoch as its promised epoch. If instead the request is coming from an older epoch, then it rejects the request. This simple policy avoids split-brain as follows: For any NameNode to successfully write edits, it has to successfully write to a majority of nodes. That means that a majority of nodes have to accept its epoch number as the newest. When a new NameNode becomes active, it has an epoch number higher than any previous NameNode. So, it simplify makes a call to all of the JournalNodes, causing them to increment their promised epochs. If it succeeds on a majority, it considers the new epoch to be successfully established. Because the two majorities above must intersect, it follows that the old NameNode, despite thinking it is active, will no longer be able to make any successful calls to a majority. Hence it is prevented from making any successful namespace modifications. Testing Though Paxos is simple on paper, and proven to be correct, it is notoriously difficult to implement correctly. Therefore, while developing this system, we spent more than half of our time on testing and verification. We found a few techniques to be particularly crucial: MiniCluster testing - early on, we wrote a simple Java class called MiniJournalCluster, which runs several JournalNodes in the same JVM. This allowed us to automate distributed scenarios in the context of a JUnit functional test case. Mock/spy testing�- we wrote a number of unit tests using Mockito to inject spies between the QuorumJournalManager client and the JournalNodes within the same JVM. For example, a Mockito spy can easily be instructed to throw an IOException at a function call matching a particular argument. We used this to write deterministic tests for a number of different failure and crash scenarios identified during design discussions. Randomized fault testing - although we were able to write tens of tests by hand for different fault scenarios, these tests were limited to the types of failures we could easily mentally generate. In my experience building distributed systems, the ones that are more worrisome are the ones you can’t�easily identify a priori. So, we introduced randomized fault tests based on deterministic seeds: given a random seed, the test case introduces a series of faults into the protocol which are completely determined by that seed. For example, a given seed may cause the 2nd, 3rd, 8th, and 45th RPC from the NameNode to the second JournalNode to fail. These tests simulate hundreds of failovers between nodes while injecting faults, and simultaneously verify that no committed transactions are lost. In addition to running the above tests by traditional means, we also built a test harness to run the randomized fault test on a MapReduce cluster. The test harness is a simple Hadoop Streaming job which runs the randomized fault test for several minutes, then outputs its test log to HDFS. The input to the test is a 5000-line file containing random seeds, and uses an NLineInputFormat to pass each seed to a separate task. Thus, 5000 instances of the random test case can easily be run in parallel on a large cluster. Upon finishing, a second streaming job runs grep�against the results, looking either for test failures or for unexpected messages in the log such as AssertionErrors or NullPointerExceptions. Using this harness, we were able to soak-test millions of failover scenarios and run for several CPU-years, and discovered several bugs along the way. The testing was in fact so comprehensive that we found two new bugs in Jetty, an HTTP Server component used internally by Hadoop. We rolled the fixes for these Jetty bugs into CDH4.1 as well. Summary The new feature described in this post is available in CDH4.1 and, thanks to hard work by the Cloudera Manager team, is very easy to deploy and monitor with just a few mouse clicks in CM4.1. Like all of our HDFS development, the project was developed in the open in the Apache Software Foundation repositories and tracked on the ASF JIRA at�HDFS-3077�. The new code has been merged into the Apache trunk branch, to be included in an upstream Apache HDFS release in the near future. Acknowledgements I would like to acknowledge a number of people from the wider Apache HDFS community who contributed to this project: Aaron T. Myers and Eli Collins for code reviews and contributions around security, configuration, and docs Sanjay Radia, Suresh Srinivas, Aaron Myers, Eli Collins, Henry Robinson, Patrick Hunt, Ivan Kelly, Andrew�Purtell, Flavio Junqueira, Ben Reed, Nicholas Sze, Bikas Saha, and Chao Shi for design discussions Brandon Li and Hari Mankude for their work on the HDFS-3092 branch which formed some of the early building blocks for the JournalNode Stephen Chu and Andrew Purtell for their help with cluster testing Vinithra Varadharajan, Chris Leroy, and the Cloudera Manager team for help with integration testing, metrics and configuration</snippet></document><document id="347"><title>Cloudera Manager 4.1 Now Available; Supports Impala Beta Release</title><url>http://blog.cloudera.com/blog/2012/10/cloudera-manager-4-1-now-available-supports-impala-beta-release/</url><snippet>I am very pleased to announce the availability of Cloudera Manager 4.1. This release adds support for the Cloudera Impala beta release, and management and monitoring of key CDH features. Here are the highlights of Cloudera Manager 4.1: Support for Quorum-based Storage HDFS High Availability Cloudera Impala management and monitoring Flume NG management and monitoring ZooKeeper monitoring Directory disk-space monitoring Host decommissioning Reduced monitoring latency Maintenance mode Several usability, performance, and other improvements Details on these features and more can be found in the Release Notes. To download the free edition of Cloudera Manager 4.1, please visit our downloads page. You can upgrade to Cloudera Manager 4.1 if you have an existing installation of Cloudera Manager.</snippet></document><document id="348"><title>Cloudera Impala: Real-Time Queries in Apache Hadoop, For Real</title><url>http://blog.cloudera.com/blog/2012/10/cloudera-impala-real-time-queries-in-apache-hadoop-for-real/</url><snippet>After a long period of intense engineering effort and user feedback, we are very pleased, and proud, to announce the Cloudera Impala project. This technology is a revolutionary one for Hadoop users, and we do not take that claim lightly. When Google published its Dremel paper in 2010, we were as inspired as the rest of the community by the technical vision to bring real-time, ad hoc query capability to Apache Hadoop, complementing traditional MapReduce batch processing. Today, we are announcing a fully functional, open-sourced codebase that delivers on that vision – and, we believe, a bit more – which we call Cloudera Impala. An Impala binary is now available in public beta form, but if you would prefer to test-drive Impala via a pre-baked VM, we have one of those for you, too. (Links to all downloads and documentation are here.) You can also review the source code and testing harness at Github right now. Impala raises the bar for query performance while retaining a familiar user experience. With Impala, you can query data, whether stored in HDFS or Apache HBase – including SELECT, JOIN, and aggregate functions – in real time. Furthermore, it uses the same metadata, SQL syntax (Hive SQL), ODBC driver and user interface (Hue Beeswax) as Apache Hive, providing a familiar and unified platform for batch-oriented or real-time queries. (For that reason, Hive users can utilize Impala with little setup overhead.) The first beta drop includes support for text files and SequenceFiles; SequenceFiles can be compressed as Snappy, GZIP, and BZIP (with Snappy recommended for maximum performance). Support for additional formats including Avro, RCFile, LZO text files, and the Parquet�columnar format is planned for the production drop. To avoid latency, Impala circumvents MapReduce to directly access the data through a specialized distributed query engine that is very similar to those found in commercial parallel RDBMSs. The result is order-of-magnitude faster performance than Hive, depending on the type of query and configuration. (See FAQ below for more details.) Note that this performance improvement has been confirmed by several large companies that have tested Impala on real-world workloads for several months now. A high-level architectural view is below: There are many advantages to this approach over alternative approaches for querying Hadoop data, including:: Thanks to local processing on data nodes, network bottlenecks are avoided. A single, open, and unified metadata store can be utilized. Costly data format conversion is unnecessary and thus no overhead is incurred. All data is immediately query-able, with no delays for ETL. All hardware is utilized for Impala queries as well as for MapReduce. Only a single machine pool is needed to scale. We encourage you to read the documentation for further technical details. Finally, we�d like to answer some questions that we anticipate will be popular: Is Impala open source? Yes, Impala is 100% open source (Apache License). You can review the code for yourself at Github today. How is Impala different than Dremel? The first and principal difference is that Impala is open source and available for everyone to use, whereas Dremel is proprietary to Google. Technically, Dremel achieves interactive response times over very large data sets through the use of two techniques: A novel columnar storage format for nested relational data/data with nested structures Distributed scalable aggregation algorithms, which allow the results of a query to be computed on thousands of machines in parallel. The latter is borrowed from techniques developed for parallel DBMSs, which also inspired the creation of Impala. Unlike Dremel as described in the 2010 paper, which could only handle single-table queries, Impala already supports the full set of join operators that are one of the factors that make SQL so popular. In order to realize the full performance benefits demonstrated by Dremel, Hadoop will shortly have an efficient columnar binary storage format called�Parquet. But contrary to Dremel, Impala supports a range of popular file formats. This lets users run Impala on their existing data without having to �load� or transform it. It also lets users decide if they want to optimize for flexibility or just pure performance. To sum it up, Impala plus Parquet will achieve the query performance described in the Dremel paper, but surpass what is described there in SQL functionality. How much faster are Impala queries than Hive ones, really? The precise amount of performance improvement is highly dependent on a number of factors: Hardware configuration: Impala is generally able to take full advantage of hardware resources and specifically generates less CPU load than Hive, which often translates into higher observed aggregate I/O bandwidth than with Hive. Impala of course cannot go faster than the hardware permits, so any hardware bottlenecks will limit the observed speedup. For purely I/O bound queries, we typically see performance gains in the range of 3-4x. Complexity of the query: Queries that require multiple MapReduce phases in Hive or require reduce-side joins will see a higher speedup than, say, simple single-table aggregation queries. For queries with at least one join, we have seem performance gains of 7-45X. Availability of main memory as a cache for table data: If the data accessed through the query comes out of the cache, the speedup will be more dramatic thanks to Impala�s superior efficiency. In those scenarios, we have seen speedups of 20x-90x over Hive even on simple aggregation queries. Is Impala a replacement for MapReduce or Hive – or for traditional data warehouse infrastructure, for that matter? No. There will continue be many viable use cases for MapReduce and Hive (for example, for long-running data transformation workloads) as well as traditional data warehouse frameworks (for example, for complex analytics on limited, structured data sets). Impala is a complement to those approaches, supporting use cases where users need to interact with very large data sets, across all data silos, to get focused result sets quickly. Does the Impala Beta Release have any technical limitations? As mentioned previously, supported file formats in the first beta drop include text files and SequenceFiles, with many other formats to be supported in the upcoming production release. Furthermore, currently all joins are done in a memory space no larger than that of the smallest node in the cluster; in production, joins will be done in aggregate memory. Lastly, no UDFs are possible at this time. What are the technical requirements for the Impala Beta Release? You will need to have CDH4.1 installed on RHEL/CentOS 6.2. We highly recommend the use of Cloudera Manager (Free or Enterprise Edition) to deploy and manage Impala because it takes care of distributed deployment and monitoring details automatically. What is the support policy for the Impala Beta Release? If you are an existing Cloudera customer with a bug, you may raise a Customer Support ticket and we will attempt to resolve it on a best-effort basis. If you are not an existing Cloudera customer, you may use our public JIRA instance or the impala-user mailing list, which will be monitored by Cloudera employees. When will Impala be generally available for production use? A production drop is planned for the first quarter of 2013. Customers may obtain commercial support in the form of a Cloudera Enterprise RTQ subscription at that time. We hope that you take the opportunity to review the Impala source code, explore the beta release, download and install the VM, or any combination of the above. Your feedback in all cases is appreciated; we need your help to make Impala even better. We will bring you further updates about Impala as we get closer to production availability. (Update: Read about Impala 1.0.) Impala resources: – Impala source code – Impala downloads (Beta Release and VM) – Impala documentation – Public JIRA – Impala mailing list - Free�Impala training�(Screencast) (Added 10/30/2012) Third-party articles about Impala: - GigaOm:�Real-time query for Hadoop�democratizes access to big data�analytics (Oct. 22, 2012) - Wired:�Man Busts Out of Google, Rebuilds Top-Secret Query Machine (Oct. 24, 2012) -�InformationWeek:�Cloudera Debuts Real-Time Hadoop Query�(Oct. 24, 2012) -�GigaOm: Cloudera Makes SQL a First-Class Citizen on Hadoop�(Oct. 24, 2012)� - ZDNet:�Cloudera�s Impala Brings Hadoop to SQL and BI (Oct. 25, 2012) -�Wired: Marcel Kornacker Profile (Oct. 29, 2012) - Dr. Dobbs:�Cloudera Impala – Processing Petabytes at The Speed Of Thought (Oct. 29, 2012) Marcel Kornacker is the architect of Impala. Prior to joining Cloudera, he was the lead developer for the query engine of Google�s F1 project. Justin Erickson is the product manager for Impala.</snippet></document><document id="349"><title>Cloudera, The Platform for Big Data</title><url>http://blog.cloudera.com/blog/2012/10/cloudera-the-platform-for-big-data/</url><snippet>Today we�re proud to announce a new addition to the Apache Hadoop ecosystem: Cloudera Impala, a parallel SQL engine that runs natively on Hadoop storage.�The salient points are: Hive compatible 10x the performance of Hive/MapReduce, on average 100% open source, under the Apache License v2 � just like Hadoop Tested to run on CDH4.1 or higher There�s a blog post that follows mine that provides more details about Impala and how it works. I�d like to touch on a few related points. Impala brings useful new capabilities to the platform in its own right.�It enables interactive SQL on Hadoop data, whether stored in HDFS or in HBase, where previously there was only batch.�It substantially improves the quality of experience for business intelligence users.�It improves the economics of running ELT workloads on Hadoop clusters.�But perhaps just as important as what Impala *brings* to the platform is what Impala *says* about the platform.� It says that we are building on a fundamentally new and better architecture for data management.�It is architecture in which: We can add new forms of computation to an elastic, economical, linearly scalable, secure and durable pool of storage. You can work from a shared, open metadata model. You can store, explore, process, analyze and serve data without having to bolt together several disparate systems and repeatedly copy data among them. All of this is delivered as a completely open-source platform, where customers pay for results from technology, not technology itself. At Cloudera, we believe there will continue to be strong demand for all sorts of data management products like data warehouses, XML databases and document stores that each excel in their respective niche.�But from where we stand, CDH represents the best possible point of departure for Big Data applications.�We�ve come a long way from the filesystem and a batch processing engine that is Apache Hadoop. I think we now can see the outlines of a single platform for big data. Impala is immediately available as a public beta. You can find links to the download, documentation and installation information here.�We hope you try it out, give us your feedback and get involved in its evolution.</snippet></document><document id="350"><title>Your Guide to Cloudera @ Strata + Hadoop World This Week</title><url>http://blog.cloudera.com/blog/2012/10/your-guide-to-cloudera-strata-hadoop-world-this-week/</url><snippet>Cloudera is co-presenting the sold-out Strata Conference + Hadoop World�in New York this week, and if you’re an attendee, you have a great week ahead! Here�s a quick guide to where you can find Clouderans during the conference. There are of course many other great activities planned as well that are not covered here. Keynotes Cloudera CEO�Mike Olson (@mikeolson)�takes the stage on Wednesday to kick off the plenary sessions, and Apache Hadoop Co-founder and Apache Software Foundation chair�Doug Cutting (@cutting)�is part of the Thursday morning plenary sessions. (See abstracts here.) Arrive early to make sure you have a seat; you won�t want to miss their vision for the future of Hadoop. Presentations &amp; Tutorials As we have previously blogged, experts on the Cloudera team have put together some compelling and informative tutorials and presentations. They include: Given Enough Monkeys � Some Thoughts on Randomness Large Scale ETL with Hadoop HDFS � What is New and Future High Availability for the HDFS Namenode: Phase 2 Upcoming Enterprise Features in Apache HBase 0.96 Data Science on Hadoop: What�s There and What�s Missing Designing Scalable Network Architectures for Fast Moving Big Data Taming the Elephant � Learn How Monsanto Manages Their Hadoop Cluster to Enable Genome/Sequence Processing An Introduction to Hadoop Testing Hadoop Applications Using HBase Building a Large-Scale Data Collection System Using Flume NG Free Books/Meet the Authors The week is a great one for Hadoop book fans – book authors Tom White (@tom_e_white),�Lars George (@larsgeorge),�Eric Sammer (@esammer),�Amandeep Khurana (@amansk)�and�Nick Dimiduk (@xefyr)�be available in the Cloudera booth to sign your copies and answer your questions.�If you don’t have a copy yet, we’ll be giving away 100 copies of each book on Wednesday and Thursday respectively during specific times – check out the book giveaway schedule here. Meetups Make the most of your evenings and attend one of the many meetups taking place. It�s a great way to network and meet others that are working on similar projects and challenges. Check the schedule here. Find Cloudera at Booth #100 Visit our booth to check out our demos, learn what’s new, engage with our team and get a free t-shirt or book. If you can’t attend, at least follow us on Twitter and join the conversation. #strataconf We look forward to seeing you in New York City!</snippet></document><document id="351"><title>Sneak Peek into Skybox Imaging�s Cloudera-powered Satellite System</title><url>http://blog.cloudera.com/blog/2012/10/sneak-peek-into-skybox-imagings-cloudera-powered-satellite-system/</url><snippet>This is a guest post by Oliver Guinan, VP Ground Software, at Skybox Imaging. Oliver is a 15-year veteran of the internet industry and is responsible for all ground system design, architecture and implementation at Skybox. One of the great promises of the big data movement is using networks of ubiquitous sensors to deliver insights about the world around us. Skybox Imaging is attempting to do just that for millions of locations across our planet. Skybox is developing a low cost imaging satellite system and web-accessible big data processing platform that will capture video or images of any location on Earth within a couple of days. The low cost nature of the satellite opens the possibility of deploying tens of satellites which, when integrated together, have the potential to image any spot on Earth within an hour. Skybox satellites are designed to capture light in the harsh environment of outer space. Each satellite captures multiple images of a given spot on Earth. Once the images are transferred from the satellite to the ground, the data needs to be processed and combined to form a single image, similar to those seen within online mapping portals. With any sensor network, capturing raw data is only the beginning of the story. We at Skybox are building a system to ingest and process the raw data, allowing data scientists and end users to ask arbitrary questions of the data, then publish the answers in an accessible way and at a scale that grows with the number of satellites in orbit. We selected Cloudera to support this deployment. Processing raw imagery is a complex computer vision task that involves many pixel-level calculations over multiple images. Image Scientists create algorithms in C and C++ to efficiently perform these calculations. Hadoop prefers MapReduce jobs written in Java, so we have developed a proprietary framework called BusBoy to wrap the native algorithms into a standard Hadoop job. This allows our Hadoop engineers to develop efficient storage and publication solutions while our Image Scientists focus on developing better image processing algorithms. Developing against CDH and using Puppet to manage our deployed extensions and configurations allows Skybox to develop our architecture on our in-house cluster. Once the solution is robust, we then have the option to deploy our solution at scale using Amazon’s EC2 hardware or other scalable computation and storage platforms. We have tested a large number of hardware configurations to validate our scalability assumptions and to determine the right balance between CPU, memory, disk, and network resources. This information informs the purchasing process for our next in-house cluster. Making all data available on spinning disk allows data scientists to efficiently ask any question of the data. Traditional systems tend to archive older data to tape based systems. This makes speculative examination of the data prohibitively expensive. The Hadoop ecosystem of large scale compute and storage coupled with Apache Oozie‘s ability to chain complex processing jobs together that publish results to accessible, structured storage in Apache Hive and Apache HBase is allowing Skybox to create a sensor network that takes the pulse of the planet 24×7. About Skybox Imaging Skybox Imaging is a commercial, remote sensing start-up revolutionizing access to information that describes daily activity on our planet. Founded in 2009 and backed by leading venture firms, the company is designing, manufacturing, and operating the world�s first coordinated constellation of high-resolution microsatellites. With its constellation, Skybox will deliver timely, global imagery and video as well as an analytics platform capable of creating new sources of value from such data. Skybox is headquartered in Mountain View, California, and was named to MIT Technology Review�s �Top 50 Most Innovative Companies� for 2012. For more information, visit www.skyboximaging.com or follow Skybox Imaging on Twitter.</snippet></document><document id="352"><title>Apache Hadoop 2.0.2-alpha Released</title><url>http://blog.cloudera.com/blog/2012/10/apache-hadoop-2-0-2-alpha-released/</url><snippet>Earlier this month the Apache Hadoop PMC released Apache Hadoop 2.0.2-alpha, which fixes over 600 issues�since the previous release in the 2.0 series, 2.0.1-alpha, back in July. This is a tremendous rate of development, of which all contributors to the project should feel proud. Some of the more noteworthy changes in this release include: HDFS HA supports automatic failover using ZooKeeper (HDFS-3042). The FUSE-DFS module now supports secure HDFS clusters (HDFS-3568). The (non-standard) Kerberos over SSL has been replaced with SPNEGO for image transfers and for secure HDFS web access in general (HDFS-2617). SASL encryption can be enabled for block data transfers in HDFS (HDFS-3637), and the MapReduce shuffle can be encrypted using HTTPS (MAPREDUCE-4417). There is also HTTPS support for the web UIs (HADOOP-8581). A new type of Hadoop Metric, a quantile metric, has been added to provide latency histograms for various HDFS metrics (HDFS-3650). The Capacity Scheduler now supports delay scheduling (YARN-80). There are various performance improvements including support for fadvise in the shuffle handler (MAPREDUCE-3289) and datanode (HDFS-3697) YARN is now a subproject of Hadoop (YARN-1). The separation will make it easier for folks who want to write YARN applications that are independent of MapReduce. (See Harsh Chouraria’s “MR2 and YARN Briefly Explained” post for more on the relationship between YARN and MapReduce.) Try It Out! You can download the release from an Apache mirror. Alternatively, you can try CDH 4.1, since it includes most of the changes from Apache Hadoop 2.0.2-alpha. Note that MR2 in CDH 4.1 is still experimental�in line with the Apache release�however MR1 in CDH 4.1 is stable and fully supported in production. A Note on Release Numbering Historically the numbering of Apache Hadoop releases has been somewhat confusing, but things have improved since the Hadoop community voted to adopt 1.x for the current stable branch (renamed from the 0.20.x series) and the 2.x branch for the new line of development (previously 0.23.x), which is still currently unstable as mentioned above. Some confusion lingers in that there is still an 0.23 branch which is still producing releases (Robert Evans is the release manager). However this branch is a special case: it is an earlier version of the branch-2 line that Yahoo! is using to stabilize YARN for their own use, with plans to move to a 2.x sometime next year. The Yahoo! Hadoop team are also backporting fixes in the 2.x branch to the 0.23 branch as needed, and of course all changes that go into 0.23 go into trunk and 2.x first, so all the valuable stabilization work they are doing will benefit future 2.x releases.� From a feature point of view, the biggest difference between 0.23 and 2.x is that 0.23 lacks HDFS High Availability. Acknowledgements I would like to thank the many people from many different organizations who contributed to this release�from the smallest bug report to the largest feature, all contributions are appreciated. Also, thanks to Arun C Murthy who acted as release manager for this release.</snippet></document><document id="353"><title>What’s New in CDH4.1 Hue</title><url>http://blog.cloudera.com/blog/2012/10/whats-new-in-cdh4-1-hue/</url><snippet>Hue is a Web-based interface that makes it easier to use Apache Hadoop. Hue 2.1 (included in CDH4.1) provides a new application on top of Apache Oozie (a workflow scheduler system for Apache Hadoop) for creating workflows and scheduling them repetitively. For example, Hue makes it easy to group a set of MapReduce jobs and Hive scripts and run them every day of the week. In this post, we’re going to focus on the Workflow component of the new application. Workflow Editor Workflows consist of one or multiple actions that can be executed sequentially or in parallel. Each action will run a program that can be configured with parameters (e.g. output=${OUTPUT} instead of hardcoding a directory path) in order to be easily reusable. The current types of programs are: MapReduce Pig Hive Sqoop Java Shell Ssh Streaming jobs DistCp The application comes with a set of examples: Workflows can be shared with other users and cloned. Forks are supported and enable actions to run at the same time. The Workflow Editor lets you compose your workflow. Let�s take the Sequential Java (aka TeraSort) example and add an Hive action, HiveGen, that will generate some random data. TeraGen is a MapReduce job doing the same thing and both actions will run in parallel. Finally, the TeraSort action will read both outputs and sort them together You can see how this would look in Hue via the screenshot below. Workflow Dashboard Our TeraGen workflow can then be submitted and controlled in the Dashboard. Parameters values (e.g. ${OUTPUT} of the output path of the TeraSort action) are prompted when clicking on the submit button. Jobs can be filtered/killed/restarted and detailed information (progress, logs) is available within the application and in the Job Browser Application. Individual management of a workflow can be done on its specific page. We can see the active actions in orange below: Summary Before CDH4.1, Oozie users had to deal with XML files and command line programs. Now, this new application allows users to build, monitor and control their workflows within a single Web application. Moreover, the Hue File Browser (for listing and uploading workflows) and Job Browser (for accessing fine grained details of the jobs) are leveraged. The next version of the Oozie application will focus on improving the general experience, increasing the number of supported Oozie workflows and prettifying the Editor. In the meantime, feel free to report feedback and wishes to hue-user!</snippet></document><document id="354"><title>What’s New in CDH4.1 Pig</title><url>http://blog.cloudera.com/blog/2012/10/whats-new-in-cdh4-1-pig/</url><snippet>Apache Pig�is a platform for analyzing large data sets that provides a high-level language called Pig Latin. Pig users can write complex data analysis programs in an intuitive and compact manner using Pig Latin. Among many other enhancements, CDH4.1, the newest release of Cloudera’s open-source Hadoop distro, upgrades Pig from version 0.9 to version 0.10. This post provides a summary of the top seven new features introduced in CDH4.1 Pig. Boolean Data Type Pig Latin is continuously evolving. As with other actively developed programming languages, more data types are being added to Pig. CDH4.1 adds the boolean type. The boolean type is internally mapped to the Java Boolean class, and the boolean constants �TRUE� and �FALSE� are case-insensitive. Here are some example uses of boolean type: a = LOAD 'a.txt' AS (a0:boolean, a1:(a10:boolean), a2);
b = FOREACH a GENERATE a0, a1, (boolean)a2;
c = FILTER b BY a2 == TRUE;
 Note that if you have UDFs that implement the LoadCaster and StoreCaster interfaces in releases prior to CDH4.1, they will have to be modified to implement new methods called bytesToBoolean() and toBytes(), which were added to LoadCaster and StoreCaster respectively. Nested FOREACH and CROSS Pig now supports CROSS and FOREACH within a FOREACH statement in addition to already supported operators such as DISTINCT, FILTER, LIMIT, and ORDER BY. A nested FOREACH is particularly useful when you want to iterate through nested relations grouped by the same key. For example, if you want to compute the Cartesian product of two nested relations co-grouped by the same key and do some processing on them, you can achieve this in a concise manner as follows: a = LOAD 'a.txt' AS (a0, a1);
b = LOAD 'b.txt' AS (b0, b1);
c = COGROUP a BY a0, b BY b0;
d = FOREACH c {
��� d0 = CROSS a, b;
��� d1 = FOREACH d0 GENERATE a1 + b1;
��� GENERATE d1;
}
dump d;
 Note that only two levels of nesting are supported. Ruby UDFs Scripting UDFs are not new to Pig. In fact, Python and JavaScript have been supported since CDH3. But in CDH4.1, another popular scripting language is added: Ruby. Just as Python UDFs interact with Pig via Jython, Ruby UDFs do the same via JRuby internally. To register Ruby UDFs in Pig scripts, the REGISTER command is used as follows: REGISTER 'myfuncs.rb' USING jruby AS myfuncs;
 Similar to Python UDFs, there are two ways of defining the return type of UDFs: defining it in Pig syntax with an �outputSchema� decorator if the return type is static, or defining a �schemaFunction� if the return type is dynamic. For more details on Ruby UDF decorators, please refer to the Pig 0.10 documentation. LIMIT / SPLIT by Expression Prior to CHD4.1, only constants were allowed in the expression of LIMIT and SPLIT. In CDH4.1, the expression may include scalar variables as well. Here is an example of getting the top 10% of records from a relation using LIMIT: a = LOAD '1.txt' AS val;
b = GROUP a ALL;
c = FOREACH b GENERATE COUNT(a) AS sum;
d = ORDER a BY val;
e = LIMIT d c.sum/10;
 It is worth noting that only scalar variables can be used in the expression; column references cannot be used. In the above example, �c.sum� is implicitly cast to a scalar since �c� is a relation that contains a single long type record. A statement like e = LIMIT d val is not valid. Default SPLIT Destination The SPLIT operator is used to partition records of a relation into multiple relations. Prior to CDH4.1, records that did not meet any condition in the expressions were simply discarded. But in CDH4.1 it is possible to define the default destination via OTHERWISE as follows: SPLIT a INTO x IF $0&gt;10, y IF $0&gt;5, z OTHERWISE;
 Note that this feature introduces a new keyword – OTHERWISE, and this may break existing Pig scripts if they use it as an alias. Syntactical Sugar for TOTUPLE, TOBAG, and TOMAP Syntactic sugar for TOTUPLE, TOBAG, and TOMAP has been added. Now Pig automatically converts �( )�, �{ }�, and �[ ]� to tuple, bag, and map respectively. b = FOREACH a GENERATE (x, y); /* ((x,y))���� */
c = FOREACH a GENERATE {x, y}; /* ({(x),(y)}) */
d = FOREACH a GENERATE [x, y]; /* ([x#y])���� */
 AvroStorage Improvements AvroStorage now supports path globbing. Since Hadoop�s path globbing is used internally, the syntax is the same as that of Hadoop: a = LOAD '/{foo,bar}/*.avro' USING AvroStorage();
 AvroStorage can also load and store an Avro record type that has self-referencing fields. The problem with self-referencing fields is that it is not possible to convert them to Pig schema because recursive records are by definition infinite. As a workaround, Pig now converts them to bytearrays when detecting recursive records; therefore, it is possible to load and store an Avro record type that has self-referencing fields. Here is an example: a = LOAD '1.avro' USING AvroStorage('no_schema_check');

STORE a INTO '2' USING AvroStorage('no_schema_check', 'schema', '
�� { "type" : "record",
���� "name" : "recursive_record",
���� "fields" : [ { "name" : "value",
������������������� "type" : "int" },
����������������� { "name" : "next",
������������������� "type" : [ "null", "recursive_record" ] } ]
�� }
');

STORE a INTO '3' USING AvroStorage('no_schema_check', 'same', '1.avro');
 Note that a new option ‘no_schema_check’ is passed to both the load function and store function. This is necessary because by mapping recursive records to bytearrays, discrepancies between Avro and Pig schemas are introduced. Therefore, we must disable schema check or the load and store will fail with an exception during job compilation. For the store function, there are two ways to specify output schemas. Either it can be specified via the �schema� option with a JSON string, or the schema of an existing Avro file can be used via the �same’ option. There are more new features coming in a future release including but not limited to a date/time data type, the rank function, and a cube operator. These will let you write even more powerful and concise Pig scripts. Please try out the CDH4.1 Pig today. It can be downloaded from the Downloads page.</snippet></document><document id="355"><title>Axemblr’s Java Client for the Cloudera Manager API</title><url>http://blog.cloudera.com/blog/2012/10/axemblrs-java-client-for-the-cloudera-manager-api/</url><snippet>Axemblr, purveyors of a cloud-agnostic MapReduce Web Service, have recently announced the availability of an Apache-licensed Java Client for the Cloudera Manager API. The task at hand, according to Axemblr, is to�”deploy Hadoop on Cloud with as little user interaction as possible. We have the code to provision the hosts but we still need to install and configure Hadoop on all nodes and make it so the user has a nice experience doing it.” And voila, the answer is Cloudera Manager,�with the process made easy via the REST API introduced in Release 4.0. Thus, says Axemblr: “In the pursuit of our greatest desire (second only to coffee early in the morning), we ended up writing a Java client for Cloudera Manager�s API. Thus we achieved to automate a CDH3 Hadoop installation on Amazon EC2 and Rackspace Cloud. We also decided to open source the client so other people can play along.” Congrats to Axemblr for building this client and bringing the code to the community, and let a thousand others bloom! (And by the way – if there is anyone else writing similar wrappers or plugins out there, please let us know in comments.)</snippet></document><document id="356"><title>Analyzing Twitter Data with Apache Hadoop, Part 2: Gathering Data with Flume</title><url>http://blog.cloudera.com/blog/2012/10/analyzing-twitter-data-with-hadoop-part-2-gathering-data-with-flume/</url><snippet>This is the second article in a series about analyzing Twitter data using some of the components of the Hadoop ecosystem available in CDH, Cloudera’s open-source distribution of Apache Hadoop and related projects. In the first article, you learned how to pull CDH components together into a single cohesive application, but to really appreciate the flexibility of each of these components, we need to dive deeper. Every story has a beginning, and every data pipeline has a source. So, to build Hadoop applications, we need to get data from a source into HDFS. Apache Flume is one way to bring data into HDFS using CDH. The Apache Flume website describes Flume as “a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data.” At the most basic level, Flume enables applications to collect data from its origin and send it to a resting location, such as HDFS. At a slightly more detailed level, Flume achieves this goal by defining dataflows consisting of three primary structures: sources, channels and sinks. The pieces of data that flow through Flume are called events, and the processes that run the dataflow are called agents. In the Twitter example, we used Flume to collect data from the Twitter Streaming API, and forward it to HDFS. Looking closer at the Flume pipeline from that example, we come away with a system like this:   In the rest of this post, we’ll take an in-depth look at the pieces of Flume that are used to build dataflows, and specifically, how they were used in the example. Sources A source is just what it sounds like: the part of Flume that connects to a source of data, and starts the data along its journey through a Flume dataflow. A source processes events and moves them along by sending them into a channel. Sources operate by gathering discrete pieces of data, translating the data into individual events, and then using the channel to process events one at a time, or as a batch. Sources come in two flavors: event-driven or pollable. The difference between event-driven and pollable sources is how events are generated and processed. Event-driven sources typically receive events through mechanisms like callbacks or RPC calls. Pollable sources, in contrast, operate by polling for events every so often in a loop. Another good way to frame this differentiation is as a push-versus-pull model, where event-driven sources have events pushed to them, and pollable sources pull events from a generator. Examining the TwitterSource In our Twitter analysis example, we built a custom source called TwitterSource. To understand how sources operate more thoroughly, let�s look at how the TwitterSource was built. We can start with a very generic piece of boilerplate code: /**
 * A template for a custom, configurable Flume source
 */
public class BoilerplateCustomFlumeSource extends AbstractSource
    implements EventDrivenSource, Configurable {

  /**
   * The initialization method for the Source. The context contains all the
   * Flume configuration info, and can be used to retrieve any configuration
   * values necessary to set up the Source.
   */
  @Override
  public void configure(Context context) {
    // Get config params with context.get* methods
    // Example: stringParam = context.getString("stringParamName")
  }

  /**
   * Start any dependent systems and begin processing events.
   */
  @Override
  public void start() {
    // For an event-driven source, the start method should spawn
    // a thread that will receive events and forward them to the
    // channel
    super.start();
  }

  /**
   * Stop processing events and shut any dependent systems down.
   */
  @Override
  public void stop() {
    super.stop();
  }
}
  With this code, we have a configurable source that we can plug into Flume, although at this stage, it won�t do anything. The start() method contains the bulk of the source�s logic. In the TwitterSource, the twitter4j library is used to get access to the Twitter Streaming API, using this block of code: // The StatusListener is a twitter4j API, which can be added to a Twitter
// stream, and will execute callback methods every time a message comes in
// through the stream.
StatusListener listener = new StatusListener() {
� // The onStatus method is a callback executed when a new tweet comes in.
� public void onStatus(Status status) {
��� Map headers = new HashMap();
��� // The EventBuilder is used to build an event using the headers and
��� // the raw JSON of a tweet
��� headers.put("timestamp", String.valueOf(status.getCreatedAt().getTime()));
��� Event event = EventBuilder.withBody(
������� DataObjectFactory.getRawJSON(status).getBytes(), headers);

��� try {
����� getChannelProcessor().processEvent(event);
��� } catch (ChannelException e) {
����� // If we catch a channel exception, it�s likely that the memory channel
����� // does not have a high enough capacity for our rate of throughput, and
����� // we tried to put too many events in the channel. Error handling or
����� // retry logic would go here.
����� throw e;
��� }
� }
����� �
� // This listener will ignore everything except for new tweets
� public void onDeletionNotice(StatusDeletionNotice statusDeletionNotice) {}
� public void onTrackLimitationNotice(int numberOfLimitedStatuses) {}
� public void onScrubGeo(long userId, long upToStatusId) {}
� public void onException(Exception ex) {}
};
 The StatusListener implements a set of callbacks that will be called when receiving a new tweet, represented by a Status object. There are other callbacks available but for the purposes of this source, we�re only concerned with new tweets. As can be seen in the TwitterSource, the StatusListener is created and registered in the start() method. Looking a bit closer, we can pick out the code that actually builds an event out of a tweet: headers.put("timestamp", String.valueOf(status.getCreatedAt().getTime()));
Event event = EventBuilder.withBody(
  ��� DataObjectFactory.getRawJSON(status).getBytes(), headers));
 The EventBuilder interface takes a byte array and an optional set of headers, and creates an event, which we�re putting on the end of a list. The source processes events as they come in and passes them along to the channel: channel.processEvent(event);
 In order to connect to the Twitter APIs, we need access to some application-specific secrets. In the TwitterSource, these are variables like the consumerKey and consumerSecret, which are used to setup the Twitter stream: twitterStream.setOAuthConsumer(consumerKey, consumerSecret);
 So, where did the consumerKey and consumerSecret get defined? For this source, these variables are configuration parameters. Taking a look at the configure() method, we can see where the variables are defined: consumerKey = context.getString(TwitterSourceConstants.CONSUMER_KEY_KEY);
consumerSecret = context.getString(TwitterSourceConstants.CONSUMER_SECRET_KEY);
 The context object contains all the configuration parameters for the source, which can be pulled out and stored in instance variables using a variety of get accessors. With this code in place, the custom source will be able to process tweets as events. The next step is to define where those events should go and how they should get there. Configuring the Flume Agent Before we discuss how to actually configure a Flume agent, we need to know what a configuration looks like. For the Twitter analysis example, we used this configuration: TwitterAgent.sources = Twitter
TwitterAgent.channels = MemChannel
TwitterAgent.sinks = HDFS

TwitterAgent.sources.Twitter.type = com.cloudera.flume.source.TwitterSource
TwitterAgent.sources.Twitter.channels = MemChannel
TwitterAgent.sources.Twitter.consumerKey = [required]
TwitterAgent.sources.Twitter.consumerSecret = [required]
TwitterAgent.sources.Twitter.accessToken = [required]
TwitterAgent.sources.Twitter.accessTokenSecret = [required]
TwitterAgent.sources.Twitter.keywords = hadoop, big data, analytics, bigdata, cloudera, data science, data scientist, business intelligence, mapreduce, data warehouse, data warehousing, mahout, hbase, nosql, newsql, businessintelligence, cloudcomputing

TwitterAgent.sinks.HDFS.channel = MemChannel
TwitterAgent.sinks.HDFS.type = hdfs
TwitterAgent.sinks.HDFS.hdfs.path = hdfs://hadoop1:8020/user/flume/tweets/%Y/%m/%d/%H/
TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream
TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text
TwitterAgent.sinks.HDFS.hdfs.batchSize = 1000
TwitterAgent.sinks.HDFS.hdfs.rollSize = 0
TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000
TwitterAgent.sinks.HDFS.hdfs.rollInterval = 600

TwitterAgent.channels.MemChannel.type = memory
TwitterAgent.channels.MemChannel.capacity = 10000
TwitterAgent.channels.MemChannel.transactionCapacity = 100
 Each object that is defined will be referenced by these names throughout the rest of the configuration. Most Flume configuration entries will follow a format very similar to the configuration of log4j appenders. An entry will look like [agent_name].[object_type].[object_name].[parameter_name], where [object_type] is one of sources, channels, or sinks. Channels Channels act as a pathway between the sources and sinks. Events are added to channels by sources, and later removed from the channels by sinks. Flume dataflows can actually support multiple channels, which enables more complicated dataflows, such as fanning out for replication purposes. In the case of the Twitter example, we’ve defined a memory channel: TwitterAgent.channels.MemChannel.type = memory
 Memory channels use an in-memory queue to store events until they’re ready to be written to a sink. Memory channels are useful for dataflows that have a high throughput; however, since events are stored in memory in the channel, they may be lost if the agent experiences a failure. If the risk of data loss is not tolerable, this situation can be remedied using a different type of channel – i.e., with one that provides stronger guarantees of data durability like a FileChannel. Sinks The final piece of the Flume dataflow is the sink. Sinks take events and send them to a resting location or forward them on to another agent. In the Twitter example, we utilized an HDFS sink, which writes events to a configured location in HDFS. The HDFS sink configuration we used does a number of things: First, it defines the size of the files with the rollCount parameter, so each file will end up containing 10,000 tweets. It also retains the original data format, by setting the fileType to DataStream and setting writeFormat to Text. This is done instead of storing the data as a SequenceFile or some other format. The most interesting piece, however, is the file path: TwitterAgent.sinks.HDFS.hdfs.path = hdfs://hadoop1:8020/user/flume/tweets/%Y/%m/%d/%H/
 The file path, as defined, uses some wildcards to specify that the files will end up in a series of directories for the year, month, day and hour during which the events occur. For example, an event that comes in at 9/20/2012 3:00PM will end up in HDFS at hdfs://hadoop1:8020/user/flume/tweets/2012/09/20/15/. Where does the timestamp information come from? If you’ll recall, we added a header to each event in the TwitterSource: headers.put("timestamp", String.valueOf(status.getCreatedAt().getTime()));
 This timestamp header is used by Flume to determine the timestamp of the event, and is used to resolve the full path where the event should end up. Starting the Agent Now that we understand the configuration of our source, channel and sink, we need to start up the agent to get the dataflow running. Before we actually start the agent, we need to set the agent to have the appropriate name as defined in the configuration. The file /etc/default/flume-ng-agent contains one environment variable defined called FLUME_AGENT_NAME. In a production system, for simplicity, the FLUME_AGENT_NAME will typically be set to the hostname of the machine on which the agent is running. However, in this case, we set it to TwitterAgent, and we’re ready to start up the process. We can start the process by executing $ /etc/init.d/flume-ng-agent start
 Once it’s going, we should start to see files showing up in our /user/flume/tweets directory: natty@hadoop1:~/source/cdh-twitter-example$ hadoop fs -ls /user/flume/tweets/2012/09/20/05
  Found 2 items
  -rw-r--r--�� 3 flume hadoop � 255070 2012-09-20 05:30 /user/flume/tweets/2012/09/20/05/FlumeData.1348143893253
  -rw-r--r--�� 3 flume hadoop � 538616 2012-09-20 05:39 /user/flume/tweets/2012/09/20/05/FlumeData.1348143893254.tmp
 As more events are processed, Flume writes to files in the appropriate directory. A temporary file, suffixed with .tmp, is the file currently being written to. That .tmp suffix is removed when Flume determines that the file contains enough events or enough time has passed to roll the file. Those thresholds are determined in the configuration of the HDFS sink, as we saw above, by the rollCount and rollInterval parameters, respectively. Conclusion In this article, you�ve seen how to develop a custom source and process events from Twitter. The same approach can be used to build custom sources for other types of data. Also, we�ve looked at how to configure a basic, complete dataflow for Flume, to bring data into HDFS as it is generated. A distributed filesystem isn�t particularly useful unless you can get data into it, and Flume provides an efficient, reliable way to achieve just that. In Part 3, we’ll examine�how to query this data with Apache Hive. Jon Natkins (@nattybnatkins) is a Software Engineer at Cloudera</snippet></document><document id="357"><title>HBase at ApacheCon Europe 2012</title><url>http://blog.cloudera.com/blog/2012/10/hbase-at-apachecon-europe-2012/</url><snippet>Apache HBase will have a notable profile at ApacheCon Europenext month.�Clouderan and HBase committer Lars George has two sessions on the schedule: HBase Sizing and Schema Design Abstract: This talk will guide the HBase novice to consider important details when designing HBase backed storage systems. Examples of schemas are given and discussed, as well as rules of thumb that will help to avoid common traps. With the right knowledge of how HBase works internally, it will be much easier to come to terms with performance implications of different data schemas. HBase Status Quo Abstract: This talk�focuses on what happened to HBase since version 0.90. The idea is to introduce and discuss all the major changes in 0.92, 0.94, and trunk, aka 0.96. This spans from coprocessors and security, to distributed log splitting in 0.92, to prefix compression and lazy seek optimizations in 0.94 and so on. But also more subtle – yet often equally important – features like WALPlayer, or the handling of checksums, are presented as they improve operations and performance. The goal is to show the audience the large strides HBase and its community have taken towards a 1.0 release. HBase user Christian G�gi of Sentric in Zurich, who is also an organizer of the Swiss Big Data User Group, has a session as well: Operating HBase: Things You Need to Know Abstract: If you�re running HBase in production, you have to be aware of many things. In this talk we will share our experience in running and operating an HBase production cluster for a customer. To avoid common pitfalls, we�ll discuss problems and challenges we�ve faced as well as practical solutions (real-world techniques) for repair.�Even though HBase provides internal tools for diagnosing issues and for repair, running a healthy cluster can still be challenging for an administrator. We’ll cover some background on these tools as well as on HBase internals such as compaction, region splits and their distribution.� Finally, Steve Watt of HP offers some learning from his company’s experiences with respect to HBase: Taking the Guesswork Out of Your Hadoop Infrastructure Abstract: Apache Hadoop is clearly one of the fastest growing big data platforms to store and analyze arbitrarily structured data in search of business insights. However, applicable commodity infrastructures have advanced greatly in the last number of years and there is not a lot of information to assist the community in optimally designing and configuring Hadoop Infrastructure based on specific requirements. For example, how many disks and controllers should you use? Should you buy processors with 4 or 6 cores? Do you need a 1GbE or 10GbE Network? Should you use SATA or MDL SAS? Small or Large Form Factor Disks? How much memory do you need? How do you characterize your Hadoop workloads to figure out whether your are I/O, CPU, Network or Memory bound? In this talk we’ll discuss the lessons learned and outcomes from the work HP has done to optimally design and configure infrastructure for both MapReduce and HBase.� So, if you happen to be in Sinsheim, Germany, during the first week of November, you could do worse with your free time than brush up on your HBase knowledge at ApacheCon!</snippet></document><document id="358"><title>Meet the Engineer: Todd Lipcon</title><url>http://blog.cloudera.com/blog/2012/10/meet-the-engineer-todd-lipcon/</url><snippet>In this installment of “Meet the Engineers”, meet Todd Lipcon (@tlipcon), PMC member/committer for the Hadoop, HBase, and Thrift projects. What do you do at Cloudera, and in which Apache project are you involved? I’m a software engineer in Cloudera’s Platform Engineering group, more specifically on the HDFS team. In my time at Cloudera I’ve also done a significant amount of work on other components of CDH including Hue, HBase, and MapReduce. I spend most of my time developing new features for these open source components – recently I’ve been primarily focused on designing and implementing High Availability for the HDFS NameNode. I’m also a performance nut, and have spent a lot of time over the last several years improving the performance of the Hadoop stack. Why do you enjoy your job? Getting paid to work on open source is pretty great. Almost all of my time goes to working with the open source community, whether it’s developing new code, reviewing patches from new contributors, or speaking at events like Hadoop user groups and conferences. From an engineering perspective, Hadoop is quite interesting and challenging to work on. It runs at enormous scale and in critical production workloads at some of the world’s biggest companies, so you really have to think through all the corner cases to make a robust design and implementation. (If you’re interested in learning more about the Cloudera work environment, I recently wrote a more lengthy post on this.) Systems programming is particularly interesting to me since it encourages a “full-stack” perspective. To make Hadoop efficient, you have to really understand all the layers of the system, from the Linux kernel to the JVM to TCP/IP up through distributed consensus protocols. Jumping between these layers to solve bugs or improve performance keeps every day fresh and interesting. What is your favorite thing about Hadoop? From an engineer’s perspective, I like working on Hadoop because it’s very challenging. There are very few open source systems that operate at this kind of scale or are making this big of an impact. From a user’s perspective, I think Hadoop is exciting because it levels the playing field between technical powerhouses like Google who have built this kind of technology in-house and more traditional enterprises. I imagine that working on Hadoop today is very much like what it was like to work on Linux in the mid to late 90s. What is your advice for someone who is interested in participating in any open source project for the first time? Walk before you run. One mistake I’ve seen new contributors make is that they try to start off with a huge chunk of work at the core of the system. Instead, learn your way around the source code by doing small improvements, bug fixes, etc. Then, when you want to propose a larger change, the rest of the community will feel more comfortable accepting it. One great way to build karma in the community is to look at recently failing unit tests, file bugs, and fix them up. At what age did you become interested and programming, and why? I started out with Apple Basic and LOGO on an Apple IIc when I was 5 or 6 years old, probably because there weren’t that many exciting games to play on the machine, and drawing spirographs and making “guess-the-number” games was pretty cool. We even had some kind of adapter to hook up to our TV and display color! I progressed from there through various other beginner languages until Perl and C++ when I was 14 or so. By that point, I’d say I was interested because it was more challenging than working at a grocery store and paid a bit better too! Look for our next “Meet the Engineer” profile in a week or two. See you then!</snippet></document><document id="359"><title>New Additions to the Apache HBase Team</title><url>http://blog.cloudera.com/blog/2012/10/new-additions-to-the-hbase-team/</url><snippet>StumbleUpon (SU) and Cloudera have signed a technology collaboration agreement. Cloudera will support the SU clusters, and in exchange, Cloudera will have access to a variety of production deploys on which to study and try out beta software. As part of the agreement, the StumbleUpon Apache HBase+Apache Hadoop team — Jean-Daniel Cryans, Elliott Clark and I — have joined Cloudera. From our new perch up in the Cloudera San Francisco office — 10 blocks north and 11 floors up — we will continue as first-level support for SU clusters, tending and optimizing them as we have always done. The rest of our time will be spent helping develop Apache HBase as the newest additions to Cloudera’s HBase team. We do not foresee this transition disrupting our roles as contributors to HBase. If anything, we look forward to contributing even more than in the past. As we see it, our job at SU was effectively done. We had put in place a stable, scalable data store used both for low latency serving of the SU frontend, and by bigger, backend batch clusters used by scientists and analysts running all kinds of processing and reporting MapReduce jobs. The front-end clusters are set up so they replicate to the batch and backup clusters across datacenters. All clusters are multi-tenant serving a variety of schemas, features and a wide breadth of access patterns. As the SU dataset �and processing demands continue to grow, all they need do to scale their data store is add boxes. While we once furiously made HBase customizations to facilitate new SU features, there is less of that of late and most of our SU-specific work has long since been pushed upstream. We reached a state whereby software updates to the SU HBase+Hadoop�stack, apart from the odd bug fix, came whenever the HBase project put up a new release candidate for the community to try. At SU we tested the release candidate by rolling it up through the SU clusters from dev, through batch, and eventually out to the front-end-serving cluster if all proved stable. We therefore were spending most of our time working in open source on the HBase project helping releases along, with the remainder spent operating our HBase+Hadoop clusters and educating folks a bit about how best to use the datastore (and Hadoop). It became apparent after a while that if we could hand off operations, there was little to tie us to SU in particular. Cloudera has a wide variety of customer HBase installs. It also already has a strong HBase team who, to tell you the truth, were barely keeping up with growth in the customer base. We wanted to give them a helping hand. �We also wanted to be able to take on some larger features and fix-ups — snapshots and backups, to mention but a few — projects that take more than a single developer and a weekend to finish. Being part of a larger team, we will be able to do this. At Cloudera, we would also be better positioned to improve HBase integration with the rest of the Hadoop�stack. Finally, Cloudera is in the support business. We could set up an arrangement whereby we could continue to look after SU. This move strikes us as a natural progression – “coming home,” as Mike Olson calls it. We are super-psyched to be joining Cloudera even if we are going to miss our old haunt, the super supportive SU, which generously sponsored and pioneered HBase these last three years. – Michael Stack</snippet></document><document id="360"><title>How-to: Set Up an Apache Hadoop/Apache HBase Cluster on EC2 in (About) an Hour</title><url>http://blog.cloudera.com/blog/2012/10/set-up-a-hadoophbase-cluster-on-ec2-in-about-an-hour/</url><snippet>Note (added July 8, 2013): The information below is deprecated; we suggest that you refer to this post for current instructions. Today we bring you one user’s experience using Apache Whirr to spin up a CDH cluster in the cloud. This post was originally published here by George London (@rogueleaderr) based on his personal experiences; he has graciously allowed us to bring it to you here as well in a condensed form. (Note: the configuration described here is intended for learning/testing purposes only.) I�m going to walk you through a (relatively) simple set of steps that will get you up and running MapReduce programs on a cloud-based, six-node distributed Apache Hadoop/Apache HBase cluster as fast as possible. This is all based on what I�ve picked up on my own, so if you know of better/faster methods, please let me know in comments! We�re going to be running our cluster on Amazon EC2, and launching the cluster using Apache Whirr and configuring it using Cloudera Manager Free Edition.� Then we�ll run some basic programs I�ve posted on Github that will parse data and load it into Apache HBase. All together, this tutorial will take a bit over one hour and cost about $10 in server costs. Step 1: Get the Cluster Running I�m going to assume you already have an Amazon Web Services account (because it�s awesome, and the basic tier is free.) If you don�t,�go get one. Amazon�s directions for getting started are pretty clear, or you can easily find a guide with Google. We won�t actually be interacting with the Amazon management console much, but you will need two pieces of information, your�AWS Access Key ID�and your�AWS Secret Access Key. To find these, go to�https://portal.aws.amazon.com/gp/aws/securityCredentials. You can write these down, or better yet add them to your shell startup script by doing: $ echo "export AWS_ACCESS_KEY_ID=" &gt; ~/.bashrc
$ echo "export AWS_SECRET_ACCESS_KEY="your_key_here&gt;" &gt; ~/.bashrc
$ exec $SHELL You will also need a security certificate and private key that will let you use the command-line tools to interact with AWS.�From the AWS Management Console go to Account &gt; Security Credentials &gt; Access Credentials, select the �X.509 Certificates� tab and click on Create a new Certificate. Download and save this somewhere safe (e.g. ~/.ec2) Then do: $ export EC2_PRIVATE_KEY=~/.ec2/.pem
$ export EC2_CERT=~/.ssh/.pem
 Finally, you�ll need a�different�key to log into your servers using SSH. To create that, do: $ mkdir ~/.ec2
$ ec2-add-keypair --region us-east-1 hadoop | sed 1d &gt; ~/.ec2/hadoop
$ chmod 600 ~/.ec2/hadoop
(to lock down the permissions on the key so that SSH will agree to use it)
 You have the option of manually creating a bunch of EC2 nodes, but that�s a pain. Instead, we�re going to use Whirr, which is specifically designed to allow push-button setup of clusters in the cloud. To use Whirr, we are going to need to create one node manually, which we are going to use as our �control center.� I�m assuming you have the EC2 command-line tools installed (if not, go�here�and follow directions). We�re going to create an instance running Ubuntu 10.04 (it�s old, but all of the tools we need run on it in stable fashion), and launch it in the USA-East region. You can find AMIs for other Ubuntu versions and regions here. So, do: $ ec2-run-instances ami-1db20274 -k "hadoop"
 This creates an EC2 instance using a minimal Ubuntu image, with the SSH key �hadoop_tutorial� that we created a moment ago. The command will produce a bunch of information about your instance. Look for the �instance id� that starts with i- , then do: $ ec2-describe-instance [i-whatever] This will tell you the IP of your new instance (it will start ec2-). Now we�re going to remotely log in to that server. $ ssh -i ~/.ec2/hadoop ubuntu@ec2-54-242-56-86.compute-1.amazonaws.com
 Now we�re in! This server is only going to run two programs, Whirr and the Cloudera Manager. First we�ll install Whirr.� Find a mirror at (http://www.apache.org/dyn/closer.cgi/whirr/), then download to your home directory using wget: $ wget http://www.motorlogy.com/apache/whirr/whirr-0.8.0/whirr-0.8.0.tar.gz
 Untar and unzip: $ tar -xvf whirr-0.8.0.tar.gz
$ cd whirr-0.8.0
 Whirr will launch clusters for you in EC2 according to a �properties� file you pass it. It�s actually quite powerful and allows a lot of customization (and can be used with non-Amazon cloud providers) or you to set up complicated servers using Chef scripts. But for our purposes, we�ll keep it simple. Create a file called hadoop.properties: $ nano hadoop.properties
 And give it these contents: whirr.cluster-name=whirrly
whirr.instance-templates=6 noop
whirr.provider=aws-ec2
whirr.identity=
whirr.credential=
whirr.cluster-user=huser
whirr.private-key-file=${sys:user.home}/.ssh/id_rsa
whirr.public-key-file=${sys:user.home}/.ssh/id_rsa.pub
whirr.env.repo=cdh4
whirr.hardware-id=m1.large
whirr.image-id=us-east-1/ami-1db20274
whirr.location-id=us-east-1
 This will launch a cluster of six unconfigured��large� EC2 instances. (Whirr refused to create small or medium instances for me. Please let me know in comments if you know how to do that.) Before we can use Whirr, we�re going to need to install Java, so do: $ sudo apt-get update
$ sudo apt-get install openjdk-6-jre-headless Next we need to create that SSH key that will let our control node log into to our cluster.� $ ssh-keygen -t rsa -P ''
 And hit [enter] at the prompt. Now we�re ready to launch! $ bin/whirr launch-cluster --config hadoop.properties
 This will produce a bunch of output and end with commands to SSH into your servers. We�re going to need these IPs for the next step, so copy and paste these lines into a new file: $ nano hosts.txt
 Then use this bit of regular expression magic to create a file with just the IP�s: $ sed -rn "\|.*@(.*)'| s/.*@(.*)'/\1/p" hosts.txt &gt;&gt; ips.txt
 Step 2: Configure the Cluster From your Control Node, download Cloudera Manager; we will install the Free Edition, which can be used for up to 50 nodes: $ wget http://archive.cloudera.com/cm4/installer/latest/cloudera-manager-installer.bin
 Then install it: $ sudo chmod +x cloudera-manager-installer.bin
$ sudo ./cloudera-manager-installer.bin
 This will pop up an extreme green install wizard; just hit “yes” to everything. Cloudera Manager works poorly with textual browsers like Lynx. (It has an API, but we won’t cover that here.) Luckily, we can access the web interface from our laptop by looking up the public DNS address we used to log in to our control node, and appending �:7180� to the end in our web browser. First, you need to tell Amazon to open that port. The manager also needs a pretty ridiculously long list of open ports to work, so we�re just going to tell Amazon to open all TCP ports. That�s not great for security, so you can add the individual ports if you care enough (lists here): $ ec2-authorize default -P tcp -p 0-65535 -o "jclouds#whirrly"
$ ec2-authorize default -P tcp -p 7180 -o 0.0.0.0/0
$ ec2-authorize default -P udp -p 0-65535 -o "jclouds#whirrly"
$ ec2-authorize default -P icmp -t -1:-1 -o "jclouds#whirrly"
 Then fire up Chrome and visit�http://ec2-.compute-1.amazonaws.com:7180/ . Log in with the default credentials user: �admin� pass: �admin� Click �just install the free edition�, �continue�, then �proceed� in tiny text at the bottom right of the registration screen. Now go back to that ips.txt file we created in the last part and copy the list of IPs. Past them into the box on the next screen, and click �search�, then �install CDH on selected hosts.� Next the manager needs credentials that�ll allow it to log into the nodes in the cluster to set them up. You need to give it a SSH key, but that key is on the server and can�t be directly accessed from you laptop. So you need to copy it to your laptop.  $ scp -r -i ~/.ec2/hadoop_tutorial.pem ubuntu@ec2-54-242-62-52.compute-1.amazonaws.com:/home/ubuntu/.ssh ~/Downloads/hadoop_tutorial
 (�scp� is a program that securely copies files through ssh, and the �r flag will copy a directory.) Now you can give the manager the username �huser�, and the SSH keys you just downloaded: Click �start installation,� then �ok� to log in with no passphrase. Now wait for a while as CDH is installed on each node. Next, Cloudera Manager will inspect the hosts and issues some warnings but just click “continue.” Then it will ask you which services you want to start � choose �custom� and then select Zookeeper, HDFS, HBase, and MapReduce. Click “continue” on the �review configuration changes� page, then wait as the manager starts your services. Click “continue” a couple more times when prompted, and now you�ve got a functioning cluster. Step 3: Do Something To use your cluster, you need to SSH login to one of the nodes. Pop open the �hosts.txt� file we made earlier, grab any of the lines, and paste it into the terminal. $ ssh -i /home/ubuntu/.ssh/id_rsa -o "UserKnownHostsFile /dev/null" \
 -o StrictHostKeyChecking=no huser@75.101.233.156
 If you already know how to use Hadoop and HBase, then you�re all done. Your cluster is good to go. If you don�t, here�s a brief overview: The basic Hadoop workflow is to run a �job� that reads some data from HDFS, �maps� some function onto that data to process it, �reduces� the results back to a single set of data, and then stores the results back to HDFS. You can also use HBase as the input and/or output to your job. You can interact with HDFS directly from the terminal through commands starting �hadoop fs�. In CDH, Cloudera’s open-source Hadoop distro, you need to be logged in as the �hdfs� user to manipulate HDFS, so let�s log in as hdfs, create a users directory for ourselves, then create an input directory to store data. $ sudo su - hdfs
$ hadoop fs -mkdir -p /user/hdfs/input You can list the contents of HDFS by typing: $ hadoop fs -ls -R /user
 To run a program using MapReduce, you have two options. You can either: Write a program in Java using the MapReduce API and package it as a JAR Use Hadoop Streaming, which allows you to write your mapper and reducer�scripts in whatever language you want and transmit data between stages by reading/writing to StdOut. If you�re used to scripting languages like Python or Ruby and just want to crank through some data, Hadoop Streaming is great (especially since you can add more nodes to overcome the relative CPU slowness of a higher level language). But interacting programmatically with HBase is a lot easier through Java. (Interacting with HBase is tricky but not impossible with Python. There is a package called �Happybase� which lets you interact �pythonically� with HBase; the problem is that you have to run a special service called Thrift on each server to translate the Python instructions into Java, or else transmit all of your requests over the wire to a server on one node, which I assume will heavily degrade performance. Cloudera Manager will not set up Thrift for you, though you could do it by hand or using Whirr+Chef.) So I�ll provide a quick example of Hadoop streaming and then a more extended HBase example using Java. Now, grab my example code repo off Github. We�ll need git. (If you�re still logged in as hdfs, do �exit� back to �huser� since hdfs doesn�t have sudo privileges by default.) $ sudo apt-get install -y git-core
$ sudo su - hdfs
$ git clone https://github.com/rogueleaderr/Hadoop_Tutorial.git $ cd Hadoop_Tutorial/hadoop_tutorial
 Cloudera Manager won’t tell the nodes where to find the configuration files it needs to run (i.e. �set the classpath�), so let�s do that now: $ export HADOOP_CLASSPATH=/etc/hbase/conf.cloudera.hbase1/:/etc/hadoop/conf.cloudera.mapreduce1/:/etc/hadoop/conf.cloudera.hdfs1/
 Hadoop Streaming Michael Noll has a good tutorial on Hadoop streaming with Python here. I�ve stolen the code and put it in Github for you, so to get going. Load some sample data into hdfs: $ hadoop fs -copyFromLocal data/sample_rdf.nt input/sample_rdf.nt
$ hadoop fs -ls -R
(to see that the data was copied)
 Now let�s Hadoop: $ hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-2.0.0-mr1-cdh4.0.1.jar \
-file python/mapper.py -mapper python/mapper.py \
-file python/reducer.py -reducer python/reducer.py \
-input /user/hdfs/input/sample_rdf.nt -output /user/hdfs/output/1
 That�s a big honking statement, but what it�s doing is telling Hadoop (which Cloudera Manager installs in /usr/lib/hadoop-0.20-mapreduce) to execute the �streaming� jar, to use the mapper and reducer �mapper.py� and �reducer.py�, passing those actual script files along to all of the nodes, telling it to operate on the sample_rdf.nt file, and to store the output in the (automatically created) output/1/ folder. Let that run for a few minutes, then confirm that it worked by looking at the data: $ hadoop fs -cat /user/hdfs/output/1/part-00000
 That�s Hadoop Streaming in a nutshell. You can execute whatever code you want for your mappers/reducers (e.g. Ruby or even shell commands like �cat�. If you want to use non-standardlib Python packages – e.g. �rdflib� for actually parsing the RDF – you need to zip the packages and pass those files to hadoop streaming using -file [package.zip].) Hadoop/HBase API If you want to program directly into Hadoop and HBase, you�ll do that using Java. The necessary Java code can be pretty intimidating and verbose, but it�s fairly straightforward once you get the hang of it. The Github repo we downloaded in Step 3 contains some example code that should�just run if you�ve followed this guide carefully, and you can incrementally modify that code for your own purposes. (The basic code is adapted from the code examples in Lars George’s HBase, the Definitive Guide. The full original code can be found here.� That code has its own license, but my marginal changes are released into the public domain.) All you need to run the code is Maven. Grab that: (If you�re logged in as user �hdfs�, type �exit� until you get back to huser. Or give hdfs sudo privileges with �visudo� if you know how.) $ sudo apt-get install maven2
$ sudo su - hdfs $ cd Hadoop_Tutorial/hadoop_tutorial
 When you run Hadoop jobs from the command line, Hadoop is literally shipping your code over the wire to each of the nodes to be run locally. So you need to wrap your code up into a JAR file that contains your code and all the dependencies. (There are other ways to bundle or transmit your code but I think fully self-contained �fat jars� are the easiest. You can make these using the �shade� plugin which is included in the example project.�) Build the jar file by typing: $ export JAVA_HOME=/usr/lib/jvm/j2sdk1.6-oracle/
 (to tell maven where java lives)
 $ mvn package
 That will take an irritatingly long time (possibly 20+ minutes) as Maven downloads all the dependencies, but it requires no supervision. (If you�re curious, you can look at the code with a text editor at /home/users/hdfs/Hadoop_Tutorial/hadoop_tutorial/src/main/java/com/tumblr/rogueleaderr/hadoop_tutorial/HBaseMapReduceExample.java). There�s a lot going on there, but I�ve tried to make it clearer via comments. Now we can actually run our job: $  cd /var/lib/hdfs/Hadoop_Tutorial/hadoop_tutorial
$ hadoop jar target/uber-hadoop_tutorial-0.0.1-SNAPSHOT.jar com.tumblr.rogueleaderr.hadoop_tutorial.HBaseMapReduceExample
 If you get a bunch of connection errors, make sure your classpath is set correctly by doing: $ export HADOOP_CLASSPATH=/etc/hbase/conf.cloudera.hbase1/:/etc/hadoop/conf.cloudera.mapreduce1/:/etc/hadoop/conf.cloudera.hdfs1/
 Confirm that it worked by opening up the hbase commandline shell: $ hbase shell
hbase(main):001:0&gt; scan "parsed_lines"
 If you see a whole bunch of lines of data, then – congratulations! You�ve just parsed RDF data using a six-node Hadoop Cluster, and stored the results in HBase! Next Steps If you�re planning on doing serious work with Hadoop and HBase, just buy the books: Hadoop, the Definitive Guide HBase, the Definitive Guide The official tutorials for�Whirr,�Hadoop, and�HBase�are okay, but pretty intimidating for beginners. Beyond that, you should be able to Google up some good tutorials.</snippet></document><document id="361"><title>Videos: Get Started with Hadoop Using Cloudera Enterprise</title><url>http://blog.cloudera.com/blog/2012/10/videos-get-started-with-hadoop-using-cloudera-enterprise/</url><snippet>Our video animation factory has been busy lately. The embedded player below contains our two latest ones stitched together: Get Started with Hadoop Using Cloudera Enterprise, Part 1� To be a proactive industry leader, you must be able to consume critical data as it is produced, analyze it quickly and act on your results. Before Hadoop there was no scalable, cost-effective way to do this with Big Data. Learn how Cloudera Enterprise helps you to efficiently store all of your data, avoiding archiving and data loss, and provides the opportunity to gain detailed insights into aspects of your business never before possible. Get Started with Hadoop Using Cloudera Enterprise, Part 2 To run Hadoop effectively, you need to pair it with several other software projects. Cloudera Enterprise offers a simple click-through setup wizard for deployment and configuration of Hadoop as well as these projects, which will save weeks of developer hours that can now be spent taking advantage of the Big Data capabilities Hadoop offers. Once Hadoop is up and running it is also a large and complicated system to monitor. This is why Cloudera built Cloudera Manager, included as part of Cloudera Enterprise. Cloudera Manager helps you easily monitor things like hosts, services, system resources and security. If anything goes wrong in the Hadoop environment, Cloudera Manager will send an alert and provides easy-to-use diagnostic tools to get to the root of the issue. If the issue is not easily identifiable, a simple click will send a Cloudera Hadoop expert all the information needed to quickly diagnose the problem and get your system back to normal. � Learn more about Cloudera Enterprise � Download Cloudera Manager Free Edition Contact Cloudera to learn more: � Contact Cloudera � Get Private Training Quote</snippet></document><document id="362"><title>Data Science: The New Heart of Healthcare</title><url>http://blog.cloudera.com/blog/2012/10/data-science-the-new-heart-of-healthcare/</url><snippet>We at Cloudera are tremendously excited by the power of data to effect large-scale change in the healthcare industry. Many of the projects that our data science team worked on in the past year originated as data-intensive problems in healthcare, such as analyzing�adverse drug events�and constructing�case-control studies.�Last summer, we announced that our Chief Scientist Jeff Hammerbacher would be collaborating with the Mt. Sinai School of Medicine to leverage large-scale data analysis with Apache Hadoop for the treatment and prevention of disease. And next week, it will be my great pleasure to host a panel of data scientists and researchers at the Strata Rx Conference�(register with discount code SHARON�for 25% off) to discuss the meaningful use of natural language processing in clinical care. Of course, the cost-effective�storage and analysis of massive quantities of text�is one of Hadoop’s strengths, and Jimmy Lin’s book on text processing is an excellent way to learn how to think in MapReduce. But a close study of how the applications of natural language processing technology in healthcare have evolved over the last few years is instructive for anyone who wants to understand how to use data science in order to tackle seemingly intractable problems. Lesson 1: Choose the Right Problem A couple of months ago, Mark Madsen tweeted that many organizations are using an “underpants gnomes” strategy for data science. The plan goes something like this: Collect a lot of dirty, unstructured data. Hire a data scientist. Profit! In general, I am wary of people who come to me bearing databases and asking for “insights” into their data. The right way to approach data science is to start with a problem that has a bottom-line impact on your business, and then work backward from the problem towards the analysis and the data is needed to solve it. Insights don’t happen in a vacuum – they come with the hard work of analyzing data and building models to solve real problems. Sometimes, the link between the business problem and the application of data science will be very clear, like in the case of correctly identifying fraudulent credit card transactions.�In other cases, there can be multiple steps that separate the business problem from the data science application. For example, a rental service like Netflix is primarily interested in growing and retaining their subscribers. They could have performed an analysis that demonstrated a correlation between the number of movies in a customer’s queue and the probability that the customer will renew his subscription. This analysis might have then motivated Netflix to create a movie recommendation system that helps customers discover movies that they will love. If the users who add recommended movies to their queues are then more likely to renew their subscriptions, then the project has succeeded. In the case of natural language processing and healthcare, the right problem turned out to be computer-assisted coding�(CAC). In healthcare, coding�refers to the process of converting the narrative description of the treatments a patient received, including doctors’ notes, lab tests, and medical images, into a series of alphanumeric codes that are used for billing purposes. Medical coding is both very important (if the treatments a patient receives aren’t coded, the insurance company won’t pay for them) and very expensive (medical coders need a lot of training and skill to do the job well). To make matters worse, the coding standards are becoming more complex: the current ICD-9 standard has around 24,000 possible codes, while the next-generation ICD-10 standard will expand to 155,000 codes. Finding ways to use natural language processing to help coders be more efficient is a great problem for a data scientist to tackle: it has a quantifiable impact on the bottom line and there is a strong potential for data analysis and modeling to make a meaningful difference. Lesson 2: Build a Minimum Viable Data Product The minimum viable product strategy is also a good way of developing�data products: the first model that we use for a problem does not need to crunch massive quantities of data or leverage the most advanced machine learning algorithms. Our primary objective is to create the simplest system that will provide enough utility for its users that they are willing to use it and start providing feedback that we can use to make the product better. At first, this feedback may be explicitly communicated to the data scientists working on the system, who may incorporate the feedback by tuning the system by hand. But if we design the system well, we can use automated and implicit sources of feedback to make improvements in a more scalable fashion.� The first systems for performing computer-assisted coding were similar to the first spam classifiers: they relied almost exclusively on a static set of rules in order to make coding decisions. They also primarily targeted medical coding applications for outpatient treatments, instead of the more complex coding required for inpatient treatments. These early systems weren’t particularly great, but they were useful enough that they could gather feedback from the medical coders on when they failed to identify a code or included one that was not relevant for the problem, and as more data was gathered, the static rules could be augmented with statistical models that were capable of adjusting to new information and improving over time. Lesson 3: The One Who Has the Best Data Wins Data is like any other kind of capital – it flows to where it is wanted, and it stays where it is well-treated. Good algorithms and good people are critical for any data science project, but there is absolutely no substitute for high-quality data that you can use as inputs for your models. As your models improve, they get used more often to make decisions, receive even more feedback, and are used in a wider variety of situations, which leads to a�virtuous cycle�and the kind of network effects that we see in winner-take-all markets. Many of the computer-assisted coding products that are available today are web-based and/or integrated with electronic health record (EHR) systems, which allows them to collect feedback data quickly and reliably as well as take advantage of more information about the patient to improve the automated coding. It also becomes possible to use the feedback from many different medical coders across different healthcare institutions in order to make improvements in the underlying models more quickly. Data as Platform For many problems that can be tackled using machine learning, the choice of input features is the most important part of the overall process. Data scientists bridge the gap between messy, unstructured data and the structured inputs required by our algorithms. At scale, the skills required to generate input features are similar to the ones needed to build�ETL pipelines for data warehousing applications. You might say that ETL is the wax-on, wax-off of data science. "I don't see how this is going to help me win the All Valley Kaggle competition." One of the reasons that automatic medical coding is such a great problem for data scientists to take on is that solving it well doesn’t just save money and time, it also provides the structured information that we need as inputs for other problems,�including the adverse drug event and case-control projects that we have worked on here at Cloudera. We hope that you can join us at Strata Rx next week to join the conversation around how to effect change in healthcare via the effective, meaningful use of data.</snippet></document><document id="363"><title>What is Hadoop Metrics2?</title><url>http://blog.cloudera.com/blog/2012/10/what-is-hadoop-metrics2/</url><snippet>Metrics are collections of information about Hadoop daemons, events and measurements; for example, data nodes collect metrics such as the number of blocks replicated, number of read requests from clients, and so on. For that reason, metrics�are an invaluable resource for monitoring Apache Hadoop services and an indispensable�tool for debugging system problems.� This blog post focuses on the features and use of the Metrics2 system for Hadoop, which allows multiple metrics output plugins to be used in parallel, supports dynamic reconfiguration of metrics plugins, provides metrics filtering, and allows all metrics to be exported via JMX. Metrics vs. MapReduce Counters When speaking about metrics, a question about their relationship to MapReduce counters usually arises. This differences can be described in two ways: First, Hadoop daemons and services are generally the scope for metrics, whereas MapReduce applications are the scope for MapReduce counters (which are collected for MapReduce tasks and aggregated for the whole job). Second, whereas Hadoop administrators are the main audience for metrics, MapReduce users are the audience for MapReduce counters. Contexts and Prefixes For organizational purposes metrics are grouped into named contexts – e.g., jvm for java virtual machine metrics or dfs for the distributed file system metric. There are different sets of contexts supported by Hadoop-1 and Hadoop-2; the table below highlights the ones supported for each of them. � Branch-1 Branch-2 - jvm - rpc - rpcdetailed - metricssystem - mapred - dfs - ugi - yarn - jvm - rpc - rpcdetailed - metricssystem - mapred - dfs - ugi A Hadoop daemon collects metrics in several contexts. For example, data nodes collect metrics for the �dfs�, �rpc� and �jvm� contexts. The daemons that collect different metrics in Hadoop (for Hadoop-1 and Hadoop-2) are listed below: Branch-1 Daemons/Prefixes Branch-2 Daemons/Prefixes - namenode – datanode – jobtracker – tasktracker – maptask – reducetask   - namenode - secondarynamenode - datanode - resourcemanager - nodemanager - mrappmaster - maptask - reducetask System Design The Metrics2 framework is designed to collect and dispatch per-process metrics to monitor the overall status of the Hadoop system. Producers register the metrics sources with the metrics system, while consumers register the sinks. The framework marshals metrics from sources to sinks based on (per source/sink) configuration options. This design is depicted below. � Here is an example class implementing the MetricsSource: class MyComponentSource implements MetricsSource {
���@Override
 ���public void getMetrics(MetricsCollector collector, boolean all) {
  � collector.addRecord("MyComponentSource")
   ����� ����.setContext("MyContext")
    ����� ����.addGauge(info("MyMetric", "My metric description"), 42);
     ���}
     }
 The �MyMetric� in the listing above could be, for example, the number of open connections for a specific server. Here is an example class implementing the MetricsSink: public class MyComponentSink implements MetricsSink {
 ���public void putMetrics(MetricsRecord record) {
  � System.out.print(record);
   ���}
    ���public void init(SubsetConfiguration conf) {}
     ���public void flush() {}
     }
 To use the Metric2s framework, the system needs to be initialized and sources and sinks registered. Here is an example initialization: DefaultMetricsSystem.initialize(�datanode");
MetricsSystem.register(source1, �source1 description�, new MyComponentSource());
MetricsSystem.register(sink2, �sink2 description�, new MyComponentSink())
 Configuration and Filtering The Metrics2 framework uses the PropertiesConfiguration from the apache commons configuration library. Sinks are specified in a configuration file (e.g., “hadoop-metrics2-test.properties”), as: test.sink.mysink0.class=com.example.hadoop.metrics.MySink
 The configuration syntax is: [prefix].[source|sink|jmx|].[instance].[option]
 In the previous example, test is the prefix and mysink0 is an instance name. DefaultMetricsSystem would try to load hadoop-metrics2-[prefix].properties first, and if not found, try the default hadoop-metrics2.properties in the class path. Note, the [instance] is an arbitrary name to uniquely identify a particular sink instance. The asterisk (*) can be used to specify default options. Here is an example with inline comments to identify the different configuration sections: # syntax: [prefix].[source|sink].[instance].[options]
# Here we define a file sink with the instance name �foo�
*.sink.foo.class=org.apache.hadoop.metrics2.sink.FileSink
# Now we specify the filename for every prefix/daemon that is used for
# dumping metrics to this file. Notice each of the following lines is
# associated with one of those prefixes.
namenode.sink.foo.filename=/tmp/namenode-metrics.out
secondarynamenode.sink.foo.filename=/tmp/secondarynamenode-metrics.out
datanode.sink.foo.filename=/tmp/datanode-metrics.out
resourcemanager.sink.foo.filename=/tmp/resourcemanager-metrics.out
nodemanager.sink.foo.filename=/tmp/nodemanager-metrics.out
maptask.sink.foo.filename=/tmp/maptask-metrics.out
reducetask.sink.foo.filename=/tmp/reducetask-metrics.out
mrappmaster.sink.foo.filename=/tmp/mrappmaster-metrics.out
# We here define another file sink with a different instance name �bar�
*.sink.bar.class=org.apache.hadoop.metrics2.sink.FileSink
# The following line specifies the filename for the nodemanager daemon
# associated with this instance. Note that the nodemanager metrics are
# dumped into two different files. Typically you�ll use a different sink type
# (e.g. ganglia), but here having two file sinks for the same daemon can be
# only useful when different filtering strategies are applied to each.
nodemanager.sink.bar.filename=/tmp/nodemanager-metrics-bar.out
 Here is an example set of NodeManager metrics that are dumped into the NodeManager sink file: 1349542623843 jvm.JvmMetrics: Context=jvm, ProcessName=NodeManager, SessionId=null, Hostname=ubuntu, MemNonHeapUsedM=11.877365, MemNonHeapCommittedM=18.25, MemHeapUsedM=2.9463196, MemHeapCommittedM=30.5, GcCountCopy=5, GcTimeMillisCopy=28, GcCountMarkSweepCompact=0, GcTimeMillisMarkSweepCompact=0, GcCount=5, GcTimeMillis=28, ThreadsNew=0, ThreadsRunnable=6, ThreadsBlocked=0, ThreadsWaiting=23, ThreadsTimedWaiting=2, ThreadsTerminated=0, LogFatal=0, LogError=0, LogWarn=0, LogInfo=0
1349542623843 yarn.NodeManagerMetrics: Context=yarn, Hostname=ubuntu, AvailableGB=8
1349542623843 ugi.UgiMetrics: Context=ugi, Hostname=ubuntu
1349542623843 mapred.ShuffleMetrics: Context=mapred, Hostname=ubuntu
1349542623844 rpc.rpc: port=42440, Context=rpc, Hostname=ubuntu, NumOpenConnections=0, CallQueueLength=0
1349542623844 rpcdetailed.rpcdetailed: port=42440, Context=rpcdetailed, Hostname=ubuntu
1349542623844 metricssystem.MetricsSystem: Context=metricssystem, Hostname=ubuntu, NumActiveSources=6, NumAllSources=6, NumActiveSinks=1, NumAllSinks=0, SnapshotNumOps=6, SnapshotAvgTime=0.16666666666666669
 Each line starts with a time followed by the context and metrics name and the corresponding value for each metric. Filtering By default, filtering can be done by source, context, record and metrics. More discussion of different filtering strategies can be found in the Javadoc and wiki. Example: mrappmaster.sink.foo.context=jvm
# Define the classname used for filtering
*.source.filter.class=org.apache.hadoop.metrics2.filter.GlobFilter
*.record.filter.class=${*.source.filter.class}
*.metric.filter.class=${*.source.filter.class}
# Filter in any sources with names start with Jvm
nodemanager.*.source.filter.include=Jvm*
# Filter out records with names that matches foo* in the source named "rpc"
nodemanager.source.rpc.record.filter.exclude=foo*
# Filter out metrics with names that matches foo* for sink instance "file" only
nodemanager.sink.foo.metric.filter.exclude=MemHeapUsedM
 Conclusion The Metrics2 system for Hadoop provides a gold mine of real-time and historical data that help�monitor and debug problems associated with the Hadoop services and jobs.� Ahmed Radwan is a software engineer at Cloudera, where he contributes to various platform tools and open-source projects. �</snippet></document><document id="364"><title>MR2 and YARN Briefly Explained</title><url>http://blog.cloudera.com/blog/2012/10/mr2-and-yarn-briefly-explained/</url><snippet>With CDH4 onward, the Apache Hadoop component introduced two new terms for Hadoop users to wonder about: MR2 and YARN.�Unfortunately, these terms are mixed up so much that many people are confused about them. Do they mean the same thing, or not? This post aims to clarify these two terms. What is YARN? YARN stands for �Yet-Another-Resource-Negotiator�. It is a new framework that facilitates writing arbitrary distributed processing frameworks and applications. YARN provides the daemons and APIs necessary to develop generic distributed applications of any kind, handles and schedules resource requests (such as memory and CPU) from such applications, and supervises their execution. YARN’s execution model is more generic than the earlier MapReduce implementation. YARN can run applications that do not follow the MapReduce model, unlike the original Apache Hadoop MapReduce (also called MR1). What is MR2? With the advent of YARN, there is no longer a single JobTracker to run jobs and a TaskTracker to run tasks of the jobs. The old MR1 framework was rewritten to run within a submitted application on top of YARN. This application was christened MR2, or MapReduce version 2. It is the familiar MapReduce execution underneath, except that each job now controls its own destiny via its own ApplicationMaster taking care of execution flow (such as scheduling tasks, handling speculative execution and failures, etc.). It is a more isolated and scalable model than the MR1 system where a singular JobTracker does all the resource management, scheduling and task monitoring work. MR2 and a new proof-of-concept application called the DistributedShell are the first two applications using the YARN API in CDH4. Summary YARN is a generic platform for any form of distributed application to run on, while MR2 is one such distributed application that runs the MapReduce framework on top of YARN. For a more extensive post on this topic, click here.</snippet></document><document id="365"><title>Applying Parallel Prediction to Big Data</title><url>http://blog.cloudera.com/blog/2012/10/applying-parallel-prediction-to-big-data/</url><snippet>This guest post is provided by Dan McClary,�Principal Product Manager for Big Data and Hadoop at Oracle. One of the constants in discussions around Big Data is the desire for richer analytics and models. However, for those who don’t have a deep background in statistics or machine learning, it can be difficult to know not only just what techniques to apply, but on what data to apply them. Moreover, how can we leverage the power of Apache Hadoop to effectively operationalize the model-building process? In this post we’re going to take a look at a simple approach for applying well-known machine learning approaches to our big datasets. We’ll use Pig and Hadoop to quickly parallelize a standalone machine-learning program written in Jython. Playing Weatherman I’d like to predict the weather. Heck, we all would – there’s personal and business value in knowing the likelihood of sun, rain, or snow. Do I need an umbrella? Can I sell more umbrellas? Better yet, groups like the National Climatic Data Center offer public access to weather data stretching back to the 1930s. I’ve got a question I want to answer and some big data with which to do it. On first reaction, because I want to do machine learning on data stored in HDFS, I might be tempted to reach for a massively scalable machine learning library like Mahout. For the problem at hand, that may be overkill and we can get it solved in an easier way, without understanding Mahout. Something becomes apparent on thinking about the problem: I don’t want my climate model for San Francisco to include the weather data from Providence, RI. Weather is a local problem and we want to model it locally. Therefore what we need is many models across different subsets of data. For the purpose of example, I’d like to model the weather on a state-by-state basis. But if I have to build 50 models sequentially, tomorrow’s weather will have happened before I’ve got a national forecast. Fortunately, this is an area where Pig shines. Parallel Pipelines with Pig We want to build models by state, but unfortunately NCDC’s global surface temperature data doesn’t come tagged by state. Instead, what we have is day-over-day data organized by station ID number. NCDC provides a map from station ID to locations, but we’ll need to join it to our weather data. However, the number of stations is much, much smaller than the number of temperature observations — in fact, it will fit into a modest amount of memory. Pig’s JOIN operator allows us to specify join behavior when we understand our bags are of uneven sizes. In this case, we can use the “replicated” directive with the JOIN operator to force all but the first bag to reside in memory. Pig’s GROUP operator allows us to quickly organize our dataset by a feature (e.g., state) resulting in an outer bag for which each group is a training set. Once the data’s organized, dispatching our model-building code is as simple as using Pig’s FOREACH operator. That accomplished, we’re going to need some model-building code. Pig alone isn’t suited to this task, but its ability to leverage scripting languages in the JVM makes it easy to tap into a wealth of machine learning methods. Using Jython and Weka for Model Building Decision trees are one of the fundamental techniques in machine learning and are applicable to a wide set of industry problems (e.g., advertisement targeting, quality evaluation). I’d like to build a simple C4.5 tree for each state’s weather. While I won’t get into the details of C4.5 here, the concept is simple: we want to organize our tree so that as we travel from the root to the leaves our decisions are ordered by the amount of information that can be gleaned from that feature. For example, if one of our features is latitude, then the model for California might place that decision near the top of the tree. Why? Because northern and southern California have very different climates and latitude tells us more about weather outcome across California than, say, the wind speed. As I mentioned above, Pig isn’t suitable for constructing a C4.5 tree. Besides, C4.5′s been around since the 1990s, someone has surely open-sourced an implementation of it. In fact, the Weka machine-learning library contains a very good Java implementation of C4.5 called J48. Since Pig can register JARs for use as UDFs, it should be easy enough for us to route our FOREACH call into Weka. However, I’m in a hurry, and may want to try out a bunch of different modeling techniques. I don’t want to write and package Java code if I can just write Python instead. Fortunately, Pig has support for Jython UDFs built in. All we need to do is make sure Jython is on the Pig classpath and make sure our code knows where to find our UDF. It looks a little like this: REGISTER weka.jar;
REGISTER 'c45_udf.py' USING jython AS c45;
  And we’ll send the bags of training data to Jython like this:   models = FOREACH training_groups GENERATE c45.build_instances(group, training_data);
  But what does the Jython code do to leverage Weka? The code needs to accomplish a few things: Import the necessary Weka classes Define an output schema so that Pig understands how the resulting tuples are structured Transform the input data into a Weka dataset and build a C4.5 tree Turn the model into data that can be used to test or make predictions in the future and return it to Pig Importing the classes and defining the output schema are simple:       import sys
      sys.path += ["/usr/lib/jython/Lib","/usr/lib/jython/Lib/site-packages"]
      import weka.core.Instances as Instances
      import weka.core.Instance as Instance
      import weka.core.FastVector as FastVector
      import weka.core.Attribute as Attribute
      import weka.classifiers.trees.J48 as J48

      @outputSchema("state:chararray, model:chararray")
      def build_instances(state,dataset): That output decorator tells Pig what to expect in the return tuple. The mechanics of transforming the input data into Weka vectors and training a model are less easily summarized, so you can find a code sample here: REGISTER weka.jar;
REGISTER 'c45_udf.py' USING jython AS c45;
rmf /user/oracle/weather/models
weather_obs = LOAD '/user/oracle/weather/cleaned_history'
    using PigStorage('\u0001') as
    (usaf:int, wban:int, year:int, month:int, day:int, temp:float,
     dewp:float, weather:chararray);                 

stations = LOAD '/user/oracle/weather/stations' USING PigStorage() as
(stn:int, wban:int, country:chararray, state:chararray, lat:float,
 lon:float);
observations = JOIN weather_obs BY usaf, stations BY stn using 'replicated';
training_data = FOREACH observations
                  GENERATE state,lat, lon, day,temp,dewp,weather;
training_groups = GROUP training_data BY state;
models = FOREACH training_groups
                  GENERATE c45.build_instances(group, training_data); STORE models INTO '/user/oracle/weather/models' USING PigStorage();,
        …or read about integrating Weka and Jython here. Once we’ve done this, we end up with trees like this one for California:         lon &lt;= -124.160004
          |   dewp &lt;= 49.299999
          |   |   temp &lt;= 50.599998
          |   |   |   dewp &lt;= 45.700001
          |   |   |   |   temp &lt;= 42.299999
          |   |   |   |   |   day &lt;= 6: Fog (2.0)
          |   |   |   |   |   day &gt; 6: Sunny (18.0/2.0)
          |   |   |   |   temp &gt; 42.299999
          |   |   |   |   |   dewp &lt;= 44.299999
          |   |   |   |   |   |   temp &lt;= 42.599998: Rain (2.0)
          |   |   |   |   |   |   temp &gt; 42.599998: Sunny (156.0/38.0)
          |   |   |   |   |   dewp &gt; 44.299999
          |   |   |   |   |   |   temp &lt;= 50.299999
          |   |   |   |   |   |   |   dewp &lt;= 44.599998: Rain (10.0)
          |   |   |   |   |   |   |   dewp &gt; 44.599998
          |   |   |   |   |   |   |   |   day &lt;= 18: Sunny (8.0)
          |   |   |   |   |   |   |   |   day &gt; 18: Rain (4.0)
          |   |   |   |   |   |   temp &gt; 50.299999: Sunny (4.0)

          ...
            While my code includes some half-backed packaging of the model, consider serialization an exercise left to the reader. Takeaway This model snippet doesn’t tell me much, other than perhaps that it’s always sunny in California. But what we have here is more than just a fun example of how to play with public data using Pig and Python; rather it’s a simple methodology for applying existing modeling approaches to Big Data. By adding in your own aggregation logic and modeling code, you can get up and running with analytics on Hadoop with very little effort.  </snippet></document><document id="366"><title>Data Science: Hot or Not?</title><url>http://blog.cloudera.com/blog/2012/10/data-science-hot-or-not/</url><snippet>You may have noticed that Harvard Business Review is calling data science “the sexiest job of the 21st century.” So our answer to the question is: Hot. Definitely hot.� If you need an explanation, watch the “Definition of a Data Scientist”�talk embedded below from Cloudera data science director Josh Wills, which was�hosted by Cloudera partner Lilien LLC�recently in Portland, Ore. The key take-away is, you don’t literally have to be a “scientist,” just someone with the�curiosity�of one.</snippet></document><document id="367"><title>CDH4.1 Now Released!</title><url>http://blog.cloudera.com/blog/2012/10/cdh4-1-now-released/</url><snippet>Update time!� As a reminder, Cloudera releases major versions of CDH, our 100% open source distribution of Apache Hadoop and related projects, annually and then updates to CDH every three months.� Updates primarily comprise bug fixes but we will also add enhancements.� We only include fixes or enhancements in updates that maintain compatibility, improve system stability and still allow customers and users to skip updates as they see fit. We�re pleased to announce the availability of CDH4.1.� We�ve seen excellent adoption of CDH4.0 since it went GA at the end of June and a number of exciting use cases have moved to production.� CDH4.1 is an update that has a number of fixes but also a number of useful enhancements.� Among them: Quorum based storage ��Quorum-based Storage for HDFS�provides the ability for HDFS to store its own NameNode edit logs,�allowing you to run a highly available NameNode without external�storage or custom fencing. Hive security and concurrency � we�ve fixed some long standing issues with running Hive.� With CDH4.1, it is now possible to run a shared Hive instance where users submit queries using Kerberos authentication.� In addition this new Hive server supports multiple users submitting queries at the same time. Support for DataFu � the LinkedIn data science team was kind enough to open source their library of Pig UDFs that make it easier to perform common jobs like sessionization or set operations.� Big thanks to the LinkedIn team!!! Oozie workflow builder � since we added Apache Oozie to CDH more than two years ago, we have often had requests to make it easier to develop Oozie workflows.� The newly enhanced job designer in Hue enables users to use a visual tool to build and run Oozie workflows. Apache FlumeNG improvements �� since its release, FlumeNG has become the backbone for some exciting data collection projects, in some cases collecting as much as 20TB of new event data per day.� In CDH4.1 we added an HBase sink as well as metrics for monitoring as well as a number of performance improvements. Various performance improvements � CDH4.1 users should experience a boost in their MapReduce performance from CDH4.0. Various security improvements � CDH4.1 enables users to configure the system to encrypt data in flight during the shuffle phase.� CDH now also applies Hadoop security to users who access the filesystem via a FUSE mount. CDH4.1 is available on all of the usual platforms and form factors.� You can install it via Cloudera Manager or learn how to install the packages manually here.</snippet></document><document id="368"><title>Apache Hadoop Wins Duke’s Choice Award, is a Java Ecosystem “MVP”</title><url>http://blog.cloudera.com/blog/2012/09/apache-hadoop-wins-dukes-choice-award-is-a-java-ecosystem-mvp/</url><snippet>For those of you new to it, the Duke’s Choice Awards�program was initiated by Sun Microsystems in 2002 in an effort to “celebrate extreme innovation in the world of Java technology” – in essence, it’s the “MVP” of the Java ecosystem. Since it acquired Sun in 2009, Oracle has continued the tradition of bestowing the award, and in fact has made the process more community-oriented by accepting nominations from the public and involving Java User Groups in the judging effort. For the 2012 awards, I’m happy to report that Apache Hadoop is among the awardees�- which also include the United Nations High Commission for Refugees, Liquid Robotics, and Java cloud company Jelastic Inc., among others. As Doug Cutting, the Hadoop project’s founder, current ASF chairman, and Cloudera’s chief architect, explains in the Java Magazine writeup about the award, “Java is the primary language of the Hadoop ecosystem…and Hadoop is the de facto standard operating system for big data. So, as the big data trend spreads, Java spreads too.” Update (10/2/2012): And here’s documentary evidence – a group photo of the winners! That’s Clouderan committer Eli Collins in the back row, second from right. So, a big congrats to Apache Hadoop for getting the recognition it deserves from the Java community!</snippet></document><document id="369"><title>About Apache Flume FileChannel</title><url>http://blog.cloudera.com/blog/2012/09/about-apache-flume-filechannel/</url><snippet>The post below was originally published via blogs.apache.org and is republished below for your reading pleasure. This blog post is about Apache Flume�s File Channel. Apache Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming�data flows.�It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application. FileChannel is a persistent Flume channel that supports writing to multiple disks in parallel and encryption. Overview When using Flume, each flow has a Source, Channel, and Sink. A typical example would be a webserver writing events to a Source via RPC (e.g.�Avro Source), the sources writing to�MemoryChannel, and�HDFS Sink�consuming the events, writing them to HDFS. MemoryChannel provides high throughput but loses data in the event of a crash or loss of power. As such the development of a persistent Channel was desired. FileChannel was implemented in�FLUME-1085. The goal of FileChannel is to provide a reliable high throughput channel. FileChannel guarantees that when a transaction is committed, no data will be lost due to a subsequent crash or loss of power. It’s important to note that FileChannel does not do any replication of data itself. As such, it is only as reliable as the underlying disks. Users who use FileChannel because of its durability should take this into account when purchasing and configuring hardware. The underlying disks should be RAID, SAN, or similar. Many systems trade a small amount of data loss (fsync�from memory to disk every few seconds for example) for higher throughput. The Flume team decided on a different approach with FileChannel. Flume is a transactional system and multiple events can be either Put or Taken in a single transaction. The batch size can be used to control throughput. Using large batch sizes, Flume can move data through a flow with no data loss and high throughput. The batch size is completely controlled by the client. This is an approach users of RDBMS’s will be familiar with. A Flume transaction consists of either Puts or Takes, but not both, and either a commit or a rollback. Each transaction implements both a Put and Take method. Sources do Puts onto the channel and Sinks do Takes from the channel. Design FileChannel is based on a write ahead log or�WAL�in addition to an in-memory queue.�Each transaction is written to the WAL based on the transaction type (Take or Put) and the queue is modified accordingly. Each time a transaction is committed, fsync is called on the appropriate file to ensure the data is actually on disk and a pointer to that event is placed on a queue. The queue serves just like any other queue: it manages what is yet to be consumed by the sink. During a take, a pointer is removed from the queue. The event is then read directly from the WAL. Due to the large amount of RAM available today, it’s very common for that read to occur from the operating system file cache. After a crash, the WAL can be replayed to place the queue in the same state it was immediately�preceding�the crash such that no committed transactions are lost. Replaying WALs can be time consuming, so the queue itself is written to disk periodically. Writing the queue to disk is called a checkpoint. After a crash, the queue is loaded from disk and then only committed transactions after the queue was saved to disk are replayed, significantly reducing the amount of WAL, which must be read.� For example, a channel that has two events will look like this: The WAL contains three important items: the transaction id, sequence number, and event data. Each transaction has a unique transaction id, and each event has a unique sequence number. The transaction id is used simply to group events into a transaction while the sequence number is used when replaying logs. In the above example, the transaction id is 1 and the sequence numbers are 1, 2, and 3. When the queue is saved to disk – a checkpoint – the sequence number is incremented and saved as well. At restart, first the queue from disk is loaded and then any WAL entries with a greater sequence number than the queue, are replayed. During the checkpoint operation the channel is locked so that no Put or Take operations can alter it’s state. Allowing modification of the queue during the checkpoint would result in an inconsistent snapshot of the queue stored on disk. In the example queue above, a checkpoint occurs after the commit of transaction 1 resulting in the queue being saved to disk with both events (“a” and “b”) and a sequence number of 4. After that point, event a is Taken from the queue in transaction 2: If a crash occurs, the queue checkpoint is read from disk. Note that since the checkpoint occurred before transaction 2, both events a and b currently exist on the queue. Then the WAL is read and any committed transaction with a sequence number greater than 4 is applied resulting in “a” being removed from the queue. Two items are not covered by the design above. Takes and Puts which are in progress at the time the checkpoint occurs are lost. Assume the checkpoint occurred instead after the take of “a”: If a crash occurred at this point, under the design described above, event “b” would be on the queue and on replay any WAL entry with a sequence number greater than 5 would be replayed. The Rollback for transaction 2 would be replayed, but the Take for transaction 2 would not be replayed. As such, “a” would not be placed on the queue resulting in data loss. A similar scenario is played out for Puts. For this reason, when a queue checkpoint occurs, transactions which are still in progress are also written out so that this scenario can be handled appropriately. Implementation FileChannel is stored in the flume-file-channel module of the Flume project and it’s Java package name is org.apache.flume.channel.file. The queue described above is named�FlumeEventQueue�and the WAL is named�Log. The queue itself is a circular array and is backed by a�Memory Mapped File�while the WAL is a set of files written and read from using the�LogFile�class and its subclasses. Conclusion FileChannel provides Flume users with durability in the face of hardware, software, and environmental failures while perserving high throughput. It is the�recommended�channel for most topologies where both aspects are important. Brock Noland is a software engineer at Cloudera.</snippet></document><document id="370"><title>How-to: Enable User Authentication and Authorization in Apache HBase</title><url>http://blog.cloudera.com/blog/2012/09/understanding-user-authentication-and-authorization-in-apache-hbase/</url><snippet>With the default Apache HBase configuration,�everyone is allowed to read from and write to all tables available in the system. For many enterprise setups, this kind of policy is unacceptable.� Administrators can set up firewalls that decide�which machines are allowed to communicate with HBase. However, machines that can pass the firewall are still allowed to�read from and write to all tables. �This kind of mechanism is effective but insufficient because HBase still cannot differentiate between multiple users that use the same client machines, and there is still no granularity with regard to HBase table, column family, or column qualifier access. In this post, we will discuss how Kerberos is used with Hadoop and HBase to provide�User Authentication,�and how HBase implements�User�Authorization�to grant users permissions for particular actions on a specified set of data. Secure HBase: Authentication &amp; Authorization A secure HBase aims to protect against sniffers, unauthenticated/unauthorized users and network-based attacks. It does not protect against authorized users who accidentally delete all the data.� HBase can be configured to provide�User Authentication, which ensures that only authorized users can communicate with HBase. The authorization system is implemented at the�RPC�level, and is based on the Simple Authentication and Security Layer (SASL), which supports (among other authentication mechanisms) Kerberos. SASL allows authentication, encryption negotiation and/or message integrity verification on a per connection basis ( �hbase.rpc.protection� configuration property). The next step after enabling�User Authentication�is�to give an�admin the ability to�define a series of User Authorization�rules that allow or deny particular actions. The Authorization system, also known as the Access Controller Coprocessor or Access Control List (ACL), is available from HBase 0.92 (CDH4) onward and gives the ability to define authorization policy (Read/Write/Create/Admin), with table/family/qualifier granularity, for a specified user. Kerberos Kerberos is a networked authentication protocol. It is designed to provide strong authentication for client/server applications by using secret-key cryptography. The Kerberos protocol uses�strong cryptography (AES, 3DES, …) so that a client can prove its identity to a server (and vice versa) across an insecure network connection. After a client and server have used Kerberos to prove their identities, they can also encrypt all of their communications to assure privacy and data integrity as they go about their business. Ticket exchange protocol At a high level, to access a service using Kerberos, each client must follow three steps: Kerberos Authentication: The client authenticates itself to the Kerberos Authentication Server and receive a Ticket Granting Ticket (TGT). Kerberos Authorization: The client request a service ticket from the Ticket Granting Server, which issues a ticket and a session key if the client TGT sent with the request is valid. Service Request: The client uses the service ticket to authenticate itself to the server that is providing the service the client is using (e.g. HDFS, HBase,��) HBase, HDFS, ZooKeeper�SASL Since HBase depends on HDFS and ZooKeeper, secure�HBase relies on a secure HDFS and a secure ZooKeeper. This means that the HBase servers�need�to create a secure service session, as described above, to communicate with HDFS and ZooKeeper. All the files written by HBase are stored in HDFS. As in Unix filesystems, the access control provided by HDFS is based on users, groups and permissions. �All the files created by HBase have �hbase� as user, but this access control is based on the username provided by the system, and everyone that can access the machine is potentially able to �sudo� as the user �hbase�. Secure HDFS adds the authentication steps that guarantee that the �hbase� user is trusted. ZooKeeper has an Access Control List (ACL) on each znode that allows read/write access to the users based on user information in a similar manner to HDFS. HBase ACL Now that our users are authenticated via Kerberos, we are sure that the username that we received is one of our trusted users. Sometimes this is not enough granularity – we want to control that a specified user is able to read or write a table. To do that, HBase provides an Authorization mechanism that allows restricted access for specified users. To enable this feature, you must enable the Access Controller coprocessor, by adding it to hbase-site.xml under the master and region server coprocessor classes. (See how to setup the HBase security configuration here.) A coprocessor is code that runs inside each HBase Region Server and/or Master. �It is able to intercept most operations (put, get, delete, �), and run arbitrary code before and/or after the operation is executed.� Using this ability to execute some code before each operation, the Access Controller coprocessor can check the user rights and decide if the user can or cannot execute the operation. Rights�management�and _acl_ table The HBase shell has a couple of commands that allows�an admin to manage the user rights: grant [table] [family] [qualifier] revoke [table] [family] [qualifier] As you see, an admin has the ability to restrict user access based on the table schema: Give User-W only read rights to Table-X/Family-Y (grant 'User-W', 'R', 'Table-X', 'Family-Y') Give User-W the full read/write rights to Qualifier-Z (grant 'User-W', 'RW', 'Table-X', 'Family-Y', 'Qualifier-Z') An admin also has the ability to grant global rights, which�operate�at the cluster level, such as creating tables,�balancing regions,�shutting down�the cluster and so on: Give User-W the ability to create tables (grant 'User-W', 'C') Give User-W the ability to manage the cluster (grant 'User-W', 'A') All the permissions are stored in a table created by the Access Controller coprocessor, called _acl_. The primary key of this table is the table name that you specify in the grant command. The _acl_�table has just one column family and each qualifier�describes the granularity of rights for a particular table/user. �The value�contains the actual rights granted. As you can see, the HBase�shell commands are tightly related to how the data is stored. The grant command adds or updates one row, and the revoke command removes one row from the _acl_ table. Access�Controller under the hood As mentioned previously, the Access Controller coprocessor uses the ability to intercept each user request, and�check�if the user has�the rights to execute the operations. For each operation, the Access Controller�needs�to query�the _acl_ table to see if the user has the rights to execute the operation. However, this operation can have a negative impact on performance. The solution to fix this problem is using the _acl_ table for persistence and�ZooKeeper to speed up the rights lookup.�Each region server�loads the _acl_ table in memory and get notified of changes by the ZkPermissionWatcher. In this way, every region server has the updated value every time and each permission check is performed by using an in-memory map.� Roadmap While Kerberos is a stable, well-tested and proven authentication system, the�HBase ACL�feature is still very basic and�its semantics are still evolving. HBASE-6096 is the umbrella JIRA as reference for�all the improvements�to ship�in a v2 of the ACL feature. Another open topic on authorization and access control is implementing a per-KeyValue security system (HBASE-6222) that will give the ability to have different values on the same cell associated with a security tag. That would allow to showing a particular piece of information based on the user’s permissions. Conclusion HBase Security adds two extra features that allow you to�protect your data against sniffers or other network attacks (by using Kerberos to authenticate users and encrypt communications between services), and allow you to define User Authorization policies, restrict operations,�and limit data visibility for particular users.� Matteo Bertozzi is a Software Engineer at Spotify and an HBase Consultant at Cloudera.�</snippet></document><document id="371"><title>Apache ZooKeeper 3.4.4 Has Been Released!</title><url>http://blog.cloudera.com/blog/2012/09/apache-zookeeper-3-4-4-has-been-released/</url><snippet>Apache ZooKeeper release 3.4.4 is now available. This is a bug fix release covering 50 issues, four of which were considered blockers. Some of the more serious issues include: ZOOKEEPER-1419 Leader election never settles for a 5-node cluster ZOOKEEPER-1489 Data loss after truncate on transaction log ZOOKEEPER-1466 QuorumCnxManager.shutdown missing synchronization ZOOKEEPER-1412 java client watches inconsistently triggered on reconnect ZOOKEEPER-1427 Writing to local files is done non-atomically ZOOKEEPER-1465 Cluster availability following new leader election takes a long time with large datasets – is correlated to dataset size ZOOKEEPER-1514 FastLeaderElection – leader ignores the round information when joining a quorum Stability, Compatibility and Testing We are pleased to announce that 3.4.4 marks the first stable release of 3.4. This release will be incorporated into CDH4.2. Getting Involved The Apache ZooKeeper project is working on a number of new features. Our How To Contribute page is a great place to start if you�re interested in getting involved as a developer, or dive right into an open issue. Acknowledgements A special thanks to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc), especially Mahadev Konar for acting as release manager.</snippet></document><document id="372"><title>Schedule This! Strata + Hadoop World Speakers from Cloudera</title><url>http://blog.cloudera.com/blog/2012/09/schedule-this-strata-hadoop-world-speakers-from-cloudera/</url><snippet>We’re getting really close to Strata Conference + Hadoop World 2012 (just over a month away), schedule planning-wise. So you may want to consider adding the tutorials, sessions, and keynotes below to your calendar! (Start times are always subject to change of course.) The ones listed below are led or co-led by Clouderans, but there is certainly a wide range of attractive choices beyond what you see here. We just want to ensure that you put these particular ones high on your consideration list. If you’re interested in community meetups as well, refer to my post from last week on that subject – several are planned.� Session/Tutorial/Keynote Name Presenter(s) Date Time An Introduction to Hadoop Mark Fei Tues., Oct. 23 9am Using HBase Amandeep Khurana, Matteo Bertozzi Tues., Oct. 23 9am Testing Hadoop Applications Tom Wheeler Tues., Oct. 23 9am Building a Large-scale Data Collection System Using Flume NG Hari Shreedharan, Will McQueen, Arvind Prabhakar, Prasad Mujumdar, Mike Percy Tues., Oct. 23 1:30pm Given Enough Monkeys – Some Thoughts on Randomness Jesse Anderson Tues., Oct. 23 3:20pm Keynote: Big Answers Mike Olson Weds., Oct. 24 8:55am Large Scale ETL with Hadoop Eric Sammer Weds., Oct. 24 11:40am HDFS – What is New and Future Todd Lipcon (co-presenter) Weds., Oct. 24 4:10pm High Availability for the HDFS NameNode: Phase 2 Aaron Myers, Todd Lipcon Weds., Oct. 24 5pm Plenary Session: Beyond Batch Doug Cutting Thurs., Oct. 25 9:20am Upcoming Enterprise Features in Apache HBase 0.96 Jon Hsieh Thurs., Oct. 25 11:40am Data Science on Hadoop: What�s There and What�s Missing Justin Erickson Thurs., Oct. 25 1:40pm Taming the Elephant – Learn How Monsanto Manages Their Hadoop Cluster to Enable Genome/Sequence Processing Bala Venkatrao, Aparna Ramani (with others) Thurs., Oct. 25 4:10pm Knitting Boar Josh Patterson, Michael Katzenellenbogen Thurs., Oct. 25 4:10pm</snippet></document><document id="373"><title>Meet the Engineer: Jon Natkins</title><url>http://blog.cloudera.com/blog/2012/09/meet-the-engineer-jon-natkins/</url><snippet>In this installment of “Meet the Engineers”, meet Jonathan Natkins, �also known as “Natty” by his friends and colleagues.� What do you do at Cloudera, and in which Apache project are you involved? For the last year and a half, I’ve been an engineer on the Enterprise team. We’re the guys who build Cloudera Manager, and all the goodies that make it easy to manage and administer Apache Hadoop clusters. Specifically, I’ve worked on a number of things across the product, like scale and performance for the databases underlying the various monitoring tools available in the Enterprise edition of Cloudera Manager. I’ve also worked extensively on our operational reporting and HDFS file search capabilities. While I don’t work full-time on any of the Apache projects, I have been known to contribute to Apache Hive and Hadoop on rainy days. Why do you enjoy your job? It’s the people. Cloudera has outrageously smart employees, both technical and non-technical, and for me, that’s really important. I feel like I’ve got a lot to learn from the people around me. The culture is fantastic, as well. I feel just as comfortable in the office with these folks as I do at the bar. Getting to work on one of the hottest enterprise technologies in years is just icing on the cake. What is your favorite thing about Hadoop? My favorite thing about Hadoop has to be the breadth of the ecosystem. It’s pretty cool that we’re working on not just a filesystem, but a data processing system, and a data ingest system, and systems to handle real-time workloads, as well as offline workloads. It’s amazing to see all the use cases that users have been able to build on top of the platform. What is your advice for someone who is interested in participating in any open source project for the first time? Get involved! If you’re interested in participating in the open source community, find a project that strikes your fancy, check out their bug list, and look for something that seems easy enough to bite off. I know the HBase community tags some bugs as “newbie” bugs. Start with those. When in doubt, reach out. Most of the projects have developer mailing lists, and that’s a great place to say hi and ask for some help finding some small issues to cut your teeth on. At what age did you become interested and programming, and why? Well�I built Web pages when I was 12. For a long time, as a kid, I wanted nothing more than to build video games. Then I got older and decided I was going to law school, so I went to college for International Relations. That plan crashed and burned when I realized that I didn’t really�do books. I ended up falling in love with a computer science course I took the following semester, and the rest is history. Look for our next “Meet the Engineer” profile soon. See you then!</snippet></document><document id="374"><title>How-to: Analyze Twitter Data with Apache Hadoop</title><url>http://blog.cloudera.com/blog/2012/09/analyzing-twitter-data-with-hadoop/</url><snippet>Social media has gained immense popularity with marketing teams, and Twitter is an effective tool for a company to get people excited about its products. Twitter makes it easy to engage users and communicate directly with them, and in turn, users can provide word-of-mouth marketing for companies by discussing the products. Given limited resources, and knowing we may not be able to talk to everyone we want to target directly, marketing departments can be more efficient by being selective about whom we reach out to. In this post, we�ll learn how we can use Apache Flume, Apache HDFS, Apache Oozie, and Apache Hive to design an end-to-end data pipeline that will enable us to analyze Twitter data. This will be the first post in a series. The posts to follow to will describe, in more depth, how each component is involved and how the custom code operates. All the code and instructions necessary to reproduce this pipeline is available on the Cloudera Github. Who is Influential? To understand whom we should target, let�s take a step back and try to understand the mechanics of Twitter. A user – let�s call him Joe – follows a set of people, and has a set of followers. When Joe sends an update out, that update is seen by all of his followers. Joe can also retweet other users� updates. A retweet is a repost of an update, much like you might forward an email. If Joe sees a tweet from Sue, and retweets it, all of Joe�s followers see Sue�s tweet, even if they don�t follow Sue. Through retweets, messages can get passed much further than just the followers of the person who sent the original tweet. Knowing that, we can try to engage users whose updates tend to generate lots of retweets. Since Twitter tracks retweet counts for all tweets, we can find the users we�re looking for by analyzing Twitter data. Now we know the question we want to ask: Which Twitter users get the most retweets? Who is influential within our industry? How Do We Answer These Questions? SQL queries can be used to answer this question: We want to look at which users are responsible for the most retweets, in descending order of most retweeted. However, querying Twitter data in a traditional RDBMS is inconvenient, since the Twitter Streaming API outputs tweets in a JSON format which can be arbitrarily complex. In the Hadoop ecosystem, the Hive project provides a query interface which can be used to query data that resides in HDFS. The query language looks very similar to SQL, but allows us to easily model complex types, so we can easily query the type of data we have. Seems like a good place to start. So how do we get Twitter data into Hive? First, we need to get Twitter data into HDFS, and then we�ll be able to tell Hive where the data resides and how to read it. The diagram above shows a high-level view of how some of the CDH (Cloudera’s Distribution Including Apache Hadoop) components can be pieced together to build the data pipeline we need to answer the questions we have. The rest of this post will describe how these components interact and the purposes they each serve. Gathering Data with Apache Flume The Twitter Streaming API will give us a constant stream of tweets coming from the service. One option would be to use a simple utility like curl to access the API and then periodically load the files. However, this would require us to write code to control where the data goes in HDFS, and if we have a secure cluster, we will have to integrate with security mechanisms. It will be much simpler to use components within CDH to automatically move the files from the API to HDFS, without our manual intervention. Apache Flume is a data ingestion system that is configured by defining endpoints in a data flow called sources and sinks. In Flume, each individual piece of data (tweets, in our case) is called an event; sources produce events, and send the events through a channel, which connects the source to the sink. The sink then writes the events out to a predefined location. Flume supports some standard data sources, such as syslog or netcat. For this use case, we�ll need to design a custom source that accesses the Twitter Streaming API, and sends the tweets through a channel to a sink that writes to HDFS files. Additionally, we can use the custom source to filter the tweets on a set of search keywords to help identify relevant tweets, rather than a pure sample of the entire Twitter firehose. The custom Flume source code can be found here. Partition Management with Oozie Once we have the Twitter data loaded into HDFS, we can stage it for querying by creating an external table in Hive. Using an external table will allow us to query the table without moving the data from the location where it ends up in HDFS. To ensure scalability, as we add more and more data, we�ll need to also partition the table. A partitioned table allows us to prune the files that we read when querying, which results in better performance when dealing with large data sets. However, the Twitter API will continue to stream tweets and Flume will perpetually create new files. We can automate the periodic process of adding partitions to our table as the new data comes in. Apache Oozie is a workflow coordination system that can be used to solve this problem. Oozie is an extremely flexible system for designing job workflows, which can be scheduled to run based on a set of criteria. We can configure the workflow to run an ALTER TABLE command that adds a partition containing the last hour’s worth of data into Hive, and we can instruct the workflow to occur every hour. This will ensure that we’re always looking at up-to-date data. The configuration files for the Oozie workflow are located here. Querying Complex Data with Hive Before we can query the data, we need to ensure that the Hive table can properly interpret the JSON data. By default, Hive expects that input files use a delimited row format, but our Twitter data is in a JSON format, which will not work with the defaults. This is actually one of Hive’s biggest strengths. Hive allows us to flexibly define, and redefine, how the data is represented on disk. The schema is only really enforced when we read the data, and we can use the Hive SerDe interface to specify how to interpret what we’ve loaded. SerDe stands for Serializer and Deserializer, which are interfaces that tell Hive how it should translate the data into something that Hive can process. In particular, the Deserializer interface is used when we read data off of disk, and converts the data into objects that Hive knows how to manipulate. We can write a custom SerDe that reads the JSON data in and translates the objects for Hive. Once that’s put into place, we can start querying. The JSON SerDe code can be found here. The SerDe will take a tweet in JSON form, like the following: {
   "retweeted_status": {
      "contributors": null,
      "text": "#Crowdsourcing � drivers already generate traffic data for your smartphone to suggest alternative routes when a road is clogged. #bigdata",
      "geo": null,
      "retweeted": false,
      "in_reply_to_screen_name": null,
      "truncated": false,
      "entities": {
         "urls": [],
         "hashtags": [
            {
               "text": "Crowdsourcing",
               "indices": [
                  0,
                  14
               ]
            },
            {
               "text": "bigdata",
               "indices": [
                  129,
                  137
               ]
            }
         ],
         "user_mentions": []
      },
      "in_reply_to_status_id_str": null,
      "id": 245255511388336128,
      "in_reply_to_user_id_str": null,
      "source": "SocialOomph",
      "favorited": false,
      "in_reply_to_status_id": null,
      "in_reply_to_user_id": null,
      "retweet_count": 0,
      "created_at": "Mon Sep 10 20:20:45 +0000 2012",
      "id_str": "245255511388336128",
      "place": null,
      "user": {
         "location": "Oregon, ",
         "default_profile": false,
         "statuses_count": 5289,
         "profile_background_tile": false,
         "lang": "en",
         "profile_link_color": "627E91",
         "id": 347471575,
         "following": null,
         "protected": false,
         "favourites_count": 17,
         "profile_text_color": "D4B020",
         "verified": false,
         "description": "Dad, Innovator, Sales Professional. Project Management Professional (PMP).  Soccer Coach,  Little League Coach  #Agile #PMOT - views are my own -",
         "contributors_enabled": false,
         "name": "Scott Ostby",
         "profile_sidebar_border_color": "404040",
         "profile_background_color": "0F0F0F",
         "created_at": "Tue Aug 02 21:10:39 +0000 2011",
         "default_profile_image": false,
         "followers_count": 19005,
         "profile_image_url_https": "https://si0.twimg.com/profile_images/1928022765/scott_normal.jpg",
         "geo_enabled": true,
         "profile_background_image_url": "http://a0.twimg.com/profile_background_images/327807929/xce5b8c5dfff3dc3bbfbdef5ca2a62b4.jpg",
         "profile_background_image_url_https": "https://si0.twimg.com/profile_background_images/327807929/xce5b8c5dfff3dc3bbfbdef5ca2a62b4.jpg",
         "follow_request_sent": null,
         "url": "http://facebook.com/ostby",
         "utc_offset": -28800,
         "time_zone": "Pacific Time (US &amp; Canada)",
         "notifications": null,
         "friends_count": 13172,
         "profile_use_background_image": true,
         "profile_sidebar_fill_color": "1C1C1C",
         "screen_name": "ScottOstby",
         "id_str": "347471575",
         "profile_image_url": "http://a0.twimg.com/profile_images/1928022765/scott_normal.jpg",
         "show_all_inline_media": true,
         "is_translator": false,
         "listed_count": 45
      },
      "coordinates": null
   },
   "contributors": null,
   "text": "RT @ScottOstby: #Crowdsourcing � drivers already generate traffic data for your smartphone to suggest alternative routes when a road is  ...",
   "geo": null,
   "retweeted": false,
   "in_reply_to_screen_name": null,
   "truncated": false,
   "entities": {
      "urls": [],
      "hashtags": [
         {
            "text": "Crowdsourcing",
            "indices": [
               16,
               30
            ]
         }
      ],
      "user_mentions": [
         {
            "id": 347471575,
            "name": "Scott Ostby",
            "indices": [
               3,
               14
            ],
            "screen_name": "ScottOstby",
            "id_str": "347471575"
         }
      ]
   },
   "in_reply_to_status_id_str": null,
   "id": 245270269525123072,
   "in_reply_to_user_id_str": null,
   "source": "web",
   "favorited": false,
   "in_reply_to_status_id": null,
   "in_reply_to_user_id": null,
   "retweet_count": 0,
   "created_at": "Mon Sep 10 21:19:23 +0000 2012",
   "id_str": "245270269525123072",
   "place": null,
   "user": {
      "location": "",
      "default_profile": true,
      "statuses_count": 1294,
      "profile_background_tile": false,
      "lang": "en",
      "profile_link_color": "0084B4",
      "id": 21804678,
      "following": null,
      "protected": false,
      "favourites_count": 11,
      "profile_text_color": "333333",
      "verified": false,
      "description": "",
      "contributors_enabled": false,
      "name": "Parvez Jugon",
      "profile_sidebar_border_color": "C0DEED",
      "profile_background_color": "C0DEED",
      "created_at": "Tue Feb 24 22:10:43 +0000 2009",
      "default_profile_image": false,
      "followers_count": 70,
      "profile_image_url_https": "https://si0.twimg.com/profile_images/2280737846/ni91dkogtgwp1or5rwp4_normal.gif",
      "geo_enabled": false,
      "profile_background_image_url": "http://a0.twimg.com/images/themes/theme1/bg.png",
      "profile_background_image_url_https": "https://si0.twimg.com/images/themes/theme1/bg.png",
      "follow_request_sent": null,
      "url": null,
      "utc_offset": null,
      "time_zone": null,
      "notifications": null,
      "friends_count": 299,
      "profile_use_background_image": true,
      "profile_sidebar_fill_color": "DDEEF6",
      "screen_name": "ParvezJugon",
      "id_str": "21804678",
      "profile_image_url": "http://a0.twimg.com/profile_images/2280737846/ni91dkogtgwp1or5rwp4_normal.gif",
      "show_all_inline_media": false,
      "is_translator": false,
      "listed_count": 7
   },
   "coordinates": null
}
 and translate the JSON entities into queryable columns: SELECT created_at, entities, text, user
FROM tweets
WHERE user.screen_name='ParvezJugon'
 �AND retweeted_status.user.screen_name='ScottOstby';
  which will result in: created_at                        entities                                                                                                                 text                                                                                                                                            user
Mon Sep 10 21:19:23 +0000 2012    {"urls":[],"user_mentions":[{"screen_name":"ScottOstby","name":"Scott Ostby"}],"hashtags":[{"text":"Crowdsourcing"}]}    RT @ScottOstby: #Crowdsourcing � drivers already generate traffic data for your smartphone to suggest alternative routes when a road is  ...    {"screen_name":"ParvezJugon","name":"Parvez Jugon","friends_count":299,"followers_count":70,"statuses_count":1294,"verified":false,"utc_offset":null,"time_zone":null}
 We’ve now managed to put together an end-to-end system, which gathers data from the Twitter Streaming API, sends the tweets to files on HDFS through Flume, and uses Oozie to periodically load the files into Hive, where we can query the raw JSON data, through the use of a Hive SerDe. Some Results In my own testing, I let Flume collect data for about three days, filtering on a set of keywords: � hadoop, big data, analytics, bigdata, cloudera, data science, data scientist, business intelligence, mapreduce, data warehouse, data warehousing, mahout, hbase, nosql, newsql, businessintelligence, cloudcomputing The collected data was about half a GB of JSON data, and here is an example of what a tweet looks like. The data has some structure, but certain fields may or may not exist. The retweeted_status field, for example, will only be present if the tweet was a retweet. Additionally, some of the fields may be arbitrarily complex. The hashtags field is an array of all the hashtags present in the tweets, but most RDBMS�s do not support arrays as a column type. This semi-structured quality of the data makes the data very difficult to query in a traditional RDBMS. Hive can handle this data much more gracefully. The query below will find usernames, and the number of retweets they have generated across all the tweets that we have data for: SELECT
 �t.retweeted_screen_name,
 �sum(retweets) AS total_retweets,
 �count(*) AS tweet_count
FROM (SELECT
�       retweeted_status.user.screen_name as retweeted_screen_name,
  ���	retweeted_status.text,
  ���	max(retweet_count) as retweets
      FROM tweets
  �   GROUP BY retweeted_status.user.screen_name,
  �������������retweeted_status.text) t
GROUP BY t.retweeted_screen_name
ORDER BY total_retweets DESC
LIMIT 10; For the few days of data, I found that these were the most retweeted users for the industry:   retweeted_screen_name		total_retweets		tweet_count
  mauricefreedman	        493		        1
  HarvardBiz			362                     6
  TechCrunch			314			7
  googleanalytics		244			10
  BigDataBorat			201			6
  stephen_wolfram		182		        1
  CloudExpo			153			28
  TheNextWeb			150			1
  GonzalezCarmen		121			10
  IBMbigdata			100			37
  From these results, we can see whose tweets are getting heard by the widest audience, and also determine whether these people are communicating on a regular basis or not. We can use this information to more carefully target our messaging in order to get them talking about our products, which, in turn, will get other people talking about our products. Conclusion In this post we�ve seen how we can take some of the components of CDH and combine them to create an end-to-end data management system. This same architecture could be used for a variety of applications designed to look at Twitter data, such as identifying spam accounts, or identifying clusters of keywords. Taking the system even further, the general architecture can be used across numerous applications. By plugging in different Flume sources and Hive SerDes, this application can be customized for many other applications, like analyzing web logs, to give an example. Grab the code, and give it a shot yourself. Jon Natkins (@nattybnatkins) is a Software Engineer at Cloudera, where he has worked on Cloudera Manager and Hue, and has contributed to a variety of projects in the Apache Hadoop ecosystem. Prior to Cloudera, Jon wrangled databases at Vertica. He holds an Sc.B in Computer Science from Brown University.</snippet></document><document id="375"><title>Community Meetups at Strata + Hadoop World 2012</title><url>http://blog.cloudera.com/blog/2012/09/community-meetups-at-strata-hadoop-world-2012/</url><snippet>Strata Conference + Hadoop World�(Oct. 23-25 in New York City) is a bonanza for Hadoop and big data enthusiasts – but not only because of the technical sessions and tutorials. It’s also an important gathering place for the developer community, most of whom are eager to share info from their experiences in the “trenches”. Just to make that process easier, Cloudera is teaming up with local meetups during that week to organize a series of meetings on a variety of topics. (If for no other reason, stop into one of these meetups for a chance to grab a coveted Cloudera t-shirt.) As you can see, these meetups are highly parallel, so you will either have to make careful choices or have very quick feet. The good news is: there’s something for everybody. (In fact: I will send an item of to-be-determined swag to anyone who can submit proof of attendance to every meetup on this list. For example – a photo of yourself standing next to the meetup organizer in an identifiable place.) Tues., Oct. 23 New York Hadoop User Group Hosts: Eli Collins and Aaron Myers Where: Foursquare offices Hive User Group Meetup NYC Host: Carl Steinbach Where: Hilton Hotel Sqoop User Meetup Host: Kathleen Ting Where: PulsePoint offices Thurs., Oct. 25 Flume User Meetup Host: Kathleen Ting Where: Hilton Hotel HBase Meetup Host: Otis Gospodneti� Where: AppNexus offices Cloudera Manager Users Meetup Host: Philip Zeyliger Where: Hilton Hotel ZooKeeper Users Meetup Host: Camille Fournier Where: Hilton Hotel �Now, who’s going? PS: For more information about Cloudera’s investments in user groups and meetups generally, see this page.</snippet></document><document id="376"><title>Exploring Compression for Hadoop: One DBA’s Story</title><url>http://blog.cloudera.com/blog/2012/09/exploring-compression-for-hadoop-one-dbas-story/</url><snippet>This guest post comes to us courtesy of Gwen Shapira (@gwenshap), a database consultant for The Pythian Group (and an Oracle ACE Director). Most western countries use street names and numbers to navigate inside cities. But in Japan, where I live now, very few streets have them. Sometimes solving technical problems is similar to navigating a city without many street names: Once you arrive at the desired location, the path seems obvious, but on the way there are many detours and interesting sights to be seen. Here’s an example: in a client engagement in Tokyo for which I was a database consultant, we needed to move about 10TB from Hadoop into an Oracle Exadata box as a one-time load. The options we considered were: Use FUSE to mount HDFS on Exadata and use Oracle external tables (an Oracle Database feature through which you can access external data as if it were stored in a table) to load the files. Install Hadoop on Oracle Exadata, use HDFS to copy the files to the compute nodes, and load them from there. Write process on the existing server that will read data from Hadoop and load it into Oracle Exadata. Use Sqoop to insert data into Exadata directly from Hadoop. Use a pre-packed Oracle connector. FUSE and Sqoop seemed like the least intrusive options, requiring minimal changes on the Exadata or Hadoop, and we debated the pros and cons of each. Ultimately using FUSE and external tables, while not a common approach, seemed the best choice for our situation. Once we had the data in text files, we could load data into our tables using any method we wanted, using all of Oracle Exadata’s features to optimize compression, parallelism, and load times. Being a consultant, I sent the customer system administrators links to relevant Hadoop-FUSE documentation and asked them to get cranking. The system administrator called me back to say that Hadoop is “backed up" nightly to NFS, and that it would be easier if we just used that data. Since I didn’t mind getting data that is few hours old (I know how to close the data gap), I agreed to their plan. I used "less" to peek at one of the files. This is what I saw: 
SEQ org.apache.hadoop.io.BytesWritable
org.apache.hadoop.io.Text
#com.hadoop.compression.lzo.LzoCodec
 An experienced Hadoop-er would know exactly what she was looking at, but I was still in learning mode. In a discussion with the application developer, I was told that the file was compressed. Just for fun, I decided to write the decompression Java code myself. Read the compressed file, output text to STDOUT, use it as an external tables pre-processor: How difficult could it be? I just needed some code samples of how to use the LZO libraries, and we would be set. Or so I thought. Almost immediately, I got as lost as a drunk tourist in Shinjuku at night. Hadoop in fact has more than one LZO library, and I was not sure if they are mutually compatible. Files can be block compressed or row compressed, and I did not know what my file was. The libraries were used only as plug-ins to Hadoop; apparently no one used them on files outside Hadoop. There was also something called LZOP, and a lot of C libraries that I tried using but that kept claiming my file was not in fact LZO. At that point, I called it a day. But before going to sleep, I took a look at Hadoop: The Definitive Guide (2nd edition), by Cloudera’s Tom White, hoping for some ideas and insights. As expected, the book proved as useful as Google Maps in Tokyo. Two hours and three glasses of Kirin later, I discovered that: The file is not just compressed – it is in fact a SequenceFile. I heard about SequenceFiles at my Cloudera developer course, but I had not actually used one before. SequenceFiles have very descriptive headers that include the key data type (BytesWritable), value data type (Text), whether the file is compressed (YES), block compressed (YES) and codec (LZO). Tom White had thoughtfully included example code that reads SequenceFiles. The book didn’t mention compression, but from my readings I gathered that SequenceFile.Reader might be able to handle that data automatically if I include the LZO libraries in my class path. This looked easy enough. I just needed to get Tom’s example to work with my environment (within Exadata, no Hadoop installed, to be used as input for external tables). The following steps turned out to be the correct ones: Step 1: Prepare the environment. To do this I needed the Hadoop core package and the Hadoop-LZO package. Hadoop core (hadoop-0.20.2-cdh3u2-core.jar) was copied from the Hadoop server (also Linux 64-bit, so it was compatible), LZO was installed from Hadoop-GPL-Packing, an excellent little project that saved me much pain. The location of the packages was added to CLASS_PATH environment variable. You will also need commons-logging.jar somewhere in the class path as Hadoop uses log4j. I also needed native libraries for Hadoop and LZO. I copied libhadoop.so from same server, the hadoop-gpl-packing package included native LZO libraries; the locations of both went into LD_LIBRARY_PATH. Step 2: Compile SequenceFileReaderDemo (from the book) and test it on a sample file: 
import java.io.IOException;
import java.net.URI;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import com.hadoop.compression.lzo.LzoCodec;
import org.apache.hadoop.util.ReflectionUtils;

public class SequenceFileReadDemo {
        public static void main(String[] args) throws IOException {
                String uri = args[0];
                Configuration conf = new Configuration();
                FileSystem fs = FileSystem.get(URI.create(uri), conf);
                Path path = new Path(uri);
                SequenceFile.Reader reader = null;
                try {
                        reader = new SequenceFile.Reader(fs, path, conf);
                        BytesWritable key = (BytesWritable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);
                        Text value = (Text) ReflectionUtils.newInstance(reader.getValueClass(), conf);
                        long position = reader.getPosition();
                        while (reader.next(key, value)) {
                                String syncSeen = reader.syncSeen() ? "*" : "";
                                //System.out.printf("[%s%s]\t%s\t%s\n", position, syncSeen, key, value);
                                System.out.printf("%s\n", value);
                                                        position = reader.getPosition();
                                // beginning of next record
                        }
                }
                finally {IOUtils.closeStream(reader);}
        }
}
 To my joy and disbelief, this "just worked". I had to add import org.apache.hadoop.io.BytesWritable to support our particular key, but once I set the environment right, javac SequenceFileReaderDemo.java ran without issues. Testing the reader of a sample file also worked with no special problems other than those that led to the discovery of native libraries: 
export PATH=/usr/java/latest/bin:/usr/kerberos/bin:
/usr/local/bin:/usr/bin:/bin:/usr/X11R6/bin:
/home/shapira/bin:/home/oracle/work/shapira/DWH
export CLASSPATH=/home/oracle/work/shapira/DWH/hadoop-0.20.2-cdh3u2-core.jar:
/opt/hadoopgpl/lib/hadoop-lzo.jar:
/home/oracle/app/oracle/product/11.2.0/dbhome_2/jlib/commons-logging-1.0.4.jar:
/home/oracle/work/shapira/DWH export LD_LIBRARY_PATH=:
/opt/hadoopgpl/native/Linux-i386-32/:/home/oracle/work/shapira/DWH

java SequenceFileReadDemo $1  2&gt;/dev/null
 Step 3: Use SequenceFileReaderDemo as pre-processor for external tables. This was probably the most difficult step (relatively speaking). Integration usually is. First, I had to tweak the output of SequenceFileReaderDemo to match the expectations of Oracle Database. Don’t output row numbers, or keys, or alignment marks – just the values and nothing but the values. Also, Hadoop will try to send info messages to STDERR, which causes Oracle Database to assume something is wrong with the External table and quit. Rather than learn how to configure log4j parameters for my non-existing Hadoop, I just piped STDERR into null, which although sloppy would do for a POC. Step 4: Load the data from the external table into a real table. I used Create Table as Select for the POC, but there are many other options. (That’s one of the nicest things about external tables; the feature is so flexible that you can even read sequence files!) To test the output, I also had to set one more environment variable: export NLS_LANG=JAPANESE_JAPAN.UTF8. Otherwise, most text came out as junk: -- external table test

--create directory nfs_dir as '/home/oracle/work/shapira/DWH';
--create directory exec_dir as '/home/oracle/work/shapira/DWH';

--grant read,write on directory nfs_dir to dw_test;
--grant execute on directory exec_dir to dw_test;

conn dw_test/dw_test
drop table ext_shop;
drop table test1;
drop table test6;

CREATE TABLE ext_test (
        db_name varchar2(30),
        test_id number,
        db      varchar2(30),
        system  varchar2(30),
        url     varchar2(1024),
        test_name varchar2(1024),
        test_name_kana varchar2(1024),
        time_stamp timestamp
        )
ORGANIZATION EXTERNAL (
        TYPE oracle_loader
        default directory nfs_dir
        access parameters (
                records delimited by newline characterset utf8
                preprocessor exec_dir:'test_reader.sh'
                fields terminated by "|"
                missing field values are null
                (db_name,test_id,db,system,url,test_name,test_name_kana,time_stamp char(19)
                  DATE_FORMAT TIMESTAMP 'yyyy-mm-dd hh24:mi:ss')
                )
        location ('000009_0'))
PARALLEL
REJECT LIMIT UNLIMITED;
set pagesize 300 linesize 200;
select * from ext_test where rownum&lt;=2;

set timi on
create table test1 as select * from ext_test;

--Create table test2 compress for query low as select * from ext_test;
--Create table test3 compress for query high as select * from ext_test;
--Create table test4 compress for archive low as select * from ext_test;
--Create table test5 compress for archive high as select * from ext_test;
create table test6 compress for oltp as select * from ext_test;

select segment_name,blocks,bytes/1024 from dba_segments where segment_name like 'test%';
select count(*) from test1;
select count(*) from test6;
exit;
 Of course, this was just enough to show the reluctant developers how things are done. I think they were happy with the results. But this was not real production code. The most glaring issue is that the error handling is dubious at best. But what was more worrying for me was that most of the data is stored in many small files - not exactly the best way to Hadoop. The overhead of creating a new JVM to load each file would become an issue. I could have had SequenceFileReader accept entire directories as inputs and process all files in the directories at once, but directories have highly variable sizes and there are not enough of them. I decided that the input would have to be a range of dates or filenames within a directory, and used this to partition the loading process manually. Maybe I was obsessing over the details of a one-time load, but no one complained. Japanese are famous for getting the small details right!</snippet></document><document id="377"><title>Cloudera Enterprise in Less Than Two Minutes</title><url>http://blog.cloudera.com/blog/2012/09/cloudera-enterprise-in-less-than-two-minutes/</url><snippet>What’s to love about Cloudera Enterprise? A lot! But rather than bury you in documentation today, we’d rather bring you a less-than-two-minute-long video: For a quick on-ramp, download Cloudera Manager Free Edition right now – free to use for up to 50 nodes with no term limit. See also the Installation Guide.</snippet></document><document id="378"><title>How-to: Automate Your Cluster with Cloudera Manager API</title><url>http://blog.cloudera.com/blog/2012/09/automating-your-cluster-with-cloudera-manager-api/</url><snippet>API access was a new feature introduced in Cloudera Manager 4.0 (download free edition here.). Although not visible in the UI, this feature is very powerful, providing programmatic access to cluster operations (such as configuration and restart) and monitoring information (such as health and metrics). This article walks through an example of setting up a 4-node HDFS and MapReduce cluster via the Cloudera Manager (CM) API. Cloudera Manager API Basics The CM API is an HTTP REST API, using JSON serialization. The API is served on the same host and port as the CM web UI, and does not require an extra process or extra configuration. The API supports HTTP Basic Authentication, accepting the same users and credentials as the Web UI. API users have the same privileges as they do in the web UI world. You can read the full API documentation here. Interacting with the API The most basic way to use the API is by making HTTP calls directly using tools like curl. For example, to obtain the status of service hdfs2 in cluster dev01 (note: italics are used for interactive shell code throughout): $ curl -u 'admin:admin' http://cm_host:7180/api/v1/clusters/dev01/services/hdfs2 { ��"name" : "hdfs2", ��"type" : "HDFS", ��"clusterRef" : { ����"clusterName" : "dev01" ��}, ��"serviceState" : "STARTED", ��"healthSummary" : "GOOD", ��"configStale" : false, ��... }   The API also comes with a Python client for your convenience. To do the same in Python: &gt;&gt;&gt; from cm_api.api_client import ApiResource &gt;&gt;&gt; api = ApiResource('cm_host', username='admin', password='admin')&gt;&gt;&gt; dev01 = api.get_cluster('dev01') &gt;&gt;&gt; hdfs = dev01.get_service('hdfs2') &gt;&gt;&gt; print hdfs.serviceState, hdfs.healthSummary STARTED GOOD
   You can expect to see client bindings in more languages. A Java client is in the works right now. Setting up a Cluster Next I will demonstrate an API Python script that defines, configures and starts a cluster. You are about to see some of the low-level details of Cloudera Manager. Compared with the UI wizard, the API route is more tedious. But the API provides flexibility and programmatic control. You will also notice that this setup process does not require my cluster to be online (until the very last step where I start the services.) This has proven useful to people who are stamping out pre-configured clusters. Step 1. Define the Cluster #!/usr/bin/env python

import socket
from cm_api.api_client import ApiResource

CM_HOST = "centos56-17.ent.cloudera.com"

api = ApiResource(CM_HOST, username="admin", password="admin")
cluster = api.create_cluster("prod01", "CDH4")
   This creates a handle on the API. The ApiResource object also accept other optional arguments such as port, TLS, and API version. With that, I created a cluster called prod01 on version CDH4. The handle to the cluster is returned as part of the call. Step 2. Create HDFS Service and Roles Now we can create the services. HDFS comes first: hdfs = cluster.create_service("hdfs01", "HDFS")
   At this point, if I query the different role types supported by hdfs01, I will get: &gt;&gt;&gt; print hdfs.get_role_types() [u'DATANODE', u'NAMENODE', u'SECONDARYNAMENODE', u'BALANCER', u'GATEWAY', u'HTTPFS', u'FAILOVERCONTROLLER']
   Now I am going to create 1 NameNode, 1 Secondary NameNode and 4 DataNodes. HOSTNAMES = [
��"centos56-17.ent.cloudera.com",
��"centos56-18.ent.cloudera.com",
��"centos56-19.ent.cloudera.com",
��"centos56-20.ent.cloudera.com"
]
hosts = [ ]�����������������������������# API host handles

for name in HOSTNAMES:
��host = api.create_host(
������name,�����������������������������# Host id
������name,�����������������������������# Host name (FQDN)
������socket.gethostbyname(name),�������# IP address
������"/default_rack")������������������# Rack
��hosts.append(host)

nn = hdfs.create_role("hdfs01-nn", "NAMENODE", hosts[0].hostId)
snn = hdfs.create_role("hdfs01-snn", "SECONDARYNAMENODE", hosts[0].hostId)
for i in range(4):
��hdfs.create_role("hdfs01-dn" + str(i), "DATANODE", hosts[i].hostId)
   Most of the code is performing host creation. That is required for role creation, as each role needs to be assigned to a host. In the end, the first host is assigned the NameNode, the Secondary NameNode and a DataNode. The rest are DataNodes. At this point, if I query the first host, I can see the correct roles assigned to it: &gt;&gt;&gt; print api.get_host(HOSTNAMES[0]).roleRefs [{'clusterName': 'prod01', 'roleName': 'hdfs01-snn', 'serviceName': 'hdfs01'}, �{'clusterName': 'prod01', 'roleName': 'hdfs01-dn0', 'serviceName': 'hdfs01'}, �{'clusterName': 'prod01', 'roleName': 'hdfs01-nn', 'serviceName': 'hdfs01'}]   Step 3. Configure HDFS Service configuration is separated into service-wide configuration and role type configuration. Service-wide configuration is typically settings that affect multiple role types, such as HDFS replication factor. Role type configuration is a template that gets inherited by specific role instances. For example, at the role type template level, I can set all DataNodes to use 3 data directories. And I can override that for specific DataNodes by setting the role-level configuration. hdfs_service_config = {
��'dfs_replication': 2,
}
nn_config = {
��'dfs_name_dir_list': '/dfs/nn',
��'dfs_namenode_handler_count': 30,
}
snn_config = {
��'fs_checkpoint_dir_list': '/dfs/snn',
}
dn_config = {
��'dfs_data_dir_list': '/dfs/dn1,/dfs/dn2,/dfs/dn3',
��'dfs_datanode_failed_volumes_tolerated': 1,
}
hdfs.update_config(
����svc_config=hdfs_service_config,
����NAMENODE=nn_config,
����SECONDARYNAMENODE=snn_config,
����DATANODE=dn_config)

# Use a different set of data directories for DN3
hdfs.get_role('hdfs01-dn3').update_config({
����'dfs_data_dir_list': '/dn/data1,/dn/data2' })
 How do I find out the configuration keys used by CM? For example, how do I know that dfs_replication is the key for setting replication factor? I query the service: &gt;&gt;&gt; service_conf, roletype_conf = hdfs.get_config(view="full") &gt;&gt;&gt; print service_conf {u'catch_events': , �u'dfs_block_access_token_enable': , �u'dfs_block_size': , �... &gt;&gt;&gt; for k, v in sorted(service_conf.items()): ... print "\n------ ", v.displayName, "\n Key:", k, \ ... "\n Value:", v.value, "\n Default:", v.default, \ ... "\n AKA:", v.relatedName, "\n Desc:", v.description ... ------ Enable log event capture ��Key: catch_events ��Value: None ��Default: true ��AKA: None ��Desc: When set, each role will identify important log events and forward them to Cloudera Manager. ------ Enable block access token ��Key: dfs_block_access_token_enable ��Value: None ��Default: true ��AKA: dfs.block.access.token.enable ��Desc: If true, access tokens are used as capabilities for accessing DataNodes. If false, no access tokens are checked on accessing DataNodes. ------ Block Size ��Key: dfs_block_size ��Value: None ��Default: 134217728 ��AKA: dfs.blocksize ��Desc: The default block size for new HDFS files. ��...
   Note the view="full" argument. Without it, the API returns only the configs that are set to non-default values: &gt;&gt;&gt; hdfs.get_config() ({u'dfs_ha_fencing_cloudera_manager_secret_key': u'xz5Yr2inDI8vWEzf16EQpIKPoBMoTg', ��u'dfs_replication': u'2'}, �{u'BALANCER': {}, ��u'DATANODE': {u'dfs_data_dir_list': u'/dfs/dn1,/dfs/dn2,/dfs/dn3', ���u'dfs_datanode_failed_volumes_tolerated': u'1'}, ��u'FAILOVERCONTROLLER': {}, ��u'GATEWAY': {}, ��u'HTTPFS': {}, ��u'NAMENODE': {u'dfs_name_dir_list': u'/dfs/nn', ���u'dfs_namenode_handler_count': u'30'}, ��u'SECONDARYNAMENODE': {u'fs_checkpoint_dir_list': u'/dfs/snn'}})   Step 4. Create MapReduce Service and Roles This step is similar to the HDFS one. I assign a TaskTracker to each node, and the JobTracker to the first one. mr = cluster.create_service("mr01", "MAPREDUCE")
jt = mr.create_role("mr01-jt", "JOBTRACKER", hosts[0].hostId)
for i in range(4):
��mr.create_role("mr01-tt" + str(i), "TASKTRACKER", hosts[i].hostId)
   Step 5. Configure MapReduce Here is the code to configure the “mr01″ service.: mr_service_config = {
�
'hdfs_service': 'hdfs01',
}
jt_config = {
��'jobtracker_mapred_local_dir_list': '/mapred/jt',
��'mapred_job_tracker_handler_count': 40,
}
tt_config = {
��'tasktracker_mapred_local_dir_list': '/mapred/local',
��'mapred_tasktracker_map_tasks_maximum': 10,
��'mapred_tasktracker_reduce_tasks_maximum': 6,
}

gateway_config
 = {
��'mapred_reduce_tasks': 10,
��'mapred_submit_replication': 2,
}
mr.update_config(
����svc_config=mr_service_config,
����JOBTRACKER=jt_config,
����TASKTRACKER=tt_config,
����GATEWAY=gateway_config)
   Two items deserve elaboration. First is the hdfs_service. Rather than asking the user for the equivalent of “fs.defaultFS”, a MapReduce service depends on an HDFS service, and derives its HDFS access parameters based on how that HDFS service is configured. Second, the “gateway” role type is unique to CM. It represents a client. A gateway role does not run any daemons. It simply receives client configuration, as part of the “deploy client configuration” process, which we will perform later. Step 6. Start HDFS HDFS is ready to start. This is the step that requires the cluster nodes to be up, CDH installed, and Cloudera Manager Agents running. (The API does not perform software installation.) As part of the preparation, I did that, and pointed the CM agents to the CM server by editing the server_host in /etc/cloudera-scm-agent/config.ini. Now I can format HDFS and start it. CMD_TIMEOUT = 180 # format_hdfs takes a list of NameNodes
cmd = hdfs.format_hdfs('hdfs01-nn')[0]
if not cmd.wait(CMD_TIMEOUT).success:
��raise Exception("Failed to format HDFS")

cmd = hdfs.start()
if not cmd.wait(CMD_TIMEOUT).success:
��raise Exception("Failed to start HDFS")
   Each of the cmd object represents an asynchronous command. I then wait for their completion and assert that they have succeeded. Then I deploy the HDFS client configuration to the host running hdfs01-nn. cmd = hdfs.deploy_client_config('hdfs01-nn')
if not cmd.wait(CMD_TIMEOUT).success:
��raise Exception("Failed to deploy HDFS client config")
   Step 7. Start MapReduce The JobTracker will not start unless /tmp exists in HDFS. [root@centos56-17 ~]# sudo -u hdfs hadoop fs -mkdir /tmp [root@centos56-17 ~]# sudo -u hdfs hadoop fs -chmod 1777 /tmp
   Now we can finish the rest: cmd = mr.start()
if not cmd.wait(CMD_TIMEOUT).success:
��raise Exception("Failed to start MapReduce")
   cmd = mr.deploy_client_config('mr01-jt')
if not cmd.wait(CMD_TIMEOUT).success:
��raise Exception("Failed to deploy MapReduce client config")
   Note that users have not been setup, and their home directories do not exist. But we can run a job as “hdfs”: [root@centos56-17 ~]# sudo -u hdfs hadoop jar /usr/lib/hadoop-0.20-mapreduce/hadoop-examples.jar pi 2 2 Number of Maps = 2 Samples per Map = 2 Wrote input for Map #0 Wrote input for Map #1 Starting Job 12/09/03 23:38:35 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same. 12/09/03 23:38:35 INFO mapred.FileInputFormat: Total input paths to process : 2 12/09/03 23:38:36 INFO mapred.JobClient: Running job: job_201209032320_0001 ... Advanced Usage The Cloudera Manager API provides a lot more than configuration and service life-cycle management. You can also obtain service health information and metrics (for the Enterprise Edition), and configure Cloudera Manager itself. Here are some resources for your exploration: API documentation API tutorial API client library (with example scripts on Github). Here you can also find a Nagios monitoring script that integrates with the API to discover the entities in the cluster and their health. bc Wong is a Software Engineering Manager at Cloudera, currently working on Cloudera Manager.</snippet></document><document id="379"><title>Meet the Engineer: Eric Sammer</title><url>http://blog.cloudera.com/blog/2012/09/meet-the-engineer-eric-sammer/</url><snippet>In this installment of “Meet the Engineer”, we meet with Eric Sammer (invariably known as just plain “Sammer”), Apache committer and author of the upcoming O’Reilly book, Hadoop Operations. What do you do at Cloudera, and in which Apache project are you involved? I’ve been lucky enough to be part of a few different teams at Cloudera since I joined. Almost three years ago, I joined Cloudera as a Solution Architect; a member of the professional services team. Most of my time was spent working with customers to build out Apache Hadoop and Apache HBase clusters, and designing data integration and processing pipelines. I also occasionally had the opportunity to fill in with the training team, teaching Cloudera’s Hadoop Developer and Administration courses to both public and private groups. There’s nothing more exciting than getting to hang out with a group of smart people and talk about Hadoop all day. I moved into a Principal Solution Architect role, spending more time in the office, working on architectural patterns and problems that repeat across customers, and working with internal teams on ways to improve CDH and Cloudera Manager. Hadoop isn’t an island, however, and as more and more of Cloudera’s partners began integrating with CDH and Cloudera Manager, I started focusing on helping them answer many of the same questions as our customers. This was the beginning of our Partner Engineering team, which fulfills mostly the same function as the Solution Architect team, but for partners. As part of this team, I got a chance to work with our partners on projects like Oracle’s Big Data Appliance, the combined Dell | Cloudera Solution for Hadoop, and HP’s recently released HP AppSystem, all of which include both CDH and Cloudera Manager. Today, I’m back in a more traditional engineering role, hacking on systems related projects. Over the last few years, Cloudera has given me the opportunity to contribute to a number of open source projects, but notably Apache Flume and Apache MRUnit, where I’m a committer and PMC member on both projects.� Why do you enjoy your job? What’s not to like? I get to work with top notch talent, hack on open source code, write a book, and speak at conferences and meetup groups. Being at Cloudera has given me a kind of back stage pass to see what other folks are doing with Hadoop and HBase, as well as be part of the process. No where else would I have worked on some of these projects. I think we do a great job of finding the intersection of what’s exciting to work on with what’s useful to users. A large part of that is open source development and contribution, something core to how things work here. It’s equally important to be able to see positive results. Whether that means shipping a release or watching a customer run their first MapReduce job on a multi-hundred node cluster, it’s nice to be able to point to something you contributed to at the end of the day. It’s a good feeling. What is your favorite thing about Hadoop? I think the best part about the Hadoop ecosystem is how far it has come while it’s still so young. When you look at what has been done, even in just the last few years, you have to be impressed. Hadoop is taking on workloads that run billion dollar businesses in a much wider set of industries than when it initially started. It’s true that all things Hadoop are the rage these days, but there’s something behind it; it’s not just smoke and mirrors. Consider the feature set and usage of Hadoop only two years ago versus today. The trajectory looks different than a lot of other systems out there. It’s also about the surrounding systems in the ecosystem. More and more, people are talking about high level data processing languages and systems, real time serving, data integration, and management rather than the nuts and bolts of core Hadoop. That’s not to dismiss the criticality of Hadoop proper, but to say that it’s the bedrock of something bigger. What is your advice for someone who is interested in participating in any open source project for the first time? Pony up and get involved. I’ve never met a developer that didn’t have a gripe with a library or application. Grab the source, fix it, and submit it back. Patches speak louder than words. Too often, developers spend time commenting on what someone else has done rather than saying, “here’s how I think it should work” by way of code. I’m not a sports guy, but there’s an analogy to the armchair quarterback here, somewhere. If you’re just starting out, find an existing project that solves a problem you have or feel passionate about. Ultimately, you always have to be working on something that means something to you. Working on someone else’s dream is something most people aren’t good at.� Be cognizant of the fact that no one has to work with you (like they do at the office) – you have to make them want to work with you. Sure, you have to be smart and you also have to be able to produce, but you have to do it in a way that makes people receptive to that contribution. If you’re thinking about contributing to a project, hang out on the mailing lists for a bit and learn how things work. It’s hard to undo a bad first impression. Recognize that the world is full of smart people; a little humility goes an awfully long way. At what age did you become interested and programming, and why? I have a background in music. My introduction to the mechanics of programming came from learning about synth programming: waveforms, LFOs, VCAs, and eventually topics like grain table synthesis (although we didn’t call it that, back then) and how much of music and sound was just about patterns and numbers. I taught myself C and learned all I could about digital signal processing somewhere around the age of fifteen. The concept of fabricating something – especially something like sound – out of nothing was absolutely incredible to me. I played around with the patterns and relationships between music and programming for some time, but by nineteen, I had a “real job” building web apps. Some time later, I realized data storage, processing, and distributed systems were far more interesting to me. In other words, my CSS-box-model-phobia reached an all time high. I’m still fascinated by recurring patterns in systems, software, music, and the rhythmic sound the San Francisco MUNI escalator at Montgomery and Market makes. It’s hypnotic. Thanks, Sammer!</snippet></document><document id="380"><title>What Do Real-Life Apache Hadoop Workloads Look Like?</title><url>http://blog.cloudera.com/blog/2012/09/what-do-real-life-hadoop-workloads-look-like/</url><snippet>Organizations in diverse industries have adopted Apache Hadoop-based systems for large-scale data processing. As a leading force in Hadoop development with customers in half of the Fortune 50 companies, Cloudera is in a unique position to characterize and compare real-life Hadoop workloads. Such insights are essential as developers, data scientists, and decision makers reflect on current use cases to anticipate technology trends. Recently we collaborated with researchers at UC Berkeley to collect and analyze a set of Hadoop traces. These traces come from Cloudera customers in e-commerce, telecommunications, media, and retail (Table 1). Here I will explain a subset of the observations, and the thoughts they triggered about challenges and opportunities in the Hadoop ecosystem, both present and in the future. Table 1. Summary of Hadoop workloads analyzed 1: Bytes moved is the sum of input, shuffle, and output sizes of all jobs in the workload. The following are some of the questions we asked about the Hadoop workloads: What are the data access patterns? Typical data set sizes? Uniform or skewed data access? Frequency of re-accessing data? How does the load vary over time? Regular cycles or unpredictable? Frequency and size of load bursts? What are the compute patterns? Ratio between compute and movement of data? Common job types? How frequently do people use higher level query languages such as Hive and Pig as opposed to Java MapReduce? How does this workload compare with other deployments? Typical Data Set Sizes Data manipulation and management are key functions of Hadoop, so it is important to understand the typical data set sizes on Hadoop clusters. The graphs below show the cummulative distribution of per-job input, shuffle, and output data across the workloads. A dot (x, y) on the graph shows that data set size less than x makes up y fraction of the jobs in the workload. For example, the arrow highlights that ~60% of jobs in the CC-b workload have input sizes of less than several KBs. Figure 1. Per-job input, shuffle, and output data sizes The graphs show that most jobs have data inputs sizes in the megabytes to terabytes and the distribution of data sizes varies greatly between workloads. It�s worth clarifying that because each workload contains a large number of jobs, the cluster capacity still needs to be quite large, even though a single job often accesses smaller data sets. These observations suggest the limits of simple performance measurement tools such as terasort or TestDFSIO: Data sets at TB and larger scales represent only a narrow sliver of real life behavior. All of the input, shuffle, and output stages need to be tested together subject to a large range of three-stage data patterns. Generic tools give generic numbers that are not guaranteed to translate to a particular workload. Data Access Skew It is not surprising that over the lifetime of data, at any given time some subsets are more valuable than the rest and get accessed more often. The nature and cause of such skew offer insights on how we can improve the Hadoop ecosystem. The graph below shows the HDFS file read frequencies against the file rank by decreasing read frequency. In other words, we count the number of jobs that read each HDFS file, then make the graph such that a data point (x, y) indicates that the xth most frequently read file is read y times. For example, the arrow indicates that the most frequently accessed file in the CC-b workload is read close to 1000 times over the duration of the trace. Figure 2. Data access skew by HDFS file paths The graph axes are both logarithmic. The combined lines from all five workloads form an approximate straight band. Straight lines on these axes represent a Zipf distribution, which indicates high data skew. In particular, for all workloads, the top 1000 most frequently read files account for all files that are read multiple times. One observation is particularly interesting. The slope of the lines are approximately the same, indicating similar data skew across several industry sectors. In order words, there are some similar business processes or data analysis needs across different industry sectors that result in the same uneven data popularity. We speculate that several factors could cause this cross-industries similarity: First, timely data is valued across industry sectors, and therefore more recent data should be more frequently accessed than archived data. Second, regardless of industry, human analysts sometimes find it necessary to restrict analysis to a small data set, which could result in a highly distilled data set being frequently accessed. Furthermore, skew could come from constant improvements in products and services, which make it less critical to analyze data about deprecated products and services.� Frameworks on Top of Hadoop and HDFS The Hadoop ecosystem encompasses many tools and ancillary components to facilitate data ingest and data access, and to provide higher level data processing languages, including Hive, Pig, Oozie, Flume, and Sqoop. We can analyze the job name strings within each workload to measure the relative slot time attributed to some of these frameworks. The graph below is a stacked bar graph, showing the relative fractions of the first word of job names. For example, in the CC-a workload, 0.48 fraction of the slot time come from jobs whose names begin with the word �insert�. We�ve color coded the words based on the frameworks to which the job belongs. The �others� sector comprises the remainder of the job names, whose contribute fraction of slot times too low to be labeled individually on the graph. This analysis would not include components that do not interact with the Job Tracker, e.g. Flume. Note that Oozie is a workflow management framework, which in turn may launch jobs in various other frameworks (Hive, Pig, Java MapReduce). We�re currently working on identifying the individual jobs within the Oozie workflows as, say, Hive or Pig or Java MapReduce. Figure 3. Task-time (slot-hours) in workloads grouped by frameworks We see that Hadoop extension frameworks make up a considerable fraction of slot times, and sometimes dominate the workload (CC-a, CC-e). Also, two frameworks are enough to account for a large fraction of slot time in each workload. The prevalence of frameworks like Hive, Pig, and Oozie suggests that people find it attractive to interact with data using query-like languages (Hive/Pig) and workflow management systems (Oozie). As these frameworks mature, it is important to explore what would be even more convenient ways to interact with the data. For example, as knowledge about use cases increase, we should try to devise automated or at least standard analyses for common business cases. Further, the fact that only two frameworks dominate each workload suggests that there are non-trivial human and business process costs that make each enterprise acquire and learn only a small number of frameworks. This means that out of the many MapReduce extension frameworks existing today, the market could eventually consolidate around a small number of proven, well-supported solutions.� Implications for Customers and Cloudera Our study highlights both a wide range of data volumes processed by different customers – from less than 3TB/day to 600TB/day – and striking similarities in the distribution of data volume processed per job. The much higher access frequency to a limited subset of data, particularly if that data is the most recently acquired, indicates there may be benefits to tiered storage. The smaller subset could be maintained in more expensive storage with lower latency and/or higher replica counts without significantly affecting total cost if HDFS placement was made sensitive to access patterns. A significant portion of the processing is executed through high-level languages like Pig Latin and Hive SQL, as opposed to through programs written in Java MapReduce, but the workload is dominated by Java MapReduce programs for at least one of the five companies. Companies may choose to use only one high-level language or to adopt more than one for different purposes. Further study may illuminate the use cases most conducive to adoption of each approach. As a software provider to a wide range of industries, company sizes, and use cases, Cloudera’s software distribution must accommodate a full spectrum of languages and processing frameworks, and be flexible enough to handle many small jobs efficiently and also very large jobs running over terabytes of input data. Yanpei is a member of the performance team at Cloudera. A more detailed version of the study appeared as a VLDB 2012 paper, which was co-authored with Sara Alspaugh and Randy Katz at UC Berkeley.</snippet></document><document id="381"><title>The Action on "HBase in Action"</title><url>http://blog.cloudera.com/blog/2012/09/the-action-on-hbase-in-action/</url><snippet>Apache HBase junkies, this one’s for you: I had an opportunity recently for a quick chat with the authors of HBase in Action (Manning Publications – download sample chapter PDF), by Nick Dimiduk and Cloudera’s Amandeep Khurana. Why did you write HBase in Action? Amandeep:�HBase In Action is about how to use Apache�HBase effectively. When we started talking about this topic initially, Nick and I scoffed at the idea of writing an entire 300-page book. After all, it really is just three API calls: Get, Put, and Scan. But as we talked further, we realized that building successful applications using HBase (or for that matter other NoSQL stores) is not as trivial as the API itself, especially for people with relational DBMS background. There was much unlearning to be done and several new concepts to be learned about thinking at scale. As we started to think about this more, we also noticed that until very recently, the focus in the HBase community was on making HBase a more solid, performant, and stable data store. Most people using it in production know the internals well enough to understand what to do and how to do it. But it also became clear that adoption is being inhibited by the fact that considerable time is needed to figure out how to use HBase effectively, and there is little content available about that. Nick: Amandeep nailed this one. This work is designed to be the “HBase User’s Guide” or “operator’s manual” rather than a “mechanic’s guide.” Continuing the automobile analogy: We’ll teach you how to choose the right car for your needs, the basic rules of the road, how to deal with the potholes you’ll encounter along the way, how to change your oil, and when to go for a tuneup. We don’t get into rebuilding the engine block or detailed wiring diagrams; those we leave for those interested in diving into the code and engaging with the committers. Who is your intended reader? Amandeep: The intention of the book is to teach developers how to build applications using HBase and thereafter deploy and operationalize them. The book is geared toward folks who have some development background and have likely built applications using other databases. During your research, did you learn anything about HBase you didn’t know? Amandeep:�There were several things (including some that were unrelated to HBase) I learned during this project. The first example is schema design – I had used most of the concepts at different times but never thought about how they all worked together, so creating a simple example for the book was a little challenging. Coprocessors and asynchbase were the other two topics that I really didn’t know a whole lot about until they made their way into the manuscript. Nick:�The big take-away for me was the actual depth of my knowledge. Everything I wrote about forced me to explore the topic even deeper than previously. This was the case regardless of how well I thought I knew it going in. I also very much enjoyed getting into the internals of OpenTSDB. There’s not many examples of HBase applications out there for people to explore. Be it literature or code or application architecture, spend time reading and it will teach you to be a better writer. Some readers will be surprised to learn that HBase offers alternative clients for non-Java development (via the Thrift gateway; see sample chapter). How important is that fact for HBase adoption? Amandeep: To date the primary client for HBase development has been the native Java client. However, having a single client out there limits adoption. There is not enough documentation and guidance available out there about other clients. That limits the adoption and therefore inhibits the investment that HBase developers put into them. With alternate clients becoming more accessible, more dev time would go into them, making them as good as the native client in the long run. This could be key for driving adoption. What’s the most interesting application of HBase you’ve ever seen? Amandeep:�In my mind the really interesting applications are where HBase is serving real-time traffic with tight SLAs. Those are not only harder to develop but also hard to operationalize and tune. That’s where the challenge lies, and that’s where the opportunity lies too. Nick: I’m most excited about the work my team at Climate is doing with HBase now and even more excited about what we’ll be doing with it next year. Chapter 8 is a simplified version of one aspect of my current project: enabling GIS applications on HBase. The next project will be hosting huge scientific datasets and exposing them using community standard tools and protocols; I’m optimistic we’ll choose HBase to tackle this challenge as well. You can also meet Amandeep and Nick at Strata Conference + Hadoop World, Oct. 23-25, in New York. Cloudera will be giving away 200 copies at our booth as part of the “Meet the Author” program. For an HBase deep dive there, catch Amandeep’s “Using HBase” session. (Use discount code CLOUDERA to get an additional 20% off conference registration!)</snippet></document><document id="382"><title>How-to: Develop CDH Applications with Maven and Eclipse</title><url>http://blog.cloudera.com/blog/2012/08/developing-cdh-applications-with-maven-and-eclipse/</url><snippet>Learn how to configure a basic Maven project that will be able to build applications against CDH Apache Maven is a build automation tool that can be used for Java projects. Since nearly all the Apache Hadoop ecosystem is written in Java, Maven is a great tool for managing projects that build on top of the Hadoop APIs. In this post, we�ll configure a basic Maven project that will be able to build applications against CDH (Cloudera�s Distribution Including Apache Hadoop) binaries. Maven projects are defined using an XML file called pom.xml, which describes things like the project’s dependencies on other modules, the build order, and any other plugins that the project uses. A complete example of the pom.xml described below, which can be used with CDH, is available on Github. (To use the example, you�ll need at least Maven 2.0 installed.) If you’ve never set up a Maven project before, you can get a jumpstart by using Maven’s quickstart archetype, which generates a small initial project layout. Choose a group ID (typically a top-level package name) and an artifact ID (the name of the project), and execute the following command with the groupId and artifactIdarguments filled in:         mvn archetype:generate \
      -DarchetypeGroupId=org.apache.maven.archetypes \
      -DarchetypeArtifactId=maven-archetype-quickstart \
      -DgroupId=&lt;your-group-id&gt; \
      -DartifactId=&lt;your-project-name&gt;
   There will be a couple of prompts for information, but you can safely just hit enter till you see the build succeed. This will create a new directory with the name you chose as the artifact ID. In that directory will be a pom.xml file, and a src directory. Since the most important part of a Maven project is the pom.xml, we’ll focus on what goes in there. Right now, this pom.xml is a bit minimalistic. It has some high-level project metadata, a properties section, and a dependencies section, with a single dependency on the JUnit test framework. Since we want to use this project for Hadoop development, we need to add some dependencies on the Hadoop libraries. Maven resolves dependencies by downloading JAR files from remote repositories, like Maven Central Repository, but none of the default repositories include CDH, so we need to add a repository. The repository is declared in the pom.xml within the top-level projectsection like this: &lt;repositories&gt;
      &lt;repository&gt;
        &lt;id&gt;cloudera-releases&lt;/id&gt;
        &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;
        &lt;releases&gt;
          &lt;enabled&gt;true&lt;/enabled&gt;
        &lt;/releases&gt;
        &lt;snapshots&gt;           &lt;enabled&gt;false&lt;/enabled&gt;
        &lt;/snapshots&gt;
      &lt;/repository&gt;
    &lt;/repositories&gt;
   This instructs Maven to pull any Hadoop binaries from the Cloudera repository, and now we can declare a dependency on Hadoop JARs. You can find all the Maven dependencies that are available from Cloudera, including Hadoop, Apache HBase, and the rest of the CDH components here. In order to specify a project dependency, you’ll add a dependency element to the dependencies section of the pom.xml: &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
      &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
      &lt;version&gt;2.0.0-mr1-cdh4.0.1&lt;/version&gt;
    &lt;/dependency&gt;
   A project with the above dependency would compile against the CDH4 MapReduce v1 library. In practice, it’s good to declare the version string as a property, since there is a high likelihood of dependencies on more than one Maven artifact with the same version. The property can be declared in the properties section of the pom.xml: &lt;hadoop.version&gt;2.0.0-mr1-cdh4.0.1&lt;/hadoop.version&gt;
   The name that is chosen for the property can then be referenced in other sections of the pom.xml. So, if we were to specify the hadoop.versionproperty, we can change our hadoop-client dependency to look like this: &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
      &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
      &lt;version&gt;${hadoop.version}&lt;/version&gt;
    &lt;/dependency&gt;
   Now, whenever we want to upgrade our code to a new CDH version, we only need to change the version string in one place, at the top of the pom.xml. Since Hadoop requires at least Java 1.6, we should also specify the compiler version for Maven to use by enabling the compiler plugin in the top-level projectsection:     &lt;build&gt;
      &lt;pluginManagement&gt;
        &lt;plugins&gt;
          &lt;plugin&gt;
            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
            &lt;version&gt;2.3.2&lt;/version&gt;
            &lt;configuration&gt;
              &lt;source&gt;1.6&lt;/source&gt;
              &lt;target&gt;1.6&lt;/target&gt;
            &lt;/configuration&gt;
          &lt;/plugin&gt;
        &lt;/plugins&gt;
      &lt;/pluginManagement&gt;
    &lt;/build&gt;
   This gets us to a point where we’ve got a fully functional project, and we can build a JAR by running mvn install. However, the JAR that gets built does not contain the project dependencies within it. This is fine, so long as we only require Hadoop dependencies, since the Hadoop daemons will include all the Hadoop libraries in their own classpaths. If the Hadoop dependencies are not sufficient, it will be necessary to package the other dependencies into the JAR. We can configure Maven to package a JAR with dependencies by adding the following XML block to the buildsection: &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
        &lt;version&gt;1.7.1&lt;/version&gt;
        &lt;executions&gt;
          &lt;execution&gt;
            &lt;phase&gt;package&lt;/phase&gt;
            &lt;goals&gt;
              &lt;goal&gt;shade&lt;/goal&gt;
            &lt;/goals&gt;
          &lt;/execution&gt;
        &lt;/executions&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;
   When executing the mvn packagecommand, the above declarations instruct Maven to package all the dependencies into a JAR file. However, the JAR now contains all the Hadoop libraries, which would conflict with the Hadoop daemons� classpaths. We can indicate to Maven that certain dependencies need to be downloaded for compile-time, but will be provided to the application at runtime by augmenting the Hadoop dependencies:     &lt;dependency&gt;
      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
      &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
      &lt;version&gt;${hadoop.version}&lt;/version&gt;
      &lt;scope&gt;provided&lt;/scope&gt;
    &lt;/dependency&gt;
   Remember to only add the provided scope to dependencies that you do not want included in the JAR. Maven also has very tight integration with a number of IDEs, such as Eclipse, NetBeans IDE, and IntelliJ IDEA. With Eclipse, the integration comes in two forms: by generating Eclipse artifacts through Maven and importing a project into Eclipse, or by using the m2eclipse Eclipse plugin, which allows you to modify a pom.xmlor run Maven builds from within Eclipse. A project can be setup to integrate with Eclipse by adding the following declarations to the pluginssection: &lt;plugin&gt;
      &lt;groupId&gt;org.apache.maven.plugins &lt;/groupId&gt;
      &lt;artifactId&gt;maven-eclipse-plugin&lt;/artifactId&gt;
      &lt;version&gt;2.9&lt;/version&gt;
      &lt;configuration&gt;
        &lt;projectNameTemplate&gt;
          ${project.artifactId}
        &lt;/projectNameTemplate&gt;
        &lt;buildOutputDirectory&gt;
          eclipse-classes
        &lt;/buildOutputDirectory&gt;
        &lt;downloadSources&gt;true&lt;/downloadSources&gt;
        &lt;downloadJavadocs&gt;false&lt;/downloadJavadocs&gt;
      &lt;/configuration&gt;
    &lt;/plugin&gt;
   In order to generate the files necessary to import projects into Eclipse, you’ll need to run the following command: mvn -Declipse.workspace=&lt;eclipse-workspace-path&gt;   eclipse:configure-workspace eclipse:eclipse
   This command will generate an Eclipse .project file. You can import the project into Eclipse by selecting File -&gt; Import, and then choosing Existing Projects Into Workspace from the General dropdown. Browse to the root directory of the project, and click OK. You should see the available projects listed in the checkbox. Select the projects you want to import, and then click Finish to complete the import. Maven will set up the classpath for the Eclipse project, so all the JARs that you have referenced as dependencies in the &lt;projectNameTemplate&gt;${project.artifactId}&lt;/projectNameTemplate&gt;�should show up under Referenced Libraries in the Eclipse project. If you add more dependencies later, the Eclipse files can be regenerated by running the mvn eclipse:eclipse command, and refreshing the project in Eclipse. For more information about Maven, and Maven documentation, see http://maven.apache.org. Jon Natkins (@nattybnatkins) is a Software Engineer at Cloudera, where he worked on Cloudera Manager and Hue, and has contributed patches to Hive and Hadoop. Prior to Cloudera, Jon was an engineer and database wrangler at Vertica. He holds an Sc.B in Computer Science from Brown University.</snippet></document><document id="383"><title>Apache Hadoop on Your PC: Cloudera’s CDH4 Virtual Machine</title><url>http://blog.cloudera.com/blog/2012/08/hadoop-on-your-pc-clouderas-cdh4-virtual-machine/</url><snippet>Today ZDNet has very helpfully published a guide to downloading, configuring, and using Cloudera’s Demo VM for CDH4 (available in three flavors, but in this case the VMware version). As the author, Andrew Brust, explains, the VM contains�a “pre-built, training-appropriate, 1-node Apache Hadoop cluster” (on top of CentOS). Perhaps most important for boot-strappers, it’s free. You can download the VM here�- and there is a Hadoop tutorial available�here.�The combo will go a long way toward jump-starting explorations. Thanks, ZDNet!</snippet></document><document id="384"><title>Meet the Engineer: Aaron T. Myers</title><url>http://blog.cloudera.com/blog/2012/08/meet-the-engineer-aaron-t-myers/</url><snippet>As I mentioned in my�inaugural�post last week, it’s important to shine a spotlight on the Cloudera engineers who have a hand in making the Hadoop projects run. It’s an obvious point, and yet an overlooked one, that a community is an aggregation of individual personalities who have diverse backgrounds and interests yet a shared passion for the group and its goals. As Jono Bacon puts it in his seminal 2009 book The Art of Community, “The building blocks of a community are its teams, and the material that makes these blocks are people.” Thus, welcome to the first installment of our “Meet the Engineers” series, in which we will briefly introduce you to some of the engineer-individuals helping to build the foundations of Hadoop. Today, it’s Aaron T. Myers, �aka ATM!� What do you do at Cloudera and in which Apache project are you involved? I’ve been a software engineer at Cloudera for over 2 years now. I started out at Cloudera working on our Enterprise products, and then transitioned to working on Apache Hadoop core, specifically focused on HDFS and Hadoop’s security features. Why do you enjoy your job? I really like working on tough technical problems – both new features and tricky bugs. At Cloudera, I get to develop and support several complex distributed systems, whose interactions routinely test my technical ability. Some of the most personally-satisfying experiences I’ve ever had have involved getting to the root cause of difficult to trace problems. I also love seeing my work directly improving users’ outcomes – not just engineering for engineering’s sake. At Cloudera, I get to be directly involved in making customers successful, which is a great feeling. What is your favorite thing about Hadoop? There are a bunch of things I like about Hadoop, but my favorite is probably that our user group meetups are usually called “HUGs.” What is your advice for someone who is interested in participating in any open source project for the first time? Start using the project first. If you don’t have an organic use for it, try following some example tutorials. If you want to submit changes, start small, with something like a little usability improvement that you encountered when first getting up and running. Every open source (or really, any software) project I’ve ever been involved with has a few rough edges that could stand to be sanded off. Usability improvements tend to have a very high appreciation-to-difficulty-of-implementation ratio. At what age did you become interested and programming, and why? Probably about age 10, programming in Lego Logo. I was into building with Legos well before that, and from a very early age I thought I wanted to be a mechanical engineer, because I loved building systems that work reliably. Once I discovered programming, I concluded that I could get all the things I loved about mechanical engineering from software engineering, just with a much more rapid turnaround on prototyping. I was hooked. Look for our next “Meet the Engineer” profile in a week or two. See you then!</snippet></document><document id="385"><title>Cloudera Manager 4.0.4 &amp; Cloudera Manager 3.7.8 Released!</title><url>http://blog.cloudera.com/blog/2012/08/cloudera-manager-4-0-4-cloudera-manager-3-7-8-released/</url><snippet>Cloudera Manager 4.0.4 and Cloudera Manager 3.7.8 are now available! These are enhancement releases for Cloudera Manager 4.x and Cloudera Manager 3.7.x respectively. Key enhancements include: Cloudera Manager 4.0.4 Fix Event store cleanup process to avoid deleting recent events Fix RegionServer heap size overrides for Kerberos environments Support custom version of SSH based on OpenSSH Explicitly allow setting of ulimits for CM Server Several bug fixes� Read the release notes�here.�For a test drive, Cloudera Manager 4.0.4 Free Edition is available for download�here. Cloudera Manager 3.7.8 Fix Event store cleanup process to avoid deleting recent events Propagate CM generated mapred.reduce.tasks value in client configurations� Several bug fixes Read the release notes here. Have fun with these new releases!</snippet></document><document id="386"><title>Process a Million Songs with Apache Pig</title><url>http://blog.cloudera.com/blog/2012/08/process-a-million-songs-with-apache-pig/</url><snippet>The following is a guest post kindly offered by Adam Kawa, a 26-year old Hadoop developer from Warsaw, Poland. This post was originally published in a slightly different form at his blog,�Hakuna MapData! Recently I have found an interesting dataset, called�Million Song Dataset�(MSD), which�contains detailed acoustic and contextual data about a million songs. For each song we can find information like title, hotness, tempo, duration, danceability, and loudness as well as artist name, popularity, localization (latitude and longitude pair), and many other things. There are no music files included here, but the links to MP3 song previews at�7digital.com can be easily constructed from the data. The dataset consists of 339 tab-separated text files. Each file contains about 3,000 songs and each song is represented as one separate line of text. The dataset is publicly available and you can find it at�Infochimps or�Amazon S3.�Since the total size of this data sums up to around 218GB, processing it using one machine may take a very long time. Definitely, a much more interesting and efficient approach is to use multiple machines and process the songs in parallel by taking advantage of open-source tools from the Apache Hadoop ecosystem (e.g. Apache Pig). If you have your own machines, you can simply use CDH (Cloudera’s Distribution including Apache Hadoop), which includes the complete Apache Hadoop stack. CDH can be installed manually (quickly and easily by typing a couple of simple commands) or automatically using Cloudera Manager Free Edition (which is Cloudera’s recommended approach). Both CDH and Cloudera Manager are freely downloadable here. Alternatively, you may rent some machines from Amazon with Hadoop already installed and process the data using Amazon’s Elastic MapReduce (here is a cool description writen by Paul Lemere how to use it and pay as low as $1, and here is my presentation about Elastic MapReduce given at the second meeting of Warsaw Hadoop User Group). Problem Definition I came up with the idea to process this dataset to find “exotic” (but still popular) songs. By exotic songs, I simply mean a song that is recorded by an artist who lives in some foreign country, far away from other artists.�The general goal is to discover a couple of fancy folk songs that are associated with the culture of some country. A funny example could be the song�“Koko Koko Euro Spoko”�by Jarz�bina which was chosen by Poles to be the official song of Polish national football team during UEFA EURO 2012. Apache Pig I have used Apache Pig to achieve this goal. Apache Pig is a convenient tool created at Yahoo! to analyze large datasets easily and efficiently. Apache Pig provides a high level and easy to learn, understand and maintain data flow programming language, called PigLatin. PigLatin is quite similar to scripting languages, it supports many useful operations like filtering, sorting, aggregation, joining, splitting and provides several complex data types (tuples, bags and maps). An exemplary PigLatin script is 20x shorter than its equivalent in Java MapReduce and takes a programmer 16x less time to implement it, at the same time being only slightly slower than Java MapReduce (see PigMix2 benchmark). Thanks to these advantages, Apache Pig is often used by many well-recognized companies and organizations e.g. Yahoo! (currently about 90% Hadoop jobs is written in PigLatin), Twitter, Nokia Ovi Maps, AOL, Mendeley, LinkedIn and ICM UW. PigLatin Script To find such popular folk and local songs, I have implemented a simple PigLatin script (about 50 lines of PigLatin code). This script uses a bit naive, but quite effective idea and tries to search for �isolated� songs. An isolated song is simply a song where the average distance between its artist localization and any other artists is as low as possible (to be more precise, I should say that an isolated song is a song recorded by an isolated artist). This approach gives us much bigger probability to discover �ear-catching� songs from Congo, Mali, Poland and Vietnam than from the US or the UK. As mentioned, the dataset contains artists� localization information in form of lat/long pairs and luckily there is a open-source library DataFu (created by LinkedIn) that provides e.g. PigLatin UDF to calculate distance between two lat/long pairs using the Haversine formula. I read the input data and filter out unpopular songs or songs without lat/long localization (relation Filtered). Then I produce all pairs of different songs (relation Different) and calculate distance between their artists localization (relation Distanced). Next, for each song, I calculate the average distance between the song�s artist localization and all other songs� artists localization (relation AvgDistanced). Now, I can limit my records to take only the most interesting results. Firstly, I want to have only the hottest song for a given location (relation Popular). Then I remain only a small number of songs with the lowest average distance (relation Limited) and finally store output results in the the format that will be useful for further visualization with Google Maps (relation Displayed). Here is the source code: SET  default_parallel $parallel
SET  pig.tmpfilecompression true
SET  pig.tmpfilecompression.codec lzo
SET  pig.maxCombinedSplitSize 134217728

REGISTER '/usr/lib/pig-0.10.0/contrib/datafu-0.0.4/dist/datafu-0.0.4.jar';
DEFINE haversineMiles datafu.pig.geo.HaversineDistInMiles();

Songs = LOAD '$input';
--"Project early and often"
Projected = FOREACH Songs GENERATE
        (double) $6 AS artistLat, (double) $8 AS artistLong, $12 AS artistName,
        (double) $43 AS songHotness, $51 AS songTitle, (int) $52 AS songPreview;

--"Filter early and often"
Filtered = FILTER Projected BY (songHotness &gt;= $hotness) AND
    (artistLat IS NOT NULL) AND (artistLong IS NOT NULL);
--"Copy useful fields from Popluar relation"
Filtered2 = FOREACH Filtered GENERATE songPreview as song2Preview, artistLat AS
    artist2Lat, artistLong AS artist2Long;

--"Produce all pairs of different songs and calculate distance between localizations
   of their artists"
Crossed = CROSS Filtered, Filtered2;
Different = FILTER Crossed BY songPreview != song2Preview;
Distanced = FOREACH Different GENERATE artistLat..songPreview,
        haversineMiles(artistLat, artistLong, artist2Lat, artist2Long) as distance;

--"For each song, calculate average distance between its artists and all other artists"
Grouped = GROUP Distanced BY artistLat..songPreview;
AvgDistanced = FOREACH Grouped {
        Distances = Distanced.distance;
        GENERATE FLATTEN(group), AVG(Distances) AS distanceAvg;
}

--"Find the most popular song for a given location"
Locationed = GROUP AvgDistanced BY (artistLat, artistLong);
Popular = FOREACH Locationed {
        OrderedSongs = ORDER AvgDistanced BY songHotness DESC;
        TopSong = LIMIT OrderedSongs 1;
        GENERATE FLATTEN(TopSong);
}
--"Find the most isolated songs which were recored by artists
    who live far away from other artists"
Ordered = ORDER Popular BY distanceAvg DESC;
Limited = LIMIT Ordered $topCount;

--"Generate results in such a format that can be easily displayed
   by Google Maps (by copy &amp; paste)"
Displayed = FOREACH Limited GENERATE
        CONCAT('[', (chararray) artistLat), artistLong, songPreview,
        CONCAT('"', CONCAT((chararray) songTitle, '"')),
        CONCAT('"', CONCAT((chararray) artistName, '"')),
        songHotness,
        CONCAT((chararray)distanceAvg, '],');

STORE Displayed INTO '$output' USING PigStorage(',');
 Running PigLatin script My script takes five parameters: input and output paths, the song’s “hotness” threshold (a float number between 0.0 and 1.0), the number of output songs, and default level of parallelism. time /usr/lib/pig-0.10.0/bin/pig -p input=input/million-song -p
hotness=0.5 -p topCount=200 -p parallel=100 -p
output=output/million-song/all.5.200.100 exotic-songs.pig
 Pig runs this script as sequence of seven MapReduce jobs. The total time of running this script was 40m 47.758s. I have optimized this script by increasing memory available for child tasks, combining small input files, compressing the output of the map tasks and the output of the intermediate jobs using LZO compression. I have also turned off the speculative execution for both map and reduce tasks. (Note that there were no other jobs running on the Hadoop cluster at that time.) I have run this script on the Hadoop cluster that belongs to ICM UW. I have used three “fat” slave nodes and a virtual machine on separate machine in the role of HDFS NameNode and Hadoop JobTracker. Each worker node has four AMD Opteron 6174 processors (48 cores in total), 192GB of RAM and can store 7TB of data. For the purpose of this Pig script, I have configured each worker node to run 45 map and 35 reduce tasks maximally. (So in total, I can run 135 map and 105 reduce tasks in parallel.) I do realize that it is not typical Hadoop configuration (definitely, it is not commodity hardware), but I just simply use what I have. Currently, there is CDH3u3 installed here, which includes Hadoop 0.20.2 and Pig 0.8.1 by default. However, I have installed Pig 0.10.0 manually on my client machine and used it to submit jobs to this cluster. The readability and performance of this script could be improved by implementing own user-defined functions (UDFs) e.g. multi-text concatenation, tuning some MapReduce and Pig parameters (e.g. pig.maxCombinedSplitSize) or setting better customized level of parallelism using PARALLEL keyword. The Results to See The results can be visualized by using some JavaScipt with GoogleMaps. The map below shows 200 songs, and for each song, I put a clickable marker in its artist’s lat/long localization that displays basic information about this song and the link to a song’s preview. � The Results to Hear Here are some songs that attracted my attention: Africa Cler Achel by Tinariwen (Mali) (hotness: 0.6590) Yolele by Papa Wemba (Democratic Republic of the Congo) (hotness: 0.6103) Smooth Operator by Sade (Nigeria) (hotness: 0.9458) Asia Cherry Town by Trilok Gurtu (India) (hotness: 0.6192) Pandi panda by Chantal Goya (Vietnam) (hotness: 0.5952) South America La Camisa Negra by Juanes (Colombia) (hotness: 0.8290) Electric Avenue by Eddy Grant (Guyana) (hotness: 0.8206) Todo se transforma by Jorge Drexler (Uruguay) (hotness: 0.7946) Abusadora by Wilfrido Vargas (Brasil) (hotness: 0.6773) Europe Laura non c’? by Nek (Italy) (hotness: 0.7786) Bracka by Grzegorz Turnau (Poland) (hotness: 0.7332) If you find more interesting examples, just let me know or simply put them in the comments bellow. I do realize that my music taste can be questionable ;) Special Thanks Since all my calculations were performed on the Hadoop cluster that belongs to ICM UW, I would like to give big thanks to the ICM’s staff for the opportunity to use the cluster.� Thanks, Adam!</snippet></document><document id="387"><title>Cloudera Software Engineer Eli Collins on Apache Hadoop and CDH4</title><url>http://blog.cloudera.com/blog/2012/08/cloudera-software-engineer-eli-collins-on-hadoop-and-cdh4/</url><snippet>In June 2012, Eli Collins (@elicollins), from Cloudera’s Platforms team, led a session at QCon New York 2012 on the subject “Introducing Apache Hadoop: The Modern Data Operating System.” During the conference, the QCon team had an opportunity to interview Eli about several topics, including important things to know about CDH4, main differences between MapReduce 1.0 and 2.0, Hadoop use cases, and more. It’s a great primer for people who are relatively new to Hadoop. You can catch the full interview (video and transcript versions) here.</snippet></document><document id="388"><title>Apache HBase Replication: Operational Overview</title><url>http://blog.cloudera.com/blog/2012/08/hbase-replication-operational-overview/</url><snippet>This is the second blogpost about Apache HBase replication. The�previous blogpost, HBase Replication Overview, discussed use cases, architecture and different modes supported in HBase replication. This blogpost is from an operational perspective and will touch upon HBase replication configuration, and key concepts for using it — such as bootstrapping, schema change, and fault tolerance. Configuration As mentioned in HBase Replication Overview, the master cluster sends shipment of WALEdits to one or more slave clusters. This section describes the steps needed to configure replication in a master-slave mode. All tables/column families that are to be replicated must exist on both the clusters. Add the following property in $HBASE_HOME/conf/hbase-site.xml on all nodes on both clusters; set it to true.� � � � � � � � � � � � � �hbase.replication � � � � � � � �true � � � � � On the master cluster, make the following additional changes: Set replication scope (REPLICATION_SCOPEattribute) on the table/column family which is to be replicated: hbase shell&gt; disable �table’ hbase shell&gt; alter �table�, {NAME =&gt; �column-family�, REPLICATION_SCOPE =&gt; 1} hbase shell&gt; enable �table� REPLICATION_SCOPE is a column-family level attribute and its value can be either 0 or 1. A value of 0 means replication is disabled, and 1 means replication is enabled. A user has to alter each column family with the alter command as shown above, for all the column families he wants to replicate. If a user wants to enable replication while creating a table, he should use the following command: hbase shell&gt; create �table�, �column-family1�, ��column-family2�, {NAME =&gt; �column-family1�, REPLICATION_SCOPE =&gt; 1} The above command will enable replication on �column-family1� of the above table. ������ 2. �� In the hbase shell, add the slave peer. A user is supposed to provide slave cluster�s zookeeper quorum, its client port, and the root hbase znode, along with a peerId: hbase shell&gt;add_peer �peerId�, “::” The peerId is a one or two character long string, and a corresponding znode is created under the peers znode, as explained in the previous blog. Once a user runs the add_peer command, Replication code instantiates a ReplicationSource object for that peer, and all the master cluster regionservers try to connect to the slave cluster�s regionservers. It also fetches the slave cluster�s ClusterId (UUID, registered on the slave cluster�s zookeeper quorum). The master cluster regionserver lists the available regionservers of the slave by reading �/hbase/rs� znode and its children on the slave cluster�s zookeeper quorum, and makes connection to it. Each regionserver at the master cluster chooses a subset from the slave regionservers, depending on the ratio specified by �replication.source.ratio�, with default value 0.1. This means that each master cluster regionserver will try to connect to 10% of the total of slave cluster regionservers. While sending the transaction batch, the master cluster regionserver will pick a random regionserver from these connected regionservers.�(Note: Replication is not done for catalog tables, .META. and _ROOT_.) To set up a master-master mode, the above steps should be repeated on both clusters. Schema Change As mentioned in the previous section, replicated table and column family must exist in both clusters. This section discusses various possible scenarios as to what happens during a schema change when the replication is still in progress: a) Delete the column family in master: Deletion of a column family will not affect the replication of any pending mutations for that family. This is because replication code reads the WAL and checks the replication scope of the column families for each WALEdit. Each WALEdit has a map of the replication enabled column families; the check is done on all the constituting keyvalue�s column family (whether they are scoped or not). If it is present in the map, it is added to the shipment. Since the WALEdit object is created before the column family was deleted, its replication will not be affected. b) Delete the column family in slave: The WALEdits are shipped from the master cluster to a particular slave regionserver, which processes it like a normal HBase client (using a HTablePool object). Since the column family is deleted, the put operation will fail and that exception is thrown to the master regionserver cluster. Start/Stop Replication Start/Stop commands work as a kill switch. When stop_replication command is run at the HBase shell, it will change the value of /hbase/replication/state to false. It will stop all the replication source objects from reading the logs; but the existing read entries will be shipped. If a user uses the stop replication command, �the newly rolled logs will not be enqueued for replication. Similarly, issuing a start_replication command will start the replication from the current WAL (which may contain some previous transactions), and not from the time when the command was issued. � Figure 1 explains the start-stop switch behavior, where the sequence of events flows in the direction of arrows. Version Compatibility Master cluster regionservers connect to slave cluster regionservers as normal HBase clients. The same rule of compatibility applies as to whether a HBase client on version xxx is supported on an HBase server on version yyy. On another point, since replication is still evolving (more and more functionalities are continually added to it), a user should be aware of the existing functionalities. For example, in CDH4, which is based on the HBase 0.92 branch, there is support for master-master and cyclic replication. Enabling/Disabling replication source at peer level is added in the HBase 0.94 branch. Boot-strapping Replication works by reading the WALs of the master cluster regionservers. If a user wants to replicate old data, they can run a copyTable command (defining start and end timestamp), while enabling the replication. The copyTable command will copy the data scoped by the start/end timestamps, and replication will take care of current data. The overall approach can be summarized as: Start the replication (note the timestamp). Run the copyTable command with an end timestamp equal to the above timestamp. Since replication starts from current WAL, there may be some keyvalues which are copied to slave by both replication and copyTable jobs. This is still okay, as it is an idempotent operation. In the case of master-master replication, one should run the copyTable job before starting the replication. This is because if a user starts a copyTable job after enabling replication, the second master will resend the data to the first master, as copyTable does not edit the clusterId in the mutation objects. The overall approach can be summarized as: Run the copyTable job, (note the start timestamp of the job). Start the replication. Run the copyTable again with starttime equal to the starttime noted in step 1. This also entails some data being pushed back and forth between the two clusters; �but it minimizes its size. Fault Tolerance Master Cluster Region Server Failover All the regionservers in the master cluster create a znode under �/hbase/replication/rs�, as mentioned in the Architecture section. A regionserver adds a child znode for each WAL, with a byte offset under its znode in the replication hierarchy, as shown in Figure 1. If a regionserver fails, other regionservers need to take care of the dead regionserver�s logs, which are listed under that regionserver�s znode. All regionservers keep a watch on other regionserver znodes (�/hbase/rs�) znode; so when a regionserver fails, other regionservers will get the event as master marks this regionserver as dead. In this case, all other regionservers race to transfer the WALs from dead regionserver znode to their znode, and attach a prefix it with slave id and dead regionserver name, in order to distinguish from other normal logs. A separate replication source (NodeFailoverWorker instance) is instantiated for the transferred logs, which dies after processing these transferred logs. If one consider Figure 1 of the HBase Replication Overview as the base figure of replication znodes hierarchy, Figure 2. shows the new replication znodes hierarchy in case server foo1.bar.com dies and foo2.bar.com takes over its queue. Note the new znode �1-foo1.bar.com,40020,1339435481973� which is created under foo2.bar.com znode /hbase/hbaseid: b53f7ec6-ed8a-4227-b088-fd6552bd6a68 �. /hbase/rs/foo2.bar.com,40020,1339435481973: /hbase/rs/foo3.bar.com,40020,1339435486713: /hbase/replication: /hbase/replication/state: true /hbase/replication/peers: /hbase/replication/peers/1: zk.quorum.slave:281:/hbase /hbase/replication/rs: /hbase/replication/rs/foo1.bar.com.com,40020,1339435084846: /hbase/replication/rs/foo1.bar.com,40020,1339435481973/1: /hbase/replication/rs/foo1.bar.com,40020, 1339435481973/1/foo1.bar.com.1339435485769: 1243232 /hbase/replication/rs/foo3.bar.com,40020,1339435481742: /hbase/replication/rs/foo3.bar.com,40020,1339435481742/1: /hbase/replication/rs/foo3.bar.com,40020, 1339435481742/1/foo3.bar..com.1339435485769: 1243232 /hbase/replication/rs/foo2.bar.com,40020,1339435089550: /hbase/replication/rs/foo2.bar.com,40020,1339435481742/1: /hbase/replication/rs/foo2.bar.com,40020, 1339435481742/1/foo2.bar..com.13394354343443: 1909033 /hbase/replication/rs/foo2.bar.com,40020,1339435481742/1- foo1.bar.com,40020,1339435481973/foo1.bar.com.1339435485769: 1243232 Figure 2. Regionserver failover znodes hierarchy Meanwhile, log splitting may kick in and may archive the dead region server logs. The replication source looks for the logs in both regular and archived directory. Slow/unresponsive slave cluster (or regionservers) When a slave cluster is down, or if there is a temporary network partition, logs which are not yet replicated to the slave will not be deleted by the HBase log cleaner. Log cleaning is handled by LogCleaner class, which keeps on running after a configured time. Replication code adds ReplicationLogCleaner plugin, to the LogCleaner class. When the latter tries to delete a specific log, ReplicationLogCleaner will look to see whether that log exists in the replication znode hierarchy (under the /hbase/replication/rs/ znode). If the log is found, it means that the log is yet to be replicated, and it will skip its deletion. Once the log is replicated, its znode will be deleted from the replication hierarchy. In its next run, LogCleaner will delete the log file successfully once it is replicated. Verification For smaller amount of data, one can simply look for the table rows using the hbase shell at the slave cluster to verify whether they are replicated or not. A standard way to verify is to run the verifyrep mapreduce job, that comes with HBase. It should be run at the master cluster and require slave clusterId and the target table name. One can also provide additional arguments such as start/stop timestamp and column families. It prints out two counters namely, GOODROWS and BADROWS, signifying the number of replicated and unreplicated rows, respectively.� Replication Metrics Replication framework exposes some useful metrics which can be used to check the replication progress. Some of the important ones are: sizeOfLogQueue: number of HLogs to process (excludes the one which is being processed) at the Replication source shippedOpsRate: rate of mutations shipped logEditsReadRate: rate of mutations read from HLogs at the replication source ageOfLastShippedOp: age of last batch that was shipped by the replication source Future Work With all the current features present in current HBase replication, �there is still scope for further improvement. It varies from performance such as decreasing the replication time-lag between master and slave, to more robust handling of region server failure (HBase-2611). Further areas of improvement include enabling peer-level table replication and proper handling of IncrementColumnValue (HBase-2804). Conclusion This post discussed HBase replication from an operator�s point of view including configuration (of various modes), bootstrapping an existing cluster, effects of schema changes and fault tolerance.</snippet></document><document id="389"><title>Developer Community Outreach from Cloudera: Better, Faster, More</title><url>http://blog.cloudera.com/blog/2012/08/developer-community-outreach-from-cloudera-better-faster-more/</url><snippet>Hello World: This is my first post as the new guy facilitating and coordinating developer community outreach for Cloudera. I am extremely excited to become a new node in the Apache Hadoop ecosystem and involved in the global, thriving community of developers coding against Hadoop and its related projects. My most recent experience involves deep interest in the Java ecosystem – which achieved ubiquity not only for technical reasons, but also thanks to a vast group of passionate users who committed themselves to Java’s adoption. There are many lessons there for those of us who want to see the Hadoop edition meet that standard. How do we get there? At this early stage, compared to very mature ecosystems, knowledge about Hadoop development and admin is concentrated in a smaller number of people. For that reason, organic crowdsourcing is still inefficient. Instead, we need to unlock the brains of Hadoop ecosystem rock stars, inside as well as outside Cloudera, and make them accessible to the community at large (not literally, of course). At the same time, we need to help more efficiently bring your feedback and experience into the community. If you were to ask me about focus areas for meeting those goals, the list would include: Helping user groups conquer the world. User group members, and especially their leaders, are the epitome of the passionate user – and passionate users are lifeblood. For that reason, we will double-down on our support for Hadoop, HBase, and related UGs worldwide – by providing more sponsorships, speakers and content (tech talks only, no sales or marketing!), swag, and other things we haven’t thought of yet whenever it’s reasonably possible. Involved in a user group? Please get in touch – whether you have a specific request or simply want to say hello. Encouraging sharing. Do you have a story about a deployment to tell or other tips/experiences to share? We want to give you a platform for doing that on this very blog. Again, send me a note if the idea of being a published author, or alternatively of being interviewed about your experience, interests you. Driving direct participation in the Apache Hadoop project and related projects. Open source technology is only as strong as its community. If you found a bug, why stand by on the sidelines? Report it, or even better, write and submit a patch yourself. We’ll offer advice over time about why, and the best way, to do this. Introducing you to the team. Fairly soon I hope to get a formal “Meet the Engineer” series up and running for this blog, so that you can learn about the background, technical interests, and personal experiences of the Cloudera engineers and product managers who have the fortune of working on Hadoop-related Apache projects as their day job. Making learning easy and fun. When the learning curve is relatively steep, it�s important that the initial development experience be not only unintimidating and productive, but also fun. We�ll be experimenting with methods � such as portable hackathons, virtual workshops, developer kits/cookbooks, and others things � for doing that. These are just a few of my initial thoughts. Have any other ideas? Would be great to see them in comments. To stay up to date about these initiatives in near real-time, follow me at @kestelyn.</snippet></document><document id="390"><title>CDH3 update 5 is now available</title><url>http://blog.cloudera.com/blog/2012/08/cdh3-update-5-is-now-available/</url><snippet>We are happy to announce the general availability of CDH3 update 5. This update is a�maintenance�release of CDH3 platform and provides a considerable amount of bug-fixes and stability enhancements. Alongside these fixes, we have also included a few new features, most notable of which are the following: Apache Flume 1.2.0 – Provides a durable file channel and many more features over the previous release. Hive AvroSerDe – Replaces the Haivvreo SerDe and provides robust support for Avro data format. WebHDFS – A full read/write REST API to HDFS. For more details on new features, please see New Features in CDH3. Refer to the CDH3 Quick Start Guide or CDH3 Installation Guide for more information on how to install this update release. As always, we are happy to hear your feedback. Please send your comments and suggestions to cdh-users@cloudera.org.</snippet></document><document id="391"><title>Seeking nominations for the 2012 Government Big Data Solutions Award</title><url>http://blog.cloudera.com/blog/2012/08/seeking-nominations-for-the-2012-government-big-data-solutions-award/</url><snippet>This post was contributed by Bob Gourley, editor, CTOvision.com. You are no doubt aware of the interesting situation we face with data today: The amount of data being created is growing faster than humans can analyze, but fast analysis over data can help humanity solve some very tough challenges. This fact is moving the globe towards new “Big Data” solutions. Government use of Big Data is of particular note. The government has special abilities to focus research in areas like Health Sciences, Economics, Law Enforcement, Defense, Geographic Studies, Environmental Studies, Bioinformatics, and Computer Security. Each of those can be well served by Big Data approaches. There are also many areas of direct government service to citizens which can benefit from Big Data solutions. Today’s citizen is being served with government information faster than ever before, thanks to Apache Hadoop-based solutions like those at USAsearch.gov. Shortly put: Big Data innovation in and around governments holds great promise of enhancing our lives. Last year the blog at CTOvision sponsored the first “Government Big Data Solutions Award” to help highlight some of the best solutions to Big Data challenges in the government space. We are doing the same this year, with the goal of highlighting government solutions to help share lessons learned and continue to accelerate successful innovation. We are seeking nominations which highlight best practices in solving Big Data challenges, especially those built around the Apache Hadoop framework of solutions. Please let us know of government solutions you believe are worthy of recognition with this award. Candidates from industry and government are being sought. Selection of the top five nominees and an overall winner will be made by a panel of judges made up of industry and government Big Data experts which include Doug Cutting of Cloudera (creator of Hadoop), Alan Wade (former government CIO), Chris Dorobek (leading government journalist and publisher of the DorobekInsider.com, and Ed Granstedt (industry leading federal sector technologist). Award winners will be written up in our online technology reviews at CTOvision.com and published in the Government Big Data Newsletter. A presentation of awards is being planned for Fall 2012. To nominate a solution see: http://ctolabs.com/big-data-award</snippet></document><document id="392"><title>HttpFS for CDH3 – The Apache Hadoop FileSystem over HTTP</title><url>http://blog.cloudera.com/blog/2012/08/httpfs-for-cdh3-the-hadoop-filesystem-over-http/</url><snippet>HttpFS is an HTTP gateway/proxy for Apache Hadoop FileSystem implementations. HttpFS comes with CDH4 and replaces HdfsProxy (which only provided read access). Its REST API is compatible with WebHDFS (which is included in CDH4 and the upcoming CDH3u5). HttpFs is a proxy so, unlike WebHDFS, it does not require clients be able to access every machine in the cluster. This allows clients to to access a cluster that is behind a firewall via the WebHDFS REST API. HttpFS also allows clients to access CDH3u4 clusters via the WebHDFS REST API. Given the constant interest we’ve seen by CDH3 users in Hoop, we have backported Apache Hadoop HttpFS to work with CDH3. Providing a bit of background, Hoop has been contributed to Apache Hadoop and is now named HttpFS. Hoop was a preview technology, and when it was contributed to Apache Hadoop it underwent significant REST API changes (http://archive.cloudera.com/cdh4/cdh/4/hadoop/hadoop-yarn/hadoop-yarn-site/WebHDFS.html). HttpFS is part of Apache Hadoop 2.x and of CDH4. CDH3 users can now use HttpFS instead of Hoop for HDFS access over HTTP. Using HttpFS facilitates a later upgrade to CDH4 as the HttpFS API in CDH3 is compatible with HttpFS in CDH4 and does not require application code changes when upgrading to CDH4. HttpFS is not distributed with CDH3. The source for the HttpFS backport for CDH3 is available at https://github.com/cloudera/httpfs/. There is one branch for CDH3u4 and another branch for CDH3u5. Limitations: The HttpFS CDH3 backport does not implement the delegation token operations. Delegation tokens are used by tools like DistCP when reading files from a secure cluster.</snippet></document><document id="393"><title>Column Statistics in Apache Hive</title><url>http://blog.cloudera.com/blog/2012/08/column-statistics-in-hive/</url><snippet>Over the last couple of months the Hive team at Cloudera has been working hard to bring a bunch of exciting new features to Apache Hive. In this blog post, I�m going to talk about one such feature – Column Statistics in Hive – and how Hive�s query processing engine can benefit from it. The feature is currently a work in progress but we expect it to be available for review imminently. Motivation While there are many possible execution plans for a query, some plans are more optimal than others. The query optimizer is responsible for generating an efficient execution plan for a given SQL query from the space of all possible plans. Currently, Hive�s query optimizer uses rules of thumbs to generate an efficient execution plan for a query. While such rules of thumb optimizations transform the query plan into a more efficient one, the resulting plan is not always the most efficient execution plan. In contrast, the query optimizer in a traditional RDBMS is cost based; it uses the statistical properties of the input column values to estimate the cost alternative query plans and chooses the plan with the lowest cost. The cost model for query plans assigns an estimated execution cost to the plans. The cost model is based on the CPU and I/O costs of query execution for every operator in the query plan. As an example consider a query that represents a join among {A, B, C} with the predicate {A.x == B.x == C.x}. Assume table A has a total of 500 records, table B �has a total of 6000 records, table C has a total of 1000 records. In the absence of cost based query optimization, the system picks the join order specified by the user. In our example, let us further assume that the result of joining A and B yields 2000 records and the result of joining A and C yields 50 records.Hence the cost of performing the join between A, B and C, without join reordering, is the cost of joining A and B + cost of joining the output of A Join B with C. �In our example this would result in a cost of (500 * 6000) + (2000 * 1000). �On the other hand, a cost based optimizer (CBO) in a RDBMS would pick the more optimal alternate order [(A Join C) Join B] thus resulting in a cost of (500 * 1000) + (50 * 6000). However, in order to pick the more optimal join order the CBO needs cardinality estimates on the join column. Today, Hive supports statistics at the table and partition level – count of files, raw data size, count of rows etc, but doesn�t support statistics on column values. These table and partition level statistics are insufficient for the purpose of building a CBO because they don�t provide any information about the individual column values. Hence obtaining the statistical summary of the column values is the first step towards building a CBO for Hive. In addition to join reordering, Hive�s query optimizer will be able to take advantage of column statistics to decide whether to perform a map side aggregation as well as estimate the cardinality of operators in the execution plan better. New Statistics The following statistics on columns – count of null values, count of true/false values, maximum value, minimum value, estimate of number of distinct values, average column length, maximum column length, and height balanced histograms – as they apply to the datatype of the underlying column have been added. Computing Statistics Initially, we plan to support statistics gathering through an explicit ANALYZE command. We have extended the ANALYZE command to compute statistics on columns both within a table and a partition. The basic requirement to compute statistics is to stream the entire table data through mappers. Since some operations such as INSERT, CTAS already stream the table data through mappers, we could piggyback on such operations and compute statistics as part of these operations. We believe that by doing so, Hive may be able to compute statistics more efficiently than most RDBMSs which don�t combine data loading and computing statistics into a single operation. Statistical summary of the column data can be logically viewed as an aggregation of the column values rolled up by either the table or the partition. Hence we have implemented statistics computation using the generic user defined aggregate function (UDAF) framework in Hive. UDAFs in Hive are executed in two stages; As the records are streamed through the mappers, a partial aggregate that is maintained in each mapper is updated. The reducers then combine the result of the partial aggregation to produce a final aggregate output. Even though the stats task is run as a batch job , we want it to be executed �as efficiently as possible. We expect to compute statistics on terabytes of data at a given time; hence we expect to scan billions of records. Therefore it is very important that the algorithms we use for computing statistics use constant memory. We have invested significant time researching algorithms for this task and have selected ones which we think provide a good tradeoff between accuracy and efficiency, and at the same time provide knobs for tuning. We have also attempted to structure the code in a pluggable fashion so that new algorithms can be easily substituted in the future. Storing Statistics The statistics that are gathered through the ANALYZE command are persisted to avoid computing them every time the optimizer has to estimate the cost of a query plan.These newly added statistics are persisted in new tables that have been added to the metastore. We have also extended the metastore Thrift API to query the metastore for column statistics at the granularity of a partition as well as a table. Running the ANALYZE command on a column will overwrite any statistics that may already be present for the column. Algorithms for Computing Statistics While computing some statistics such as average column length, minimum value, and maximum value is straightforward, computing some others statistics is not. For instance, computing the number of distinct column values accurately and efficiently is an interesting problem. While a deterministic counting algorithm will yield the exact number of distinct values in a column such an approach is not feasible for all data types. For instance, counting the number of distinct values for a boolean valued column is feasible, whereas the amount of memory required to count the number of distinct string values could be prohibitive especially when scanning terabytes of data. Instead a number of randomized counting algorithms over data streams have been proposed to estimate the number of distinct values efficiently while maintaining practical bounds on accuracy. We have implemented one such algorithm [1] proposed by Flajolet and Martin, hereafter referred to as FM algorithm. The main idea behind the FM algorithm is to let each item in the data set select at random a bit in a bit vector (V) of length ( L) and set it to 1, with a geometric distribution. The selection of the bit is done using a hash function h that maps each value in the column uniformly at random to an integer in [0..2^L-1], and determining the largest b such that the b rightmost bits in h(v) are all 0. The intuition behind the algorithm is as follows. Using a random hash function ensures that all elements with the same value will set the same bit. For each distinct value, bit (b) in vector (V) is set with probability 1/2^(b+1). �Hence we expect bit V[b] to be set if there are at least 2^(b+1) distinct values. �The algorithm finds a bit Z such that Z-1 is set whereas Z is not set. Hence there are likely greater than 2^(Z-1) but fewer than 2^Z distinct values. While a single bit vector yields estimates that may be off by a factor of 2-4, 64 bit vectors and 64 different hash functions yield estimates that are within 10% of the true value. In our implementation we have used 64 bit vectors each of length 32 bits. Hence by using 256 bytes of memory – 32 * 64 bits – we are able to obtain estimates that are within 10% of the true value. Additionally the FM algorithm is simple to implement and nicely distributes to a cluster of nodes lending itself well to the mapreduce framework. Efficient Data Structures As a test run, we attempted to compute statistics on a table containing 80+ columns and around 500 GB of data on a 10 node test cluster. We modeled the table after the fact table that is typical of a star schema in a data warehousing environment. After watching the statistics computation statement run for a day we realized that it would take another three days to compute. In search of the bottleneck we analyzed the code using Yourkit, an awesome Java profiler, and quickly determined that there were problems with Java�s built-in BitSet library. We swapped it out with Javolution�s FastBitSet library quickly and noticed a 10x improvement �in performance. At this time, we are looking to replace some of the other built-in Java data structures with Javoloution�s equivalent and more efficient data structures. Wrapping Up In this blog post, we presented the new statistics we have added to Hive, along with the details of the algorithms and data structures we have used to compute distinct values count. We also saw how Hive�s query processing engine can take advantage of these new statistics to process queries more efficiently. The theme of a lot of the current work we are doing on Hive here at Cloudera is to make Hive adopt more the good characteristics of traditional RDBMSs ( i.e., ease of use, efficient utilization of hardware resources via a CBO, security, data management, JDBC/ODBC connectivity etc, while avoiding the pitfalls by building on top of a distributed computation framework that provides fault-tolerance, the ability to scale out linearly, and the opportunity to work with both structured and unstructured data.) As I said before, we are working on a bunch of other Hive features as well, so watch out for a post from my colleague Carl Steinbach on other new features, particularly HiveServer2. [1] Flajolet, P., Martin, N.G. Probabilistic Counting Algorithms for Data Base Applications</snippet></document><document id="394"><title>Apache ZooKeeper 3.3.6 has been released</title><url>http://blog.cloudera.com/blog/2012/08/apache-zookeeper-3-3-6-has-been-released/</url><snippet>Apache ZooKeeper�release�3.3.6 is now available. This is a bug fix release covering�16�issues, two of which were considered blockers.�Some of the more serious issues include: ZOOKEEPER-1489 Data loss after truncate on transaction log ZOOKEEPER-1466 QuorumCnxManager.shutdown missing synchronization ZOOKEEPER-1395 node-watcher double-free redux ZOOKEEPER-1521 LearnerHandler initLimit/syncLimit problems specifying follower socket timeout limits ZOOKEEPER-1431 zkpython: async calls leak memory ZOOKEEPER-1318 In Python binding, get_children (and get and exists, and probably others) with expired session doesn’t raise exception properly Stability, Compatibility and Testing 3.3.6 is a stable release that’s fully backward compatible with 3.3.5. Only bug fixes relative to 3.3.5 have been applied. Getting Involved The Apache ZooKeeper project is working on a number of new features. Our�How To Contribute�page is a great place to start if you’re interested in getting involved as a developer. You can also�follow me on twitter. Acknowledgements A special thanks to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc).</snippet></document><document id="395"><title>Processing Rat Brain Neuronal Signals Using an Apache Hadoop Computing Cluster – Part III</title><url>http://blog.cloudera.com/blog/2012/08/processing-rat-brain-neuronal-signals-using-a-hadoop-computing-cluster-part-iii/</url><snippet>Up to this point, we�ve described our reasons for using Hadoop and Hive on our neural recordings (Part I), the reasons why the analyses of these recordings are interesting from a scientific perspective, and detailed descriptions of our implementation of these analyses using Apache Hadoop and Apache Hive (Part II). The last part of this story cuts straight to the results and then discusses important lessons we learned along the way and future goals for improving the analysis framework we�ve built so far. Results Here are two plots of the output data from our benchmark run.� Both plots show the same data, one in three dimensions and the other in a two-dimensional density format. � These plots show two distinct regions.� The high gamma band (in yellow) shows peaks in the middle phase bins.� The low gamma band (in dark brown) shows peaks in the lower and upper phases, which wrap around and form a single region. Performance Since the goal of this project was to see if the Hadoop ecosystem running on a computing cluster could provide performance and usability improvements to the neuronal signal processing workflow, we did a scale test.� This first graph shows the performance of 1, 2, 3, and 4 simultaneous rat runs.� It shows that we do in fact see benefit for up to three simultaneous runs, but the fourth run takes a jump in execution time.� This comes from our cluster resource limitation of 46 computing slots in the convolution step.� Each rat run requires 15 slots, so in order to process the fourth run, we have to wait until 15 slots free up. � The next graph shows the breakdown of each processing step.� The convolution and averaging steps dominated the total execution time.� For the convolution step, we saw only a slight increase in time for runs 2 and 3, compared with the time for run 1, because we had available computing slots.� The fourth run, however, needed to wait for a previous run to complete, so it took about as long to execute as twice the first run time.� The averaging step, since it consumed all processing slots for each run, showed a linearly increasing amount of processing time. � If we had a cluster with additional computing slots, we would be able to handle more rat runs in parallel, because of the convolution step characteristics, and be able to process the averaging step more quickly.� With our current cluster, a rule of thumb was we could process approximately three 15-channel rat runs per hour, so 100 rat runs would take about 2 weeks to process. Performance Comparison with Matlab Previously, the analysis described in this paper was performed with Matlab on a single workstation.� An apples-apples comparison with our Hadoop cluster implementation is difficult, because the workstation memory limitations required some compromise changes to the workflow, while the Hadoop implementation implements the ideal workflow. The Hadoop approach does the processing in this order: Squaring Averaging the 12 channels Computing the mean and standard deviation of the average channel values Subsetting the time intervals Z-scoring This took about 4 hours to process using 46 computing slots. The Matlab approach does the processing in this order: Squaring Z-scoring Subsetting the time intervals Averaging the 12 channels This took about 10 hours to process, using a small number of process cores.� Therefore, the Matlab approach was much more resource efficient, but the Hadoop approach provided better, and cheaper, scale up capability. For the convolution step, the Matlab approach could read in a channel signal from disk to memory, perform a single kernel convolution, and output the result back to disk in about 11 seconds.� The comparable number with the Hadoop implementation was about 37 seconds.� However, the Hadoop cluster could process each channel in parallel, one channel per computing slot.� Matlab could also be configured to leverage parallel processing, but at a higher cost.� Here is a more specific breakdown of the time spent in the convolution processing: Load Kernel: .33 sec Load Data: 15.82 sec Signal FFT: 1.08 sec Kernel FFT: .60 sec Product: .20 sec Inverse FFT: 1.07 sec Output Data: 38.00 sec The last four steps were repeated for each of the 196 wavelet kernels.� It was clear that while at first we expected this task would be CPU bound, the FFT processing was so efficient that we spend most of the time in data output.� If we had more disk arms on each node in our cluster, we would expect that this time would decrease significantly. Qualitative Benefits An important benefit of the Hadoop cluster approach, when compared with doing analysis on a single Matlab workstation, is that we now have the capacity to fully process the data, instead of being constrained by workstation memory, and can keep all of the intermediate data online for ad hoc analysis.� This analysis can be done with Hive. Future Work As usual, interesting work spawns ideas for other interesting work.� Here is a list of potential future research: Look at doing the channel averaging before the convolution step, which would further improve storage demands, allow for more parallel convolution rat runs and replacement of the large averaging task with a much smaller one Provide better support for incremental rat run additions to the processing data Provide better support for selecting specific subsets of data channels, which correspond to specific brain regions See if increasing the number of disk arms per node will improve I/O performance Experiment with overcommitting MapReduce slots beyond the available physical processing cores Process all existing rat run datasets For more information on big data activities at the University of St. Thomas Graduate Programs in Software, see our Center of Excellence for Big Data (CoE4BD) webpage at http://www.stthomas.edu/coe4bd, and on Twitter @coe4bd.</snippet></document><document id="396"><title>Processing Rat Brain Neuronal Signals Using an Apache Hadoop Computing Cluster – Part II</title><url>http://blog.cloudera.com/blog/2012/08/processing-rat-brain-neuronal-signals-using-a-hadoop-computing-cluster-part-ii/</url><snippet>Background As mentioned in Part I, although Apache Hadoop and other Big Data technologies are typically applied to I/O intensive workloads, where parallel data channels dramatically increase I/O throughput, there is growing interest in applying these technologies to CPU intensive workloads. �In this work, we used Hadoop and Hive to digitally signal process individual neuron voltage signals captured from electrodes embedded in the rat brain. Previously, this processing was performed on a single Matlab workstation, a workload that was both CPU intensive and data intensive, especially for intermediate output data. �With Hadoop and Apache Hive, we were not only able to apply parallelism to the various processing steps, but had the additional benefit of having all the data online for additional ad hoc analysis. �Here, we describe the technical details of our implementation, including the biological relevance of the neural signals and analysis parameters. In Part III, we will then describe the tradeoffs between the Matlab and Hadoop/Hive approach, performance results, and several issues identified with using Hadoop/Hive in this type of application. For this work, we used a university Hadoop computing cluster.� Note that it is blade-based, and is not an ideal configuration for Hadoop because of the limited number (2) of drive bays per node.� It has these specifications: 24 Nodes (1 master, 23 slaves), running Ubuntu Server Nodes are Sun Fire X2200M2 with 2x AMD Opteron 2214 DualCore 2.2GHz processors Master: 18GB RAM, 2x 1 TB drives (RAID 1 mirrored) NameNode, SecondaryNameNode, JobTracker, MySQL for Hive metastore, user local home directories Slaves: 12GB RAM, 250 GB + 1 TB drives DataNode, TaskTracker, 2 CPU core slots for mappers and reducers Slots available for map/reduce tasks: 46 Brain Waves For this problem, the brain wave frequency band known as theta, which in the rat brain runs from 7 to 10 Hz, acts much like a master clock signal that encodes other higher frequency bands at different phases, such as the peak, trough, leading, or trailing edges of the oscillation. Specifically, the low gamma band, which runs from 35 to 60 Hz, signifies that input signals are coming into the hippocampus from within the hippocampus itself, from a region known as CA3. This high frequency wave appears as a component on the leading edge and peak of the lower frequency theta wave. The high gamma band, which runs from 85 to 165 Hz, signifies signals coming into the hippocampus from other brain structures outside that region, and reflect more cognitive processing. It appears as a component on the trailing edge of the theta wave. Input Data The neuroscience researchers were interested in analyzing more than a hundred data sets, which we call rat runs, or sessions.� Each session was recorded from a 50-electrode array consisting of two reference electrodes and twelve tetrodes, or groups of 4 electrodes (tetrode is the term used to refer to a specialized electrode capable of recording four channels of data within a small region of the brain, think of a 4 channel surround-sound microphone detecting differences in sound depending on whether the source is in front, behind, left, or right of the microphone). Each set, therefore, consisted of parallel recordings of two reference channels and 48 channels from the 12 tetrodes implanted in the rat brain, which, after some preprocessing, corresponds to signals from individual neurons.� The two reference channels helped reduce both extra and intra rat brain noise.� Up to 16 channels could be selected from these 50 channels for continuous monitoring of the local electrical fields that the brain generates (LFP, Local Field Potentials; a.k.a EEG). Each rat run was about an hour long, and each channel was sampled at a rate of 2 KHz.� The researchers may be specifically interested in a subset of the 16 continuously sampled channels depending on the location they were recorded from, resulting in a set of up to 16 files, one for each continuously sampled channel, that each consisted of about 6 million records, each in turn consisting of a timestamp and a voltage value, in a coma-separated text format. The single channel of data to be analyzed consists of: Voltage values sampled with 12 bit precision Sampling rate: 2,000 samples/sec Time-stamp values: 64 bit integer values at 1/10,000 sec precision Typical recording session length: 1 to 3 hours Processing Overview The following figure shows the processing steps involved in the analysis, as well as some of the data sizes involved. Convolution The neuroscience researchers were interested in both the frequency content of the neuronal signals and the changes in the signals during different behaviors within the one-hour time period.� While traditional Fourier analysis can separate the signals into their weighted frequency components, wavelet analysis is better suited to highlighting both frequency and time-dependent variation.� Specifically, we used a set of kernels from the Morlet wavelet family.� These resemble a sign wave modulated by a Gaussian envelope. The researchers were interested in a specific range of frequencies, so we used kernels that run from 5 Hz to 200 Hz, in one Hertz increments.� Here is an example of a 10 Hz Morlet kernel. For each of the 15 signals, we needed to convolve each of the 196 kernels (5-200 Hz) with the signal.� At first, we implemented a brute-force approach to calculating these convolutions by successive dot product and shift operations.� In the end, we implemented a more efficient Fast Fourier Transform (FFT)-based solution by first taking the discrete Fourier transforms of a given signal, then taking the discrete Fourier transform of a kernel, multiplying together the two resulting sequences of complex numbers, and final taking the inverse discrete Fourier transform of that result.� We did this operation for each kernel per channel, on each channel of interest.� The output from the convolution was then squared, to get the power value of the signal. This code was written as a Java MapReduce job, leveraging an external FFT library called JTransforms https://sites.google.com/site/piotrwendykier/software/jtransforms. Since Hadoop can parallelize operations, we had to decide on the best boundary for the parallelism.� We decided that each Hadoop task would process all kernels for a single channel, so that up to 16 parallel tasks could process all of the channels for a single rat run concurrently, depending on the number of channels to be analyzed for that rat run.� Since our cluster had 46 computing slots, this allowed approximately three entire rat runs to be processed in parallel. For testing, we selected a session with 15 unique channels to validate and benchmark each processing step. We wanted to leverage Hive�s partitioning support, which allowed the query optimizer to be aware of the subdirectory structure of the MapReduce output and only read in the subdirectories needed to resolve a specific query.� Also, to avoid copying data from HDFS into a Hive table, we used Hive�s external table support to treat the HDFS output as a hive table.� This is our convolution output directory structure: Channel Averaging The convolution step produced a huge amount of data, effectively increasing the data to be stored by factor of almost 200.� We addressed this data volume issue in two ways.� First, we had been using a text output format for the convolution step, which is the norm in Hadoop processing because of its ease of processing.� We moved to a binary format, called a SequenceFile, which caused our bytes/record value to go down from 30.5 to 18.2.� In addition, we averaged all data channels taken from each of the tetrodes from the same brain region together.� It turns out that this averaging was very compute intensive, and will be the focus of future research. This code was done with a Hive SQL task that leveraged all parallel computing slots, with a custom Hive Serde (serializer-deserializer) that allowed for sequence file input. Z-Scoring The power spectrum that results from the convolution operation was weighted heavily on low frequencies, and lightly on high frequencies (e.g. following a 1/f spectrum, where f is frequency).� In order to normalize the data and make changes in power comparable across frequency bands, we do a z-scoring step.� To do this, we calculated the mean and standard deviations of the output across time, so that we could later subtract the mean, and divide by the standard deviation.� This code was done with a Hive SQL task that leveraged all parallel computing slots. Subsetting Since we are interested in correlating specific rat movements with brain signals, we subset the data by time intervals that correspond to the rat�s physical state, as recorded by the rat observer. This code was done with a Hive SQL task that leveraged all parallel computing slots. Phasing Since we are ultimately interested in how high and low gamma waves appear in relation to the leading and trailing edges of the theta wave, we need to know the phase of the theta waves relative to the timestamps.� In a pre-processing step, we used Matlab to identify the theta wave minima and maxima via a differentiation technique, which produces a file that shows the theta phase corresponding to each time value.� This file is used as input to the subsequent phase bin assignment step. Binning We next assign each signal value to a phase bin.� A phase bin is a segment of the uniformly-divided theta wave phase data.� For our current experiment, we divided the phase (0 to 359 degrees) into 75 bins.� We then joined the average channel convolution output with this phase information, by timestamp value.� This produces a result that is independent of time, and consists of three values: a kernel frequency, a phase bin, and a z-scored, average of the channel squared convolution output for that kernel frequency. In Part III, we will describe the results of these analyses, with an emphasis on the technical findings related to implementing the workflow just described.</snippet></document><document id="397"><title>Processing Rat Brain Neuronal Signals Using an Apache Hadoop Computing Cluster – Part I</title><url>http://blog.cloudera.com/blog/2012/07/processing-rat-brain-neuronal-signals-using-a-hadoop-computing-cluster-part-i/</url><snippet>Introduction In this three-part series of posts, we will share our experiences tackling a scientific computing challenge that may serve as a useful practical example for those readers considering Apache Hadoop and Apache Hive as an option to meet their growing technical and scientific computing needs. This first part describes some of the background behind our application and the advantages of Hadoop that make it an attractive framework in which to implement our solution. Part II dives into the technical details of the data we aimed to analyze and of our solution. Finally, we wrap up this series in Part III with a description of some of our main results, and most importantly perhaps, a list of things we learned along the way, as well as future possibilities for improvements. Background About a year ago, after hearing increasing buzz about big data in general, and Hadoop in particular, I (Brad Rubin) saw an opportunity to learn more at our Twin Cities (Minnesota) Java User Group.� Brock Noland, the local Cloudera representative, gave an introductory talk.� I was really intrigued by the thought of leveraging commodity computing to tackle large-scale data processing.� I teach several courses at the University of St. Thomas Graduate Programs in Software, including one in information retrieval.� While I had taught the abstract principles behind the scale and performance solutions for indexing web-sized document collections, I saw an opportunity to integrate a real-world solution into the course. Our department had an idle computing cluster.� While it wasn�t an ideal Hadoop platform, because of the limited disk arms available in the blade configuration, our computing support staff and a grad student installed Ubuntu and Hadoop.� We immediately had trouble with frequent crashes, and Brock came by to diagnose our problem as a hardware memory configuration issue.� We got the cluster running just in time for use by a few student projects in my information retrieval class.� We decided to go with Cloudera�s Distribution Including Apache Hadoop (CDH) because initially learning about the technologies and bringing up a new cluster is complex enough, and we wanted the benefit of having a software collection that was already configured to work together, including patches.� The mailing lists were also an important benefit, allowing search for problem solutions posted by others, and quick responses to new questions by Cloudera employees and other users. In December, I had lunch with a faculty member from our University�s biology department, Jadin Jackson.� He is a clinical faculty member and a neuroscientist who recently joined our University after finishing several post-doc research positions.� Jadin described his work analyzing rat brain EEG waveforms with a Matlab workstation to try to understand the neural communication between different brain regions while the rats run a maze.� The task is very compute and data intensive.� I described my recent interest in Hadoop.� We soon wondered if Hadoop might be a good solution for Jadin�s digital signal processing application.� While most existing Hadoop applications are I/O intensive, we thought it would be interesting to explore this CPU intensive task with this computing architecture.� Since any real-world application would provide a great vehicle for improving our knowledge of the Hadoop ecosystem, we agreed to purse this. Jadin has a background in electrical engineering and physics, as well as neuroscience, and has always been interested in computing.� He viewed this project not only as a way to tackle his backlog of rat brain signal data to further his research, but also as a way to explore recent developments in cluster computing.� Jadin had already developed the computational, statistical, and visualization techniques for this analysis, so he was our domain expert, guiding development, providing test data, and addressing questions. One of the students from my information retrieval class, Ashish Singh, was interested in an independent study opportunity to improve his Hadoop knowledge, so he joined our team.� The three of us then spent the 2012 spring semester using Hadoop, with Hive added in as well, to analyze rat brain neuronal signals.� Our university has a Faculty Partnership Program that encourages interdisciplinary collaboration, so Jadin taught me some neuroscience and I taught Jadin about the Hadoop ecosystem.� When I saw that Brock was teaching a local Cloudera Hadoop development class, I signed up and also became certified. Problem Statement Prior to starting this work, Jadin had data previously gathered by himself and from neuroscience researchers who are interested in the role of the brain region called the hippocampus. In both rats and humans, this region is responsible for both spatial processing and memory storage and retrieval.� For example, as a rat runs a maze, neurons in the hippocampus, each representing a point in space, fire in sequence.� When the rat revisits a path, and pauses to make decisions about how to proceed, those same neurons fire in similar sequences as the rat considers the previous consequences of taking one path versus another.� In addition to this binary-like firing of neurons, brain waves, produced by ensembles of neurons, are present in different frequency bands.� These act somewhat like clock signals, and the phase relationships of these signals correlate to specific brain signal pathways that provide input to this sub-region of the hippocampus. The goal of the underlying neuroscience research is to correlate the physical state of the rat with specific characteristics of the signals coming from the neural circuitry in the hippocampus.� Those signal differences reflect the origin of signals to the hippocampus.� Signals that arise within the hippocampus indicate actions based on memory input, such as reencountering previously encountered situations. Signals that arise outside the hippocampus correspond to other cognitive processing.� In this work, we digitally signal process the individual neuronal signal output and turn it into spectral information related to the brain region of origin for the signal input. Hadoop Overview While the initial impetus for this project was to learn more about the Hadoop ecosystem, there are several traditional compelling technical advantages of the recent big data technologies in general, and Hadoop in particular. Improved throughput for processing large datasets, and having all of the data online are two key advantages. Hadoop is a parallel computing architecture designed to take advantage of the increased I/O bandwidth (the speed at which data can be read or written) that results from having data spread across many disks. As the processor speeds for individual microprocessor cores have plateaued in recent years, the number of cores per processor has increased. However, this increase in core number has not been accompanied by dramatic I/O speed increases for reading from and writing to disk. Additionally, disk access latencies (the time it takes to find a location to read from or write to) have remained relatively constant for nearly a decade or more. This means that for a single computer, increasing processing speeds or the number of processor cores is of only limited value, since getting the data to the processor, i.e. I/O bandwidth, is a major bottleneck affecting performance for many types of applications.� Since the Hadoop architecture spreads the data across the many hard disks in the machines (or nodes) within a cluster, with the help of a storage system called the Hadoop Distributed File System (HDFS), the effective I/O bandwidth is multiplied by the number of nodes (and disks within those nodes) available for use. The Hadoop MapReduce architecture takes the processing to the data, rather than relying on moving data across or between nodes within the cluster.� This means that Hadoop is especially adept at working with very large data sets that are spread across the large storage capacity comprised of all of the hard disks in the cluster�s nodes.� The MapReduce paradigm starts with the map step, where the same operation is performed on each node where the data of interest resides. The map task on each node sends its results to a reduce task in the reduce step. The reduce tasks compile and collate the results, performing the aggregation operations needed to output the end product of the MapReduce job. Importance of Hive Since MapReduce jobs within Hadoop often require writing specialized Java functions for each type of job, Hive was developed to provide a standard interface for database programmers that closely resembles SQL, the standard language for data access, to handle large amounts of data. Hive brings a data warehousing system to Hadoop and for data stored in HDFS. Hive takes queries (commands to search, combine, or reorganize data) and executes them as MapReduce jobs within Hadoop, thereby simplifying the development of complex analyses that use steps commonly used in standard database queries. We quickly found that the standard advice of �first, try coding the function in Hive, and if you can�t then do so in Java MapReduce� held true.� We were able to use Hive for most of the processing in this project, with high productivity, and leverage our existing SQL skills. In Part II, we will discuss the types of neural signals in this analysis, and technical details of its implementation in Hadoop and Hive.</snippet></document><document id="398"><title>Apache HBase Replication Overview</title><url>http://blog.cloudera.com/blog/2012/07/hbase-replication-overview-2/</url><snippet>Apache HBase Replication is a way of copying data from one HBase cluster to a different and possibly distant HBase cluster. It works on the principle that the transactions from the originating cluster are pushed to another cluster. In HBase jargon, the cluster doing the push is called the master, and the one receiving the transactions is called the slave. This push of transactions is done asynchronously,� and these transactions are batched in a configurable size (default is 64MB).� Asynchronous mode incurs minimal overhead on the master, and shipping edits in a batch increases the overall throughput. This blogpost discusses the possible use cases, underlying architecture and modes of HBase replication as supported in CDH4 (which is based on 0.92). We will discuss Replication configuration, bootstrapping, and fault tolerance in a follow up blogpost. Use cases HBase replication supports replicating data across datacenters. This can be used for disaster recovery scenarios, where we can have the slave cluster serve real time traffic in case the master site is down. Since HBase replication is not intended for automatic failover, the act of switching from the master to the slave cluster in order to start serving traffic is done by the user. Afterwards, once the master cluster is up again, one can do a CopyTable job to copy the deltas to the master cluster (by providing the start/stop timestamps) as described in the CopyTable blogpost. Another replication use case is when a user wants to run load intensive MapReduce jobs on their HBase cluster; one can do so on the slave cluster while bearing a slight performance decrease on the master cluster. Architecture The underlying principle of HBase replication is to replay all the transactions from the master to the slave. This is done by replaying the WALEdits (Write Ahead Log entries) in the WALs (Write Ahead Log) from the master cluster, as described in the next section. These WALEdits are sent to the slave cluster region servers, after filtering (whether a specific edit is scoped for replication or not) and shipping in a customized batch size (default is 64MB). In case the WAL Reader reaches the end of the current WAL, it will ship whatever WALEdits have been read till then. Since this is an asynchronous mode of replication, the slave cluster may lag behind from the master in a write heavy application by the order of minutes. WAL/WALEdits/Memstore In HBase, all the mutation operations (Puts/Deletes) are written to a memstore which belongs to a specific region and also appended to a write ahead log file (WAL)� in the form of a WALEdit. A WALEdit is an object which represents one transaction, and can have more than one mutation operation. Since HBase supports single row-level transaction, one WALEdit can have entries for only one row. The WALs are repeatedly rolled after a configured time period� (default is 60 minutes) such that at any given time, there is only one active WAL per regionserver. IncrementColumnValue, a CAS (check and substitute) operation, is also converted into a Put when written to the WAL. A memstore is an in-memory, sorted map containing keyvalues of the composing region; there is one memstore per each column family per region.� The memstore is flushed to the disk as an HFile once it reaches the configured size (default is 64MB). Writing to WAL is optional, but it is required to avoid data loss because in case a regionserver crashes, HBase may lose all the memstores hosted on that region server. In case of regionserver failure, its WALs are replayed by a Log splitting process to restore the data stored in the WALs. For Replication to work, write to WALs must be enabled. ClusterId Every HBase cluster has a clusterID, a UUID type auto generated by HBase. It is kept in underlying filesystem (usually HDFS) so that it doesn�t change between restarts. This is stored inside the /hbase/hbaseid znode. This id is used to acheive master-master/acyclic replication. A WAL contains entries for a number of regions which are hosted on the regionserver. The replication code reads all the keyvalues and filters out the keyvalues which are scoped for replication. It does this by looking at the column family attribute of the keyvalue, and matching it to the WALEdit’s column family map data structure. In the case that a specific keyvalue is scoped for replication, it edits the clusterId parameter of the keyvalue to the HBase cluster Id. ReplicationSource The ReplicationSource is a java Thread object in the regionserver process and is responsible for replicating WAL entries to a specific slave cluster. It has a priority queue which holds the log files that are to be replicated. As soon as a log is processed, it is removed from the queue. The priority queue uses a comparator that compares the log files based on their creation timestamp, (which is appended to the log file name); so, logs are processed in the same order as their creation time (older logs are processed first). If there is only one log file in the priority queue, it will not be deleted as it represents the current WAL. Role of Zookeeper Zookeeper plays a key role in HBase Replication, where it manages/coordinates almost all the major replication activity, such as registering a slave cluster, starting/stopping replication, enqueuing new WALs, handling regionserver failover, etc. It is advisable to have a healthy Zookeeper quorum (at least 3 or more nodes) so as to have it up and running all the time. Zookeeper should be run independently (and not by HBase). The following figure shows a sample of replication related znodes structure in the master cluster (the text after colon is the data of the znode): /hbase/hbaseid: b53f7ec6-ed8a-4227-b088-fd6552bd6a68
�.
/hbase/rs/foo1.bar.com,40020,1339435481742:
/hbase/rs/foo2.bar.com,40020,1339435481973:
/hbase/rs/foo3.bar.com,40020,1339435486713:
/hbase/replication:
/hbase/replication/state: true
/hbase/replication/peers:
/hbase/replication/peers/1: zk.quorum.slave:281:/hbase
/hbase/replication/rs:
/hbase/replication/rs/foo1.bar.com.com,40020,1339435084846:
/hbase/replication/rs/foo1.bar.com,40020,1339435481973/1:
/hbase/replication/rs/foo1.bar.com,40020,1339435481973/1/foo1.bar.com.1339435485769: 1243232
/hbase/replication/rs/foo3.bar.com,40020,1339435481742:
/hbase/replication/rs/foo3.bar.com,40020,1339435481742/1:
/hbase/replication/rs/foo3.bar.com,40020,1339435481742/1/foo3.bar..com.1339435485769: 1243232
/hbase/replication/rs/foo2.bar.com,40020,1339435089550:
/hbase/replication/rs/foo2.bar.com,40020,1339435481742/1:
/hbase/replication/rs/foo2.bar.com,40020,1339435481742/1/foo2.bar..com.13394354343443: 1909033 �� ��� ���� Figure 1. Replication znodes hierarchy As per Figure 1, there are three regionservers in the master cluster, namely foo[1-3].bar.com. There are three znodes related to replication: state: This znode tells whether replication is enabled or not. All fundamental steps (such as whether to enqueue a newly rolled log in a replication queue, read a log file to make WALEdits shipments, etc), check this boolean value before processing. This is set by the setting �hbase.replication� property to true in the hbase-conf.xml. Another way of altering its value is to use the �start/stop replication� command in hbase shell. This will be discussed in the second blogpost. peers: This znode has the connected peers/slaves as children. In the figure, there is one slave with the peerId = 1, and its value is the connection string (Zookeeper_quorum_of_slave:Zookeeper_client_port:root_hbase_znode), where the Zookeeper_quorum_of_slave is a comma separated list of zookeeper servers. The peerId znode name is the same as the one given while adding a peer. rs: This znode contains a list of active regionservers in the master cluster. Each regionserver znode has a list of WALs that are to be replicated, and the value of these log znodes is either null (in case log is not opened for replication yet), or the byte offset to the point where the log has been read. The byte offset value for a WAL znode indicates the byte offset of the corresponding WAL file up to which it has been read and replicated. As there can be more than one slave cluster, and replication progress can vary across them (one may be down for example), all the WALs are self contained in a peerId znode under rs. Thus, in the above figure, WALs znodes are under the are /rs//1, where �1� is the peerId. Replication Modes There are three modes for setting up HBase Replication: Master-Slave: In this mode, the replication is done in a single direction, i.e., transactions from one cluster are pushed to other cluster. Note that the slave cluster is just like any other cluster, and can have its own tables, traffic, etc. Master-Master: In this mode, replication is sent across in both the directions, for different or same tables, i.e., both the clusters are acting both as master and slave. In the case that they are replicating the same table, one may think it may lead to a never ending loop, but this is avoided by setting the clusterId of a Mutation (Put/Delete) to the clusterId of the originating cluster. Figure 2 explains this by using two clusters, namely Sun, and Earth. In figure 2, we have two blocks representing the two HBase clusters. They have clusterId 100, and 200 respectively. Each of the clusters has an instance of ReplicationSource, corresponding to the slave cluster it wants to replicate to; it knows cluster #Ids of both the clusters. �� ��� ���� Figure 2. Sun and Earth, two HBase clusters Lets say cluster#Sun receives a fresh and valid mutation M on a table and column family which is scoped for replication in both the clusters. It will have a default clusterID (0L). The replication Source instance ReplicationSrc-E will set its cluster#Id equal to the originator id (100), and ships it to cluster#Earth. When cluster#Earth receives the mutation, it replays it and the mutation is saved in its WAL, as per the normal flow. The cluster#Id of the mutation is kept intact in this log file at cluster#Earth.The Replication source instance at cluster#Earth, (ReplicationSrc-S, will read the mutation and checks its cluster#ID with the slaveCluster# (100, equal to cluster#Sun). Since they are equal, it will skip this WALEdit entry. Cyclic: In this mode, there are more than two HBase clusters that are taking part in replication setup, and one can have various possible combinations of master-slave and master-master set up between any two clusters. The above two covers those cases well; one corner situation is when we have set up a cycle Figure 3. A circular replication set up Figure 3. shows a circular replication setup, where cluster#Sun is replicating to cluster#Earth, cluster#Earth is replicating to cluster#Venus, and cluster#Venus is replicating to cluster#Sun. Let�s say cluster#Sun receives a fresh mutation M and is scoped to replication across all the above clusters. It will be replicated to cluster#Earth as explained above in the master master replication. The replication source instance at cluster#Earth, ReplicationSrc-V, will read the WAL and see the mutation and replicate it to cluster#Venus. The cluster#Id of the mutation is kept intact (as of cluster#Sun), at cluster#Venus. At this cluster, the replication source instance for cluster#Sun, ReplicationSrc-S, will see that the mutation has the same clusterId as its slaveCluster# (as of cluster#Sun), and therefore,� skip this from replicating. Conclusion HBase Replication is powerful functionality which can be used in a disaster recovery scenario. It was added as a preview feature in the 0.90 release, and has been evolving along with HBase in general, with addition of functionalities such as master-master replication, acyclic replication (both added in 0.92), and enabling-disabling replication at peer level (added in 0.94). In the next blogpost, we will discuss various operational features such as Configuration, etc, and other gotchas with HBase Replication.</snippet></document><document id="399"><title>Why we build our platform on HDFS</title><url>http://blog.cloudera.com/blog/2012/07/why-we-build-our-platform-on-hdfs/</url><snippet>It�s not often the case that I have a chance to concur with my colleague E14 over at Hortonworks but his recent blog post gave the perfect opportunity.� I wanted to build on a few of E14�s points and add some of my own. A recent GigaOm article presented 8 alternatives to HDFS.� They actually missed at least 4 others.� For over a year, Parascale marketed itself as an HDFS alternative (until it became an asset sale to Hitachi).� Appistry continues to market its HDFS alternative.� I�m not sure if it�s released yet but it is very evident that Symantec�s Veritas unit is proposing its Clustered Filesystem (CFS) as an alternative to HDFS as well.� HP Ibrix has also supported the HDFS API for some years now. The GigaOm article implies that the presence of twelve other vendors promoting alternatives must speak to some deficiencies in HDFS for what else would motivate so many offerings?� This really draws the incorrect conclusion.�� I would ask this: What can we conclude from the fact that there are: Twelve different filesystems promoting themselves as HDFS alternatives, most of the twelve are 6-14 years older than HDFS, yet HDFS today stores overwhelmingly more enterprise data – several hundreds of petabytes industry-wide – than any alternative, and HDFS has the broadest base of large vendor support (Cisco, Dell, HP, IBM, NetApp, Oracle, SAP, SGI, SuperMicro)? We (Cloudera) conclude that HDFS is in the process of overrunning these legacy filesystems as the industry standard for data management at scale. In fact we have seen this story before.� If we go back 20 years we can recall a similar situation.� In that �market: There were more than a dozen alternatives.� They went by names like AIX, HP-UX, Solaris, Sequent, Darwin, BSD, SCO and Unixware. Every alternative had long ago reached feature saturation, a reality that enterprise marketers labored to conceal.� Trivia question: does anyone remember the functional difference between SCO and OpenBSD? They were often tightly coupled to expensive proprietary hardware. Their fragmentation made it a nightmare for application developers and hardware manufacturers to target a broad swath of the market with one R&amp;D cycle. This environment was the tinderbox waiting for the Linux fire.� As Linux grew in maturity and popularity, many Unix vendors tried to fight the trend.� Many marketing and PR dollars were spent to create fear, uncertainty and doubt about this newcomer of an operating system. �But this was futile and over time Linux has gone on to have an outsized impact on the IT industry.� It has led to lower hardware costs by creating a level playing field for all hardware vendors.� It led to more widespread application adoption due to less platform fragmentation.� It also led to a system of shared industry R&amp;D where software, hardware and device manufacturers contribute back to Linux to ensure compatibility. HDFS is poised to play this role in a market where customers are also tired of fragmentation, excessive margins and breathless marketing of marginal features of dubious utility.� Today proprietary Unix operating systems are still in widespread use.� No doubt the same will hold true for proprietary filesystems.� Old products never die, the just become less relevant. Eric highlighted HDFS�s economics, data processing bandwidth and reliability.� On a functional level I�ll add it has excellent security, resiliency and high availability (that’s right folks, drop the SPOF claims, you can�download CDH4 here!).� Perhaps more important than features for enterprise customers HDFS offers: Choice � Customers get to work with any leading hardware vendor and let the best possible price / performer win the decision, not whatever the vendor decided to bundle in. Portability � It is possible for customers running Hadoop distributions based on HDFS to move between those different distributions without having to reformat the cluster or copy massive amounts of data.� When you�re talking about petabytes of data, this kind of portability is vital.� Without it, your vendor has incredible leverage when it comes time to negotiate the next purchase. Shared industry R&amp;D � We at Cloudera are proud of our employees’ own contributions to HDFS, and they collaborate with their colleagues at Hortonworks. �But today you will find that IBM, Microsoft and VMware are also contributing to HDFS to make it work better with their products.� In the future I predict you�ll find hard drive, networking and server manufacturers also add patches to HDFS to ensure their technologies run optimally with it. It�s rare when you get to see history repeat itself so completely as it is with HDFS.� Today HDFS may not be the best filesystem for content addressable storage or nearline archive.� But then 15 years ago who would have thought Linux would find its way into laptops, routers, mobile phones and airport kiosks? Linux drew us the map. �The smart money is already following it.</snippet></document><document id="400"><title>Cloudera Manager 4.0.3 Released!</title><url>http://blog.cloudera.com/blog/2012/07/cm4-0-3-released/</url><snippet>We are pleased to announce the availability of Cloudera Manager 4.0.3. This is an enhancement release, with several improvements to configurability and usability. Some key enhancements include: Configurable user/group settings for Oozie, HBase, YARN, MapReduce, and HDFS processes. Support new configuration parameters for MapReduce services. Auto configuration of reserved space for non-DFS use parameter for HDFS service. Improved cluster upgrade process. Support for LDAP users/groups that belong to more than one Organization Unit (OU). Flexibility with distribution of key tabs when using existing Kerberos infrastructure (e.g. Active Directory). Detailed release notes available at:��� https://ccp.cloudera.com/display/ENT4DOC/New+Features+in+Cloudera+Manager+4.0 Cloudera Manager 4.0.3 is available to download from: https://ccp.cloudera.com/display/SUPPORT/Downloads</snippet></document><document id="401"><title>Experimenting with MapReduce 2.0</title><url>http://blog.cloudera.com/blog/2012/07/experimenting-with-mapreduce-2-0/</url><snippet>In�Building and Deploying MR2,�we presented a brief introduction to MapReduce in Apache Hadoop 0.23 and focused on the steps to setup a single-node cluster. In MapReduce 2.0 in Hadoop 0.23, we discussed the new architectural aspects of the MapReduce 2.0 design. This blog post highlights the main issues to consider when migrating from MapReduce 1.0 to MapReduce 2.0. Note that both MapReduce 1.0 and MapReduce 2.0 are included in CDH4. It is important to note that, at the time of writing this blog post, MapReduce 2.0 is still Alpha, and it is not recommended to use it in production. In the rest of this post, we shall first discuss the Client API, followed by configurations and testing considerations, and finally commenting on the new changes related to the Job History Server and Web Servlets. We will use the terms MR1 and MR2 to refer to MapReduce in Hadoop 1.0 and Hadoop 2.0, respectively. Client API The Java client APIs for MR2 are compatible with the corresponding APIs in MR1 (see for example JobClient). There is no change required to any code that was written using the old client APIs; applications using such client APIs can directly switch to use MR2. This is applicable to MR1 in both CDH3 and CDH4. However, it is important to note that users have to recompile applications that use the org.apache.hadoop.mapreduce�API and also pipes applications (see jira) when moving from CDH3 to CDH4 (or generally from Hadoop 1 to 2). CDH4 provides a new Hadoop-Client Maven-based way of managing client-side Hadoop API dependencies; that eliminates the need to figure out the exact names and locations of all the needed Hadoop JAR files. This wasn�t an issue in prior Hadoop versions since the release contained a single Jar file, but it is essential with the new Maven-based releases, which contain multiple Jar files. Configurations Configuration properties are heavily used in MapReduce and Hadoop in general. While the Java client API is compatible in MR1 and MR2, configuration properties are generally not. Due to the major changes that were introduced in MR2, a number of old configuration properties are no longer valid and new properties were added. Please refer to this list for configuration property names that are deprecated in Hadoop 2, and their replacements. There is also an open jira for removing unused MR1 configurations. One new key configuration property is mapreduce.framework.name; this property is now used to control if the MapReduce job will be submitted to a YARN cluster or will be run locally using the LocalJobRunner. The valid values for this property are yarn and local. Another property you need to specify if running the resource manager on a separate host is yarn.resourcemanager.address which should be set to a &lt;host&gt;:&lt;port&gt;�combination. These properties roughly correspond to the MR1 property mapred.job.tracker that was either set to a &lt;host&gt;:&lt;port&gt; combination for submitting the job to the JobTracker, or set to local for using the LocalJobRunner. This old MR1 property is no longer valid in MR2 and, if specified, will be ignored by the system. As a general rule, all properties referring to the JobTracker or TaskTracker are no longer valid in MR2. Corresponding and new properties for The ResourceManager, ApplicationMaster and NodeManager have been added on the other hand. Testing using MapReduce Mini Cluster If a�MapReduce�cluster is needed for testing, developers can use the newly added MiniMRClientClusterFactory instead of�directly constructing a MiniMRCluster (deprecated in MR2).�This MiniMRClientClusterFactory provides a wrapper MiniMRClientCluster interface around the MiniMRYarnCluster. This same factory was also added to MR1 to provide an easy migration of tests between MR1 and MR2. MapReduce JobHistory Server As previously described in MapReduce 2.0 in Hadoop 0.23,the JobTracker no longer exists in MR2, and the job life cycle management functionality is now the responsibility of the short-lived Application Masters. For this reason, a new MapReduce JobHistory server was added to MR2, which maintains information about submitted MapReduce jobs after their Application Master terminates. The Resource Manager Web UI manages such forwarding of requests to the JobHistory server when the Application Master completes. Web Servlets Another class of incompatibility between MR2 and MR1 is the use of HTTP servlets. A number of the MR1 servlets are no longer available in MR2 and new servlets were added. Depending on what your old MR1 application was using, there should be a way to achieve the same or similar functionality using MR2. An example is the TaskLogServlet, which in MR2 is part of the MapReduce Job History Server. There is an open jira to document these changes. MR2 introduces a number of promising features and enhancements, but there are considerations to keep in mind when migrating your applications. We highlighted some of these main considerations in an effort to help users who are interested in experimenting with this new system. References: Please refer to the complete API docs for additional details. Also refer to the CDH MapReduce 2.0 API Known Issues.�</snippet></document><document id="402"><title>Apache HBase Log Splitting</title><url>http://blog.cloudera.com/blog/2012/07/hbase-log-splitting/</url><snippet>In the recent blog post about the Apache HBase Write Path, we talked about the write-ahead-log (WAL), which plays an important role in preventing data loss should a HBase region server failure occur. �This blog post describes how HBase prevents data loss after a region server crashes, using an especially critical process for recovering lost updates called log splitting. Log splitting As we mentioned in the write path blog post, HBase data updates are stored in a place in memory called memstore for fast write. In the event of a region server failure, the contents of the memstore�are lost because they have not been saved to disk yet. To prevent data loss in such a scenario, the updates are persisted in a WAL file before they are stored�in the memstore. In the event of a region server failure, the lost contents in the memstore can be regenerated by replaying the updates (also called edits) from the WAL file. A region server serves many regions. �All of the regions in a region server share the same active WAL file. Each edit in the WAL file has information about which region it belongs to. �When a region is opened, we need to replay those edits in the WAL file that belong to that region. �Therefore, edits in the WAL file must be grouped by region so that particular sets can be replayed to regenerate the data in a particular region. The process of grouping the WAL edits by region is called log splitting. It is a critical process for recovering data if a region server fails. Log splitting is done by HMaster as the cluster starts or by ServerShutdownHandler as a region server shuts down. Since we need to guarantee consistency, affected regions are unavailable until data is restored. So we need to recover and replay all WAL edits before letting those regions become available again. As a result, regions affected by log splitting are unavailable until the process completes and any required edits are applied. When log splitting starts, the log directory is renamed as follows: /hbase/.logs/&lt;host&gt;,
&lt;port&gt;,&lt;startcode&gt;-splitting For example: /hbase/.logs/srv.example.com,60020,1254173957298-splitting It is important that HBase renames the folder.�A�region server may still be up when the master thinks it is down. The region server may not respond immediately and consequently doesn�t heartbeat its ZooKeeper session. HMaster may interpret this as an indication that the region server has failed. If the folder is renamed, any existing, valid WAL files still being used by an active but busy�region server�are not accidentally written to. Each log file is split one at a time. The log splitter reads the log file one edit entry at a time and puts each edit entry into the buffer corresponding to the edit�s region. At the same time, the splitter starts several writer threads. Writer threads pick up a corresponding buffer and write the edit entries in the buffer to a temporary recovered edit file. The file location and name is of the following form: /hbase/
&lt;table_name&gt;/&lt;region_id&gt;/recovered.edits/.temp The &lt;sequenceid&gt; shown above is the sequence id of the first log entry written to the file. The temporary recovered edit file is used for all the edits in the WAL file for this region. Once log splitting is completed, the temporary file is renamed to: /hbase/
&lt;table_name&gt;/&lt;region_id&gt;/recovered.edits/&lt;sequenceid&gt; In the preceding example, the is the highest�(most recent) edit sequence id of the entries in the recovered edit file. As a result, when replaying the recovered edits, it is possible to determine if all edits have been written. If the last edit that was written to the HFile is greater than or equal to the edit sequence id included in the file name, it is clear that all writes from the edit file have been completed. When the log splitting is completed, each affected region�is assigned to a region server. When the region is opened, the recovered.edits�folder is checked for recovered edits files. If any such files are present, they are replayed by reading the edits and saving them to the memstore. After all edit files are replayed, the contents of the memstore are written to disk (HFile) and the edit files are deleted. Times to complete single threaded log splitting vary, but the process may take several hours if multiple region servers have crashed. Distributed log splitting�was added in HBase version 0.92 (HBASE-1364) by Prakash Khemani from Facebook. �It reduces the time to complete the process dramatically, and hence improves the availability of regions and tables. For example, we knew a cluster crashed. With single threaded log splitting, it took around 9 hours to recover. �With distributed log splitting, it just took around 6 minutes. Distributed log splitting HBase 0.90�log splitting is all done by the HMaster. For one log splitting invocation, all the log files are processed sequentially. �After a cluster restarts from crash, unfortunately, all region servers are idle and waiting for the master to finish the log splitting. �Instead of having all the region servers remain idle, why not make them useful and help in the log splitting process? This is the insight behind distributed log splitting With distributed log splitting, the master is the boss. �It has a split log manager�to manage all log files which should be scanned and split. Split log manager�puts all the files under the splitlog ZooKeeper node (/hbase/splitlog) as tasks. For example, while in zkcli, �ls /hbase/splitlog� returns: [hdfs%3A%2F%2Fhost2.sample.com%3A56020%2Fhbase%2F.logs%2Fhost8.sample.com%2C57020%2C1340474893275-splitting%2Fhost8.sample.com%253A57020.1340474893900,�hdfs%3A%2F%2Fhost2.sample.com%3A56020%2Fhbase%2F.logs%2Fhost3.sample.com%2C57020%2C1340474893299-splitting%2Fhost3.sample.com%253A57020.1340474893931, hdfs%3A%2F%2Fhost2.sample.com%3A56020%2Fhbase%2F.logs%2Fhost4.sample.com%2C57020%2C1340474893287-splitting%2Fhost4.sample.com%253A57020.1340474893946] After some characters are converted into plain ASCII, it is: [hdfs://host2.sample.com:56020/hbase/.logs/host8.sample.com,57020,1340474893275-splitting/host8.sample.com%3A57020.1340474893900, hdfs://host2.sample.com:56020/hbase/.logs/host3.sample.com,57020,1340474893299-splitting/host3.sample.com%3A57020.1340474893931, hdfs://host2.sample.com:56020/hbase/.logs/host4.sample.com,57020,1340474893287-splitting/host4.sample.com%3A57020.1340474893946] It is a list of WAL file names to be scanned and split, which is a list of log splitting tasks. Once split log manager�publishes all the tasks to the splitlog znode, it monitors these task nodes and waits for them to be processed. In each region server, there is a daemon thread called split log worker. Split log worker does the actual work to split the logs. The worker watches the splitlog znode all the time. �If there are new tasks, split log worker retrieves the task paths, and then loops through them all to grab any one which is not claimed by other worker yet. �After it grabs one, it tries to claim the ownership of the task, to work on the task if successfully owned, and to�update the task�s state properly based on the splitting outcome. After the split worker completes the current task, it tries to grab another task to work on if any remains. This feature is controlled by the configuration hbase.master.distributed.log.splitting�property. By default, it is enabled. (Note that distributed log splitting is backported to CDH3u3 which is based on 0.90. However, it is disabled by default in CDH3u3. To enable it, you need to set configuration parameter hbase.master.distributed.log.splitting�to true). When HMaster starts up, a split log manager�instance is created if this parameter is not explicitly set to false. The split log manager creates a monitor thread. The monitor thread periodically does the following: Checks if there are any dead split log workers queued up. If so, it will resubmit those tasks owned by the dead workers. If the resubmit fails due to some ZooKeeper exception, the dead worker is queued up again for retry. Checks if there are any unassigned tasks. If so, create an ephemeral rescan node so that each split log worker is notified to re-scan unassigned tasks via the nodeChildrenChanged�ZooKeeper event. Checks those assigned tasks if they are expired. If so, move the task to TASK_UNASSIGNED state again so that they can be retried. These tasks could be assigned to some slow workers, or could be already finished. It is fine since the split can be retried due to the idempotency of the log splitting task; that is, the same log splitting task can be processed many times without causing any problem. Split log manager�watches the HBase split log znodes all the time. If any split log task node data is changed, it retrieves the node data. The node data has the current state of the task. For example, while in zkcli, �get /hbase/splitlog/hdfs%3A%2F%2Fhost2.sample.com%3A56020%2Fhbase%2F.logs%2Fhost6.sample.com%2C57020%2C1340474893287-splitting%2Fhost6.sample.com%253A57020.1340474893945� returns: unassigned host2.sample.com:57000 cZxid = 0×7115 ctime = Sat Jun 23 11:13:40 PDT 2012 mZxid = 0×7115 mtime = Sat Jun 23 11:13:40 PDT 2012 pZxid = 0×7115 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0×0 dataLength = 33 numChildren = 0 It shows this task is still unassigned. Based on the state of the task whose data is changed, the split log manager does one of the following: Resubmit the task if it is unassigned Heart beat the task if it is assigned Resubmit or fail* the task if it is resigned Resubmit or fail* the task if it is completed with errors Resubmit or fail* the task if it could not complete due to errors Delete the task if it is successfully completed or failed Note: fail a task if: The task is deleted The node doesn’t exist anymore Fails to move the state of the task to TASK_UNASSIGNED The number of resubmits is over the resubmit threshold The split�log worker is created and started by the region server. So there is a split log worker�in each region server. When the split log worker starts, it registers itself to watch HBase znodes. If any splitlog znode children change, it notifies the worker thread to wake up to grab more tasks if it is sleeping. If current task’s node data is changed, it checks if the task is taken by another worker. If so, interrupt the worker thread and stop the current task. The split log worker thread keeps checking the task nodes under splitlog znode if any node children change. For each task, it does the following: Get the task state and doesn’t do anything if it is not in TASK_UNASSIGNED state. If it is in TASK_UNASSIGNED state, try to set the state to TASK_OWNED by the worker. If it fails to set the state, it is ok, another worker will try to grab it. Split log manager will also try to ask all workers to rescan later if it remains unassigned. If the worker gets this task, it tries to get the task state again to make sure it really gets it asynchronously. In the meantime, it starts a split task executor to do the actual work: Get the HBase root folder, create a temp folder under the root, and split the log file to the temp folder. If everything is ok, the task executor sets the task to state TASK_DONE. If catches an unexpected IOException, the task is set to state TASK_ERR. If the working is shutting down, set the the task to state TASK_RESIGNED. If the task is taken by another worker, it’s ok, just log it. Split log manager returns when all tasks are completed successfully. If all tasks are completed with some failure, it throws an exception so that the log splitting can be retried. Due to an asynchronous implementation, in very rare cases, split log manager loses track of some completed tasks. So it periodically checks if there is any remaining uncompleted task in its task map or ZooKeeper. �If none, it throws an exception so that the log splitting can be retried right away instead of hanging there waiting for something that won�t happen. Conclusion In this blog post, we have presented a critical process, log splitting, to recover lost updates from region server failures. Log splitting used to be done by the HMaster sequentially. In 0.92, an improvement called distributed log splitting was introduced, and the actual work is done by region servers in parallel. Since there are many region servers in the cluster, distributed log splitting dramatically reduces the log splitting time, and improves regions’ availability.</snippet></document><document id="403"><title>Watching the Clock: Cloudera’s Response to Leap Second Troubles</title><url>http://blog.cloudera.com/blog/2012/07/watching-the-clock-clouderas-response-to-leap-second-troubles/</url><snippet>At 5 pm PDT on June 30, a leap second was added to the Universal Coordinated Time (UTC). Within an hour, Cloudera Support started receiving reports of systems running at 100% CPU utilization. The Support Team worked quickly to understand and diagnose the problem and soon published a solution. Bugs due to the leap second coupled with the Amazon Web Services outage would make this Cloudera�s busiest support weekend to date. Since Hadoop is written in Java and closely interoperates with the underlying OS, Cloudera Support troubleshoots not only all 17 components in the Hadoop ecosystem, but also any underlying Linux and Java bugs. Last weekend many of our customers were affected by the now infamous “leap second” bugs. Initially, many assumed that Java and Linux would process the leap second gracefully. However, we soon discovered that this wasn�t the case and depending on the version of Linux being used, several distinct issues were observed. Background Leap seconds are added to the UTC to correct for Earth’s slowing rotation. The latest leap second was added last Saturday (6/30) at 23:59:60 UTC (5 pm PDT). Due to a missed function call in the Linux timekeeping code, the leap second was not accounted for properly. As a result, after the leap second, timers expired one second earlier than requested. Many applications use a recurring timer of 1 second or less; such timers expired immediately, causing the application to immediately try to set another timer, ad infinitum. This infinite loop led to CPU load spikes that launched 21 separate support tickets. Plot showing the difference UT1?UTC in seconds. Vertical segments correspond to leap seconds. Red part of graph was prediction (future values) at the time the file was made. From http://en.wikipedia.org/wiki/Leap_second Diagnosis While the tickets streamed in with varied symptoms, the underlying theme was high CPU load, resulting in responsive but slow machines. Given the symptom vagueness, we focused on accurate diagnosis in the form of three verification checks: If the cluster is managed by Cloudera Manager (CM), Activity Monitoring will clearly indicate a load average close to 100%. Check the kernel message buffer (run dmesg) and look for output confirming the leap second injection: Clock: inserting leap second 23:59:60 UTC Check CPU utilization through OS reporting mechanisms: # ps -C java -o pid,pcpu,euser,comm --sort cpu If the Java processes are using 100% of the CPU the system is likely experiencing the Leap Second problem. Cloudera Manager showing CPU Spike Caused by Leap Second Bug Remedy Just as varied as the symptoms were the suggested workarounds found in various official bug reports. Weighing the time to test each workaround against the urgency of reviving our customers� production clusters, we decided to first pilot several workarounds on our internal Hadoop clusters, rather than on our customers�. Unfortunately we weren’t able to confirm that the workarounds repaired the issue in all cases. Internal testing revealed CPU usage returning to a high number after restarting applications, which used external components such as MySQL, or the ntpd daemon. Based on its observed effectiveness in our test environments, we opted to recommend issuing 'date -s "$(date)"' to correctly set the real time clock, which immediately lowered the CPU load, followed by a rolling restart on all nodes in the Hadoop cluster in order to avoid any potential problems with other processes or kernel modules that could be affected. Ganglia showing CPU Spike Caused by Leap Second Bug Summary With both diagnosis and remedy in hand, the Support Team executed swiftly, sharing knowledge internally and leveraging internal engineering and QA teams. Rapidly disseminating information, the Support Team spoke globally, 24/7, and with one voice. Once the team had determined a reliable diagnosis and repair procedure, resolution time for customers dropped dramatically: while the first customers waited nearly three hours for a full fix, this quickly decreased to under ten minutes.</snippet></document><document id="404"><title>The Apache Hadoop Ecosystem, Visualized in Datameer</title><url>http://blog.cloudera.com/blog/2012/07/the-hadoop-ecosystem-visualized-in-datameer/</url><snippet>This is a guest re-post from Datameer’s Director of Marketing, Rich Taylor. The original post can be found on the Datameer blog. Datameer uses D3.js to power our Business Infographic� designer. I thought I would show how we visualized the Apache Hadoop ecosystem connections. First using only D3.js, and second using Datameer 2.0. Many people asked about the image above that was on our booth at the Hadoop Summit. Here�s how the image was created: A .csv file was created from public press releases and partner pages with the connections of companies and some technologies in the Hadoop ecosystem. Our visualization engineer, Christophe, coded a graphic (specifically this one) in D3.js using this data set. Our graphic designer then took the image and did a few modifications (increased some font size and added Datameer�s Hadoop distribution partners). Not including the data collection process, in short, it took three people a good amount of time (probably 4-6 hours) to create this graphic. The above Business Infographic� was created by me, Rich Taylor, a Director of Marketing, all by myself using Datameer 2.0, no coding or separate design tool required. Using the same .csv file, I uploaded the file into Datameer. I went to the Business Infographic� designer, chose the circular network graphic and dragged over my data from my uploaded file. Next I made some edits to the graphic, uploaded a few partner logos and added some text. To take it a little further, I opened the data into a Datameer workbook, did some analytics (groupby, groupcount, join and filter) to find who has the most partners/connections and threw that into my infographic. In short, it took one person (a business user) about 30 minutes to put this together. I even got carried away and tried out a few different layouts, which just took a few more minutes. Sound easy enough? Try it out for yourself by downloading our free trial! No Hadoop cluster needed, you can get started today right on your laptop. http://www.datameer.com/technology/download.php Oh and think you can make a better infographic with the same data? Show us! Here�s the Hadoop Ecosystem .csv file you can use to make your own visualization: Hadoop Ecosystem Datameer Spreadsheet</snippet></document><document id="405"><title>Apache Flume Development Status Update</title><url>http://blog.cloudera.com/blog/2012/07/apache-flume-development-status-update/</url><snippet>Apache Flume is a scalable, reliable, fault-tolerant, distributed system designed to collect, transfer, and store massive amounts of event data into HDFS. Apache Flume recently graduated from the Apache Incubator as a Top Level Project at Apache. Flume is designed to send data over multiple hops from the initial source(s) to the final destination(s). Click here for details of the basic architecture of Flume. In this article, we will discuss in detail some new components in Flume 1.x (also known as Flume NG), which is currently on the trunk branch, techniques and components that can be be used to route the data, configuration validation, and finally support for serializing events. In the past several months, contributors have been busy adding several new sources, sinks and channels to Flume. Flume now supports Syslog as a source, where sources have been added to support Syslog over TCP and UDP. Flume now has a high performance persistent channel – the File Channel. This means if the agent fails for any reason before events committed by the source are not removed and the transaction committed by the sink, the events will reloaded from disk and can be taken when the agent starts up again. The events will only be removed from the channel when the transaction is committed by the sink. The File channel uses a Write Ahead Log to save events. Something that was high on our list of priorities was to give applications the ability to easily write data to flume agents. So the RPC code was refactored into a separate Flume SDK, which can be used to push events into flume. The SDK has two clients, one which simply is able to append data to a single agent and the second client supports failover (marked in the figure as FC) . It takes a list of agents through configuration, and will connect to them, one by one, when a failure happen. We have also added the capability to plug in an EventSerializer which can serialize an event and write to an output stream. This event serializer interface can be used to convert the event data into a format required by the user, such as converting an event body into text and writing to an output stream. Also recently added was a new Channel Selector called the MultiplexingChannelSelector. The multiplexing channel selector allows the user to divert the events from the source to one or more of the channels linked to it. The MultiplexingChannelSelector is configured to look at a specific event header to find pre-defined values. The configuration defines mappings of specific values of the header to a subset of the channels wired to that source. If one of the configured values for the header is found, the channel selector writes the event to the channel(s) which are mapped to that value. If the value of the header does not match any of the values defined in the configuration, then the event is written to a set of channels which are defined as �default channels.� An example configuration is below: host2.sources.src1.channels = ch1 ch2 host2.sources.src1.selector.type = multiplexing host2.sources.src1.selector.header = myheader host2.sources.src1.selector.mapping.header1 = ch1 host2.sources.src1.selector.mapping.header2 = ch2 host2.sources.src1.selector.default = ch2 This configuration will cause events with header �myheader� whose value is header1 to go to ch1, and value, header2 to go to ch2. Any events with no header, or different value of the header will go to ch2. We also are adding a new configuration system, which allows each component to specify the configuration it requires through configuration stubs. These stubs can be used to validate the configuration of individual components, and hence the entire agent. The main component which does the validation has been committed to the Flume code base, while individual configuration stubs and a standalone configuration tool are in the final stages of development/review. Flume now has the capability to modify events in flight. Interceptors are Flume plugins (intended to allow for both “stock” plugins as well as custom Java code) that sit between a Source and a Channel. An installed Interceptor is given full access to each event that flows out of the Source it is attached to, and it may inspect, transform, or drop each event. Interceptors may be chained together to form a pipeline of plugin code that is executed for each event. Interceptors are the somewhat constrained answer in Flume 1.x to the concept of Decorators in Flume 0.9.x. We are continuing to add more features to Flume like an HBase Sink, which was recently committed. The HBase Sink also provides a serializer interface which users can implement to serialize the events into HBase Put/Increments. Flume has a very active and diverse developer community which has been continuously contributing more features and enhancing existing ones. As a result, Flume has proved to be extremely high performance, and showed that a single agent was able to write more than 70,000 events per second to HDFS. You can find more details of the performance analysis here thanks to Mike Percy and Will McQueen. We want to make sure Flume becomes as useful as possible to anyone who would want to stream large amounts of data to HDFS and/or HBase. To get started with Flume, please take a look at the Flume Getting Started Guide.</snippet></document><document id="406"><title>Update on Apache Bigtop (incubating)</title><url>http://blog.cloudera.com/blog/2012/07/update-on-apache-bigtop-incubating/</url><snippet>Introduction Ever since Cloudera decided to contribute the code and resources for what would later become�Apache Bigtop (incubating), we’ve been answering a very basic question: what exactly is Bigtop and why should you or anyone in the Apache (or�Hadoop) community care? The earliest and the most succinct answer (the one used for the�Apache Incubator proposal) simply stated that “Bigtop is a project for the development of packaging and tests of the Hadoop ecosystem”. That was a nice explanation of how Bigtop relates to the rest of the�Apache Software Foundation’s (ASF)�Hadoop ecosystem projects, yet it doesn’t really help you understand the aspirations of Bigtop. The history Cloudera was the first company to create an open source distribution that included Apache Hadoop, releasing the first version (CDH1) back in March, 2009.� The initial goal of CDH was to make Apache Hadoop easier to adopt, providing packaging to enable users to install Hadoop on popular Linux operating systems and not have to compile from source. In mid-2010 Cloudera announced a major change in CDH that eventually came to recast what defined an Apache Hadoop based distribution.� We observed that users were typically running not just Apache Hadoop but also a collection of other open source systems and components that were quickly becoming essential to have a fully functioning data management system.� But in order to run such a system, organizations needed to do a great deal of work: assembling and integrating sometimes as many as a dozen different components.� Each open source component had its own release schedule, dependencies, interfaces and standards for quality. CDH3 was the first time a great many of these components were provided together all as an integrated system.� Since that time we�ve updated the distribution on a regularly quarterly schedule and recently released a new major version (CDH4). That notion of a Hadoop distribution has become the industry�s prevailing definition: An integrated set of open source components that make up a Apache Hadoop based data management system Integrated &amp; tested to work together Tested &amp; packaged to work on a standardized set of platforms Today, all providers of Apache Hadoop distributions essentially follow this model and many in fact simply choose to redistribute CDH. The motivation Building and supporting CDH taught us a great deal about what was required to be able to repeatedly assemble a truly integrated, Apache Hadoop based data management system.� The build, testing and packaging cost was considerable, and we regularly observed that different projects made different design choices that made ongoing integration difficult.� We also realized that more and more mission critical workload was running on CDH and the customer demand for stability, predictability and compatibility was increasing. Apache Bigtop was part of our answer to solve these two different problems.� Initiate an Apache open source project that focused on creating the testing and integration infrastructure of an Apache-Hadoop based distribution.� With it we hoped that: We could better collaborate within the extended Apache community to contribute to resolving test, integration &amp; compatibility issues across projects We could create a kind of developer-focused distribution that would be able to release frequently, unencumbered by the enterprise expectations for long-term stability and compatibility. This would enable us to make progress faster, iterating quickly with new releases of all the projects included in the distribution without worrying about a� high rate of change or compatibility breaking that would be difficult to inject into our stable, supported enterprise distribution. We could help create a community process around the development of a distribution itself.� We imagined this would be beneficial both for Apache Bigtop and ultimately for CDH. Progress so far It�s been nearly 1 year since Apache Bigtop was proposed to the incubator and we�ve been thrilled with the progress.� There have been 4 releases so far, keeping with a goal of delivering fixed-time, variable scope �train� releases.� The project started with a diverse range of contributors and this diversity has broadened over time.� We�ve seen new contributions from various corporate sponsors but more importantly from members of related communities.� Apache Hama was added to Bigtop for example and a member of that community was added to the project in the process.� There�s been a similar investment to add Apache Giraph (incubating) to the project. The overall rate of activity within the Apache Bigtop is accelerating.� More patches are contributed each month.� More individuals are joining the user and developer lists.� This project comes at an important time in the evolution of the Hadoop stack.� There are more than half a dozen new projects that have recently spawned to extend the feature set of the Apache Hadoop stack and Bigtop represents an opportunity to integrate more of them more quickly into the context of a larger more strategic data management system. What Apache Bigtop means for you If you are: A casual user (a big data hacker):�Bigtop provides a fully integrated, packaged, and validated stack of big data management software based on the Apache Hadoop ecosystem specially tailored for your favorite version of Linux OS (and perhaps other OS’s in the future). The packaged artifacts and the deployment experience will be very similar to CDH, but with two key exceptions: CDH leverages backporting of patches to provide long term support on a stable release while providing stability and compatibility, but a Bigtop distribution will be much more aggressive in tracking the very latest versions of Hadoop ecosystem components even if it injects instability or compatibility changes from release to release. Apache Bigtop will include a wider range of systems and components, for many of which Cloudera may not provide production support (e.g. Apache Hama, Apache Giraphe). For OS vendors:�Bigtop provides a readily available source of packaging, validation, and deployment code that can be used as a basis for integration of Apache Hadoop into the OS bundles. A company building its own distribution that includes Apache Hadoop:�Bigtop could be a good point of departure and a treasure trove of wheels that don’t need to be reinvented. Parting Thoughts Apache Bigtop (incubating) is still a very young project. We have some ambitious goals in mind, but we can’t possibly achieve them without your help. We need your feedback and we need your involvement.� As always, patches are welcome.</snippet></document><document id="407"><title>Apache HBase I/O – HFile</title><url>http://blog.cloudera.com/blog/2012/06/hbase-io-hfile-input-output/</url><snippet>Introduction Apache HBase is the Hadoop open-source, distributed, versioned storage manager well suited for random, realtime read/write access. Wait wait? random, realtime read/write access? How is that possible? Is not Hadoop just a sequential read/write, batch processing system? Yes, we�re talking about the same thing, and in the next few paragraphs, I�m going to explain to �you how HBase achieves the random I/O, how it stores data and the evolution of the HBase�s HFile format. Apache Hadoop I/O file formats Hadoop comes with a SequenceFile[1] file format that you can use to append your key/value pairs but due to the hdfs append-only capability, the file format cannot allow modification or removal of an inserted value. The only operation allowed is append, and if you want to lookup a specified key, you�ve to read through the file until you find your key. As you can see, you�re forced to follow the sequential read/write pattern… but how is it possible to build a random, low-latency read/write access system like HBase on top of this? To help you solve this problem Hadoop has another file format, called MapFile[1], an extension of the SequenceFile. �The MapFile, in reality, is a directory that contains two SequenceFiles: the data file �/data� and the index file �/index�. The MapFile allows you to append sorted key/value pairs and every N keys (where N is a configurable interval) it stores the key and the offset in the index. This allows for quite a fast lookup, since instead of scanning all the records you scan the index which has less entries. Once you�ve found your block, you can then jump into the real data file. MapFile is nice because you can lookup key/value pairs quickly but there are still two problems: How can I delete or replace a key/value? When my input is not sorted, I can not use MapFile. HBase &amp; MapFile The HBase Key consists of: the row key, column family, column qualifier, timestamp and a type. To solve the problem of deleting key/value pairs, the idea is to use the �type� field to mark key as deleted (tombstone markers). �Solving the problem of replacing key/value pairs is just a matter of picking the later timestamp (the correct value is near the end of the file, append only means last inserted is near the end). To solve the �non-ordered� key problem we keep the last added key-values in memory. �When you�ve reached a threshold, HBase flush it to a MapFile. �In this way, you end up adding sorted key/values to a MapFile. HBase does exactly this[2]: when you add a value with table.put(), your key/value is added to the MemStore (under the hood MemStore is a sorted ConcurrentSkipListMap). When the per-memstore threshold (hbase.hregion.memstore.flush.size) is reached or the RegionServer is using too much memory for memstores (hbase.regionserver.global.memstore.upperLimit), data is flushed on disk as a new MapFile. The result of each flush is a new MapFile, and this means that to find a key you have to search in more than one file. �This takes more resources and is potentially slower. Each time a get or a scan is issued, HBase scan through each file to find the result, to avoid jumping around too many files, there�s a thread that will detect when you�ve reached a certain number of files (hbase.hstore.compaction.max). It then tries to merge them together in a process called compaction, which basically �creates a new large file as a result of the file merge. HBase has two types of compaction: one called �minor compaction� that just merges two or more small files into one, and the other called �major compaction� that picks up all the files in the region, merges them and performs some cleanup. �In a major compaction, deleted key/values are removed, this new file doesn�t contain the tombstone markers and all the duplicate key/values (replace value operations) are removed. Up to version 0.20, HBase has used the MapFile format to store the data but in 0.20 a new HBase-specific MapFile was introduced (HBASE-61). HFile v1 In HBase 0.20, MapFile is replaced by HFile: a specific map file implementation for HBase. The idea is quite similar to MapFile, but it adds more features than just a plain key/value file. Features such as support for metadata and the index is now kept in the same file. The data blocks contain the actual key/values as a MapFile. �For each “block close operation” the first key is added to the index, and the index is written on HFile close. The HFile format also adds two extra �metadata� block types: Meta and FileInfo. �These two key/value blocks are written upon file close. The Meta block is designed to keep a large amount of data with its key as a String, while FileInfo is a simple Map preferred for small information with keys and values that are both byte-array. Regionserver’s StoreFile uses Meta-Blocks to store a Bloom Filter, and FileInfo for Max SequenceId, Major compaction key and Timerange info. This information is useful to avoid reading the file if there�s no chance that the key is present (Bloom Filter), if the file is too old (Max SequenceId) or if the file is too new (Timerange) to contain what we�re looking for. HFile v2 In HBase 0.92, the HFile format was changed a bit (HBASE-3857) to improve the performance when large amounts of data are stored. One of the main problems with the HFile v1 is that you need to load all the monolithic indexes and large Bloom Filters in memory, and to solve this problem v2 introduces multi-level indexes and a block-level Bloom Filter. As a result, HFile v2 �features improved speed, memory, and cache usage. The main feature of this v2 are �inline blocks�, the idea is to break the index and Bloom Filter per block, instead of having the whole index and Bloom Filter of the whole file in memory. In this way you can keep in ram just what you need. Since the index is moved to block level you then have a multi-level index, meaning each block has its own index (leaf-index). �The last key of each block is kept to create the intermediate/index that makes the multilevel-index b+tree like. The block header now contains some information: The �Block Magic� field was replaced by the �Block Type� field that describes the content of the block �Data�, Leaf-Index, Bloom, Metadata, Root-Index, etc. Also three fields (compressed/uncompressed size and offset prev block) were added to allow fast backward and forward seeks. Data Block Encodings Since keys are sorted and usually very similar, it is possible to design a better compression than what a general purpose algorithm can do. HBASE-4218 tried to solve this problem, and in HBase 0.94 you can choose between a couple of different algorithms: Prefix and Diff Encoding. The main idea of Prefix Encoding is to store the common prefix only once, since the rows are sorted and the beginning is typically the same. The Diff Encoding pushes this concept further. Instead of considering the key as an opaque sequence of bytes, the Diff Encoder splits each key field in order to compress each part in a better way. This being that the column family is stored once. If the key length, value length and type are the same as the row prior, the field is omitted. Also, for increased compression, the timestamp is stored is stored as a Diff from the previous one. Note that this feature is off by default since writing and scanning are slower but more data is cached. To enable this feature you can set DATA_BLOCK_ENCODING = PREFIX | DIFF | FAST_DIFF in the table info. HFile v3 HBASE-5313 contains a proposal to restructure the HFile layout to improve compression: Pack all keys together at beginning of the block and all the value together at the end of the block. In this way you can use two different algorithms to compress key and values. Compress timestamps using the XOR with the first value and use VInt instead of long. Also, a columnar format or a columnar encoding is under investigation, take a look at AVRO-806 for a columnar file format by Doug Cutting. As you may see the trend in evolution is to be more aware about what the file contains, to get better compression or better location awareness that translates into less data to write/read from disk. Less I/O means more speed! [1] http://blog.cloudera.com/blog/2011/01/hadoop-io-sequence-map-set-array-bloommap-files/ [2] http://blog.cloudera.com/blog/2012/06/hbase-write-path/</snippet></document><document id="408"><title>Apache Oozie (incubating) 3.2.0 release</title><url>http://blog.cloudera.com/blog/2012/06/apache-oozie-incubating-3-2-0-release-hadoop-workflow-scheduler/</url><snippet>This blog was originally posted on the Apache�Blog for Oozie. In June 2012, we released Apache Oozie (incubating) 3.2.0. Oozie is currently undergoing incubation at The Apache Software Foundation (see http://incubator.apache.org/oozie). Oozie is a workflow scheduler system for Apache Hadoop jobs. Oozie Workflows are Directed Acyclical Graphs (DAGs), and they can be scheduled to run at a given time frequency and when data becomes available in HDFS. Oozie 3.1.3 was the first incubating release. Oozie 3.1.3 added Bundle job capabilities to Oozie. A bundle job is a collection of coordinator jobs that can be managed as a single application. This is a key feature for power users that need to run complex data-pipeline applications. Oozie 3.2.0 is the second incubating release, and the first one to include features and fixes done in the context of the Apache Community. The Apache Oozie Community is growing organically with more users, more contributors, and new committers. Speaking as one of the initial developers of Oozie, it is exciting and fulfilling to see the Apache Oozie project gaining traction and mindshare. While Oozie 3.2.0 is a minor upgrade, it adds significant new features and fixes that make the upgrade worthwhile. Here are the most important new features: Support for Hadoop 2 (YARN Map-Reduce) Built in support for new workflow actions: Hive, Sqoop, and Shell Kerberos SPNEGO authentication for Oozie HTTP REST API and Web UI Support for proxy-users in the Oozie HTTP REST API (equivalent to Hadoop proxy users) Job ACLs support (equivalent to Hadoop job ACLs) Tool to create and upgrade Oozie database schema (works with Derby, MySQL, Oracle, and PostgreSQL databases) Improved Job information over HTTP REST API New Expression Language functions for Workflow and Coordinator applications Share library per action (including only the JARs required for the specific action) Oozie 3.2.0 also includes several improvements for performance and stability, as well as bug fixes. And, as with previous Oozie releases, we are ensuring 100% backwards compatibility with applications written for previous versions of Oozie. At the Hadoop Summit 2012 in San Jose, an Oozie meet-up gathering occurred. It was very nice to meet new people and to match faces to familiar email addresses and IRC IDs. During the meet-up, Michelle Chiang from Yahoo! Oozie QE team- explained the comprehensive certification process that Yahoo has in place for Oozie, which includes reliability, scalability, and compatibility tests, some of which run for 7 days. Mona Chitnis from Yahoo! Oozie Engineering team described how the integration with Hadoop 2 (YARN Map-Reduce) was done, the challenges that were faced, and how we worked together with the Apache Hadoop community to accomplish this goal. Finally, we discussed current Oozie pain points and ideas on how to address them. Already there is activity in the Apache Oozie JIRA on this front. We are already working at full speed on new features and fixes to make Oozie easier to use and I’m personally thrilled to see how Oozie is helping manage complex processing in Hadoop clusters. If you need additional information, please feel free to drop an email on the project�s user or developer lists, or alternatively file the appropriate JIRA issues. Your contribution in any form is welcome on the project. Project Website: http://incubator.apache.org/oozie Oozie Quick Start: http://incubator.apache.org/oozie/docs/3.2.0-incubating/docs/DG_QuickStart.html Mailing Lists: http://incubator.apache.org/oozie/MailingLists.html Issue Tracking: https://issues.apache.org/jira/browse/OOZIE IRC Channel: #oozie on irc.freenode.net</snippet></document><document id="409"><title>Apache Hadoop Beyond MapReduce, Part 1: Introducing Kitten</title><url>http://blog.cloudera.com/blog/2012/06/hadoop-beyond-mapreduce-introducing-kitten/</url><snippet>This week, a team of researchers at Google will be presenting a paper describing a system they developed that can learn to identify objects, including the faces of humans and cats, from an extremely large corpus of unlabeled training data. It is a remarkable accomplishment, both in terms of the system’s performance (a 70% improvement over the prior state-of-the-art) and its scale: the system runs on over 16,000 CPU cores and was trained on 10 million 200×200 pixel images extracted from YouTube videos. Doug Cutting has described Apache Hadoop as “the kernel of a distributed operating system.” Until recently, Hadoop has been an operating system that was optimized for running a certain class of applications: the ones that could be structured as a short sequence of MapReduce jobs. Although MapReduce is the workhorse programming framework for distributed data processing,�there are many difficult and interesting problems– including combinatorial optimization problems, large-scale graph computations, and machine learning models that identify pictures of cats– that can benefit from a more flexible execution environment. Hadoop 2.0 introduced a substantial re-design of the core resource scheduling and task tracking system that will allow developers to create entirely new classes of applications for Hadoop. Cloudera’s Ahmed Radwan has written an excellent overview of the architecture of the�new resource scheduling system, known as YARN. Hadoop’s open-source foundation and its broad adoption by industry, academia, and government labs means that, for the first time in history, developers can assume that a common platform for distributed computing will be available at organizations all over the world, and that there will be a market for applications that take advantage of that common platform to solve problems at scales that have never been considered before. Kitten: For Developers Who Want to Play with YARN Hadoop 2.0 ships with an example YARN application called Distributed Shell, which lets a user create some number of tasks on a Hadoop cluster and run a shell command in each of them. Distributed Shell is intended to be a simple example to illustrate what is required to create a YARN application. Although the YARN API is powerful and flexible, that power and flexibility comes at a cost: the simple Distributed Shell application consists of over�1600 lines of Java code that are primarily concerned with configuring parameters and executing RPCs to communicate with YARN’s resource scheduler. A developer creating a YARN application using the raw API In our experience, most distributed applications have a similar lifecycle: they are initialized with a set of resources, they run for awhile and need to be monitored for results and/or failures, and they require some cleanup to release the resources they were using when they are finished executing. We implemented these lifecycle patterns in�Kitten, a set of Java libraries and�Lua-based configuration files that handle configuring, launching, and monitoring YARN applications, allowing developers to spend most of their time writing the logic of an application, not its configuration. In the same way that Java libraries like�Crunch�encode common MapReduce patterns into tools that allow developers to solve problems without needing to worry about low-level implementation details, Kitten implements a series of patterns for configuring and managing the typical lifecycle of YARN applications without requiring developers to know every detail of the YARN APIs. A developer creating a YARN application with Kitten   Free as in Kittens The source code for Kitten is available on GitHub, along with surprisingly ample documentation, all of which is released under the Apache 2.0 License.�Kitten was developed against the experimental YARN module that is distributed with CDH4, and�is intended for use by developers who are interested in developing distributed systems against the YARN APIs; we do not recommend running production jobs (MapReduce or otherwise) under YARN at the present time. In the second part of this series, we will introduce BranchReduce, a framework for building distributed�branch-and-bound�solvers�that we developed using Kitten.</snippet></document><document id="410"><title>A Big Thank You to All Who Participated In Making HBaseCon and the HBase Hack-a-thon A Success</title><url>http://blog.cloudera.com/blog/2012/06/a-big-thank-you-to-all-who-participated-in-making-hbasecon-and-the-hbase-hack-a-thon-a-success/</url><snippet>HBaseCon 2012 summation provided by Michael Stack, PMC Chair of the Apache HBase Project. HBase Hack-a-thon summation provided by David Wang, Engineering Manager for the Cloudera HBase team. HBaseCon 2012 Summation HBaseCon 2012, the first ever Apache HBase focused conference has come and gone. Maybe folks were being polite but going by my informal survey, HBaseCon 2012 was a high-quality, community event (The after-parties weren’t bad either). By all accounts, it was a great day out. A lot of hard work went into making HBaseCon 2012 go off so smoothly. I would like to thank all who made it happen. The program committee — Nicolas, Lars, Todd, Jean-Daniel, and Andrew — reviewed a mountain of submissions and helped run the conference tracks on the day. Our hosts, Cloudera did an excellent job organizing and managing the event. They made Apache HBase look well-run, and professional. A big thank you to all of the HBaseCon 2012 speakers! There are too many of you to name individually, but if it were not for you such an informative and valuable conference would not have been possible. Thank you to all of the HBaseCon 2012 Sponsors! Without your support, both in sponsoring the event itself and in your ongoing contributions to the Apache HBase project, the conference could not have been such a success. Last but not least, the attendees. Thank you all for coming to San Francisco and participating in HBaseCon. Without you, there is no conference, so once again thank you so much for your attendance and support or interest in the Apache HBase project. Hopefully we will be able to do this again next year! Slides and videos from majority of the conference sessions are available! Find session videos and slides linking from the�HBaseCon 2012 Agenda. HBase Hack-a-thon Summation More than 50 people attended the HBase Hackathon the day after HBaseCon. The day started with a series of informal talks and discussions around: Performance Testing Source modularization Wire compatibility Durable sync Release candidate testing Folks then broke out into huddles and started working on various topics. There was also a session for new folks to the community that walked through the development process and a high-level overview of the codebase. In addition to HBase work getting done, the hackathon was also a chance for folks to associate faces with the names on JIRAs and mailing list messages, thereby reinforcing the sense of community that was also evident at HBaseCon.</snippet></document><document id="411"><title>Apache HBase Write Path</title><url>http://blog.cloudera.com/blog/2012/06/hbase-write-path/</url><snippet>Apache HBase is the Hadoop database, and is based on the Hadoop Distributed File System (HDFS). HBase makes it possible to randomly access and update data stored in HDFS, but files in HDFS can only be appended to and are immutable after they are created. �So you may ask, how does HBase provide low-latency reads and writes? In this blog post, we explain this by describing the write path of HBase — how data is updated in HBase. The write path is how an HBase completes put or delete operations. This path begins at a client, moves to a region server, and ends when data eventually is written to an HBase data file called an HFile. Included in the design of the write path are features that HBase uses to prevent data loss in the event of a region server failure. Therefore understanding the write path can provide insight into HBase�s native data loss prevention mechanism. Each HBase table is hosted and managed by sets of servers which fall into three categories: One active master server One or more backup master servers Many region servers Region servers contribute to handling the HBase tables. Because HBase tables can be large, they are broken up into partitions called regions. Each region server handles one or more of these regions. Note that because region servers are the only servers that serve HBase table data, a master server crash cannot cause data loss. HBase data is organized similarly to a sorted map, with the sorted key space partitioned into different shards or regions. An HBase client updates a table by invoking put or delete commands. When a client requests a change,�that request is routed to a region server�right away by default. However, programmatically, a client can cache the changes in the client side, and flush these changes to region servers in a batch, by turning the autoflush off. If autoflush is turned off, the changes are cached until flush-commits is invoked, or the buffer is full depending on the buffer size set programmatically or configured with parameter �hbase.client.write.buffer�. Since the row key is sorted, it is easy to determine which region server manages which key. A change request is for a specific row. Each row key belongs to a specific region which is served by a region server. So based on the put or delete�s key, an HBase client can locate a proper region server. At first, it locates the address of the region server hosting the -ROOT- region from the ZooKeeper quorum. �From the root region server, the client finds out the location of the region server hosting the -META- region. �From the meta region server, then we finally locate the actual region server which serves the requested region. �This is a three-step process, so the region location is cached to avoid this expensive series of operations. If the cached location is invalid (for example, we get some unknown region exception), it�s time to re-locate the region and update the cache. After the request is received by the right region server, the�change cannot be written to a HFile immediately because the data in a HFile must be sorted by the row key. This allows searching for random rows efficiently when reading the data. Data cannot be randomly inserted into the HFile. Instead, the change must be written to a new file. If each update were written to a file, many small files would be created. Such a solution would not be scalable nor efficient to merge or read at a later time. Therefore, changes are not immediately written to a new HFile. Instead, each change is stored in a place in memory called the memstore, which cheaply and efficiently supports random writes. Data in the memstore is sorted in the same manner as data in a HFile. When the memstore accumulates enough data, the entire sorted set is written to a new HFile in HDFS. Completing one large write task is efficient and takes advantage to HDFS’ strengths. Although writing data to the memstore is efficient, it also introduces an element of risk: Information stored in memstore is stored in volatile memory, so if the system fails, all memstore information is lost. To help mitigate this risk, HBase saves updates in a write-ahead-log (WAL) before writing the information to memstore. In this way, if a region server fails, information that was stored in that server�s memstore can be recovered from its WAL. Note: By default, WAL is enabled, but the process of writing the WAL file to disk does consume some resources. WAL may be disabled, but this should only be done if the risk of data loss is not a concern. If you choose to disable WAL, consider implementing your own disaster recovery solution or be prepared for the possibility of data loss. The data in a WAL file is organized differently from HFile. WAL files contain a list of edits, with one edit representing a single put or delete. The edit includes information about the change and the region to which the change applies. Edits are written chronologically, so, for persistence, additions are appended to the end of the WAL file that is stored on disk. Because WAL files are ordered chronologically, there is never a need to write to a random place within the file. As WALs grow, they are eventually closed and a new, active WAL file is created to accept additional edits. This is called �rolling� the WAL file. Once a WAL file is rolled, no additional changes are made to the old file. By default, WAL file is rolled when its size is about 95% of the HDFS block size. You can configure the multiplier using parameter: “hbase.regionserver.logroll.multiplier”, and the block size using parameter: “hbase.regionserver.hlog.blocksize”. WAL file is also rolled periodically based on configured interval �hbase.regionserver.logroll.period�, an hour by default, even the WAL file size is smaller than the configured limit. Constraining�WAL file size facilitates efficient file replay if a recovery is required. This is especially important during replay of a region�s WAL file because while a file is being replayed, the corresponding region is not available. The intent is to eventually write all changes from each WAL file to disk and persist that content in an HFile. After this is done, the WAL file can be archived and it is eventually deleted by the LogCleaner daemon thread. �Note that WAL files serve as a protective measure. WAL files need only be replayed to recover updates that would otherwise be lost after a region server crash. A region server serves many regions, but does not have a WAL file for each region. Instead, one active WAL file is shared among all regions served by the region server. Because WAL files are rolled periodically, one region server may have many WAL files. Note that there is only one active WAL per region server at a given time. Assuming the default HBase root of �/hbase�, all the WAL files for a region server instance are stored under the same root folder, which is as follows: /hbase/.logs/&lt;host&gt;,
&lt;port&gt;,&lt;startcode&gt; For example: /hbase/.logs/srv.example.com,60020,1254173957298 WAL log files are named as follows: /hbase/.logs/&lt;host&gt;,
&lt;port&gt;,&lt;startcode&gt;/&lt;host&gt;%2C
&lt;port&gt;%2C&lt;startcode&gt;.&lt;timestamp&gt; For example: /hbase/.logs/srv.example.com,60020,1254173957298/srv.example.com%2C60020%2C1254173957298.1254173957495 Each edit in the WAL file has a unique sequence id. This id increases to preserve the order of edits. Whenever a log file is rolled, the next sequence id and the old file name are put in an in-memory map. This information is used to track the maximum sequence id of each WAL file so that we can easily figure out if a file can be archived at a later time when some memstore is flushed to disk. Edits and their sequence ids are unique within a region. Any time an edit is added to the WAL log, the edit�s sequence id is also recorded as the last sequence id written. When the memstore is flushed to disk, the last sequence id written for this region is cleared. If the last sequence id written to disk is the same as the maximum sequence id of a WAL file, it can be concluded that all edits in a WAL file for this region have been written to disk. If all edits for all regions in a WAL file have been written to disk, it is clear that no splitting or replaying will be required, and the WAL file can be archived. WAL file rolling and memstore flush are two separate actions, and don�t have to happen together. However, we don�t want to keep too many WAL files per region server so as to avoid time-consuming recovery in case a region server failure. Therefore, when a WAL file is rolled, HBase checks if there are too many WAL files, and decide what regions should be flushed so that some WAL files can be archived. In this post, we explain the HBase write path, which is how data in HBase is created and/or updated. Some important parts of it are: How a client locates a region server, Memstore which supports fast random writes, WAL files as the way to avoid data loss in case region server failures. We will talk about HFile formats, WAL file splitting and so on in subsequent posts.</snippet></document><document id="412"><title>The Elephant in the Enterprise</title><url>http://blog.cloudera.com/blog/2012/06/the-elephant-in-the-enterprise-hadoop-discussion-panel/</url><snippet>On Tuesday, June 12th The Churchill Club of Silicon Valley hosted a panel discussion on Hadoop’s evolution from an open-source project to becoming a standard component of today’s enterprise computing fabric. The lively and dynamic discussion was moderated by Cade Metz, Editor, Wired Enterprise. Panelists included: Michael Driscoll, CEO, Metamarkets Andrew Mendelsohn, SVP, Oracle Server Technologies Mike Olson, CEO, Cloudera Jay Parikh, VP Infrastructure Engineering, Facebook John Schroeder, CEO, MapR By the end of the evening, this much was clear: Hadoop has arrived as a required technology. Whether provisioned in the cloud, on-premise, or using a hybrid approach, companies need Hadoop to harness the massive data volumes flowing through their organizations today and into the future. The power of Hadoop is due in part to the way it changes the economics of large-scale computing and storage, but — even more importantly — because it gives organizations a new platform to discover, analyze and ultimately monetize all of their data. To learn more about how market leaders view Hadoop and the reasons for its accelerated adoption into the heart of the enterprise, view the above video.</snippet></document><document id="413"><title>The Singularity: Apache HBase Compatibility and Extensibility</title><url>http://blog.cloudera.com/blog/2012/06/the-singularity-hbase-compatibility-and-extensibility/</url><snippet>Overview One of the major features of the upcoming Apache HBase 0.96 release is improved support for compatibility and extensibility across different HBase versions. �This includes support for the following: Upgrading with no downtime: support for a rolling upgrade across a single major version (e.g. 0.96 to 0.98). �Because HBase has lacked this feature, moving to a new major version has required �lock step� upgrades: the entire cluster has to be shut down, the components upgraded, and then the cluster restarted. �This has been a major source of downtime and unavailability in HBase clusters. Accessing multiple HBase clusters running different versions: support for master/slave replication and multiple sharded clusters with individual clusters running different versions of HBase. �Like upgrading, this feature is supported across a single major version. To achieve this support, the remote procedure calls (RPC) and persistent data formats are being converted to use protobufs. �The latest version of trunk has 42 rpc calls and 130 data types defined via protobufs. �These definitions represent, for the first time, a clear specification of HBase client protocol, which should make it easier to write new clients (e.g. clients in other languages). The conversion to protobufs also allows HBase to become more extensible: developers can add additional parameters to RPC calls and additional fields to the data formats without breaking existing clients. �This has been a limitation of HBase in the past: bugs have lingered in older versions and improvements have been unable to be backported, because doing so would break compatibility. �See for example HBASE-5904 and HBASE-6009. �Thus, improved extensibility means a quicker cadence for fixing bugs and adding new features. Michael Stack, chair of the HBase PMC, has dubbed the conversion to protobufs and the release of 0.96 as �the Singularity� because it will not be backwards compatible, but once deployed, will be forward compatible with future versions of HBase. �Therefore, a �lock-step� upgrade will be necessary to move from a 0.92/0.94 release to 0.96. �No additional work is necessary to migrate data to the new formats: HBase 0.96 can read the old formats and will automatically migrate them when they are first read. �In addition, existing applications do not need to be rewritten or even recompiled; linking against a new version of the client will be sufficient to get applications working against 0.96.&gt; We expect wire compatibility to be incorporated into CDH in CDH5. Work so far Jimmy Xiang and I gave an overview of the wire compatibility work at the March HBase Meetup. �Slides are available here. �Since that time, the HBase community has been hard at work getting all the system components ready for the 0.96 release, which is currently targeted for a summer release. HBASE-5305 lists the individual subtasks. �Here are some highlights of the work that has been done so far: HBASE-5443�split the existing HRegionInterface into administrative (e.g. open/close region) and client (e.g. get/put/scan) operations and converted the RPC calls to use protobufs. HBASE-5453�converted the on-disk data storage formats to protobuf. �These formats include Reference files (used in region splitting), ClusterId files (used for replication), and HTableDescriptors (metadata information about tables). HBASE-5446�converted the data stored inside of ZooKeeper nodes to protobufs. �These nodes store, among other things, the location of the -ROOT- table as well as the location of the active and backup HMaster. HBASE-5445�(in review) and�HBASE-5444�converted the RPC functions of the HMaster to protobufs. �These functions include creating and deleting tables, handling RegionServer errors, and reporting cluster statistics to the client. Remaining Work While the majority of the RPC calls and file formats are now forward compatible, much work remains to ensure compatibility across the broad HBase feature set. �Coprocessors, filters, and replication, to name a few, still require work to be ready for �the singularity� of HBase 0.96. Want to help improve the compatibility and extensibility of HBase? �Get started by reading about Getting Involved in the HBase Reference Guide, send an e-mail to the development mailing list, or look into one of the subtasks of HBASE-5305. �Contributions are always welcomed!</snippet></document><document id="414"><title>CDH4 and Cloudera Enterprise 4.0 Now Available</title><url>http://blog.cloudera.com/blog/2012/06/cdh4-and-cloudera-enterprise-4-0-now-available/</url><snippet>I�m very pleased to announce the immediate General Availability of�CDH4 and Cloudera Manager 4 (part of the Cloudera Enterprise 4.0�subscription). �These releases are an exciting milestone for Cloudera�customers, Cloudera users and the open source community as a whole. Functionality Both CDH4 and Cloudera Manager 4 are chock full of new features.�Many new features will appeal to enterprises looking to move �more important workloads onto the Apache Hadoop platform. CDH4 includes high�availability for the filesystem, ability to support multiple�namespaces, Apache HBase table and column level security, improved�performance, HBase replication and greatly improved usability and browser support for the Hue web interface. Cloudera Manager 4 includes�multi-cluster and multi-version support, automation for high�availability and MapReduce2, multi-namespace support, cluster-wide�heatmaps, host monitoring and automated client configurations. Other features will appeal to developers and ISV�s looking to build�applications on top of CDH and / or Cloudera Manager. HBase�coprocessors enable the development of new kinds of real-time�applications. �MapReduce2 opens up Hadoop clusters to new data�processing frameworks other than MapReduce. There are new REST API�s�both for the Hadoop distributed filesystem and for Cloudera Manager. Quality Our investment in QA has reached a new level with the CDH4 and�Cloudera Manager 4 releases. Both systems have been subjected to�extensive functional, security and scale testing. �Both releases have�also been tested by customers, users and partners over the course of a public beta process where we�ve experienced thousands of downloads and�active feedback from our user list. Continuity The CDH4 and Cloudera Manager 4 releases build on our tradition of�delivering customers and users both innovation but also stability and�continuity. Counting updates, CDH4 is Cloudera�s 11th release of an�open source data management platform. It�s the 3rd year that we�ve�kept with our practice of annual releases and quarterly updates. Customers and users can upgrade to CDH4 from any of the prior updates to�CDH3 (u0, u1, u2, u3, u4) or future updates. Customers and users can use Cloudera Manager 4 with either CDH3 or CDH4�clusters (or some combination of the two) and Cloudera Manager 4 will�assist users in the CDH upgrade process. Ecosystem CDH has attracted a very large commercial ecosystem and this is one of�the great benefits customers and users get from CDH. �Many software and�systems partners have already certified their software against CDH4.�Among them are Datameer, Dell, Hstreaming, Informatica, Kapow,�Microstrategy, Netezza, Oracle, Revolution Analytics, Streambase, Teradata and Wibidata.�An additional two dozen more such partners are already in the process�of re-certifying against CDH4. More to come I firmly believe that the most exciting innovations of the Hadoop�ecosystem have yet to come. �We have an ambitious R&amp;D agenda set�before us for CDH5 and Cloudera Manager 4.5. �We plan to continue our�practice of delivering non-disruptive enhancements in quarterly�updates so you won�t have to wait another year to take advantage of�our work or that of the larger open source community. Both CDH4 and Cloudera Manager 4 (part of the Cloudera Enterprise 4.0�subscription) are available immediately. To download CDH4 as well a�free edition of Cloudera Manager 4, please visit our downloads page. Stay tuned and, as always, thank you for your support.</snippet></document><document id="415"><title>Hue 2.0 Packs New Features in a Refreshing UI</title><url>http://blog.cloudera.com/blog/2012/06/hue-2-0-packs-new-features/</url><snippet>Hue 2.0.1�has just been released. 2.0.1 represents major improvement on top of the Hue 1.x series. To list a few key new features: Frontend has been re-implemented as full screen pages. Hue supports LDAP (OpenLDAP and Active Directory). Hue can be configured to authenticate against LDAP. Additionally, Hue can import users and groups from LDAP, and refresh group membership from LDAP. Hue supports per-application authorization. Administrators can grant or limit group access to applications. Hue has a new Shell application, which allows access to the HBase shell, Pig shell, and more. The Job Designer now submits jobs through Oozie, which is more secured. Please see the release notes for a complete reference. � A New Frontend In particular, I am really excited about the new frontend. The Hue 1.x frontend renders application UIs via Javascript in desktop-like windows, which coexist in a single browser window. This desktop-like frontend turns out to be hard to maintain, as well as inconvenient and inflexible for third party application developers. In Hue 2.0, each application gets its own browser window or tab:� � For end users, this means that every page view has its own URL and can be bookmarked. Users also have better control of the windowing behaviours (maximize, minimize, alt-tab) and browsing history. And for enterprise users, Hue 2.0 works on Internet Explorer, which is plagued by memory reclamation issues with Hue 1.x. For third party application developers, this greatly reduces the complexity of writing an application frontend. Developers also have full control of the rendered HTML, and can therefore employ their favourite Javascript and CSS libraries (jQuery, Bootstrap,�knockout.js, Highcharts, etc.). Hue 2.0 itself uses jQuery and Bootstrap extensively, which has sped up our own frontend development cycles. Compatibility Applications written for Hue 1.x are not compatible with Hue 2.0. Fortunately, the transition is straightforward and is documented in the SDK guide.�For example, Hue 1.x provides an “HtmlTable” widget that supports banding, column sorting and more. In Hue 2.0, the same functionality is provided by the�DataTables. Hue 2.0.1 is compatible with (and included in) CDH4. It works with HA NameNode, since it communicates with HDFS via the HttpFS REST API. It can submit jobs to YARN, since job submission is executed via Oozie. But it cannot browse any YARN jobs. It supports Hive 0.8.1. Acknowledgement This release is possible thanks to the contributions from the team.�Your feedback is greatly appreciated. Drop us a note in our user list.</snippet></document><document id="416"><title>Online Apache HBase Backups with CopyTable</title><url>http://blog.cloudera.com/blog/2012/06/online-hbase-backups-with-copytable-2/</url><snippet>CopyTable is a simple Apache HBase utility that, unsurprisingly, can be used for copying individual tables within an HBase cluster or from one HBase cluster to another. In this blog post, we�ll talk about what this tool is, why you would want to use it, how to use it, and some common configuration caveats. Use cases: CopyTable is at its core an Apache Hadoop MapReduce job that uses the standard HBase Scan read-path interface to read records from an individual table and writes them to another table (possibly on a separate cluster) using the standard HBase Put write-path interface. It can be used for many purposes: Internal copy of a table (Poor man�s snapshot) Remote HBase instance backup Incremental HBase table copies Partial HBase table copies and HBase table schema changes Assumptions and limitations: The CopyTable tool has some basic assumptions and limitations. First, if being used in the multi-cluster situation, both clusters must be online and the target instance needs to have the target table present with the same column families defined as the source table. Since the tool uses standards scans and puts, the target cluster doesn’t have to have the same number of nodes or regions.� In fact, it can have different numbers of tables, different numbers of region servers, and could have completely different region split boundaries. Since we are copying entire tables, you can use performance optimization settings like setting larger scanner caching values for more efficiency. Using the put interface also means that copies can be made between clusters of different minor versions. (0.90.4 -&gt; 0.90.6, CDH3u3 -&gt; CDH3u4) or versions that are wire compatible (0.92.1 -&gt; 0.94.0). Finally, HBase only provides row-level ACID guarantees; this means while a CopyTable is going on, newly inserted or updated rows may occur and these concurrent edits will either be completely included or completely excluded. While rows will be consistent, there is no guarantees about the consistency, causality, or order of puts on the other rows. Internal copy of a table (Poor man�s snapshot) Versions of HBase up to and including the most recent 0.94.x versions do not support table snapshotting. Despite HBase�s ACID limitations, CopyTable can be used as a naive snapshotting mechanism that makes a physical copy of a particular table. Let�s say that we have a table, tableOrig with column-families cf1 and cf2. We want to copy all its data to tableCopy. We need to first create tableCopy with the same column families: srcCluster$ echo "create 'tableOrig', 'cf1', 'cf2'" | hbase shell We can then create and copy the table with a new name on the same HBase instance: srcCluster$ hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name=tableCopy tableOrig This starts an MR job that will copy the data. Remote HBase instance backup Let�s say we want to copy data to another cluster. This could be a one-off backup, a periodic job or could be for bootstrapping for cross-cluster replication. In this example, we�ll have two separate clusters: srcCluster and dstCluster. In this multi-cluster case, CopyTable is a push process — your source will be the HBase instance your current hbase-site.xml refers to and the added arguments point to the destination cluster and table. This also assumes that all of the MR TaskTrackers can access all the HBase and ZK nodes in the destination cluster. This mechanism for configuration also means that you could run this as a job on a remote cluster by overriding the hbase/mr configs to use settings from any accessible remote cluster and specify the ZK nodes in the destination cluster. This could be useful if you wanted to copy data from an HBase cluster with lower SLAs and didn�t want to run MR jobs on them directly. You will use the the –peer.adr setting to specify the destination cluster�s ZK ensemble (e.g. the cluster you are copying to). For this we need the ZK quorum�s IP and port as well as the HBase root ZK node for our HBase instance. Let�s say one of these machine is srcClusterZK (listed in hbase.zookeeper.quorum) and that we are using the default zk client port 2181 (hbase.zookeeper.property.clientPort) and the default ZK znode parent /hbase (zookeeper.znode.parent). (Note: If you had two HBase instances using the same ZK, you�d need a different zookeeper.znode.parent for each cluster. # create new tableOrig on destination cluster
dstCluster$ echo "create 'tableOrig', 'cf1', 'cf2'" | hbase shell
# on source cluster run copy table with destination ZK quorum specified using --peer.adr
# WARNING: In older versions, you are not alerted about any typo in these arguments!
srcCluster$ hbase org.apache.hadoop.hbase.mapreduce.CopyTable --peer.adr=dstClusterZK:2181:/hbase tableOrig Note that you can use the –new.name argument with the –peer.adr to copy to a differently named table on the dstCluster. # create new tableCopy on destination cluster
dstCluster$ echo "create 'tableCopy', 'cf1', 'cf2'" | hbase shell
# on source cluster run copy table with destination --peer.adr and --new.name arguments.
srcCluster$ hbase org.apache.hadoop.hbase.mapreduce.CopyTable --peer.adr=dstClusterZK:2181:/hbase --new.name=tableCopy tableOrig This will copy data from tableOrig on the srcCluster to the dstCluster�s tableCopy table. Incremental HBase table copies Once you have a copy of a table on a destination cluster, how do you do copy new data that is later written to the source cluster? Naively, you could run the CopyTable job again and copy over the entire table. However, CopyTable provides a more efficient incremental copy mechanism that just copies the updated rows from the srcCluster to the backup dstCluster specified in a window of time. Thus, after the initial copy, you could then have a periodic cron job that copies data from only the previous hour from srcCluster to the dstCuster. This is done by specifying the –starttime and –endtime arguments. Times are specified as decimal milliseconds since unix epoch time. # WARNING: In older versions, you are not alerted about any typo in these arguments!
# copy from beginning of time until timeEnd�
# NOTE: Must include start time for end time to be respected. start time cannot be 0.
srcCluster$ hbase org.apache.hadoop.hbase.mapreduce.CopyTable ... --starttime=1 --endtime=timeEnd ...
# Copy from starting from and including timeStart until the end of time.
srcCluster$ hbase org.apache.hadoop.hbase.mapreduce.CopyTable ... --starttime=timeStart ...
# Copy entries rows with start time1 including time1 and ending at timeStart excluding timeEnd.
srcCluster$ hbase org.apache.hadoop.hbase.mapreduce.CopyTable ... --starttime=timestart --endtime=timeEnd Partial HBase table copies and HBase table schema changes By default, CopyTable will copy all column families from matching rows. CopyTable provides options for only copying data from specific column-families. This could be useful for copying original source data and excluding derived data column families that are added by follow on processing. By adding these arguments we only copy data from the specified column families. –families=srcCf1 –families=srcCf1,srcCf2 Starting from 0.92.0 you can copy while changing the column family name: –families=srcCf1:dstCf1 copy from srcCf1 to dstCf1� –families=srcCf1:dstCf1,dstCf2,srcCf3:dstCf3 copy from srcCf1 to destCf1, copy dstCf2 to dstCf2 (no rename), and srcCf3 to dstCf3 Please note that dstCf* must be present in the dstCluster table! Starting from 0.94.0 new options are offered to copy delete markers and to include a limited number of overwritten versions. Previously, if a row is deleted in the source cluster, the delete would not be copied — instead that a stale version of that row would remain in the destination cluster. This takes advantage of some of the 0.94.0 release�s advanced features. –versions=vers where vers is the number of cell versions to copy (default is 1 aka the latest only) –all.cells� also copy delete markers and deleted cells Common Pitfalls The HBase client in the 0.90.x, 0.92.x, and 0.94.x versions always use zoo.cfg if it is in the classpath, even if an hbase-site.xml file specifies other ZooKeeper quorum configuration settings. This �feature� causes a problem common in CDH3 HBase because its packages default to including a directory where zoo.cfg lives in HBase�s classpath. This can and has lead to frustration when trying to use CopyTable (HBASE-4614). The workaround for this is to exclude the zoo.cfg file from your HBase�s classpath and to specify ZooKeeper configuration properties in your hbase-site.xml file. http://hbase.apache.org/book.html#zookeeper Conclusion CopyTable provides simple but effective disaster recovery insurance for HBase 0.90.x (CDH3) deployments. In conjunction with the replication feature found and supported in CDH4�s HBase 0.92.x based HBase, CopyTable�s incremental features become less valuable but its core functionality is important for bootstrapping a replicated table. While more advanced features such as HBase snapshots (HBASE-50) may aid with disaster recovery when it gets implemented, CopyTable will still be a useful tool for the HBase administrator.</snippet></document><document id="417"><title>Cloudera Manager 3.7.6 released!</title><url>http://blog.cloudera.com/blog/2012/06/cloudera-manager-3-7-6-released/</url><snippet>We are pleased to announce that Cloudera Manager 3.7.6 is now available! The most notable updates in this release are: Support for multiple Hue service instances Separating RPC queue and processing time metrics for HDFS Performance tuning of the Resource Manager components Several bug fixes and performance improvements The detailed Cloudera Manager 3.7.6 release notes are available at: https://ccp.cloudera.com/display/ENT/Cloudera+Manager+3.7.x+Release+Notes Cloudera Manager 3.7.6 is available to download from: https://ccp.cloudera.com/display/SUPPORT/Downloads</snippet></document><document id="418"><title>NameNode Recovery Tools for the Hadoop Distributed File System</title><url>http://blog.cloudera.com/blog/2012/05/namenode-recovery-tools-for-the-hadoop-distributed-file-system/</url><snippet>Warning: The procedure described below can cause data loss. Contact Cloudera Support before attempting it. Most system administrators have had to deal with a bad hard disk at some point. One moment, the hard disk is a mechanical marvel; the next, it is an expensive paperweight. The HDFS (Hadoop Distributed File System) community has been steadily working to diminish the impact of disk failures on overall system availability. In this article, I’m going to be mostly talking about how to minimize the impact of hard disk failures on the NameNode. The NameNode’s function is to store metadata. In filesystem jargon, metadata is “data about data”– things like the owners of files, permission bits, and so forth. HDFS stores its metadata on the NameNode in two main places: the FSImage, and the edit log. Edit Log Failover It is a good practice to configure your NameNode to store multiple copies of its metadata. By storing two copies of the edit log and FSImage, on two separate hard disks, a good system administrator can avoid bringing down the NameNode if one of those disks fails. During the NameNode’s startup process, it reads both the FSImage and the edit log. But what if the first place it looks is unreadable, because of a hardware problem or disk corruption? Previously, the NameNode would abort the startup process if it encountered an error while reading an edit log. The administrator would have to remove the corrupt edit log and restart the NameNode. With edit log failover, the NameNode will mark that location as failed automatically, and continue trying the other locations. More Robust end-of-file Validation When it’s stored on-disk, the edit log file contains padding at the end. Because we have padding at the end of the file, we can’t simply keep reading the edit log until we get an end-of-file (EOF) condition. Instead, we have to rely on other clues to know where the file ends. Formerly, the clue we relied on was finding an OP_INVALID opcode. As soon as we read an OP_INVALID opcode, we would immediately assume that there was nothing more to read. However, this is not the most robust way to determine where a file ends. Because an OP_INVALID opcode is a single byte, the likelihood that random corruption could produce an early EOF was unacceptably high. How can we do better? Well, in most cases, we know what transaction ID an edit log ends on. So we can simply verify that the last edit log operation we read from the file matched this. In cases where we don’t know the end transaction ID, we can verify that the padding at the end of the file contains only padding bytes. This makes the edit log code even more robust. HDFS FSCK When your local ext3 or ext4 filesystem has become corrupted, the fsck command can usually repair it. Fsck is an offline process which examines on-disk structures and usually offers to fix them if they are damaged. HDFS has its own fsck command, which you can access by running �hdfs fsck.� Similar to the ext3 fsck, HDFS fsck determines which files contain corrupt blocks, and gives you options about how to fix them. However, HDFS fsck only operates on data, not metadata. On a local filesystem, this distinction is irrelevant, because data and metadata are stored in the same place. However, for HDFS, metadata is stored on the NameNode, whereas data is stored on the DataNodes. Manual NameNode Metadata Recovery When properly configured, HDFS is much more robust against metadata corruption than a local filesystem, because it stores multiple copies of everything. However, because HDFS is a truly robust system, we added the capability for an administrator to recover a partial or corrupted edit log. This new functionality is called manual NameNode recovery. Similar to fsck, NameNode recovery is an offline process. An administrator can run NameNode recovery to recover a corrupted edit log. This can be very helpful for getting corrupted filesystems on their feet again. NameNode Recovery in Action Let’s test out recovery mode. To activate recovery mode, you start the NameNode with the -recover flag, like so: ./bin/hadoop namenode -recover At this point, the NameNode will ask you whether you want to continue. You have selected Metadata Recovery mode.  This mode is intended to recover
lost metadata on a corrupt filesystem.  Metadata recovery mode often
permanently deletes data from your HDFS filesystem.  Please back up your edit
log and fsimage before trying this!

Are you ready to proceed? (Y/N)
 (Y or N) Once you answer yes, the recovery process will read as much of the edit log as possible. When there is an error or an ambiguity, it will ask you how to proceed. In this example, we encounter an error when trying to read transaction ID 3: 11:10:41,443 ERROR FSImage:147 - Error replaying edit log at offset 71.  Expected transaction ID was 3
Recent opcode offsets: 17 71
org.apache.hadoop.fs.ChecksumException: Transaction is corrupt. Calculated checksum is -1642375052 but read checksum -6897
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$Reader.validateChecksum(FSEditLogOp.java:2356)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$Reader.decodeOp(FSEditLogOp.java:2341)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$Reader.readOp(FSEditLogOp.java:2247)
        at org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.nextOp(EditLogFileInputStream.java:110)
        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:74)
        at org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:140)
        at org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:74)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:138)
        at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:683)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:639)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:247)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:498)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:390)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:354)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.doRecovery(NameNode.java:1033)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1103)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1164)
11:10:41,444 ERROR MetaRecoveryContext:96 - We failed to read txId 3
11:10:41,444  INFO MetaRecoveryContext:64 -
Enter 'c' to continue, skipping the bad section in the log
Enter 's' to stop reading the edit log here, abandoning any later edits
Enter 'q' to quit without saving
Enter 'a' to always select the first choice in the future without prompting. (c/s/q/a) There are four options here– continue, stop, quit, and always Continue will try to skip over the bad section in the log. If the problem is just a stray byte or two, or a few bad sectors, this option will let you bypass it. Stop stops reading the edit log and saves the current contents of the FSImage. In this case, all the edits that still haven’t been read will be permanently lost. Quit exits the NameNode process without saving a new FSImage. Always selects continue, and suppresses this prompt in the future. Once you select always, Recovery mode will stop prompting you and always select continue in the future. In this case, I’m going to select continue, because I think there may be more edits following the corrupt region that I want to salvage. The next prompt informs me that an edit is missing– which is to be expected, considering the previous one was corrupt. 12:22:38,829  INFO MetaRecoveryContext:105 - Continuing.
12:22:38,860 ERROR MetaRecoveryContext:96 - There appears to be a gap in the edit log.  We expected txid 3, but got txid 4.
12:22:38,860  INFO MetaRecoveryContext:64 -
Enter 'c' to continue, ignoring missing  transaction IDs
Enter 's' to stop reading the edit log here, abandoning any later edits
Enter 'q' to quit without saving
Enter 'a' to always select the first choice in the future without prompting. (c/s/q/a) Again I enter ‘c’ to continue. Finally, recovery completes. 12:22:42,205  INFO MetaRecoveryContext:105 - Continuing.
12:22:42,207  INFO FSEditLogLoader:199 - replaying edit log: 4/5 transactions completed. (80%)
12:22:42,208  INFO FSImage:95 - Edits file /opt/hadoop/run4/name1/current/edits_0000000000000000001-0000000000000000005 of size 1048580 edits # 4 loaded in 4 seconds.
12:22:42,212  INFO FSImage:504 - Saving image file /opt/hadoop/run4/name2/current/fsimage.ckpt_0000000000000000005 using no compression
12:22:42,213  INFO FSImage:504 - Saving image file /opt/hadoop/run4/name1/current/fsimage.ckpt_0000000000000000005 using no compression Then, the NameNode exits. Now, I can restart the NameNode and resume normal operation. The corruption has been fixed, although we have lost a small amount of metadata. When Manual Recovery is the Best Choice If there is another valid copy of the edit log somewhere else, it is preferrable to use that copy rather than trying to recover the corrupted copy. This is a case where high availability can help a lot. If there is a standby NameNode ready to take over, there should be no need to recover the edit log on the primary. Manual recovery is a good choice when there is no other copy of the edit log available. Conclusion The best recovery process is the one that you never need to do. High availability, combined with edit log failover, should mean that manual recovery is almost never necessary. However, it’s good to know that HDFS has tools to deal with whatever comes up. Recovery mode will be available in CDH4. A more limited version of recovery mode without edit log failover will be available in CDH3.</snippet></document><document id="419"><title>Apache MRUnit Is Now A Top Level Project</title><url>http://blog.cloudera.com/blog/2012/05/apache-mrunit-is-now-a-top-level-project/</url><snippet>This posted was originally posted to the Apache Software Foundation MRUnit blog. The Apache MRUnit team has graduated from the Apache Incubator to an Apache TLP (Top Level Project)! MRUnit is a Java library that helps developers unit test Apache Hadoop MapReduce jobs. Unit testing is a technique for improving project quality and reducing overall costs by writing a small amount of code that can automatically verify the software you write performs as intended. This is considered a best practice in software development since it helps identify defects early, before they're deployed to a production system. In its monthly meeting in May of 2012, the board of Apache Software Foundation (ASF) resolved to grant a Top-Level Project status to Apache MRUnit, thus graduating it from the Incubator. This is a significant milestone in the life of MRUnit, which has come a long way since its inception as a Hadoop Contrib project in HADOOP-5518 contributed by Aaron Kimball. May 2012 MRUnit graduates from the Incubator to become a TLP May 2012 Version 0.9.0-incubating released. April 2012 Dave Beech added as a new committer. April 2012 Jarek Jarcec Cecho added as a new committer. April 2012 New website created using the CMS. March 2012 Version 0.8.1-incubating released. March 2012 Jim Donofrio added as a new committer. Feburary 2012 Version 0.8.0-incubating released. November 2011 Version 0.5.0-incubating released. October 2011 Brock Noland added as a new committer. March 2011 Project enters incubation. April 2009 Doug Cutting commits Aaron's patch to Hadoop March 2009 Aaron Kimball contributes MRunit to Hadoop as a contrib project Below is the graduation resolution: X. Establish the Apache MRUnit Project

WHEREAS, the Board of Directors deems it to be in the best
interests of the Foundation and consistent with the
Foundation's purpose to establish a Project Management
Committee charged with the creation and maintenance of
open-source software related to unit testing Apache Hadoop map
reduce jobs for distribution at no charge to the public.

NOW, THEREFORE, BE IT RESOLVED, that a Project Management
Committee (PMC), to be known as the "Apache MRUnit Project",
be and hereby is established pursuant to Bylaws of the
Foundation; and be it further

RESOLVED, that the Apache MRUnit Project be and hereby is
responsible for the creation and maintenance of software
related to unit testing Apache Hadoop map reduce jobs;
and be it further

RESOLVED, that the office of "Vice President, Apache MRUnit" be
and hereby is created, the person holding such office to
serve at the direction of the Board of Directors as the chair
of the Apache MRUnit Project, and to have primary responsibility
for management of the projects within the scope of
responsibility of the Apache MRUnit Project; and be it further

RESOLVED, that the persons listed immediately below be and
hereby are appointed to serve as the initial members of the
Apache MRUnit Project:

* Brock Noland - brock@apache.org
* Patrick Hunt - phunt@apache.org
* Nigel Daley - nigel@apache.org
* Eric Sammer - esammer@apache.org
* Aaron Kimball - kimballa@apache.org
* Konstantin Boudnik - cos@apache.org
* Garrett Wu - gwu@apache.org
* Jim Donofrio - jdonofrio@apache.org
* Jarek Jarcec Cecho - jarcec@apache.org
* Dave Beech - dbeech@apache.org

NOW, THEREFORE, BE IT FURTHER RESOLVED, that Brock Noland
be appointed to the office of Vice President, Apache MRUnit, to
serve in accordance with and subject to the direction of the
Board of Directors and the Bylaws of the Foundation until
death, resignation, retirement, removal or disqualification,
or until a successor is appointed; and be it further

RESOLVED, that the initial Apache MRUnit PMC be and hereby is
tasked with the creation of a set of bylaws intended to
encourage open development and increased participation in the
Apache MRUnit Project; and be it further

RESOLVED, that the Apache MRUnit Project be and hereby
is tasked with the migration and rationalization of the Apache
Incubator MRUnit podling; and be it further

RESOLVED, that all responsibilities pertaining to the Apache
Incubator MRUnit podling encumbered upon the Apache Incubator
Project are hereafter discharged.</snippet></document><document id="420"><title>Apache HBase 0.94 is now released</title><url>http://blog.cloudera.com/blog/2012/05/apache-hbase-0-94-is-now-released/</url><snippet>Apache HBase 0.94.0 has been released! This is the first major release since the January 22nd HBase 0.92 release. In the HBase 0.94.0 release the main focuses were on performance enhancements and the addition of new features (Also, several major bug fixes). Performance Related JIRAs Below are a few of the important performance related JIRAs: Read Caching improvements: HDFS stores data in one block file and its corresponding metadata (checksum) in another block file. This means that every read into the HBase block cache may consume up to two disk ops, one to the datafile and one to the checksum file. HBASE-5074: “Support checksums in HBase block cache” adds a block level checksum in the HFile itself in order to avoid one disk op,� boosting up the read performance. This feature is enabled by default. Seek optimizations: Till now, if there were several StoreFiles for a column family in a region, HBase would seek in each such files and merge the results, even if the row/column we are looking for is in the most recent file.��HBASE-4465: “Lazy Seek optimization of StoreFile Scanners” optimizes scanner reads to read the most recent StoreFile first by lazily seeking the StoreFiles. This is achieved by introducing a fake keyvalue with its timestamp equal to the maximum timestamp present in the particular StoreFile. Thus, a disk seek is avoided until the KeyValueScanner for a StoreFile is bubbled up the heap, implying a need to do a real read operation.� This should provide a significant read performance boost, especially for IncrementColumnValue operations where we care only for latest value. This feature is enabled by default. Write to WAL optimizations: HBase write throughput is upper bounded by the write rate of WAL where the log is replicated to a number of datanodes, depending on the replication factor. HBASE-4608: “HLog Compression” adds a custom dictionary-based compression of HLogs for faster replication on HDFS datanodes, thus improving overall write rate for HBase. This feature is considered experimental and is off by default. New Feature Related JIRAs Here is a list of some of the important JIRAs related to adding new features: More powerful first aid box: The previous HBck tool did a good job of fixing inconsistencies related to region assignments but lacked some basic features like fixing orphaned regions, region holes, overlapping regions, etc. HBASE-5128: “Uber hbck”, adds these missing features to the first aid box. Simplified Region Sizing: Deciding a region size is always tricky as it varies on a number of dynamic parameters such as data size, cluster size, workload, etc.�HBASE-4365: “Heuristic for Region size” adds a heuristic where it increases the split size threshold of a table region as the data grows, thus limiting the number of region splits. Smarter transaction semantics: Though HBase supports single row level transaction, if there are a number of updates (Puts/Deletes) to an individual row, it will lock the row for each of these operations. HBASE-3584: “Atomic Put &amp; Delete in a single transaction” enhances the HBase single row locking semantics by allowing Puts and Deletes on a row to be executed in a single call. This feature is on by default. This major release has a number of new features and bug fixes; a total of 397 resolved JIRAs with 140 enhancements and 180 bug fixes. It is compatible with 0.92. This opens up a window of opportunity to backport some of the cool features back in CDH4, which is based on the 0.92 branch. Acknowledgements Thanks to everyone who contributed to this release and a hat tip to Lars Hofhansl of Salesforce for being the release manager.</snippet></document><document id="421"><title>Meet the Presenter: Todd Lipcon</title><url>http://blog.cloudera.com/blog/2012/05/meet-the-presenter-todd-lipcon/</url><snippet>Today�s interview features Todd Lipcon, software engineer for Cloudera. Todd will be presenting Optimizing MapReduce Job Performance at Hadoop Summit. Question: Tell us about your current role and how you interact with Apache Hadoop? Todd: I�m a software engineer on Cloudera�s platform engineering team, where I spend most of my time contributing code to open source projects like Apache Hadoop and Apache HBase. Most recently I�ve been implementing the automatic HA failover feature in Hadoop 2.0, but I�ve also spent a lot of time working on understanding and improving performance of the Hadoop stack. Question: Tell us about your Hadoop Summit presentation? Todd: At this year�s summit, I will be presenting about the internals of MapReduce and how you can tune your MapReduce jobs for optimal performance. A lot of developers see MapReduce as a black box, but looking inside that box can help you understand where you might have bottlenecks or easy opportunities to improve performance by changing a few configuration parameters. Question: What do you expect will be the key takeaway for folks attending your session? Todd: I hope attendees will walk away with a better understanding of each of the phases of MapReduce task execution, and a few key configuration parameters they can play with to get better performance without changing their code. Question: What other presentations are you most looking forward to attending? Todd: I�m really looking forward to Josh Wills� talk on BranchReduce: Distributed Branch-and-Bound on YARN. There are a lot of optimization problems which can be solved by branch-and-bound approaches, and it�s only recently with the introduction of YARN that these types of algorithms can be efficiently built on Hadoop. Not only this a fresh topic, Josh is also an entertaining speaker!</snippet></document><document id="422"><title>Cloudera Manager 4.0 Beta released</title><url>http://blog.cloudera.com/blog/2012/05/cloudera-manager-4-0-beta-released/</url><snippet>We’re happy to announce the Beta release of Cloudera Manager 4.0.� This version of Cloudera Manager includes support for CDH4 Beta2�and several new features for both the Free edition and the Enterprise edition. Please try it out�and send your comments to beta@cloudera.com.�As always, we look forward to your feedback.�</snippet></document><document id="423"><title>CDH3 update 4 is now available</title><url>http://blog.cloudera.com/blog/2012/05/cdh3-update-4-is-now-available/</url><snippet>We are happy to officially announce the general availability of CDH3 update 4. This update consists primarily of reliability enhancements as well as a number of minor improvements. First, there have been a few notable HBase updates. In this release, we’ve upgraded Apache HBase to upstream version 0.90.6, improving system robustness and availability. Also, some of the recent hbck changes were incorporated to better detect and handle various types of corruptions. Lastly,�HDFS append support is now disabled by default in this release as it is no longer needed for HBase. Please see the CDH3 Known Issues and Workarounds page�for details. In addition to the HBase updates, CDH3 update 4 also includes the latest release of Apache Flume (incubating) – version 1.1.0. A detailed description of what it brings to the table is found�in a previous Cloudera blog post describing its architecture. Please note that we will continue to ship Flume 0.9.4 as well. More information about how to download or upgrade to CDH3 update 4 can be found in the CDH packaging information webpage. �The patches and JIRAs for update 4 are described in the changes files that can be found in the CDH3 downloads area. �Additional details are available in the�CDH3 release notes. Feedback is always welcome, so please email your thoughts and suggestions to cdh-user@cloudera.com.</snippet></document><document id="424"><title>Meet the Presenters: Aaron Myers from Cloudera and Suresh Srinivas from Hortonworks</title><url>http://blog.cloudera.com/blog/2012/05/meet-the-presenters-aaron-myers-from-cloudera-and-suresh-srinivas-from-hortonworks/</url><snippet>This was originally posted on the Hadoop Summit 2012 blog. Today�s �Meet the Presenters� interview features two speakers: Aaron Myers from Cloudera and Suresh Srinivas from Hortonworks. Aaron and Suresh will be presenting on�HDFS NameNode High Availability, one of the hottest topics in the Apache Hadoop space today. Question:�Tell us about your current role and how you interact with Apache Hadoop? Aaron:�I work full-time developing Hadoop and supporting Hadoop�s many users. My efforts are primarily focused on HDFS and Hadoop�s security infrastructure. Suresh:�I have been working on Hadoop for about 4 years. Currently I am on HDFS full-time, with focus on improving reliability, scalability and developing enterprise features. I also work on expanding Apache Hadoop APIs and interfaces to enable new use cases and simplify integration of other solutions with HDFS. Question: Tell us about your Hadoop Summit presentation? Suresh:�The HDFS NameNode is a robust and reliable service as seen in practice in production at Yahoo! and other organizations. However, the NameNode does not have automatic failover support. A hot failover solution called HA NameNode is currently under active development (HDFS-1623). This talk will cover the architecture, design and setup. We will also discuss the future direction for HA NameNode. Question: What do you expect will be the key takeaway for folks attending your session?� Aaron:�Because we will be sharing best practices and architectural details, we�expect attendees to walk away with a good understanding of what�s required to deploy and operate a highly available HDFS NameNode. Question: What are you most looking forward to at Hadoop Summit? Aaron:�Chatting in-person with the Hadoop developers and other community members who I interact with frequently, but don�t get to see often. Suresh:�Interacting with the community and learning from them their Hadoop experiences. I�m also interested in getting feedback on things we can improve and important new features desired by the community. Question:�What other presentations are you most looking forward to attending? Aaron: �I�accidentally the Namenode: Hadoop Distributed Filesystem Reliability and Durability at Facebook�by Andrew Ryan Optimizing MapReduce Job Performance�by Todd Lipcon �Suresh: Like Aaron, �I�accidentally the Namenode: Hadoop Distributed Filesystem Reliability and Durability at Facebook�by�Andrew Ryan Apache Hadoop and Virtual Machines�by Richard McDougall and Sanjay Radia</snippet></document><document id="425"><title>Announcing Apache Hive 0.9.0</title><url>http://blog.cloudera.com/blog/2012/05/announcing-apache-hive-0-9-0/</url><snippet>This past Monday marked the official release of Apache Hive 0.9.0. Users�interested in taking�this release of Hive�for a spin can download a copy from the�Apache archive site.�The following�post is a�quick�summary of�new features and improvements�users can expect to find in this update of the popular�data warehousing system for Hadoop. The 0.9.0 release continues the trend of extending Hive’s SQL support. Hive now understands the�BETWEEN�operator�and the�NULL-safe equality operator, plus�several new user defined functions�(UDF)�have�now�been added. New UDFs include�printf(),�sort_array(),�and�java_method().�Also, the�concat_ws()�function has been modified to support input parameters consisting of arrays of strings. This�Hive�release also includes several significant improvements to the query compiler and execution engine.�HIVE-2642�improved�Hive’s ability to optimize UNION queries,�HIVE-2881�made the�the map-side JOIN algorithm more efficient, and Hive’s ability to generate optimized execution plans for queries that contain multiple GROUP BY clauses was significantly improved in�HIVE-2621. HBase users will also be interested in several improvements to Hive’s HBase StorageHandler,�mainly: The ability to access primitive types stored in binary format�within HBase (HIVE-1634), And support for filter-pushdown for keys (HIVE-2861,�HIVE-2815,�HIVE-2771). Finally, I’d like to commend Ashutosh Chauhan on a job well done as the release manager for Hive 0.9.0. Ashutosh became a Hive committer six months ago and since then has had a significant impact on the project by doing lots of code reviews, helping answer questions on the mailing list, and through continued patch submissions. He did a great job as a first-time release manager, and I hope that he will reprise this role in the future!</snippet></document><document id="426"><title>How Treato Analyzes Health-related Social Media Big Data with Hadoop and HBase</title><url>http://blog.cloudera.com/blog/2012/05/treato-analyzes-health-related-big-data-with-hadoop/</url><snippet>This is a guest post by Assaf Yardeni, Head of R&amp;D for Treato, an online social healthcare solution, headquartered in Israel. Three years ago I joined Treato, a social healthcare analysis firm to help treato.com scale up to its present capability. Treato is a new source for healthcare information where health-related user generated content (UGC) from the Internet is aggregated and organized into usable insights for patients, physicians and other healthcare professionals. With oceans of patient-written health-related information available on the Web, and more being published each day, Treato needs to be able to collect and process vast amounts of data � Treato is Big Data par excellence, and my job has been to bring Treato to this stage. Before the Hadoop era When I arrived at Treato, the team had already developed a Microsoft-based prototype that could organize a limited amount of health-related UGC into relevant insights, as a proof of concept. The system would: Crawl the Web and fetch raw HTML sources, Extract the user-generated content (i.e. user�s posts) out of the raw sources, Extract concepts from the posts and index them, Execute semantic analysis on the posts using natural language processing (NLP) algorithms And calculate statistics. The prototype was able to prove the initial hypothesis that relevant medical insights can be found in social media, you just have to know how to analyze it. We collected data from dozens of websites and individual social media posts in the tens of millions. We had a handful of text analysis algorithms and could only process a couple million posts per day, but the results were impressive. We found that we were able to identify side effects through social media long before initial FDA or pharmaceutical companies issued warnings about them. For example, when we looked at the discussions about Singulair � an asthma medication � we found that almost half of the user generated content discussed mental disorders. When we looked back through the historical data, we learned that this would have been identifiable in our data four years before the official warning. In order to gain even more health-related insights, we knew we needed a solution that could crawl and process a larger quantity of data � larger by an order of magnitude. That was the point at which Web scale joined the game. In order to collect massive amounts of posts, we needed to add thousands of data sources. And, of course, all the data we collected would need to be analyzed. Dealing with a few dozen websites was difficult and costly. But we were able to scale up our Microsoft code to handle collection from a several hundred sites, and could process around 250 million posts. We were running a few old IBM boxes that did the collection work and had developed a job manager that�administered crawling and fetching tasks. Different servers ran the indexing and the stats calculations, and we had developed a distributed job manager to direct task executions. Different servers were used for serving the data. We didn’t have any storage solution, and all of the boxes worked with local drives. Besides the fact that administering the process was hell, it was expensive in terms of CPU, network and input/output (I/O); e.g., after each stage, the data needed to be moved to a different server for the next stage. In addition, our job manager didn�t deal with failures; every time a task failed we needed to handle it manually. Needless to say, supporting collection and analysis of�thousands of websites would have been impossible using this�approach. Looking at scale In the beginning of 2010, we started searching for solutions that could support the capabilities we wanted. The requirements included: Reliable and scalable storage. Reliable and scalable processing infrastructure. Search engine capabilities (for retrieving posts) with high availability (HA). Scalable real-time store for�retrieving�stats, with HA. We wanted the ability to periodically reprocess the data in a timely manner, so new algorithms or other analysis improvements would take effect on all historical data. We wanted to know how much it costs to deal with X number of posts, and to be able to scale according to this formula. We wanted a technology and architecture that would scale with the business. We searched for answers to questions such as: “How does Google do it?� and it didn’t take too long to find Google’s papers, documentation on Hadoop and MapReduce, and so on. We started digging deeper in these�areas. After a short investigation, it was clear that the Hadoop Distributed File System (HDFS) would support our storage demands, and MapReduce would be a good fit for the processing infrastructure. First Hadoop cluster in the lab While looking for Hadoop�distributions, I encountered Cloudera’s Distribution including Apache Hadoop (CDH), however, I decided to start with a manual installation since this usually helps me better understand how things work.�We started a pilot, setting up a 2 node cluster on Linux boxes. As mentioned, the first installation was done totally manually using the binaries downloaded from�Apache, and gently configuring the system. This process was ugly: I needed to download all sorts of binaries from different sources, deal with networking issues, exchange of SSH keys between the nodes, formatting the FS and all sorts of OS tweaks. We started testing the�behavior of the new technology, first with some simple WordCount and pi calculations, and then we quickly wrote MapReduce (Java) code that did parts of our processing and tested it on real HTML sources. The little cluster just worked: I was able to submit jobs &amp; monitor them; I tested recovery from task failures, crash of a node, etc. Next, I wanted to see how this Hadoop solution scaled. To do this, I installed an additional box and added it to our little Hadoop cluster. It was�awesome: after adding the new slave to the cluster, everything was transparent.�Suddenly we had more capacity on the file system and more horsepower for processing. The job submission was the same as before; the job submitter (Hadoop client) didn’t even know that the cluster had changed, it simply got the results quicker. We were able to crunch some numbers and got a dollar-per-post cost. So, the evaluation was great, but still there was the awful installation and maintenance process. That�s when we started to test Cloudera’s Distribution including Apache Hadoop; I think it was version 2 of CDH back then. We�re-installed�our little cluster from�scratch using this Hadoop distribution. The installation process was much easier, and the documentation helped. The setup took only a couple of hours. (CDH3 takes less than an hour).� After we found a good package, we wanted to set up a bigger cluster for prototyping, and deeper tests and evaluations. Amazon seemed to be the perfect place for that. Using CDH we set up a 10 node (small instances) cluster on EC2. This was used for performance evaluation and the processing rate was about 10M-20M posts per day — approximately 6 times higher than the performance from our pre-Hadoop solution. We decided to go with Hadoop. This was a dramatic�decision, as we took a company with a Microsoft-oriented development team, ported all the code into Java, all the while adopting a new and very complicated technology stack. This actually meant starting implementation from the�beginning, opening a new integrated development environment (IDE) and starting to code from scratch.� In order to reduce risks and avoid critical mistakes, we searched for someone who has “been there, done that” so we could learn from them and validate our overall planned new architecture. Cloudera was our first choice; it made sense to go with a company that has�multiple setups behind them, some of which are very large clusters. Cloudera sent Solutions Architect, Lars George, to our offices for two days, and we gave him our suggested design in advance.�We felt lucky to have Lars, an HBase committer and author of HBase: The Definitive Guide, as our consultant since HBase was one of the core technologies we were using. For the first implementation phase, we decided to go with HDFS, MapReduce &amp; HBase. Our in-house-developed crawlers were using HBase as the store for the list of URLs to be fetched. This table should be able to scale to billions of rows. The fetcher (the component in charge of fetching the raw HTML sources) gets the URL queues out of HBase, runs HTTP requests, and stores the raw HTML sources in large files on top of HDFS (few gigs per file). Both the crawler and fetcher don�t use any relational database or any other kind of store except HDFS &amp; HBase. These two components are network and I/O intensive, but CPU is not much of an issue. Next comes the processing. Each line in the HDFS files contains an HTML source and metadata related to this source. For each directory of files in HDFS, the following processing jobs need to be executed: Turn the unstructured HTML into a list of post entities (content, timestamp, etc.) Each post needs to be processed as follows: Index key terms � extract medical concepts out of the post content, using Treato’s extensive knowledge base Execute text analysis algorithms Calculate all statistics and update the HBase stats tables. Post all documents (user�s posts) into our search engine (Solr). During this process, many database queries and updates are needed. For example, each post retrieved may�potentially already exist in our system, and of course we don’t want to add a duplicate post to our system, nor invest processing power on documents we already have. In order to accomplish this, we need to calculate a hash for each post, and then check it against a database containing all of the existing hashes. For this�purpose HBase works perfectly in terms of both latency�and load. After the design phase, we started implementation. All R&amp;D teams worked on porting their code into Java, and our Ops team worked on planning the data center (we decided on co-location data center setup). For the initial setup, we had 11 boxes that comprised our Hadoop cluster, two of which were name nodes in an active / passive mode (one was in standby for manual failover in case the active NameNode failed). Nine nodes were slaves hosting DataNodes, TaskTrackers and Region-Servers daemons. In addition to this we had three boxes running Zookeeper services. The new system was capable of analyzing 50M posts per day. This was a significant�performance improvement. In addition, it was reasonable to maintain, reliable and worked quite�smoothly. Of course, there were bumps in the road, but in the end we managed to overcome them all. We have continued to improve and expand the solution, and today we can process 150 � 200 million user posts per day. In subsequent blog posts, I will share in greater detail our system design, use of HBase, and cluster architecture.</snippet></document><document id="427"><title>Apache MRUnit 0.9.0-incubating has been released!</title><url>http://blog.cloudera.com/blog/2012/05/apache-mrunit-0-9-0-incubating-has-been-released/</url><snippet>This post was originally posted on the Apache Software Foundation’s blog. We (the Apache MRUnit team) have just released Apache MRUnit 0.9.0-incubating (tarball, nexus, javadoc). Apache MRUnit is an Apache Incubator project that is a Java library which helps developers unit test Apache Hadoop MapReduce jobs. Unit testing is a technique for improving project quality and reducing overall costs by writing a small amount of code that can automatically verify the software you write performs as intended. This is considered a best practice in software development since it helps identify defects early, before they’re deployed to a production system. The MRUnit project is quite active, 0.9.0 is our fourth release since entering the incubator and we have added 4 new committers beyond the projects initial charter! We are very interested in having new contributors and committers join the project! Please join our mailing list to find out how you can help! The MRUnit build process has changed to produce mrunit-0.9.0-hadoop1.jar and mrunit-0.9.0-hadoop2.jar instead of mrunit-0.9.0-hadoop020.jar, mrunit-0.9.0-hadoop100.jar and mrunit-0.9.0-hadoop023.jar. The hadoop1 classifier is for all Apache Hadoop versions based off the 0.20.X line including 1.0.X. The hadoop2 classifier is for all Apache Hadoop versions based off the 0.23.X line including the unreleased 2.0.X. This release contains 2 new features, 15 improvements and 6 bug fixes. I will highlight a few below: Support custom counter checking in MRUNIT-68 runTest() should optionally ignore output order in MRUNIT-91 Driver.runTest throws RuntimeException should it throw AssertionError in MRUNIT-54 o.a.h.mrunit.mapreduce.MapReduceDriver should support a combiner in MRUNIT-67 Better support for other serializations besides Writable: MRUNIT-70, MRUNIT-86, MRUNIT-99, MRUNIT-77 Better error messages from validate, null checking and forgetting to set mappers and reducers: MRUNIT-74, MRUNIT-66, MRUNIT-65 add static convenience methods to PipelineMapReduceDriver class in MRUNIT-89 Test and Deprecate Driver.{*OutputFromString,*InputFromString} Methods in MRUNIT-48 Support custom counter checking It has always been possible to check the counter values like so: assertEquals(2, mapDriver.getCounters().findCounter(CustomMapper.CustomCounter.NAME).getValue());
 but this is quite tedious. As such Jarek Jarcec Cecho (our second newest committer) added this feature directly to the drivers: .withCounter(CustomMapper.CustomCounter.Name, 2);
 runTest() should optionally ignore output order Previous to this change MRUnit required Mapper/Reducer classes to output key value pairs in the order specified on the test. Well defined output order is common, but strictly not universal. Dave Beech (our newest committer) contributed a patch so you optionally turn this ordered requirement off by using: .runTest(false)
 instead of .runTest()
 Driver.runTest throws RuntimeException should it throw AssertionError Previous versions of MRUnit threw a RuntimeException when a test failed. This worked well, but it meant that testing frameworks saw the the test as having erred, not failed. We have changed this to AssertionError so that testing frameworks see the tests as failed. The distinction is small but important. o.a.h.mrunit.mapreduce.MapReduceDriver should support a combiner Previously the MRUnit only supported a combiner in the mapred MapReduceDriver class but now the mapreduce MapReduceDriver also supports a combiner by: MapReduceDriver.newMapReduceDriver(mapper, reducer, combiner) or .withCombiner(combiner) or .setCombiner(combiner) Better support for other serializations besides Writable Previous versions of MRUnit did not support JavaSerialization, Avro or other Serialization frameworks well. We improved alternative serialization support by not forcing K2 in MapReduceDriver to be Comparable and supporting serializations that cannot clone into a object or that do not have default constructors. Better error messages from validate, null checking and forgetting to set mappers and reducers We have improved checking of parameters passed to MRUnit and the error messages when the parameters are invalid including throwing NullPointerException immediately when receiving a null value and throwing a IllegalStateExcpetion when no mapper or reducer class is provided instead of a NullPointerException. Add static convenience methods to PipelineMapReduceDriver class add static convenience constructors similar to those in the other driver classes: PipelineMapReduceDriver.newPipelineMapReduceDriver() or PipelineMapReduceDriver.newPipelineMapReduceDriver(list of Pair) Test and Deprecate Driver.{*OutputFromString,*InputFromString} Methods The OutputFromString and InputFromString methods are now deprecated because they required Text inputs or outputs with no way to enforce that the inputs or outputs from a mapper or reducer were actually Text. These methods also provided little convenience as a user can just pass the string they intended to new Text(string)</snippet></document><document id="428"><title>HBaseCon 2012: A Glimpse into the Operations Track</title><url>http://blog.cloudera.com/blog/2012/04/hbasecon-2012-a-glimpse-into-the-operations-track/</url><snippet>HBaseCon 2012 is only a month away! The conference takes place May 22 in San Francisco, California and the event is poised to sell out. For those unfamiliar with the Apache HBase project, HBase is open source software that allows for real-time random read/write access to your Big Data in Apache Hadoop with very low latency and high scalability. Presentations in the HBaseCon 2012 Operations track will explain the state of HBase today, how to mitigate HBase failures, and best practices in cluster deployment and cluster monitoring. Operations Track Presentations Case Study of HBase Operations at Facebook Ryan Thiessen, Facebook At Facebook we have demanding HBase installations which are used for important and real-time user activity, so failure in an HBase cluster can be a serious issue requiring immediate attention. This session will discuss a variety of real-world scenarios where we have had failures in our HBase systems, how our Operations and Engineering teams have worked to mitigate many of these issues, and where HBase still needs to improve instead of relying on workarounds. The database should never go down. This talk is aimed at developers and other users of HBase (both current and potential) who are interested in an operational perspective on the state of HBase today. HBase Backup Sunil Sitaula, Cloudera Madhuwanti Vaidya, Facebook Reliable backup and recovery is one of the main requirements for any enterprise grade applications. HBase has been very well embraced by enterprises needing random, real-time read/write access with huge volumes of data and ease of scalability. As such they are looking for backup solutions that are reliable, easy to use, and can work with existing infrastructure. HBase comes with several backup options but there is a clear need to improve the native export mechanisms. This talk will cover various options that are available out of the box, their drawbacks and what various companies are doing to make backup and recovery efficient. In particular it will cover what Facebook has done to improve performance of backup and recovery process with minimal impact to production cluster. HBase Security for the Enterprise Andrew Purtell, Trend Micro Trend Micro developed the new security features in HBase 0.92 and has the first known deployment of secure HBase in production. We will share our motivations, use cases, experiences, and provide a 10 minute tutorial on how to set up a test secure HBase cluster and a walk through of a simple usage example. The tutorial will be carried out live on an on-demand EC2 cluster, with a video backup in case of network or EC2 unavailability. Lightning Talk | Developing Real Time Analytics Applications Using HBase in the Cloud Rick Tucker, Sproxil As small companies are adapting to handle Big Data, the cloud and HBase enable developers to leverage that data to provide revenue generating real-time applications. When developing a real-time application for an existing system, one must balance incrementing counters in real-time with MapReduce jobs over the same data-set. When maintaining an analytics platform, ensuring data accuracy is essential. At Sproxil, SMS logs are ingested into HBase at a growing rate and we report metrics such as SMS throughput, unique user growth over time, and return SMS user activity in real time. Sproxil provides a versatile analytics application enabling customers to handpick statistics on demand to gain market insights enabling them to react quickly to trends. This talk will identify the most profitable metrics and demonstrate how to calculate them using Map Reduce while continually updating data as it arrives. Lightning Talk | Unique Sets on HBase and Hadoop Elliott Clark, StumbleUpon Determining the number of unique users that have interacted with a web page, game, or application is a very common use case. HBase is becoming an increasingly accepted tool for calculating sets or counts of unique individuals who meet some criteria. Computing these statistics can range in difficulty from very simple to very difficult. This session will explore how different approaches have worked or not worked at scale for counting uniques on HBase with Hadoop. Lightning Talk | Orchestrating Clusters with Ironfan and Chef Robert Berger, Runa This session will discuss how you can represent your complete cluster with one config file and have it deployed to Cloud or Bare Metal. Infochmimps� Ironfan builds on Opscode Chef to allow you to specify and orchestrate all flavors of your cluster�s deployment, monitoring and growth. Not just the core HBase/HDFS/MapReduce/Hive/Flume, etc. but all the elements including web / app servers, mysql, redis, rabbitmq and whatever other servers needed to implement your service. These same tools can manage variations for development, staging, R&amp;D as well as the target �rendering� to various Clouds, Bare Metal or even Vagrant VMs.</snippet></document><document id="429"><title>Introducing CDH4 Beta 2</title><url>http://blog.cloudera.com/blog/2012/04/introducing-cdh4-beta-2/</url><snippet>I’m pleased to inform our users and customers that we have released the Cloudera’s Distribution Including Apache Hadoop version 4 (CDH4) 2nd and final beta today. We received great feedback from the community from the first beta and this release incorporates that feedback as well as a number of new enhancements. CDH4 has a great many enhancements compared to CDH3. Availability – a high availability namenode, better job isolation, improved hard disk failure handling, and multi-version support Utilization – multiple namespaces and a slot-less resource management model Performance – improvements in HBase, HDFS, MapReduce, Flume and compression performance Usability – broader BI support, expanded API options, a more responsive Hue with broader browser support Extensibility – HBase co-processors enable developers to create new kinds of real-time big data applications, the new MapReduce resource management model enables developers to run new data processing�paradigms�on the same cluster resources and storage Security – HBase table &amp; column level security and Zookeeper authentication support Some items of note about this beta: This is the second (and final) beta for CDH4, and this version has all of the major component changes that we’ve planned to incorporate before the platform goes GA. �The second beta: Incorporates the Apache Flume, Hue, Apache Oozie and Apache Whirr components that did not make the first beta Broadens the platform support back out to our normal release matrix of Red Hat, CentOS, SUSE, Ubuntu and Debian Standardizes our release matrix of supported databases to include MySQL, PostgresSQL and Oracle Includes a number of improvements to existing components like adding auto-failover support to HDFS’s high availability feature and adding multi-homing support to HDFS and MapReduce Incorporates a number of fixes that were identified during the first beta period like removing a HBase performance regression To recap, some CDH components have undergone substantial revamps and we have transition plans for these. There is a significantly redesigned MapReduce (aka MR2) with a similar API to the old MapReduce but with new daemons, user interface and more. MR2 is part of CDH4, but we also decided it makes sense to ship with the MapReduce from CDH3 (aka MR1) which is widely used, thoroughly debugged and stable. We will support both generations of MapReduce for the life of CDH4, which will allow customers and users to take advantage of all of the new CDH4 features while making the transition to the new MapReduce in a timeframe that makes sense for them.�Similarly, Apache Flume in CDH4 is substantially revamped (aka Flume NG). �The new design is simpler, more scaleable, more manageable and more reliable. Because of the popularity of the high availability features,�we’ve created a high availability guide.��All of the other documentation artifacts�have been updated. As always, we maintain complete transparency as to the Apache project releases and patches that make up CDH4. You can find�the documentation for the Apache contents of CDH4 here. We value your feedback! Please help make this beta a success by trying out CDH4 b2 and letting us know what you think.� If you are a customer, you should give us your feedback via Zendesk. If you are a user but not a customer, please give us your feedback on�CDH Users.</snippet></document><document id="430"><title>HBaseCon 2012: A Glimpse into the Development Track</title><url>http://blog.cloudera.com/blog/2012/04/hbasecon-2012-a-glimpse-into-the-development-track/</url><snippet>HBaseCon 2012 is nearly a month away, and if the conference agenda and attendee registration numbers are good indicators, this will be an annual�event you won�t want to miss. Apache HBase is an open source software project that provides users with the ability to do real-time random read/write access to their data in Apache Hadoop. This means that when you want to use Hadoop for real-time data processing, HBase is the project you are looking for. The HBase developer community includes contributors from many organizations such as StumbleUpon, Facebook, Salesforce.com, TrendMicro, eBay, Explorys, Huawei and Cloudera. In fact, the HBaseCon Program Committee, constructors of the HBaseCon 2012 agenda, are all committers and PMC members of the Apache HBase project. Presentations in the HBaseCon 2012 Development track will explain how and why HBase is built the way it is and will also cover HBase schema design and HDFS, the file system on which HBase is most commonly deployed. Some of the presentations for this track include the following below. Development Track Presentations Learning HBase Internals Lars Hofhansl, Salesforce.com The strength of an open source project resides entirely in its developer community; a strong democratic culture of participation and hacking makes for a better piece of software. The key requirement is having developers who are not only willing to contribute, but also knowledgeable about the project�s internal structure and architecture. This session will introduce developers to the core internal architectural concepts of HBase, not just �what� it does from the outside, but �how� it works internally, and �why� it does things a certain way. We�ll walk through key sections of code and discuss key concepts like the MVCC implementation and memstore organization. The goal is to convert serious �HBase Users� into HBase Developer Users,� and give voice to some of the deep knowledge locked in the committers� heads. Lessons learned from OpenTSDB Benoit Sigoure, StumbleUpon OpenTSDB was built on the belief that, through HBase, a new breed of monitoring systems could be created, one that can store and serve billions of data points forever without the need for destructive downsampling, one that could scale to millions of metrics, and where plotting real-time graphs is easy and fast. In this presentation we�ll review some of the key points of OpenTSDB�s design, some of the mistakes that were made, how they were or will be addressed, and what were some of the lessons learned while writing and running OpenTSDB as well as asynchbase, the asynchronous high-performance thread-safe client for HBase. Specific topics discussed will be around the schema, how it impacts performance and allows concurrent writes without need for coordination in a distributed cluster of OpenTSDB instances. HBase Schema Design Ian Varley, Salesforce.com Most developers are familiar with the topic of �database design.� In the relational world, normalization is the name of the game. How do things change when you�re working with a scalable, distributed, non-SQL database like HBase? This talk will cover the basics of HBase schema design at a high level and give several common patterns and examples of real-world schemas to solve interesting problems. The storage and data access architecture of HBase (row keys, column families, etc.) will be explained, along with the pros and cons of different schema decisions. HBase and HDFS: Past, Present, and Future Todd Lipcon, Cloudera Apache HDFS, the file system on which HBase is most commonly deployed, was originally designed for high-latency high-throughput batch analytic systems like MapReduce. Over the past two to three years, the rising popularity of HBase has driven many enhancements in HDFS to improve its suitability for real-time systems, including durability support for write-ahead logs, high availability, and improved low-latency performance. This talk will give a brief history of some of the enhancements from Hadoop 0.20.2 through 0.23.0, discuss some of the most exciting work currently under way, and explore some of the future enhancements we expect to develop in the coming years. We will include both high-level overviews of the new features as well as practical tips and benchmark results from real deployments. Lightning Talk | Relaxed Transactions for HBase Francis Liu, Yahoo! For Map/Reduce programmers used to HDFS, the mutability of HBase tables poses new challenges: Data can change over the duration of a job, multiple jobs can write concurrently, writes are effective immediately, and it is not trivial to clean up partial writes. Revision Manager introduces atomic commits and point-in-time consistent snapshots over a table, guaranteeing repeatable reads and protection from partial writes. Revision Manager is optimized for a relatively small number of concurrent write jobs, which is typical within Hadoop clusters. This session will discuss the implementation of Revision Manager using ZooKeeper and coprocessors, and paying extra care to ensure security in multi-tenant clusters. Revision Manager is available as part of the HBase storage handler in HCatalog, but can easily be used stand-alone with little coding effort. Lightning Talk | Living Data: Applying Adaptable Schemas to HBase Aaron Kimball, WibiData HBase application developers face a number of challenges: schema management is performed at the application level, decoupled components of a system can break one another in unexpected ways, less-technical users cannot easily access data, and evolving data collection and analysis needs are difficult to plan for. In this talk, we describe a schema management methodology based on Apache Avro that enables users and applications to share data in HBase in a scalable, evolvable fashion. By adopting these practices, engineers independently using the same data have guarantees on how their applications interact. As data collection needs change, applications are resilient to drift in the underlying data representation. This methodology results in a data dictionary that allows less-technical users to understand what data is available to them for analysis and inspect data using general-purpose tools (for example, export it via Sqoop to an RDBMS). And because of Avro�s cross-language capabilities, HBase�s power can reach new domains, like web apps built in Ruby.   �</snippet></document><document id="431"><title>Constructing Case-Control Studies with Apache Hadoop</title><url>http://blog.cloudera.com/blog/2012/04/constructing-case-control-studies-with-hadoop-healthcare/</url><snippet>San Francisco seems to be having an unusually high number of flu cases/searches this April, and the Cloudera Data Science Team has been hit pretty hard. Our normal activities (working on Crunch, speaking at conferences, finagling a job with the San Francisco Giants) have taken a back seat to bed rest, throat lozenges, and consuming massive quantities of orange juice. But this bit of downtime also gave us an opportunity to focus on solving a large-scale data science problem that helps some of the people who help humanity the most: epidemiologists. Case-Control Studies A case-control study is a type of observational study in which a researcher attempts to identify the factors that contribute to a medical condition by comparing a set of subjects who have that condition (the ‘cases’) to a set of subjects who do not have the condition, but otherwise resemble the case subjects (the ‘controls’). They are useful for exploratory analysis because they are relatively cheap to perform, and have led to many important discoveries- most famously, the link between smoking and lung cancer. Epidemiologists and other researchers now have access to data sets that contain tens of millions of anonymized patient records. Tens of thousands of these patient records may include a particular disease that a researcher would like to analyze. In order to find enough unique control subjects for each case subject, a researcher may need to execute tens of thousands of queries against a database of patient records, and I have spoken to researchers who spend days performing this laborious task. Although they would like to parallelize these queries across multiple machines, there is a constraint that makes this problem a bit more interesting: each control subject may only be matched with at most one case subject. If we parallelize the queries across the case subjects, we need to check to be sure that we didn’t assign a control subject to multiple cases. If we parallelize the queries across the control subjects, we need to be sure that each case subject ends up with a sufficient number of control subjects. In either case, we still need to query the data an arbitrary number of times to ensure that the matching of cases and controls we come up with is feasible, let alone optimal. Designing and analyzing a case-control study is a problem for a statistician. Constructing a case-control study is a problem for a data scientist. Applied Auction Theory We can think of constructing a case-control study as an�assignment problem: we have a bipartite graph, where one set of nodes represents the cases, one set of nodes represents the controls, and the edges between the cases and controls are weighted by the quality of the match between the subjects as determined by the researcher. If a particular case-control pair would not be a suitable match under any circumstances because the patients are not similar enough, there is no edge between them. A small assignment problem Although MapReduce is great for finding compatible case-control pairs and computing the weights we want to assign to those matches, it’s not ideal for the kinds of iterative, graph-based computations that we need to do in order to solve the assignment problem. After we use MapReduce to prepare the input, we turn to Apache Giraph, a Java library that makes it easy to perform fast, distributed graph processing on Apache Hadoop clusters, to assign cases to controls. Although there are lots of different algorithms for solving the assignment problem, our implementation is based on�Bertsekas‘ auction algorithm. The core idea of the algorithm is that the case subjects will bid for the right to be matched with control subjects over a series of rounds, with the bids computed based on the edge weights.�Assuming that all of the weights are integers, the auction algorithm is guaranteed to converge to an assignment of cases to controls that maximizes the sum of the weights of the matched pairs. Bertsekas’ algorithm is also very easy to parallelize, and has excellent performance on assignment problems that are relatively sparse (i.e., each node is only connected to a small fraction of the total nodes.) An optimal matching Do It Yourself Our toolkit for constructing case-control studies is available on Cloudera’s github repository, and is released under the Apache License. To get started, you will need a cluster that has Apache Zookeeper installed, which is easy to do on local servers using the free edition of Cloudera Manager, or in a cloud environment via the version of�Apache Whirr in CDH3. If you are just getting started with Hadoop and run into any issues, Cloudera Support is happy to help. This work, like a lot of the work we do, started out as a conversation with a Cloudera customer about a challenge they were facing. If you have a data problem, if no one else can help, and if you can provide chicken soup, maybe you can hire the Cloudera Data Science Team.</snippet></document><document id="432"><title>Sqoop Graduation Meetup</title><url>http://blog.cloudera.com/blog/2012/04/sqoop-graduation-meetup/</url><snippet>This blog was originally posted on the Apache Blog: https://blogs.apache.org/sqoop/entry/sqoop_graduation_meetup Cloudera hosted the Apache Sqoop Meetup last week at Cloudera HQ in Palo Alto. About 20 of the Meetup attendees had not�used Sqoop before, but were interested enough to participate in the Meetup on�April 4th. We believe this healthy interest in Sqoop will contribute to its wide�adoption.� Not only was this Sqoop’s second Meetup but also a celebration for�Sqoop’s graduation from the Incubator, cementing its status as a Top-Level Project in Apache Software Foundation. Sqoop’s come a long way since its beginnings three years ago as a contrib module for Apache Hadoop submitted by Aaron Kimball. As a result, it was fitting that Aaron gave the first talk of the night by discussing its history: “Sqoop: The Early Days.” From Aaron, we learned that Sqoop�s original name was “SQLImport” and that it was conceived out of his frustration from the inability to easily query both unstructured and structured data at the same time. Closing out the evening, Arvind Prabhakar described the “Highlights of Sqoop 2.� Sqoop 2 will enable users to use�Sqoop effectively with a minimal understanding of its details. For instance, by having a web-application run Sqoop, Sqoop can be installed once and used from anywhere.�Among other goals for the project are ease of the development of connectors and security enhancements. With the conclusion of the scheduled talks, the graduation cake was cut, the swag was passed out, and the hallway talks commenced on the anticipated features of Sqoop 2. We encourage you to participate in and contribute to Sqoop 2′s Design and Implementation.</snippet></document><document id="433"><title>Apache HBase Hackathon at Cloudera</title><url>http://blog.cloudera.com/blog/2012/04/hbase-hackathon-at-cloudera/</url><snippet>Cloudera will be hosting an Apache HBase hackathon on May 23rd, 2012, the day after HBaseCon 2012. �The overall theme of the event will be 0.96 stabilization. �If you are in the area for HBaseCon, please come down to our offices in Palo Alto the next day to attend the hackathon. �This is a great opportunity to contribute some code towards the project and hang out with other HBasers. More details are on the hackathon’s�Meetup page. �Please RSVP so we can better plan lunch, room size, and other logistics for the event. �See you there!</snippet></document><document id="434"><title>HBaseCon 2012: A Glimpse into the Applications Track</title><url>http://blog.cloudera.com/blog/2012/04/hbasecon-2012-a-glimpse-into-the-applications-track/</url><snippet>HBaseCon 2012 is coming to San Francisco on May 22, less than 2 months away! The conference agenda continues to grow daily with exciting presentation content, which means it�s time to share a few sessions that have been added to the HBaseCon 2012 Applications Track. Apache HBase is primarily used for real-time random read/write access to Big Data as part of the Apache Hadoop ecosystem. Applications on Apache HBase are typically built to query Big Data with extremely low latency. Sessions in the HBaseCon 2012 Applications Tracks will include explanations of real-world HBase use cases, where HBase fits in an organization�s entire Big Data stack and when HBase is the �right� solution for an organization. Applications Track Presentations Building a Large Search Platform on a Shoestring Budget Jacques Nadeau, CTO at YapMap YapMap is a new kind of search platform that does multi-quanta search to better understand threaded discussions. This talk will cover how HBase made it possible for two self-funded guys to build a new kind of search platform. The presentation will discuss the YapMap data model and how YapMap uses row based atomicity to manage parallel data integration problems. Also learn where YapMap does not use HBase and instead uses a traditional SQL based infrastructure; the benefits of using MapReduce and HBase for index generation; the YapMap migration of tasks from a message based queue to the Coprocessor framework; and YapMap�s future Coprocessor use cases. Lastly, learn about YapMap�s operational experience with HBase, hardware choices and the challenges YapMap has faced. Low Latency �OLAP� with HBase Cosmin Lehene, Computer Scientist at Adobe Systems Adobe Systems uses �SaasBase Analytics� to incrementally process large heterogeneous data sets into pre-aggregated, indexed views, stored in HBase to be queried in real- time. The goal was to process new data in real- time (currently minutes) and have it ready for a large number of concurrent queries that execute in milliseconds. This set Adobe�s problem apart from what is traditionally solved with Hive or Pig. This talk will describe the design and the strategies (and hacks) used to achieve low latency and scalability, from theoretical model to the entire process of ETL to warehousing and queries. Growing Your Inbox, HBase at Tumblr Blake Matheny, Director of Platform Engineering at Tumblr This talk goes into detail about Tumblr�s experience developing Motherboy, an eventually consistent inbox style storage system built around HBase. The SLA, write concurrency, data volume, and failure modes for this application created a number of challenges in developing a solution. The user homing scheme introduced additional complexity that made capacity planning tricky as Tumblr tried to trade off availability and cost. Performance testing of our workload, and automation to support that testing, also provided a number of valuable lessons. This talk will be most useful to people considering HBase for their application, but will have enough detail to be useful to current HBase users as well. Be sure to check the agenda in the coming weeks as we are adding more sessions soon. Remember that the Early Bird registration price expires this Friday April 6 so register soon to take advantage of the discount.</snippet></document><document id="435"><title>Apache Bigtop 0.3.0 (incubating) has been released</title><url>http://blog.cloudera.com/blog/2012/04/apache-bigtop-0-3-0-incubating-has-been-released/</url><snippet>Apache Bigtop 0.3.0 (incubating) is now available. This is the first fully integrated, community-driven, 100% Apache Big Data management distribution based on Apache Hadoop 1.0. In addition to a major change in the Hadoop version, all of the Hadoop ecosystem components have been upgraded to the latest stable versions and thoroughly tested: Apache Hadoop 1.0.1 Apache Zookeeper 3.4.3 Apache HBase 0.92.0 Apache Hive 0.8.1 Apache Pig 0.9.2 Apache Mahout 0.6.1 Apache Oozie 3.1.3 Apache Sqoop 1.4.1 Apache Flume 1.0.0 Apache Whirr 0.7.0 The list of supported Linux platforms has expanded to: Fedora 15 and 16 CentOS and Red Hat Enterprise Linux 5 and 6 SuSE Linux Enterprise 11 Ubuntu 10.04 LTS Mageia 1 This, we hope, will make our user community’s experience running Apache Hadoop the most seamless Bigtop experience to date: just follow our Installation Guide and you will have your first pseudo-distributed Hadoop PI or Hive query running in no time. If you’re thinking about deploying Bigtop to a fully-distributed cluster you might find our improved Puppet code to be of assistance. There is some brief documentation� on how to run our Puppet recipes in a master-less puppet configuration, but they should work just fine in a typical Puppet master setup as well. Whatever you do, don’t forget to check us out at Apache and consider getting involved. Bigtop is a community-driven effort and we need your help. Of course, above all we need you to use Bigtop and give us your the feedback. Happy Big Data discoveries, Your faithful and tireless Bigtop development team!</snippet></document><document id="436"><title>Apache Sqoop Graduates from Incubator</title><url>http://blog.cloudera.com/blog/2012/04/apache-sqoop-graduates-from-incubator/</url><snippet>This blog was originally posted on the Apache Blog:�https://blogs.apache.org/sqoop/entry/apache_sqoop_graduates_from_incubator Apache Sqoop is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases. You can use Sqoop to import data from external structured datastores into Hadoop Distributed File System or related systems like Hive and HBase. Conversely, Sqoop can be used to extract data from Hadoop and export it to external structured datastores such as relational databases and enterprise data warehouses. In its monthly meeting in March of 2012, the board of Apache Software Foundation (ASF) resolved to grant a Top-Level Project status to Apache Sqoop, thus graduating it from the Incubator. This is a significant milestone in the life of Sqoop, which has come a long way since its inception almost three years ago. The following figure offers a brief overview of what has happened in the life of Sqoop so far: Figure 1: A timeline of Sqoop Project Sqoop started as a contrib module for Apache Hadoop in May of 2009, first submitted as a patch to HADOOP-5815 by Aaron Kimball. Over the course of next year, it saw about 56 patches submitted towards its development. Given the inertia of large projects, Aaron decided to decouple it from Hadoop and host it elsewhere to facilitate faster development and release cycles. Consequently, in April of 2010 Sqoop was taken out from Hadoop via MAPREDUCE-1644 and hosted on GitHub by Cloudera as an Apache Licensed project. Over the course of next year, Sqoop saw wide adoption along with four releases and 191 patches. An extension API was introduced early in Sqoop that allowed the development of high-speed third party connectors for rapid data transfer from specialized systems such as enterprise data warehouses. As a result, multiple connectors were developed by various vendors that plugged into Sqoop. To bolster this fledgling community of users and third party connector vendors, Cloudera decided to propose it for incubation in Apache. Sqoop was accepted for incubation by the Apache Incubator in June of 2011. Inside the Incubator, Sqoop saw a healthy growth in its community and gained four new committers. With active community and committers, Sqoop made two incubating releases. The focus of its first release was migration of code from com.cloudera.sqoop namespace to org.apache.sqoop while preserving backward compatibility. Thanks to phenomenal work by Bilung Lee, the release manager of the first incubating release, this release met all of its expectations. The second incubating release of Sqoop focused on its interoperability with various versions of Hadoop. The release manager of this release – Jarek Jarcec Cecho – was instrumental in making sure that it delivered to this requirement and could work with Hadoop versions 0.20, 0.23 and 1.0. Along with the stated goals of these incubating releases, Sqoop saw a steady growth with 116 patches by various contributors and committers. With excellent mentorship by Patrick Hunt, other mentors of the project, and from Incubator PMC members, Sqoop acquired the ability to self-govern, follow the ASF policies and guidelines, and, foster and grow the community. Sqoop successfully graduated from the Incubator in March of 2012 and is now a Top-Level Apache project. You can download its latest release artifacts by visiting http://sqoop.apache.org/. While Sqoop has no doubt delivered significant value to the community of users, it is fair to say that it is in the early stages of fulfilling requirements of data integration around Hadoop. Work has started towards the development of next major revision of Sqoop which will address more of these requirements than before. Along the way, we are looking forward to grow the community many folds, get more committers on board, and solve some real challenging problems of data movement between Hadoop and external systems. We sincerely hope you will join us in taking Sqoop towards fulfilling all these goals and to become a standard component in Hadoop deployments everywhere.</snippet></document><document id="437"><title>Apache Hadoop Versions: Looking Ahead</title><url>http://blog.cloudera.com/blog/2012/04/apache-hadoop-versions-looking-ahead-3/</url><snippet>Introduction A few months ago, my colleague Charles Zedlewski wrote a great piece explaining Apache Hadoop version numbering. The post can be summed up with the following diagram: � While Charles�s post does a great job of explaining the history of Apache Hadoop version numbering, it doesn�t help users understand where Hadoop version numbers are headed.� The Problem Hadoop has of late been frequently referred to as �an operating system for the cloud.� Without disputing the accuracy of this description, one thing is clear: if we�re going to compete for mindshare with entrenched operating systems, we�ll need to first get our version numbers up to par. For the first 5 or so years of Hadoop releases, the version number was strictly less than �1″. With the release of Hadoop 1.0 in late 2011, and the impending release of Hadoop 2.0, we�ve done a lot to catch up, but The Community must do more. The Solutions� First: Version Backdating Similarly to option backdating, several past releases will be renumbered to higher version numbers. Starting with the Hadoop 0.16 line, through the venerable 0.20 line, the major version will be incremented, beginning at 1, and proceeding with the Fibonacci sequence. However, these releases� minor versions will be kept the same, to decrease confusion. This will yield the following mapping of version numbers: � The releases previously known as �1.0.x� and �2.0.x� will likewise be increased, but this time will be explicitly set to the next two numbers in the Fibonacci sequence, and their minor versions will be reset to 0. So, 8.0 and 13.0, respectively. In order to appropriately distinguish the importance of the two releases previously known as �1.0� and �2.0�, these versions will adopt a new 6-part numbering scheme, i.e. 8.0.0.0.0.0 and 13.0.0.0.0.0. The intention of the last three parts of this scheme is that they will never be anything but �0� – the extra zeros are used solely to identify the importance of the release. More zeros may be added in the future. Note: Given the maturity of 8.x.y.0.0.0 (nee 1.0.0), this release line will henceforth only accept code modifications that are either spelling corrections, or the addition of single lines of documentation. Second: Fractional Minor Versions In order to further distinguish Hadoop from other �operating systems,� Hadoop releases will optionally, at the will of The Community, be able to append fractional version numbers. This should afford Hadoop with versioning flexibility unrivaled in the software industry. Examples might include �Apache Hadoop 13 ?�, �Apache Hadoop 33 ?�, or �Apache Hadoop 21 C/d�. Third (and Finally): Codenames Other �operating systems� are often referred to not merely by version numbers, but by catchy codenames. Hadoop must be able to compete along these lines as well. Though not yet voted on by the community, the following codenames have been proposed:� Apache Hadoop 8.x.y.0.0.0 shall alternately be called �Lion� Apache Hadoop 13.x.y.0.0.0 shall alternately be called �Vista� Apache Hadoop 21.x.y.0.0.0 shall alternately be called �Hadoop Database 11g�� These new codenames should help Hadoop spread to the so far underserved retail big data processing market, and will undoubtedly help realize Doug�s original goal of �a cluster in every home”. � Cloudera Manager Version Compatibility Layer In order to keep pace with what will surely be a proliferation of many different versions of Apache Hadoop running within each organization, Cloudera Manager will need to introduce a compatibility and versioning layer of its own. This product, being announced for the first time today, will support a single console from which an operator can monitor and manage all past, present, re-numbered, and future versions of Apache Hadoop. This product shall be called Cloudera Middle Manager. [NOTE: This was an April Fools Day post.]</snippet></document><document id="438"><title>March 2012 Bay Area HBase User Group meetup summary</title><url>http://blog.cloudera.com/blog/2012/03/march-2012-bay-area-hbase-user-group-meetup-summary/</url><snippet>The Bay Area HBase User Group�March 2012 meetup was held at the StumbleUpon offices in San Francisco, California. 80 interested Apache HBasers were in attendance to mingle and listen to the scheduled presentations. Michael Stack started the meetup by reminding folks to register for HBaseCon 2012 in San Francisco on May 22nd. �Nick Dimiduk and Cloudera’s Amandeep Khurana then announced an early access program for their upcoming book,�HBase In Action. �Interested folks can get a discount for the program by using the code “hbase38.” St.Ack then discussed various recent releases (link to slides): 0.90.6 (link to previous blog) and 0.92.1 (link to previous blog) were officially released in mid-late March. 0.94.0 is a performance-oriented release currently in its first RC. Many improvements from Facebook and others made it into the release, which should support rolling restart and be backwards compatible with 0.92. Trunk is now 0.96. It will contain changes to enable wire compatibility, and will not be backwards-compatible with previous releases; hence its nickname of the “singularity”. However, releases after 0.96 should be compatible within one major release. Folks then commenced their presentations: Moving HBase RPC to protobufs Jimmy Xiang &amp; Gregory Chanan from Cloudera talked about the background, motivation, and goals for the ongoing Apache HBase wire compatibility effort. �They also focused on the requirements, compability matrix, proposal and work breakdown. Slides are available. Comparing the native HBase client and asynchbase StumbleUpon’s Beno�t Sigoure gave an overview of asynchbase, a fully non-blocking HBase client. He touched upon its features, how it is used at StumbleUpon and gave some encouraging performance results. Using Apache Hive with HBase: Recent improvements Finally, Enis Soztutar from Hortonworks gave a presentation about Hive and HBase integration. He covered architecture and some use cases, along with some discussion of schemas and data type mappings. Afterwards, Shaneal Manek from Cloudera and Jesse Yates from Salesforce held a huddle about backups and�snapshots in HBase. A summary is presented in HBASE-50. Please note that an Apache HBase PMC meeting took place right before the meetup; comprehensive minutes are available. Thanks to StumbleUpon for hosting the meetup and providing the food and drink!</snippet></document><document id="439"><title>Apache HBase 0.92.1 now available</title><url>http://blog.cloudera.com/blog/2012/03/apache-hbase-0-92-1-now-available/</url><snippet>What’s new? Apache HBase 0.92.1 is now available. This release is a marked improvement in system correctness, availability, and ease of use. It’s also backwards compatible with 0.92.0 � except for the removal of the rarely-used transform functionality from the REST interface in�HBASE-5228. Apache HBase 0.92.1 is a bug fix release covering 61 issues – including 6 blockers and 6 critical issues, such as: HBASE-5121 which ensures scans work properly while major compactions are occurring Several fixes that prevent crashes in edge-case situations (HBASE-5279, HBASE-4890, and HBASE-5415) Fixing the build system so the release includes security sources and Javadocs HBASE-5267 which fixes some configuration problems with the slab cache � Acknowledgements: A big thanks to our tireless release manager, Michael Stack, and everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc). Cloudera is hosting the first ever HBase conference at the InterContinental hotel in San Francisco on May 22, 2012. Register by April 6 to catch the early bird price!</snippet></document><document id="440"><title>Cloudera Manager 3.7.4 released!</title><url>http://blog.cloudera.com/blog/2012/03/cloudera-manager-3-7-4-released/</url><snippet>We are pleased to announce that Cloudera Manager 3.7.4 is now available! The most notable updates in this release are: A fixed memory leak in supervisord Compatibility with a scheduled refresh of CDH3u3 Significant improvements to the alerting functionality, and the rate of ‘false positive alerts’ Support for several new multi-homing features Updates to the default heap sizes for the management daemons (these have been increased). The detailed Cloudera Manager 3.7.4 release notes are available at: https://ccp.cloudera.com/display/ENT/Cloudera+Manager+3.7.x+Release+Notes Cloudera Manager 3.7.4 is available to download from: https://ccp.cloudera.com/display/SUPPORT/Downloads</snippet></document><document id="441"><title>Apache ZooKeeper 3.3.5 has been released</title><url>http://blog.cloudera.com/blog/2012/03/apache-zookeeper-3-3-5-has-been-released/</url><snippet>Apache ZooKeeper�release�3.3.5 is now available. This is a bug fix release covering�11�issues, two of which were considered blockers.�Some of the more serious issues include: ZOOKEEPER-1367 Data inconsistencies and unexpired ephemeral nodes after cluster restart ZOOKEEPER-1412 Java client watches inconsistently triggered on reconnect ZOOKEEPER-1277 Servers stop serving when lower 32bits of zxid roll over ZOOKEEPER-1309 Creating a new ZooKeeper client can leak file handles ZOOKEEPER-1389 It would be nice if start-foreground used exec $JAVA in order to get rid of the intermediate shell process ZOOKEEPER-1089 zkServer.sh status does not work due to invalid option of nc Stability, Compatibility and Testing 3.3.5 is a stable release that’s fully backward compatible with 3.3.4. Only bug fixes relative to 3.3.4 have been applied. Version 3.3.5 will be incorporated into the upcoming CDH3U4 release. Getting Involved The Apache ZooKeeper project is working on a number of new features. Our�How To Contribute�page is a great place to start if you’re interested in getting involved as a developer. You can also�follow me on twitter. Acknowledgements A special thanks to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc).</snippet></document><document id="442"><title>Authorization and Authentication In Hadoop</title><url>http://blog.cloudera.com/blog/2012/03/authorization-and-authentication-in-hadoop/</url><snippet>One of the more confusing topics in Hadoop is how authorization and authentication work in the system. The first and most important thing to recognize is the subtle, yet extremely important, differentiation between authorization and authentication, so let’s define these terms first: Authentication is the process of determining whether someone is who they claim to be. Authorization is the function of specifying access rights to resources. In simpler terms, authentication is a way of proving who I am, and authorization is a way of determining what I can do. Authentication If Hadoop is configured with all of its defaults, Hadoop doesn’t do any authentication of users. This is an important realization to make, because it can have serious implications in a corporate data center. Let’s look at an example of this. Let�s say Joe User has access to a Hadoop cluster. The cluster does not have any Hadoop security features enabled, which means that there are no attempts made to verify the identities of users who interact with the cluster. The cluster’s superuser is hdfs, and Joe doesn’t have the password for the hdfs user on any of the cluster servers. However, Joe happens to have a client machine which has a set of configurations that will allow Joe to access the Hadoop cluster, and Joe is very disgruntled. He runs these commands: sudo useradd hdfs
sudo -u hdfs hadoop fs -rmr / The cluster goes off and does some work, and comes back and says “Ok, hdfs, I deleted everything!”. So what happened here? Well, in an insecure cluster, the NameNode and the JobTracker don’t require any authentication. If you make a request, and say you’re hdfs or mapred, the NN/JT will both say “ok, I believe that,” and allow you to do whatever the hdfs or mapred users have the ability to do. Hadoop has the ability to require authentication, in the form of Kerberos principals. Kerberos is an authentication protocol which uses “tickets” to allow nodes to identify themselves. If you need a more in depth introduction to Kerberos, I strongly recommend checking out the�Wikipedia page. Hadoop can use the Kerberos protocol to ensure that when someone makes a request, they really are who they say they are. This mechanism is used throughout the cluster. In a secure Hadoop configuration, all of the Hadoop daemons use Kerberos to perform mutual authentication, which means that when two daemons talk to each other, they each make sure that the other daemon is who it says it is. Additionally, this allows the NameNode and JobTracker to ensure that any HDFS or MR requests are being executed with the appropriate authorization level. Authorization Authorization is a much different beast than authentication. Authorization tells us what any given user can or cannot do within a Hadoop cluster, after the user has been successfully authenticated. In HDFS this is primarily governed by file permissions. HDFS file permissions are very similar to BSD file permissions.�If you’ve ever run `ls -l` in a directory, you’ve probably seen a record like this: drwxr-xr-x �2 natty hadoop �4096 2012-03-01 11:18 foo
-rw-r--r-- �1 natty hadoop ���87 2012-02-13 12:48 bar On the far left, there is a string of letters. The first letter determines whether a file is a directory or not, and then there are three sets of three letters each. Those sets denote owner, group, and other user permissions, and the �rwx� are read, write, and execute permissions, respectively. The �natty hadoop� portion says that the files are owned by natty, and belong to the group hadoop.�As an aside, a stated intention is for HDFS semantics to be �Unix-like when possible.� The result is that certain HDFS operations follow BSD semantics, and others are closer to Unix semantics. The real question here is: what is a user or group in Hadoop? The answer is: they’re strings of characters. Nothing more. Hadoop will very happily let you run a command like hadoop fs -chown fake_user:fake_group /test-dir The downside to doing this is that if that user and group really don’t exist, no one will be able to access that file except the superusers, which, by default, includes hdfs, mapred, and other members of the hadoop supergroup. In the context of MapReduce, the users and groups are used to determine who is allowed to submit or modify jobs. In MapReduce, jobs are submitted via queues controlled by the scheduler. Administrators can define who is allowed to submit jobs to particular queues via MapReduce ACLs. These ACLs can also be defined on a job-by-job basis. Similar to the HDFS permissions, if the specified users or groups don�t exist, the queues will be unusable, except by superusers, who are always authorized to submit or modify jobs. The next question to ask is: how do the NameNode and JobTracker figure out which groups a user belongs to? When a user runs a hadoop command, the NameNode or JobTracker gets some information about the user running that command. Most importantly, it knows the username of the user. The daemons then use that username to determine what groups the user belongs to. This is done through the use of a pluggable interface, which has the ability to take a username and map it to a set of groups that the user belongs to.�In a default installation, the user-group mapping implementation forks off a subprocess that runs `id -Gn [username]`. That provides a list of groups like this: natty@vorpal:~/cloudera $ id -Gn natty
natty adm lpadmin netdev admin sambashare hadoop hdfs mapred The Hadoop daemons then use this list of groups, along with the username to determine if the user has appropriate permissions to access the file being requested. There are also other implementations that come packaged with Hadoop, including one that allows the system to be configured to get user-group mappings from an LDAP or Active Directory systems. This is useful if the groups necessary for setting up permissions are resident in an LDAP system, but not in Unix on the cluster hosts. Something to be aware of is that the set of groups that the NameNode and JobTracker are aware of may be different than the set of groups that a user belongs to on a client machine. All authorization is done at the NameNode/JobTracker level, so the users and groups on the DataNodes and TaskTrackers don�t affect authorization, although they may be necessary if Kerberos authentication is enabled. Additionally, it is very important that the NameNode and the JobTracker both be aware of the same groups for any given user, or there may be undefined results when executing jobs.�If there�s ever any doubt of what groups a user belongs to, `hadoop dfsgroups` and `hadoop mrgroups` may be used to find out what groups that a user belongs to, according to the NameNode and JobTracker, respectively. Putting it all together A proper, safe security protocol for Hadoop may require a combination of authorization and authentication. Admins should look at their security requirements and determine which solutions are right for them, and how much risk they can take on regarding their handling of data. Additionally, if you are going to enable Hadoop�s Kerberos features, I strongly recommend looking into Cloudera Manager, which helps make the Kerberos configuration and setup significantly easier than doing it all by hand.</snippet></document><document id="443"><title>Apache HBase 0.90.6 is now available</title><url>http://blog.cloudera.com/blog/2012/03/apache-hbase-0-90-6-is-now-available/</url><snippet>Apache HBase 0.90.6 is now available. It is a bug fix release covering 31 bugs and 5 improvements.� Among them, 3 are blockers and 3 are critical, such as: HBASE-5008: HBase can not provide services to a region when it can’t flush the region, but considers it stuck in flushing, HBASE-4773: HBaseAdmin may leak ZooKeeper connections, HBASE-5060:�HBase client may be blocked forever when there is a temporary network failure. This release has improved system robustness and availability by fixing bugs that cause potential data loss, system unavailability, possible deadlocks, read�inconsistencies and resource leakage. The 0.90.6 release is backward compatible with 0.90.5. The fixes in this release will be included in CDH3u4. Acknowledgements: A special thanks to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc). Cloudera is hosting the first ever HBase conference at the InterContinental hotel in San Francisco on May 22, 2012. Register by April 6 to catch the early bird price!�</snippet></document><document id="444"><title>Apache MRUnit 0.8.1-incubating has been released!</title><url>http://blog.cloudera.com/blog/2012/03/apache-mrunit-0-8-1-incubating-has-been-released/</url><snippet>This blog was originally posted on the Apache Software Foundation MRUnit’s blog. We (the Apache MRUnit team) have just released Apache MRUnit 0.8.1-incubating. Apache MRUnit is an Apache Incubator project. MRUnit is a Java library that helps developers unit test Apache Hadoop MapReduce jobs. Unit testing is a technique for improving project quality and reducing overall costs by writing a small amount of code that can automatically verify the software you write performs as intended. This is considered a best practice in software development since it helps identify defects early, before they’re deployed to a production system. The MRUnit project is actively looking for contributors, even ones brand new to the world of open source software. There are many ways to contribute: documentation, bug reports, blog articles, etc. If you are interested but have no idea where to start, please email brock at cloudera dot com. If you are an experienced open source contributor, the MRUnit wiki explains How you can Contribute. The new release includes one bug fix and a number of new features including, but not limited to: Support Hadoop 0.20 and 0.23 (MRUNIT-31 and MRUNIT-56) Improved test failure messages (MRUNIT-15) Static driver factories (MRUNIT-43) Support Hadoop 0.20 and 0.23 In MAPREDUCE-954 Hadoop changed the public org.apache.hadoop.mapreduce API. The MRUnit team in MRUNIT-31 was able to provide one release artifact which supports both versions of the API. MRUNIT-56 provided three maven classifiers, hadoop020, hadoop023, and hadoop100 to provide easy dependency on the correct binary. Improved test failure messages Improving the test case failure messages is how I became involved in the project. Before this change you had to look at your console to see what errors had occurred. Now, when a test fails the error message contains an explanation of exactly what failed: java.lang.RuntimeException: 1 Error(s): (Missing expected output (cat1, 1) at position 0.)
	at org.apache.hadoop.mrunit.TestDriver.validate(TestDriver.java:194)
	at org.apache.hadoop.mrunit.MapDriverBase.runTest(MapDriverBase.java:186)
	at TestWordCount.testMapper(TestWordCount.java:34)
 Static driver factories Static driver factories are an addition which reduce much the weight imposed by Java generics. Before this new feature, to create a driver, say MapDriver, you would have to specify all the generic types: mapDriver = new MapDriver&lt;LongWritable, Text, Text, IntWritable&gt;();
 Now, the generic types are inferred and you can simply specify: mapDriver = MapDriver.newMapDriver();
 This should provide users with the ability to write simpler code. Additionally, using the MapDriver constructor will continue to work and is fully supported. Download the source release, binaries via the Maven artifact and classifier, or download them directly via the Maven repo. More information on MRUnit is available on the project website.</snippet></document><document id="445"><title>Apache HBase + Apache Hadoop + Xceivers</title><url>http://blog.cloudera.com/blog/2012/03/hbase-hadoop-xceivers/</url><snippet>Introduction Some of the configuration properties found in Apache Hadoop have a direct effect on clients, such as Apache HBase. One of those properties is called “dfs.datanode.max.xcievers”, and�belongs to the HDFS subproject. It defines the number of server side threads and – to some extent – sockets used for data connections. Setting this number too low can cause problems as you grow or increase utilization of your cluster. This post will help you to understand what happens between the client and server, and how to determine a reasonable number for this property. The Problem Since HBase is storing everything it needs inside HDFS, the hard upper boundary imposed by the�”dfs.datanode.max.xcievers” configuration property�can result in too few resources being available to HBase, manifesting itself as IOExceptions on either side of the connection. Here is an example from the HBase mailing list [1], where the following messages were initially logged on the RegionServer side:� 2008-11-11 19:55:52,451 INFO org.apache.hadoop.dfs.DFSClient:�Exception in createBlockOutputStream java.io.IOException: Could not�read from stream 2008-11-11 19:55:52,451 INFO org.apache.hadoop.dfs.DFSClient:�Abandoning block blk_-5467014108758633036_595771 2008-11-11 19:55:58,455 WARN org.apache.hadoop.dfs.DFSClient:�DataStreamer Exception: java.io.IOException: Unable to create new block. 2008-11-11 19:55:58,455 WARN org.apache.hadoop.dfs.DFSClient: Error�Recovery for block blk_-5467014108758633036_595771 bad datanode[0] 2008-11-11 19:55:58,482 FATAL�org.apache.hadoop.hbase.regionserver.Flusher: Replay of hlog required.�Forcing server shutdown Correlating this with the Hadoop DataNode logs revealed the following entry: ERROR org.apache.hadoop.dfs.DataNode:�DatanodeRegistration(10.10.10.53:50010,storageID=DS-1570581820-10.10.10.53-50010-1224117842339,infoPort=50075, ipcPort=50020):DataXceiver: java.io.IOException:�xceiverCount 258 exceeds the limit of concurrent xcievers 256�� In this example, the low value of “dfs.datanode.max.xcievers” for the DataNodes caused the entire RegionServer to shut down. This is a really bad situation. Unfortunately, there is no hard-and-fast rule that explains how to compute the required limit. It is commonly advised to raise the number from the default of 256 to something like 4096 (see [1], [2], [3], [4], and [5]�for reference). This is done by adding this property to the hdfs-site.xml file of all DataNodes (note that it is misspelled):� &lt;property&gt;� � &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt; � � &lt;value&gt;4096&lt;/value&gt; � &lt;/property&gt; Note: You will need to restart your DataNodes after making this change to the configuration file. This should help with the above problem, but you still might want to know more about how this all plays together, and what HBase is doing with these resources.�We will discuss this in the remainder of this post. But before we do, we need to be clear about why you cannot simply set this number very high, say 64K and be done with it. There is a reason for an upper boundary, and it is twofold: first, threads need their own stack, which means they occupy memory. For current servers this means 1MB per thread[6] by default. In other words, if you use up all the 4096 DataXceiver threads, you need around 4GB of heap to accommodate them. This cuts into the space you have assigned for memstores and block caches, as well as all the other moving parts of the JVM. In a worst case scenario, you might run into an OutOfMemoryException, and the RegionServer process is toast. You want to set this property to a reasonably high number, but not too high either. Second, having these many threads active you will also see your CPU becoming increasingly loaded. There will be many context switches happening to handle all the concurrent work, which takes away resources for the real work. As with the concerns about memory, you want the number of threads not grow boundlessly, but provide a reasonable upper boundary – and that is what “dfs.datanode.max.xcievers” is for. Hadoop File System Details From the client side, the HDFS library is providing the abstraction called Path. This class represents a file in a file system supported by Hadoop, represented by the FileSystem class. There are a few concrete implementation of the abstract FileSystem class, one of which is the DistributedFileSytem, representing HDFS. This class in turn wraps the actual DFSClient class that handles all interactions with the remote servers, i.e. the NameNode and the many DataNodes. When a client, such as HBase, opens a file, it does so by, for example, calling the�open()�or�create() methods of the FileSystem class, here the most simplistic incarnations � public DFSInputStream open(String src) throws IOException � public FSDataOutputStream create(Path f) throws IOException The returned stream instance is what needs a server-side socket and thread, which are used to read and write blocks of data. They form part of the contract to exchange data between the client and server. Note that there are other, RPC-based protocols in use between the various machines, but for the purpose of this discussion they can be ignored. The stream instance returned is a specialized DFSOutputStream�or DFSInputStream class, which handle all of the interaction with the NameNode to figure out where the copies of the blocks reside, and the data communication per block per DataNode. On the server side, the DataNode wraps an instance of�DataXceiverServer, which is the actual class that reads the above configuration key and also throws the above exception when the limit is exceeded. When the DataNode starts, it creates a thread group and starts the mentioned�DataXceiverServer�instance like so: � this.threadGroup = new ThreadGroup(“dataXceiverServer”); � this.dataXceiverServer = new Daemon(threadGroup, � � � new DataXceiverServer(ss, conf, this)); � this.threadGroup.setDaemon(true); // auto destroy when empty� Note that the�DataXceiverServer�thread is already taking up one spot of the thread group. The DataNode also has this internal class to retrieve the number of currently active threads in this group: � /** Number of concurrent xceivers per node. */ � int getXceiverCount() { � � return threadGroup == null ? 0 : threadGroup.activeCount(); � } Reading and writing blocks, as initiated by the client, causes for a connection to be made, which is wrapped by the�DataXceiverServer�thread into a�DataXceiver�instance. During this hand off, a thread is created and registered in the above thread group. So for every active read and write operation a new thread is tracked on the server side. If the count of threads in the group exceeds the configured maximum then the said exception is thrown and recorded in the DataNode’s logs: � if (curXceiverCount &gt; dataXceiverServer.maxXceiverCount) { � � throw new IOException(“xceiverCount ” + curXceiverCount � � � � � � � � � � � � � + ” exceeds the limit of concurrent xcievers “ � � � � � � � � � � � � � + dataXceiverServer.maxXceiverCount); � } Implications for Clients Now, the question is, how does the client reading and writing relate to the server side threads. Before we go into the details though, let’s use the debug information that the DataXceiver class logs when it is created and closed � LOG.debug(“Number of active connections is: ” + datanode.getXceiverCount()); � … � LOG.debug(datanode.dnRegistration + “:Number of active connections is: “�� � + datanode.getXceiverCount()); and monitor during a start of HBase what is logged on the DataNode. For simplicity’s sake this is done on a pseudo distributed setup with a single DataNode and RegionServer instance. The following shows the top of the RegionServer’s status page. � The important part is in the “Metrics” section, where it says “storefiles=22″. So, assuming that HBase has at least that many files to handle, plus some extra files for the write-ahead log, we should see the above logs message state that we have at least 22 “active connections”. Let’s start HBase and check the DataNode and RegionServer log files: Command Line: $ bin/start-hbase.sh … DataNode Log: 2012-03-05 13:01:35,309 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 1 2012-03-05 13:01:35,315 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 2 12/03/05 13:01:35 INFO regionserver.MemStoreFlusher: globalMemStoreLimit=396.7m, globalMemStoreLimitLowMark=347.1m, maxHeap=991.7m 12/03/05 13:01:39 INFO http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030 2012-03-05 13:01:40,003 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 1 12/03/05 13:01:40 INFO regionserver.HRegionServer: Received request to open region: -ROOT-,,0.70236052 2012-03-05 13:01:40,882 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 3 2012-03-05 13:01:40,884 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 4 2012-03-05 13:01:40,888 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 3 … 12/03/05 13:01:40 INFO regionserver.HRegion: Onlined -ROOT-,,0.70236052; next sequenceid=63083 2012-03-05 13:01:40,982 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 3 2012-03-05 13:01:40,983 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 4 … 12/03/05 13:01:41 INFO regionserver.HRegionServer: Received request to open region: .META.,,1.1028785192 2012-03-05 13:01:41,026 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 3 2012-03-05 13:01:41,027 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 4 … 12/03/05 13:01:41 INFO regionserver.HRegion: Onlined .META.,,1.1028785192; next sequenceid=63082 2012-03-05 13:01:41,109 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 3 2012-03-05 13:01:41,114 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 4 2012-03-05 13:01:41,117 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 5 12/03/05 13:01:41 INFO regionserver.HRegionServer: Received request to open 16 region(s) 12/03/05 13:01:41 INFO regionserver.HRegionServer: Received request to open region: usertable,,1330944810191.62a312d67981c86c42b6bc02e6ec7e3f. 12/03/05 13:01:41 INFO regionserver.HRegionServer: Received request to open region: usertable,user1120311784,1330944810191.90d287473fe223f0ddc137020efda25d. … 2012-03-05 13:01:41,246 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 6 2012-03-05 13:01:41,248 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 7 … 2012-03-05 13:01:41,257 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 10 2012-03-05 13:01:41,257 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 9 … 12/03/05 13:01:41 INFO regionserver.HRegion: Onlined usertable,user1120311784,1330944810191.90d287473fe223f0ddc137020efda25d.; next sequenceid=62917 12/03/05 13:01:41 INFO regionserver.HRegion: Onlined usertable,,1330944810191.62a312d67981c86c42b6bc02e6ec7e3f.; next sequenceid=62916 … 12/03/05 13:01:41 INFO regionserver.HRegion: Onlined usertable,user1361265841,1330944811370.80663fcf291e3ce00080599964f406ba.; next sequenceid=62919 2012-03-05 13:01:41,474 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 6 2012-03-05 13:01:41,491 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 7 2012-03-05 13:01:41,495 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 8 2012-03-05 13:01:41,508 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 7 … 12/03/05 13:01:41 INFO regionserver.HRegion: Onlined usertable,user1964968041,1330944848231.dd89596e9129e1caa7e07f8a491c9734.; next sequenceid=62920 2012-03-05 13:01:41,618 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 6 2012-03-05 13:01:41,621 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 7 … 2012-03-05 13:01:41,829 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 7 12/03/05 13:01:41 INFO regionserver.HRegion: Onlined usertable,user515290649,1330944849739.d23924dc9e9d5891f332c337977af83d.; next sequenceid=62926 2012-03-05 13:01:41,832 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 6 2012-03-05 13:01:41,838 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 7 12/03/05 13:01:41 INFO regionserver.HRegion: Onlined usertable,user757669512,1330944850808.cd0d6f16d8ae9cf0c9277f5d6c6c6b9f.; next sequenceid=62929 … 2012-03-05 14:01:39,711 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 4 2012-03-05 22:48:41,945 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 4 12/03/05 22:48:41 INFO regionserver.HRegion: Onlined usertable,user757669512,1330944850808.cd0d6f16d8ae9cf0c9277f5d6c6c6b9f.; next sequenceid=62929 2012-03-05 22:48:41,963 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 4 You can see how the regions are opened one after the other, but what you also might notice is that the number of active connections never climbs to 22 – it barely even reaches 10. Why is that? To understand this better, we have to see how files in HDFS map to the server-side DataXceiver’s instance – and the actual threads they represent.� Hadoop Deep Dive The aforementioned DFSInputStream and DFSOutputStream are really facades around the usual stream concepts. They wrap the client-server communication into these standard Java interfaces, while internally routing the traffic to a selected DataNode – which is the one that holds a copy of the current block. It has the liberty to open and close these connection as needed. As a client reads a file in HDFS, the client library classes switch transparently from block to block, and therefore from DataNode to DataNode, so it has to open and close connections as needed.� The�DFSInputStream�has an instance of a�DFSClient.BlockReader�class, that opens the connection to the DataNode. The stream instance calls blockSeekTo()�for every call to read()�which takes care of opening the connection, if there is none already. Once a block is completely read the connection is closed. Closing the stream has the same effect of course.� The�DFSOutputStream�has a similar helper class, the�DataStreamer. It tracks the connection to the server, which is initiated by the�nextBlockOutputStream() method. It has further internal classes that help with writing the block data out, which we omit here for the sake of brevity. Both writing and reading blocks requires a thread to hold the socket and intermediate data on the server-side, wrapped in the DataXceiver instance. Depending what your client is doing, you will see the number of connections fluctuate around the number of currently accessed files in HDFS. Back to the HBase riddle above: the reason you do not see up to 22 (and more) connections during the start is that while the regions open, the only required data is the HFile’s info block. This block is read to gain vital details about each file, but then closed again. This means that the server-side resource is released in quick succession. The remaining four connections are harder to determine. You can use JStack to dump all threads on the DataNode, which in this example shows this entry: “DataXceiver for client /127.0.0.1:64281 [sending block blk_5532741233443227208_4201]” daemon prio=5 tid=7fb96481d000 nid=0x1178b4000 runnable [1178b3000] � �java.lang.Thread.State: RUNNABLE � �… “DataXceiver for client /127.0.0.1:64172 [receiving block blk_-2005512129579433420_4199 client=DFSClient_hb_rs_10.0.0.29,60020,1330984111693_1330984118810]” daemon prio=5 tid=7fb966109000 nid=0x1169cb000 runnable [1169ca000] � �java.lang.Thread.State: RUNNABLE � �… These are the only DataXceiver entries (in this example), so the count in the thread group is a bit misleading. Recall that the DataXceiverServer daemon thread already accounts for one extra entry, which combined with the two above accounts for the three active connections – which in fact means three active threads. The reason the log states four instead, is that it logs the count from an active thread that is about to finish. So, shortly after the count of four is logged, it is actually one less, i.e. three and hence matching our head count of active threads. Also note that the internal helper classes, such as the PacketResponder occupy another thread in the group while being active. The JStack output does indicate that fact, listing the thread as such: �”PacketResponder 0 for Block blk_-2005512129579433420_4199″ daemon prio=5 tid=7fb96384d000 nid=0x116ace000 in Object.wait() [116acd000] � �java.lang.Thread.State: TIMED_WAITING (on object monitor) � � �at java.lang.Object.wait(Native Method) � � �at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder \ � � � �.lastDataNodeRun(BlockReceiver.java:779) � � �- locked (a org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder) �� � at org.apache.hadoop.hdfs.server.datanode.BlockReceiver$PacketResponder.run(BlockReceiver.java:870) �� � at java.lang.Thread.run(Thread.java:680) This thread is currently in�TIMED_WAITING�state and is not considered active. That is why the count emitted by the DataXceiver log statements is not including these kind of threads. If they become active due to the client sending sending data, the active thread count will go up again. Another thing to note its that this thread does not need a separate connection, or socket, between the client and the server. The�PacketResponder�is just a thread on the server side to receive block data and stream it to the next DataNode in the write pipeline. The Hadoop fsck command also has an option to report what files are currently open for writing: $ hadoop fsck /hbase -openforwrite FSCK started by larsgeorge from /10.0.0.29 for path /hbase at Mon Mar 05 22:59:47 CET 2012 ……/hbase/.logs/10.0.0.29,60020,1330984111693/10.0.0.29%3A60020.1330984118842 0 bytes, 1 block(s), OPENFORWRITE: ………………………………..Status: HEALTHY �Total size:���� 2088783626 B �Total dirs:���� 54 �Total files: � �45 �… This does not immediately relate to an occupied server-side thread, as these are allocated by block ID. But you can glean from it, that there is one open block for writing. The Hadoop command has additional options to print out the actual files and block ID they are comprised of: $ hadoop fsck /hbase -files -blocks FSCK started by larsgeorge from /10.0.0.29 for path /hbase at Tue Mar 06 10:39:50 CET 2012 … /hbase/.META./1028785192/.tmp &lt;dir&gt; /hbase/.META./1028785192/info &lt;dir&gt; /hbase/.META./1028785192/info/4027596949915293355 36517 bytes, 1 block(s):� OK 0. blk_5532741233443227208_4201 len=36517 repl=1 … Status: HEALTHY �Total size:���� 2088788703 B �Total dirs:���� 54 �Total files:���� 45 (Files currently being written: 1) �Total blocks (validated):���� 64 (avg. block size 32637323 B) (Total open file blocks (not validated): 1) �Minimally replicated blocks:���� 64 (100.0 %) �… This gives you two things. First, the summary states that there is one open file block at the time the command ran – matching the count reported by the “-openforwrite” option above. Secondly, the list of blocks next to each file lets you match the thread name to the file that contains the block being accessed. In this example the block with the ID “blk_5532741233443227208_4201″ is sent from the server to the client, here a RegionServer. This block belongs to the HBase .META. table, as shown by the output of the Hadoop fsck command. The combination of JStack and fsck can serve as a poor mans replacement for lsof (a tool on the Linux command line to “list open files”). The JStack also reports that there is a DataXceiver thread, with an accompanying PacketResponder, for block ID “blk_-2005512129579433420_4199″, but this ID is missing from the list of blocks reported by fsck. This is because the block is not yet finished and therefore not available to readers. In other words, Hadoop fsck only reports on complete (or synced[7][8], for Hadoop version that support this feature) blocks.� Back to HBase Opening all the regions does not need as many resources on the server as you would have expected. If you scan the entire HBase table though, you force HBase to read all of the blocks in all HFiles:� HBase Shell: hbase(main):003:0&gt; scan ‘usertable’ … 1000000 row(s) in 1460.3120 seconds DataNode Log: 2012-03-05 14:42:20,580 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 6 2012-03-05 14:43:23,293 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 7 2012-03-05 14:43:23,299 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 8 … 2012-03-05 14:49:24,332 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 11 2012-03-05 14:49:24,332 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 10 2012-03-05 14:49:59,987 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 11 2012-03-05 14:51:12,603 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 12 2012-03-05 14:51:12,605 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 11 2012-03-05 14:51:46,473 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 12 … 2012-03-05 14:56:59,420 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 15 2012-03-05 14:57:31,722 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 16 2012-03-05 14:58:24,909 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 17 2012-03-05 14:58:24,910 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 16 … 2012-03-05 15:04:17,688 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 21 2012-03-05 15:04:17,689 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 22 2012-03-05 15:04:54,545 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 21 2012-03-05 15:05:55,901 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeRegistration(127.0.0.1:50010, storageID=DS-1423642448-10.0.0.64-50010-1321352233772, infoPort=50075, ipcPort=50020):Number of active connections is: 22 2012-03-05 15:05:55,901 DEBUG org.apache.hadoop.hdfs.server.datanode.DataNode: Number of active connections is: 21 The number of active connections reaches the elusive 22 now. Note that this count already includes the server thread, so we are still a little short of what we could consider the theoretical maximum – based on the number of files HBase has to handle. What does that all mean? So, how many “xcievers (sic)” do you need? Given you only use HBase, you could simply monitor the above “storefiles” metric (which you get also through Ganglia or JMX) and add a few percent for intermediate and write-ahead log files. This should work for systems in motion. However, if you were to determine that number on an idle, fully compacted system and assume it is the maximum, you might find this number being too low once you start adding more store files during regular memstore flushes, i.e. as soon as you start to add data to the HBase tables. Or if you also use MapReduce on that same cluster, Flume log aggregation, and so on. You will need to account for those extra files, and, more importantly, open blocks for reading and writing.� Note again that the examples in this post are using a single DataNode, something you will not have on a real cluster. To that end, you will have to divide the total number of store files (as per the HBase metric) by the number of DataNodes you have. If you have, for example, a store file count of 1000, and your cluster has 10 DataNodes, then you should be OK with the default of 256 xceiver threads per DataNode. The worst case would be the number of all active readers and writers, i.e. those that are currently sending or receiving data. But since this is hard to determine ahead of time, you might want to consider building in a decent reserve. Also, since the writing process needs an extra – although shorter lived – thread (for the PacketResponder) you have to account for that as well. So a reasonable, but rather simplistic formula could be: � This formula takes into account that you need about two threads for an active writer and another for an active reader. This is then summed up and divided by the number of DataNodes, since you have to specify the “dfs.datanode.max.xcievers” per DataNode. If you loop back to the HBase RegionServer screenshot above, you saw that there were 22 store files. These are immutable and will only be read, or in other words occupy one thread only. For all memstores that are flushed to disk you need two threads – but only until they are fully written. The files are finalized and closed for good, cleaning up any thread in the process. So these come and go based on your flush frequency. Same goes for compactions, they will read N files and write them into a single new one, then finalize the new file.�As for the write-ahead logs, these will occupy a thread once you have started to add data to any table. There is a log file per server, meaning that you can only have twice as many active threads for these files as you have RegionServers. For a pure HBase setup (HBase plus its own HDFS, with no other user), we can estimate the number of needed DataXceiver’s with the following formula: Since you will be hard pressed to determine the active number of store files, flushes, and so on, it might be better to estimate the theoretical maximum instead. This maximum value takes into account that you can only have a single flush and compaction active per region at any time. The maximum number of logs you can have active matches the number of RegionServers, leading us to this formula: �� Obviously, the number of store files will increase over time, and the number of regions typically as well. Same for the numbers of servers, so keep in mind to adjust this number over time. In practice, you can add a buffer of, for example, 20%, as shown in the formula below – in an attempt to not force you to change the value too often.� On the other hand, if you keep the number of regions fixed per server[9], and rather split them manually, while adding new servers as you grow, you should be able to keep this configuration property stable for each server. Final Advice &amp; TL;DR Here is the final formula you want to use: It computes the maximum number of threads needed, based on your current HBase vitals (no. of store files, regions, and region servers). It also adds a fudge factor of 20% to give you room for growth. Keep an eye on the numbers on a regular basis and adjust the value as needed. You might want to use Nagios with appropriate checks to warn you when any of the vitals goes over a certain percentage of change. Note: Please make sure you also adjust the number of file handles your process is allowed to use accordingly[10]. This affects the number of sockets you can use, and if that number is too low (default is often 1024), you will get connection issues first.� Finally, the engineering devil on one of your shoulders should already have started to snicker about how horribly non-Erlang-y this is, and how you should use an event driven approach, possibly using Akka with Scala[11] – if you want to stay within the JVM world. Bear in mind though that the clever developers in the community share the same thoughts and have already started to discuss various approaches[12][13].� Links: [1]�http://old.nabble.com/Re%3A-xceiverCount-257-exceeds-the-limit-of-concurrent-xcievers-256-p20469958.html [2]�http://ccgtech.blogspot.com/2010/02/hadoop-hdfs-deceived-by-xciever.html [3]�https://issues.apache.org/jira/browse/HDFS-1861�”Rename dfs.datanode.max.xcievers and bump its default value” [4]�https://issues.apache.org/jira/browse/HDFS-1866�”Document dfs.datanode.max.transfer.threads in hdfs-default.xml” [5]�http://hbase.apache.org/book.html#dfs.datanode.max.xcievers [6]�http://www.oracle.com/technetwork/java/hotspotfaq-138619.html#threads_oom [7]�https://issues.apache.org/jira/browse/HDFS-200�”In HDFS, sync() not yet guarantees data available to the new readers” [8]�https://issues.apache.org/jira/browse/HDFS-265�”Revisit append” [9]�http://search-hadoop.com/m/CBBoV3z24H1�”HBase, mail # user – region size/count per regionserver” [10]�http://hbase.apache.org/book.html#ulimit�”ulimit and nproc” [11]�http://akka.io/�”Akka” [12]�https://issues.apache.org/jira/browse/HDFS-223�”Asynchronous IO Handling in Hadoop and HDFS” [13]�https://issues.apache.org/jira/browse/HDFS-918�”Use single Selector and small thread pool to replace many instances of BlockSender for reads”  </snippet></document><document id="446"><title>Real-Time Your Hadoop! Join us at HBaseCon 2012</title><url>http://blog.cloudera.com/blog/2012/03/real-time-your-hadoop-come-join-us-at-hbasecon-2012-the-industrys-first-apache-hbase-community-conference/</url><snippet>We�re excited to host the first ever HBaseCon this May 22, 2012 in San Francisco – the industry�s first Apache HBase community conference. The theme of this first HBaseCon is �Real-Time Your Hadoop,� and the conference will be a culmination of the best of the HBase community, including speakers and sponsors from across the HBase landscape. As Michael Stack, the champion of the HBase community and engineer at StumbleUpon, put it, �It�s going to be a great day out for the community. Anyone with any kind of an HBase itch at all whether apps, ops, or dev should be sure to drop by.� The call for speakers wrapped up last week and the HBaseCon Program Committee is now parsing through all the submissions to bring you the most interesting and inspiring talks the HBase community has to offer. We will be announcing the speakers later this month. According to Lars George, solutions architect for Cloudera and author of HBase: The Definitive Guide, �There is a whole ecosystem being created around HBase, and the community is thriving and incredibly involved. HBaseCon will bring the community together in a collaborative and resourceful way, and I look forward to being a part of this first conference.� We hope to see you there! Early bird registration ends April 6 and there is a discounted price for contributors. Cloudera will also offer a hands-on HBase training session after the conference, May 23 and 24, 2012 for developers already familiar with Apache Hadoop. For more information: http://www.hbasecon.com/training/</snippet></document><document id="447"><title>High Availability for the Hadoop Distributed File System (HDFS)</title><url>http://blog.cloudera.com/blog/2012/03/high-availability-for-the-hadoop-distributed-file-system-hdfs/</url><snippet>Background Apache Hadoop consists of two primary components: HDFS and MapReduce. HDFS, the Hadoop Distributed File System, is the primary storage system of Hadoop, and is responsible for storing and serving all data stored in Hadoop. MapReduce is a distributed processing framework designed to operate on data stored in HDFS. HDFS has long been considered a highly reliable file system. �An empirical study done at Yahoo! concluded that across Yahoo!�s 20,000 nodes running Apache Hadoop in 10 different clusters in 2009, HDFS lost only 650 blocks out of 329 million total blocks. The vast majority of these lost blocks were due to a handful of bugs which have long since been fixed. Despite this very high level of reliability, HDFS has always had a well-known single point of failure which impacts HDFS�s availability: the system relies on a single Name Node to coordinate access to the file system data. In clusters which are used exclusively for ETL or batch-processing workflows, a brief HDFS outage may not have immediate business impact on an organization; however, in the past few years we have seen HDFS begin to be used for more interactive workloads or, in the case of HBase, used to directly serve customer requests in real time. In cases such as this, an HDFS outage will immediately impact the productivity of internal users, and perhaps result in downtime visible to external users. For these reasons, adding high availability (HA) to the HDFS Name Node became one of the top priorities for the HDFS community. The remainder of this post discusses the implementation of a new feature for HDFS, called the �HA Name Node.� For a detailed discussion of other issues surrounding the availability of Hadoop as a whole, take a look at this excellent blog post by my colleague Eli Collins. High-level Architecture The goal of the HA Name Node project is to add support for deploying two Name Nodes in an active/passive configuration. This is a common configuration for highly-available distributed systems, and HDFS�s architecture lends itself well to this design. Even in a non-HA configuration, HDFS already requires both a Name Node and another node with similar hardware specs which performs checkpointing operations for the Name Node. The design of the HA Name Node is such that the passive Name Node is capable of performing this checkpointing role, thus requiring no additional Hadoop server machines beyond what HDFS already requires. The HDFS Name Node is primarily responsible for serving two types of file system metadata: file system namespace information and block locations. Because of the architecture of HDFS, these must be handled separately. Namespace Information All mutations to the file system namespace, such as file renames, permission changes, file creations, block allocations, etc, are written to a persistent write-ahead log by the Name Node before returning success to a client call. In addition to this edit log, periodic checkpoints of the file system, called the fsimage, are also created and stored on-disk on the Name Node. Block locations, on the other hand, are stored only in memory. The locations of all blocks are received via �block reports� sent from the Data Nodes when the Name Node is started. The goal of the HA Name Node is to provide a hot standby Name Node that can take over serving the role of the active Name Node with no downtime. To provide this capability, it is critical that the standby Name Node has the most complete and up-to-date file system state possible in memory. Empirically, starting a Name Node from cold state can take tens of minutes to load the namespace information (fsimage and edit log) from disk, and up to an hour to receive the necessary block reports from all Data Nodes in a large cluster. The Name Node has long supported the ability to write its edit logs to multiple, redundant local directories. To address the issue of sharing state between the active and standby Name Nodes, the HA Name Node feature allows for the configuration of a special shared edits directory. This directory should be available via a network file system, and should be read/write accessible from both Name Nodes. This directory is treated as being required by the active Name Node, meaning that success will not be returned to a client call unless the file system change has been written to the edit log in this directory. The standby Name Node polls the shared edits directory frequently, looking for new edits written by the active Name Node, and reads these edits into its own in-memory view of the file system state. Note that requiring a single shared edits directory does not necessarily imply a new single point of failure. It does, however, mean that the filer providing this shared directory must itself be HA, and that multiple network routes should be configured between the Name Nodes and the service providing this shared directory. Plans to improve this situation are discussed further below. Block Locations The other part of keeping the standby Name Node hot is making sure that it has up-to-date block location information. Since block locations aren�t written to the Name Node edit log, reading from the shared edits directory is not sufficient to share this file system metadata between the two Name Nodes. To address this issue, when HA is enabled, all Data Nodes in the cluster are configured with the network addresses of both Name Nodes. Data Nodes send all block reports, block location updates, and heartbeats to both Name Nodes, but Data Nodes will only act on block commands issued by the currently-active Name Node. With both up-to-date namespace information and block locations in the standby Name Node, the system is able to perform a failover from the active Name Node to the standby with no delay. Client Failover Since multiple distinct daemons are now capable of serving as the active Name Node for a single cluster, the HDFS client must be able to determine which Name Node to communicate with at any given time. The HA Name Node feature does not support an active-active configuration, and thus all client calls must go to the active Name Node in order to be served. To implement this feature, the HDFS client was extended to support the configuration of multiple network addresses, one for each Name Node, which collectively represent the HA name service. The name service is identified by a single logical URI, which is mapped to the two network addresses of the HA Name Nodes via client-side configuration. These addresses are tried in order by the HDFS client. If a client makes a call to the standby Name Node, a special result is returned to the client, indicating that it should retry elsewhere. The configured addresses are tried in order by the client until an active Name Node is found. In the event that the active Name Node crashes while in the middle of processing a request, the client will be unable to determine whether or not the request was processed. For many operations such as reads (or idempotent writes such as setting permissions, setting modification time, etc), this is not a problem — the client may simply retry after the failover has completed. For others, the error must be bubbled up to the caller to be correctly handled. In the course of the HA project, we extended the Hadoop IPC system to be able to classify each operation�s idempotence using special annotations. Current Status Active development work began on the HA Name Node in August 2011, in a branch off of Apache Hadoop trunk. Development was done under the umbrella JIRAs HDFS-1623 and HADOOP-7454. Last Friday, March 2nd 2012 we merged this branch back into Apache Hadoop trunk. We closed over 170 individual JIRAs in the course of implementing this feature. The stated intention of the community is to merge this work from HDFS trunk into the 0.23 branch, where it will be released as an update of the Apache Hadoop 0.23 release line. Much of this work is already available as part of CDH4 beta 1, released on February 13th, 2012. Once a failover has been initiated, the actual process of stopping the active and starting the standby Name Node takes a matter of seconds or less. This speed allows for little or no detectable service disruption during a failover. I�ve personally run hundreds of MR jobs over a running HA cluster, doing failovers back and forth between two HA Name Nodes, without any job failures. This first implementation of the HA Name Node supports only manual failover — that is, failure of one of the Name Nodes is not automatically detected by the system, but rather requires intervention by an operator to initiate a failover between the Name Nodes. Though this is an obvious limitation, this version should still be useful to eliminate the need for planned HDFS downtime in many cases, e.g. changing the configuration of the Name Node, scheduled hardware maintenance of a Name Node, or scheduled OS upgrade of a Name Node. Next Up The highest priority feature to add to the HA Name Node implementation is support for automatically detecting the failure of the Active Name Node and initiating a failover to the Standby when it is determined that the Active is no longer functional. HDFS-3042 and its sub-tasks are actively being worked on to provide this functionality. The dependence on an HA filer for HDFS edit logs is a limitation that we�d like to address in the near to medium term as well. Several different options have been discussed to address this: BookKeeper – BookKeeper is a highly available write-ahead logging system. Work has already been done to allow the HDFS Name Node to be able to write its edits log to BookKeeper, though this has not yet been tested with the HA Name Node. Multiple, non-HA filers – the HA Name Node presently only supports logging to a single shared edits directory. Perhaps the easiest improvement from the current situation would be to allow the Name Node to log to several shared edits directories, and require that all edits be logged to a quorum of shared edits directories. This proposal is being tracked by HDFS-2782. Stream edits to remote NNs – in addition to writing edits to a local file system, edit log entries could be sent directly to other Name Nodes over the network. The active Name Node would require a quorum of the involved Name Nodes to acknowledge receipt of the edits before responding with success to the client call. Store edit logs in HDFS itself – systems such as HBase already use HDFS to store a write-ahead log of all data mutations. If HDFS were extended to have a modicum of bootstrapping information, it is not inconceivable that HDFS edit logs could be stored in HDFS itself. This proposal is being discussed on HDFS-2601. In the next few weeks, we will be evaluating all of these options and selecting one to implement. Currently, deploying HA Name Nodes is somewhat cumbersome, requiring the operator to manually synchronize the on-disk metadata of the two Name Nodes. HDFS-2731 aims to improve the user experience of this deployment process by having the second Name Node automatically synchronize itself with the state of the first Name Node. This feature will make the process faster and less error prone. Further Reading Take a look at the CDH4 docs for detailed information on configuring the HA Name Node in CDH4. Be on the lookout for an upcoming blog post from my colleague Todd Lipcon, which will go into greater detail about some of the specific challenges encountered while implementing the HA Name Node feature, and how these issues were overcome. Acknowledgments This work has been a community effort from the start, and represents the work of many contributors. Both the architecture and implementation were the collaborative effort of many. In particular, this work would not have been possible without contributions from Todd Lipcon, Eli Collins, Uma Maheswara Rao G, Bikas Saha, Suresh Srinivas, Jitendra Nath Pandey, Hari Mankude, Brandon Li, Sanjay Radia, Mingjie Lai, and Gregory Chanan. Also thanks to Dhruba Borthakur and Konstantin Shvachko for helpful design discussions and recommendations on testing. Thanks also to Stephen Chu, Wing Yew Poon, and Patrick Ramsey for their help in testing the HA Name Node.</snippet></document><document id="448"><title>Cloudera Manager | Activity Monitoring &amp; Operational Reports Demo Video</title><url>http://blog.cloudera.com/blog/2012/03/cloudera-manager-activity-monitoring-operational-reports-demo-video/</url><snippet>In this demo video, Philip Zeyliger, a software engineer at Cloudera, discusses the Activity Monitoring and Operational Reports in Cloudera Manager. Activity Monitoring The Activity Monitoring feature in Cloudera Manager consolidates all Hadoop cluster activities into a single, real-time view. This capability lets you see who is running what activities on the Hadoop cluster, both at the current time and through historical activity views. Activities are either individual MapReduce jobs or those that are part of larger workflows (via Oozie, Hive or Pig). Activity Monitoring provides many statistics � both in tabular displays and charts � about the resources used by individual Hadoop jobs and at the aggregate cluster level. The Comparison feature in Activity Monitoring shows the performance of the selected Hadoop job compared with the performance of other similar Hadoop jobs. The Task Distribution chart creates a map of the performance of task attempts based on a number of different measures (on the Y-axis) and the length of time taken to complete the task on the X-axis. This view helps you identify problems with the user code, data skew or slow running TaskTrackers. Operational Reports Operational Reports provide a visualization of current and historical disk utilization by user, user groups and directory. In addition, it tracks MapReduce activity on the Hadoop cluster by job, user, group or job ID. These reports are aggregated over selected time periods (hourly, daily, weekly, etc.) and can be exported as XLS or CSV files. Operational Reports are useful in presentations to explain how your Hadoop cluster is being used, for capacity planning and for chasing down irreverent users not cleaning up their temporary files. Here�s a previous blog post that explains how Operational Reports can aid in capacity planning: http://blog.cloudera.com/blog/2012/01/capacity-planning-with-cloudera-manager/ More information on Cloudera Manager is available here: http://www.cloudera.com/products-services/tools/ Previous Cloudera Manager demo videos blog posts: Cloudera Manager | Service and Configuration Management demo videos (Part I and II) http://blog.cloudera.com/blog/2012/02/cloudera-manager-service-and-configuration-management-demo-videos/ Cloudera Manager | Log Management, Event Management and Alerting demo video http://blog.cloudera.com/blog/2012/02/cloudera-manager-log-management-event-management-and-alerting-demo-video/ Cloudera Manager | Service Monitoring demo video http://blog.cloudera.com/blog/2012/02/cloudera-manager-hadoop-service-monitoring-demo-video/</snippet></document><document id="449"><title>Thoughts on Cloudera and Cisco UCS reference architecture for Apache Hadoop</title><url>http://blog.cloudera.com/blog/2012/03/thoughts-on-cloudera-and-cisco-ucs-reference-architecture-for-hadoop/</url><snippet>Cloudera and Cisco jointly announced a reference architecture for running Cloudera’s Distribution Including Apache Hadoop (CDH) and Cloudera Manager on Cisco’s Unified Computing System (UCS) last November. It was the first Apache Hadoop reference architecture assembled by Cisco, and is proudly certified by Cloudera. I bring a different perspective on the Cloudera-Cisco relationship, as I worked for over five years in Cisco on the software powering the Nexus 5000 series switches and the Cisco Virtual Interface Card. I now work at Cloudera on the HBase team, and can fully appreciate the synergies that the Cloudera and Cisco reference architecture brings to the table. I know that Cisco is actively investing in development and support for the rack servers and switches that comprise UCS. Working at Cloudera now, I see the tireless effort that goes into each CDH and Cloudera Manager release from leaders in the Apache Hadoop community. The combination of CDH and UCS is a natural marriage of best-of-breed technologies that delivers clear advantages to the big data end-user: CDH is a supported and proven big data platform, with clear advantages in ease of deployment and no vendor lock-in since it is 100% open source Cloudera Manager provides easy and powerful Hadoop cluster management UCS provides massive cross-sectional network bandwidth between servers when using 10 gigabit Ethernet adapter cards. More bandwidth means faster execution of network-heavy jobs, especially within a rack. UCS provides very good cluster management capabilities through UCS Manager. UCS Manager empowers the network and server adminstrators with fully-integrated control and monitoring of compute, storage, and networking elements from management GUI, CLI, XML, and SNMP interfaces. The reference architrecture also specifies up to 16 TB of storage per node for large MapReduce workloads, and a wide range of Intel Xeon CPUs, with timely upgrades to Romley and beyond. Helpful Links More details on the Cloudera and Cisco UCS reference architecture for Hadoop A video describing the Cloudera-Cisco relationship</snippet></document><document id="450"><title>Indexing Files via Solr and Java MapReduce</title><url>http://blog.cloudera.com/blog/2012/03/indexing-files-via-solr-and-java-mapreduce/</url><snippet>Several weeks ago, I set about to demonstrate the ease with which Solr and Map/Reduce can be integrated. I was unable to find a simple, yet comprehensive, primer on integrating the two technologies. So I set about to write one. What follows is my bare-bones tutorial on getting Solr up and running to index each word of the complete works of Shakespeare. Note: Special thanks to Sematext for looking over the Solr bits and making sure they are sane. Check them out if you�re going to be doing a lot of work with Solr, ElasticSearch, or search in general and want to bring in the experts. First things first The way that I got started was by instantiating a new CentOS 6 Virtual Machine. You can pick a different flavor of Linux if that suits you; Hadoop should work fine on any (though advocated distros are SuSE, Ubuntu/Debian, RedHat/CentOS). If you are fine with CentOS and want to skip some of the manual labor here, you can download a pre-loaded Virtual Machine from the Cloudera Downloads section, that already includes an installation of Sun Java 6u21 and CDH3u3. You can then skip ahead to installing Solr and downloading sample data as outlined below. If you are proceeding with a new (virtual) machine, then follow along as follows: Make sure to disable SELinux, if applicable, and enable sshd. For CentOS6, that was done with the following commands: [user@localhost ~]$ sudo chkconfig --levels 2345 sshd on
[user@localhost ~]$ /etc/init.d/sshd start
[user@localhost ~]$ vim /etc/selinux/config [set to disabled] On that machine, download and install: Java – I’d recommend Java 6u26 as it has been tested with CDH http://www.oracle.com/technetwork/java/javase/downloads/index-jsp-138363.html Oracle Java never seems to play nicely with the /etc/alternative system (in my experience), so I force it to be the preferred JRE the old fashioned way: [user@localhost ~]$ sudo rm /usr/bin/java
[user@localhost ~]$ sudo ln -s /usr/java/jdk1.6.0_26/jre/bin/java \
 /usr/bin/java Solr – download and unzip/untar in whatever directory that you like. For the purpose of this article, I’ll refer to it as http://lucene.apache.org/solr/downloads.html Hadoop – I am, obviously, biased. But my recommendation would be to use Cloudera Manager (free up to 50 nodes) to set up your VM as a pseudo-distributed cluster http://www.cloudera.com/products-services/tools/ Sample data – Complete works of William Shakespeare. I’d recommend unzipping into a single directory. I’ll refer to it as http://www.ipl.org/div/shakespeare/ You can validate that all of the pieces are installed and running correctly by doing the following: Java [user@localhost ~]$ java -version
java version "1.6.0_21"
Java(TM) SE Runtime Environment (build 1.6.0_21-b06)
Java HotSpot(TM) 64-Bit Server VM (build 17.0-b16, mixed mode) Solr [user@localhost ~]$ cd /example
[user@localhost ~]$ java -jar start.jar The goal is to get up and running quickly here, so I am opting to use the Solr example configuration. Worth noting also that when run in this manner, the Solr server will be started with the default JVM heap size – which I believe to be the smaller of {1/4 system memory or 1GB}. Now, you should be able to access the Solr administration GUI (one of the niceties of Solr!) via a web browser inside your VM with the address: http://localhost:8983/solr/admin Hadoop You can validate that Hadoop is installed and running successfully by navigating in your VM’s browser to: http://localhost:7180, logging in as admin/admin, and seeing the following: You probably want to export the client config XML files (can be done with a single click via Cloudera Manager – see the Generate Client Configuration buttong), copy them to /usr/lib/hadoop/conf, and then copy the sample text into hdfs: [user@localhost ~]$ hadoop fs -put &lt;shakespeare&gt; shakespeare Creating the indexing code I have some history with Lucene from a past life, so the high level functionality of Solr was familiar to me. In a nutshell, you index files within Java code by creating a SolrInputDocument, which represents a single entity to index – a file or document generally – and using the .addField() to attach fields to this document that you’d later like to search. The driver code for the indexer is very simple, in that it takes input file path(s) off the command line, and runs the mapper on the files that it finds. Note that it will accept a directory, and parse all of the files that it finds within. public class IndexDriver extends Configured implements Tool {     

  public static void main(String[] args) throws Exception {
    //TODO: Add some checks here to validate the input path
    int exitCode = ToolRunner.run(new Configuration(),
     new IndexDriver(), args);
    System.exit(exitCode);
  }

  @Override
  public int run(String[] args) throws Exception {
    JobConf conf = new JobConf(getConf(), IndexDriver.class);
    conf.setJobName("Index Builder - Adam S @ Cloudera");
    conf.setSpeculativeExecution(false);

    // Set Input and Output paths
    FileInputFormat.setInputPaths(conf, new Path(args[0].toString()));
    FileOutputFormat.setOutputPath(conf, new Path(args[1].toString()));
    // Use TextInputFormat
    conf.setInputFormat(TextInputFormat.class);

    // Mapper has no output
    conf.setMapperClass(IndexMapper.class);
    conf.setMapOutputKeyClass(NullWritable.class);
    conf.setMapOutputValueClass(NullWritable.class);
    conf.setNumReduceTasks(0);
    JobClient.runJob(conf);
    return 0;
  }
} The Map code is where things get more interesting. A couple notes before we proceed: Solr servers may be used in 2 ways: Via embedding a Solr server object within your Java code using EmbeddedSolrServer Via HTTP requests, using the class CommonsHttpSolrServer with a URL (in our case, http://localhost:8983/solr) In what follows, I elected to go with the StreamingUpdateSolrServer – which is a subclass of CommonsHttpSolrServer. More comments on that towards the end. I will assume now that the reader has some familiarity with the Map/Reduce programming paradigm. The salient points for us here are that we will use a Map-only job to read through each file in the input that we provide, and index our chosen fields. Taking the path of least resistance, I used the fact that each line of text is it’s own Key/Value pair if we read the input as TextInputFormat, and I chose to index the following fields: As a unique identifier for each word, I concatenated the filename, line offset (conveniently provided to the Map code as the “Key” because we are using TextInputFormat), and the position on that line of the word The word itself The Solr server obeys field definitions (specifying field names, data types, uniqueness, etc.) as dictated by a schema file. For this example, running Solr as indicated above, the schema is defined by/example/solr/conf/schema.xml Per my choice to index 2 distinct fields, the relevant fields in the schema are: &lt;field name="id" type="string" indexed="true" \
 stored="true" required="true" /&gt;
&lt;field name="text" type="text_general" indexed="true" \
 stored="true" multiValued="true"/&gt; Without further ado, then, the code looks like the following: public class IndexMapper extends MapReduceBase implements
 Mapper &lt;LongWritable, Text, NullWritable, NullWritable&gt; {
  private StreamingUpdateSolrServer server = null;
  private SolrInputDocument thisDoc = new SolrInputDocument();
  private String fileName;
  private StringTokenizer st = null;
  private int lineCounter = 0;

  @Override
  public void configure(JobConf job) {
    String url = "http://localhost:8983/solr";
    fileName = job.get("map.input.file").substring(
      (job.get("map.input.file")).lastIndexOf(
      System.getProperty("file.separator")) +1);
      try {
        server = new StreamingUpdateSolrServer(url, 100, 5);
      } catch (MalformedURLException e) {
        e.printStackTrace();
      }
  }

  @Override
  public void map(LongWritable key, Text val,
   OutputCollector &lt;NullWritable, NullWritable&gt; output,
   Reporter reporter) throws IOException {

    st = new StringTokenizer(val.toString());
    lineCounter = 0;
    while (st.hasMoreTokens()) {
      thisDoc = new SolrInputDocument();
      thisDoc.addField("id", fileName + " "
       + key.toString() + " " + lineCounter++);
      thisDoc.addField("text", st.nextToken());
      try {
        server.add(thisDoc);
      } catch (SolrServerException e) {
        e.printStackTrace();
      } catch (IOException e) {
        e.printStackTrace();
      }
    }
  }

  @Override
  public void close() throws IOException {
  try {
      server.commit();
    } catch (SolrServerException e) {
      e.printStackTrace();
    }
  }
} Compile the code how you see fit (I am old school and still use ant), and the job is ready to run! To index all of the comedies, you can run the job with the compiled jar file as follows. Note that you must tell hadoop to include an additional Solr jar at runtime: [user@localhost SolrTest]$ hadoop jar solrtest.jar \
 -libjars &lt;solr_install_dir&gt;/dist/apache-solr-solrj-3.5.0.jar \
 shakespeare/comedies shakespeare_output If you then query the Solr server (via the web GUI at http://localhost:8983/solr/admin, the default search is *:* which works well for a quick test) you should see something like the following: &lt;response&gt;
 &lt;lst name="responseHeader"&gt;
  &lt;int name="status"&gt;0&lt;/int&gt;
  &lt;int name="QTime"&gt;35&lt;/int&gt;
  &lt;lst name="params"&gt;
    &lt;str name="indent"&gt;on&lt;/str&gt;
    &lt;str name="start"&gt;0&lt;/str&gt;
    &lt;str name="q"&gt;*:*&lt;/str&gt;
    &lt;str name="version"&gt;2.2&lt;/str&gt;
    &lt;str name="rows"&gt;10&lt;/str&gt;
  &lt;/lst&gt;
 &lt;/lst&gt;
&lt;result name="response" numFound="377452" start="0"&gt;
&lt;doc&gt;
 &lt;str name="id"&gt;troilusandcressida 0 0&lt;/str&gt;
 &lt;arr name="text"&gt;
  &lt;str&gt;TROILUS&lt;/str&gt;
 &lt;/arr&gt;
&lt;/doc&gt;
&lt;doc&gt;
 &lt;str name="id"&gt;troilusandcressida 0 1&lt;/str&gt;
 &lt;arr name="text"&gt;
  &lt;str&gt;AND&lt;/str&gt;
 &lt;/arr&gt;
&lt;/doc&gt;
&lt;doc&gt;
 &lt;str name="id"&gt;troilusandcressida 0 2&lt;/str&gt;
 &lt;arr name="text"&gt;
  &lt;str&gt;CRESSIDA&lt;/str&gt;
 &lt;/arr&gt;
&lt;/doc&gt; … Further Tuning/Investigation Opportunities Performance Implications of StreamingUpdateSolrServer – possibility of using EmbeddedSolrServer: What are the optimal tuning parameters for number of threads and batch size when using StreamingUpdateSolrServer? More investigation could be done here. It is also possible to use an EmbeddedSolrServer (per the Rackspace case study in Hadoop: The Definitive Guide), though it does add some maintenance overhead to create the indexes in a distributed fashion and then later re-combine. I opted to use the StreamingUpdateSolrServer because I believe that it is simpler to get up and running in a small test environment. How to minimize the memory requirements in the Map code: I haven’t been a full time Java developer in many years, so there are almost certainly things that I’m missing on how to minimize the memory overhead of the objects used in the Map code. Since this is called for each line in the input, it is critical to make this code as lean as possible. One tip that I came across on this topic is to use (mutable) org.apache.hadoop.io.Text objects rather than (immutable) Strings. I avoided creating any new String objects in this example Map code, but the point is worth noting for other exercises. Resources that I found useful A great primer that accomplished the indexing via Cascading: http://architects.dzone.com/articles/solr-hadoop-big-data-love Solr Tutorial: http://lucene.apache.org/solr/tutorial.html Some sample code for adding, updating, deleting documents on this wiki: http://wiki.apache.org/solr/Solrj My outstanding coworkers at Cloudera!</snippet></document><document id="451"><title>Cloudera Manager | Hadoop Service Monitoring Demo Video</title><url>http://blog.cloudera.com/blog/2012/02/cloudera-manager-hadoop-service-monitoring-demo-video/</url><snippet>In this demo video, BC Wong, a software engineer at Cloudera, discusses the Hadoop Service Monitoring feature in Cloudera Manager. Service Monitoring helps you monitor and manage your Hadoop clusters effectively. Through the Service Monitoring feature, customers can monitor dozens of service health and performance metrics about the overall service (HDFS, MapReduce, HBase). They can also examine underlying role instances (Namenode, Datanodes, JobTracker, TaskTrackers, Region Servers etc.) in your Hadoop cluster and see what�s going wrong � or what is about to go wrong. Service Monitoring presents health and performance data in a variety of formats including interactive charts through Cloudera�s new, enhanced user interface. Every Service Monitoring page also includes a widget to enable quick search for relevant Events and Logs associated with the service under consideration. Important Event and Log messages are also highlighted in the various charts. You can also monitor metrics against customizable thresholds, which results in Alerts that operators can pay attention to. This demo covers examples of identifying problems with a service, underlying roles and instances. It reviews the service summary and health checks, logs and events to determine the root cause of the problem. More information on Cloudera Manager is available here: http://www.cloudera.com/products-services/tools/ Previous Cloudera Manager demo video blog posts: Cloudera Manager | Service and Configuration Management demo videos (Part I and II) Cloudera Manager | Log Management, Event Management and Alerting demo video</snippet></document><document id="452"><title>MapReduce 2.0 in Apache Hadoop 0.23</title><url>http://blog.cloudera.com/blog/2012/02/mapreduce-2-0-in-hadoop-0-23/</url><snippet>In Building and Deploying MR2 we presented a brief introduction to MapReduce in Apache Hadoop 0.23 and focused on the steps to set up a single-node cluster. This blog provides developers with architectural details of the new MapReduce design.� Apache Hadoop 0.23 has major improvements over previous releases. Here are a few highlights on the MapReduce front; note that there are also major HDFS improvements, which are out of scope of this post. MapReduce 2.0 (a.k.a. MRv2 or YARN): The new architecture divides the two major functions of the JobTracker – resource management and job life-cycle management – into separate components: A ResourceManager (RM) that manages the global assignment of compute resources to applications. A per-application ApplicationMaster (AM) that manages the application�s life cycle. In Hadoop 0.23, a MapReduce application is a single job in the sense of classic MapReduce, executed by the MapReduce ApplicationMaster. There is also a per-machine NodeManager (NM) that manages the user processes on that machine. The RM and the NM form the computation fabric of the cluster. The design also allows plugging long-running auxiliary services to the NM; these are application-specific services, specified as part of the configuration, and loaded by the NM during startup. For MapReduce applications on YARN, shuffle is a typical auxiliary service loaded by the NMs. Note that, in Hadoop versions prior to 0.23, shuffle was part of the TaskTracker. � The per-application ApplicationMaster is a framework specific library and is tasked with negotiating resources from the ResourceManager and working with the NodeManager(s) to execute and monitor the tasks. In the YARN design, MapReduce is just one application framework; the design permits building and deploying distributed applications using other frameworks. For example, Hadoop 0.23 ships with a Distributed Shell application that permits running a shell script on multiple nodes on the YARN cluster. At the time of writing this blog post, there is also an ongoing development effort to allow running Message Passing Interface (MPI) applications on top of YARN. MapReduce 2.0 Design: Figure 1 shows a pictorial representation of a YARN cluster. There is a single Resource Manager, which has two main services: A pluggable Scheduler, which manages and enforces the resource scheduling policy in the cluster.� Note that, at the time of writing this blog post, there are two schedulers supported in Hadoop 0.23, the default FIFO scheduler and the Capacity scheduler; the Fair Scheduler is not yet supported. An Applications Manager (AsM), which manages running Application Masters in the cluster, i.e., it is responsible for starting application masters and for monitoring and restarting them on different nodes in case of failures. � Fig. 1 Figure 1 also shows that there is a NM service running on each node in the cluster. The diagram also shows two AMs (AM1 and AM2). In a YARN cluster at any given time, there will be as many running Application Masters as there are applications (jobs). Each AM manages the application’s individual tasks (starting, monitoring and restarting in case of failures). The diagram shows AM1 managing three tasks (containers 1.1, 1.2 and 1.3), while AM2 manages four tasks (containers 2.1, 2.2, 2.3 and 2.4). Each task runs within a Container on each node. The AM acquires such containers from the RM�s Scheduler before contacting the corresponding NMs to start the application�s individual tasks. These Containers can be roughly compared to Map/Reduce slots in previous Hadoop versions. However the resource allocation model in Hadoop-0.23 is more optimized from a cluster utilization perspective.�� Resource Allocation Model: In earlier Hadoop versions, each node in the cluster was statically assigned the capability of running a predefined number of Map slots and a predefined number of Reduce slots. The slots could not be shared between Maps and Reduces. This static allocation of slots wasn�t optimal since slot requirements vary during the MR job life cycle (typically, there is a demand for Map slots when the job starts, as opposed to the need for Reduce slots towards the end). Practically, in a real cluster, where jobs are randomly submitted and each has its own Map/Reduce slots requirement, having an optimal utilization of the cluster was hard, if not impossible. The resource allocation model in Hadoop 0.23 addresses such deficiency by providing a more flexible resource modeling. Resources are requested in the form of containers, where each container has a number of non-static attributes. At the time of writing this blog, the only supported attribute was memory (RAM). However, the model is generic and there is intention to add more attributes in future releases (e.g. CPU and network bandwidth). In this new Resource Management model, only a minimum and a maximum for each attribute are defined, and AMs can request containers with attribute values as multiples of these minimums. MapReduce 2.0 Main Components: In this section, we�ll go through the main components of the new MapReduce architecture in detail to understand the functionality of these components and how they interact with each other. Client � Resource Manager Figure 2 illustrates the initial step for running an application on a YARN cluster. Typically a client communicates with the RM (specifically the Applications Manager component of the RM) to initiates this process. The first step, marked (1) in the diagram, is for the client to notify the Applications Manager of the desire of submitting an application, this is done via a �New Application Request�. The RM respose, marked (2), will typically contain a newly generated unique application ID, in addition to information about cluster resource capabilities that the client will need in requesting resources for running the application�s AM. Using the information received from the RM, the client can construct and submit an �Application Submission Context�, marked (3), which typically contains information like scheduler queue, priority and user information, in addition to information needed by the RM to be able to launch the AM. This information is contained in a �Container Launch Context�, which contains the application�s jar, job files, security tokens and any resource requirements. Fig. 2 Following application submission, the client can query the RM for application reports, receive such reports and, if needed, the client can also ask the RM to kill the application. These three additional steps are pictorially depicted in fig. 3. � Fig. 3 Resource Manager � Application Master When the RM receives the application submission context from the client, it finds an available container meeting the resource requirements for running the AM, and it contacts the NM for the container to start the AM process on this node. Figure 4 depicts the following communication steps between the AM and the RM (specifically the Scheduler component of the RM). The first step, marked (1) in the diagram, is for the AM to register itself with the RM. This step consists of a handshaking procedure and also conveys information like the RPC port that the AM will be listening on, the tracking URL for monitoring the application�s status and progress, etc. The RM registration response, marked (2), will convey essential information for the AM master like minimum and maximum resource capabilities for this cluster. The AM will use such information in calculating and requesting any resource requests for the application�s individual tasks. The resource allocation request from the AM to the RM, marked (3), mainly contains a list of requested containers, and may also contain a list of released containers by this AM. Heartbeat and progress information are also relayed through resource allocation requests as shown by arrow (4). When the Scheduler component of the RM receives a resource allocation request, it computes, based on the scheduling policy, a list of containers that satisfy the request and sends back an allocation response, marked (5), which contains a list of allocated resources. Using the resource list, the AM starts contacting the associated node managers (as will be soon seen), and finally, as depicted by arrow (6), when the job finishes, the AM sends a Finish Application message to the Resource Manager and exits. Fig. 4 Application Master � Container Manager Figure 5 describes the communication between the AM and the Node Managers. The AM requests the hosting NM for each container to start it as depicted by arrow (1) in the diagram. While containers are running, the AM can request and receive a container status report as shown in steps (2) and (3), respectively. Fig. 5 Based on the above discussion, a developer writing YARN applications will be mainly concerned with the following interfaces: ClientRMProtocol: Client RM (Fig. 3). This is the protocol for a client to communicate with the RM to launch a new application (i.e. an AM), check on the status of the application or kill the application. AMRMProtocol: AM RM (Fig. 4). This is the protocol used by the AM to register/unregister itself with the RM, as well as to request resources from the RM Scheduler to run its tasks. ContainerManager: AM NM (Fig. 5). This is the protocol used by the AM to communicate with the NM to start or stop containers and to get status updates on its containers. Migrating older MapReduce applications to run on Hadoop 0.23: All client-facing MapReduce interfaces are unchanged, which means that there is no need to make any source code changes to run on top of Hadoop 0.23. Useful links: How to write a YARN Application. Hadoop 0.23 Javadocs.</snippet></document><document id="453"><title>Cloudera Manager | Log Management, Event Management and Alerting Demo Video</title><url>http://blog.cloudera.com/blog/2012/02/cloudera-manager-log-management-event-management-and-alerting-demo-video/</url><snippet>In this demo, Henry Robinson, a software engineer at Cloudera, discusses the Log Management, Event Management and Alerting features in Cloudera Manager that help make sense out of all the discrete events that take place across the Hadoop cluster. He demonstrates how to search the logs valuable information, note important events that pertain to system health and create alerts to warn you when things go wrong. Log Management Every process in a Hadoop cluster regularly writes to a log file, which captures valuable data but also creates volumes of information that is difficult to manually sort. Cloudera Manager�s comprehensive log management feature contextualizes all system logs from across the Hadoop cluster and allows the operator to search and filter by service, role, host, keyword and severity. The application also proactively scans the log files for irregularities and warns you before the Hadoop cluster is impacted. Event Management With event management, Cloudera Manager proactively reports on important events in the Hadoop cluster such as a change in service health or metrics, log messages with a certain severity or keyword, or abnormal job performance. It creates and aggregates these relevant Hadoop events, and makes them available for searching and alerting. Alerting Cloudera Manager aggregates events for easy filtering and viewing and makes them available as email alerts through the Alerting feature. This powerful feature warns you before harm is done to the cluster. More information on Cloudera Manager is available here:� http://www.cloudera.com/products-services/tools/ Previous Cloudera Manager demo videos blog posts: Cloudera Manager | Service and Configuration Management demo videos (Part I and II)</snippet></document><document id="454"><title>Apache ZooKeeper 3.4.3 has been released</title><url>http://blog.cloudera.com/blog/2012/02/apache-zookeeper-3-4-3-has-been-released/</url><snippet>Apache ZooKeeper release�3.4.3 is now available. This is a bug fix release covering 18�issues, one of which was considered a blocker.� ZooKeeper 3.4 is incorporated into CDH4 and�now available in beta 1! ZOOKEEPER-1367 is the most serious of the issues addressed, it could cause data corruption on restart. This version also adds support for compiling the client on ARM architectures. ZOOKEEPER-1367� Data inconsistencies and unexpired ephemeral nodes after cluster restart ZOOKEEPER-1343� getEpochToPropose should check if lastAcceptedEpoch is greater or equal than epoch ZOOKEEPER-1373� Hardcoded SASL login context name clashes with Hadoop security configuration override ZOOKEEPER-1089� zkServer.sh status does not work due to invalid option of nc ZOOKEEPER-973� � bind() could fail on Leader because it does not setReuseAddress on its ServerSocket ZOOKEEPER-1374� C client multi-threaded test suite fails to compile on ARM architectures. ZOOKEEPER-1348� Zookeeper 3.4.2 C client incorrectly reports string version of 3.4.1 If you are running 3.4.2 or earlier, be sure to upgrade immediately. See my earlier post for details on what’s new in 3.4. Stability, Compatibility and Testing The 3.4 series has been through a number of releases, incorporating feedback from users and addressing found issues. The Apache community is now considering the 3.4.3 release to be of beta quality.� Getting Involved The Apache ZooKeeper project is working on a number of new features. Our�How To Contribute page is a great place to start if you’re interested in getting involved as a developer. You can also�follow me on twitter. Acknowledgements A special thanks to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc).</snippet></document><document id="455"><title>Cloudera Manager | Service and Configuration Management Demo Videos</title><url>http://blog.cloudera.com/blog/2012/02/cloudera-manager-service-and-configuration-management-demo-videos/</url><snippet>Service and Configuration Management (Part I &amp; II) We�ve recently recorded a series of demo videos intended to highlight the extensive set of features and functions included with Cloudera Manager, the industry�s first end-to-end management application for Apache Hadoop. These demo videos showcase the newly enhanced Cloudera Manager interface and reveal how to use this powerful application to simplify the administration of Hadoop clusters, optimize performance and enhance the quality of service. In the first two videos of this series, Philip Langdale, a software engineer at Cloudera, walks through Cloudera Manager�s Service and Configuration Management module. He demonstrates how simple it is to set up and configure the full range of Hadoop services in CDH (including HDFS, MR and HBase); enable security; perform configuration rollbacks; and add, delete and decommission nodes. Part I Part I of the Service and Configuration Management demo focuses on managing services and configuring a cluster for optimal performance. �It also demonstrates how to administer users within Cloudera Manager, configure role-based permissions, and better manage security. Part II Part II of the Service and Configuration Management demo explores decommissioning a node as well as deleting and adding roles within the cluster. This allows the graceful start, stop and restart of services as needed. More information on Cloudera Manager is available here: http://www.cloudera.com/products-services/tools/ We released Cloudera Manager 3.7.3 on Feb 10. This release includes several bug fixes, performance enhancements and new features, including multi-master support for HBase and wizard-based validation of Cloudera Manager Database settings. More information available at: https://ccp.cloudera.com/display/ENT/Cloudera+Manager+3.7.x+Release+Notes</snippet></document><document id="456"><title>Introducing CDH4</title><url>http://blog.cloudera.com/blog/2012/02/introducing-cdh4/</url><snippet>I’m pleased to inform our users and customers that Cloudera has released its 4th version of Cloudera�s Distribution Including Apache Hadoop (CDH) into beta today. This release combines the input from our enterprise customers, partners and users with the hard work of Cloudera engineering and the larger Apache open source community to create what we believe is a compelling advance for this widely adopted platform. There are a great many improvements and new capabilities in CDH4 compared to CDH3. Here is a high level list of what�s available for you to test in this first beta release: Availability – a high availability namenode, better job isolation, hard drive failure handling, and multi-version support Utilization – multiple namespaces, co-processors and a slot-less resource management model Performance – improvements in HBase, HDFS, MapReduce and compression performance Usability – broader BI support, expanded API access, unified file formats &amp; compression codecs Security – scheduler ACL’s Some items of note about this beta: This is the first beta for CDH4.� We plan to do a second beta some weeks after the first beta.� The second beta will roll in updates to Apache Flume, Apache Sqoop, Hue, Apache Oozie and Apache Whirr that did not make the first beta.� It will also broaden the platform support back out to our normal release matrix of Red Hat, Centos, Suse, Ubuntu and Debian. Our plan is for this second beta to have the last significant component changes before CDH goes GA. Some CDH components are getting substantial revamps and we have transition plans for these. There is a significantly redesigned MapReduce (aka MR2) with a similar API to the old MapReduce but with new daemons, user interface and more. MR2 is part of CDH4, but we also decided it makes sense to ship with the MapReduce from CDH3 which is widely used, thoroughly debugged and stable. We will support both generations of MapReduce for the life of CDH4, which will allow customers and users to take advantage of all of the new CDH4 features while making the transition to the new MapReduce in a timeframe that makes sense for them. Because of the anticipated popularity of the high availability features, we’ve created a high availability guide.� All of the other documentation artifacts have been updated. As always, we maintain complete transparency as to the Apache project releases and patches that make up CDH4. You can find the documentation for the Apache contents of CDH4 here. We value your feedback! Please help make this beta a success by trying out CDH4 b1 and letting us know what you think.� If you are a customer, you should give us your feedback via Zendesk. If you are a user but not a customer, please give us your feedback on CDH Users.</snippet></document><document id="457"><title>Cloudera Connector for Tableau Has Been Released</title><url>http://blog.cloudera.com/blog/2012/02/cloudera-connector-for-tableau-has-been-released/</url><snippet>Earlier today, Cloudera proudly released the Cloudera Connector for Tableau. The availability of this connector serves both Tableau users who are looking to expand the volume of datasets they manipulate and Hadoop users who want to enable analysts like Tableau users to make the data within Hadoop more meaningful. Enterprises can now extract the full value of big data and allow a new class of power users to interact with Hadoop data in ways they priorly could not. The Cloudera Connector for Tableau is a free ODBC Driver that enables Tableau Desktop 7.0 to connect to Apache Hive. Tableau users can thus leverage Hive, Hadoop’s data warehouse system, as a data source for all the maps, charts, dashboards and other artifacts typically generated within Tableau. Hive itself is a powerful query engine that is optimized for analytic workloads, and that’s where this Connector is sure to work best. Tableau also, however, lets users ingest result sets from Hive into its in-memory analytical engine so that results returning from Hadoop can be analyzed much more quickly. Setting up your connection involves only the following steps: Download and run the Cloudera Connector for Tableau executable. Point the Windows ODBC Data Source Administrator to the Cloudera ODBC Driver for Apache Hive. Identify the Hive data source in Tableau Desktop. And you’re done! Cloudera is pleased to continue to work with Tableau and with other vendors to enable more users in the enterprise to bring out Hadoop’s fullest value.</snippet></document><document id="458"><title>CDH3, update 3 now available</title><url>http://blog.cloudera.com/blog/2012/01/cdh3-update-3-now-available/</url><snippet>Keeping with our release policy for Cloudera’s Distribution Including Apache Hadoop (CDH) I’m pleased to announce the availability of update 3 for CDH3. �As a reminder, we ship updates for our most recent GA distribution every 3 months. �Updates primarily include bug fixes but when possible we will also include features from our mid-term roadmap. �We’ll only include new features when they do not introduce instability or break compatibility. �As always, users have the option to skip updates without incurring any future upgrade cost. Update 3 contains a number of new improvements. �Several improvements positively impact performance. �Enhancements were made to HDFS and to HBase which will result in 15-150% improvements in performance compared to CDH3 update 2 depending on the workload. �Users should see performance gains in a wide range of workloads from MapReduce over HDFS style workloads to HBase scan style workloads to HBase random read / write workloads. �Todd Lipcon’s talk at Hadoop World on performance outlines a number of these improvements that have made it to update 3. �Some of these performance improvements require users to select specific configuration settings so please consult the documentation. A number of other improvements have been made that will help system stability and�recover-ability. Enhancements were made to MapReduce to better work around disk failures without impacting task locality. �We also backported the Apache HBase distributed log splitting feature that will make�recovery�from region failure much faster than it has been previously. More information about how to download or upgrade to update 3 is available here. �Additional details are available in the release notes. �As always, the exact changes (jiras, patches) for update 3 are described in the changes files that can be found here.</snippet></document><document id="459"><title>January 2012 Bay Area HBase User Group meetup summary + HBaseCon announcement</title><url>http://blog.cloudera.com/blog/2012/01/january-2012-bay-area-hbase-user-group-meetup-summary/</url><snippet>More than 150 people attended the San Francisco Bay Area HBase User Group meetup last�Thursday, January 19th, at eBay headquarters in San Jose, California. �Presenters from StumbleUpon, Facebook, eBay and MapR shared a wealth of information about Apache HBase operations and optimizations, gleaned from their experience�running HBase in production environments. One special item of note: Michael Stack announced�HBaseCon 2012, taking place this spring in the Bay Area. �This inaugural conference will focus on the growth and education of the HBase community. �While details of the event are not yet published, the call for speakers is currently open. �Submit your abstract here. Many of the talks focused on HBase operations. �Here’s a summary of those presentations: Aravind Gottipati discussed the HBase deployments at StumbleUpon, reflecting on hardware, requirements, configuration, and monitoring tools. Aravind also pointed out some operational challenges StumbleUpon has faced, and suggested some improvements for future HBase versions. �[slides] Next, Paul Tuckfield�presented on HBase operations at Facebook. He shared interesting facts about their deployment, such as how their clusters span multiple racks to avoid network uplinks as a single point of failure, and how their clusters are as slow as their slowest region server. �[slides] eBay’s Swati Agarwal and Thomas Pan gave a talk on eBay’s HBase deployments, sharing many statistics about their pre-production deployment, and discussed their need for well-distributed keys and the impact on their rowkey schema. They also talked about their HBase-related challenges, including a need for more stability and how upgrades incur significant downtime. �[slides] By now, the meeting was running a bit behind schedule, so J.D. Cryans gave�a quick presentation about some experiments he did at StumbleUpon involving different caching configurations and datasets. He showed his numbers in a couple of different runs based on a snapshot of the upcoming CDH3u3 release from Cloudera, which is currently in production at StumbleUpon. �The runs were with with no block cache, short-circuited reads, and 100% block cache. The main takeaway was that it is very important to have a good understanding of how much data that needs to be read for your specific use case, and how this data fits into HBase. �[slides] In addition to the above talks, Tomer Shiran from MapR gave an overview of MapR’s product, and Mikhail Bautin from Facebook concluded the meetup with�some slides about the various optimizations that Facebook has contributed back to the HBase community in the area of scanner performance. Slides for all presentations are available here, and the link to the meetup web page is here. Thanks to eBay for inviting the HBase User Group to their building, and providing the free pizza and beer. �See you at HBaseCon 2012!</snippet></document><document id="460"><title>Seismic Data Science: Reflection Seismology and Hadoop</title><url>http://blog.cloudera.com/blog/2012/01/seismic-data-science-hadoop-use-case/</url><snippet>When most people first hear about data science, it’s usually in the context of how prominent web companies work with very large data sets in order to predict clickthrough rates, make personalized recommendations, or analyze UI experiments. The solutions to these problems require expertise with statistics and machine learning, and so there is a general perception that data science is intimately tied to these fields. However, in my conversations at academic�conferences and with Cloudera customers, I have found that many kinds of scientists– such as astronomers, geneticists, and geophysicists– are working with very large data sets in order to build models that do not involve statistics or machine learning, and that these scientists encounter data challenges that would be familiar to data scientists at Facebook, Twitter, and LinkedIn. The Practice of Data Science The term “data science” has been subject to criticism on the grounds that it doesn’t mean anything, e.g., “What science doesn’t involve data?” or “Isn’t data science a rebranding of statistics?” The source of this criticism could be that data science is not a solitary discipline, but rather a set of techniques used by many scientists to solve problems across a wide array of scientific fields. As DJ Patil wrote in his excellent overview of building data science teams, the key trait of all data scientists is the understanding “that the heavy lifting of [data] cleanup and preparation isn’t something that gets in the way of solving the problem: it�is�the problem.” I have found a few more characteristics that apply to the work of data scientists, regardless of their field of research: Inverse problems.�Not every data scientist is a statistician, but all data scientists are interested in extracting information about complex systems from observed data, and so we can say that data science is related to the study of inverse problems. Inverse problems arise in almost every branch of science, including�medical imaging, remote sensing, and astronomy. We can also think of DNA sequencing as an inverse problem, in which the genome is the underlying model that we wish to reconstruct from a collection of observed DNA fragments. Real-world inverse problems are often ill-posed or ill-conditioned, which means that scientists need substantive expertise in the field in order to apply reasonable regularization conditions in order to solve the problem. Data sets that have a rich set of relationships between observations.�We might think of this as a kind of Metcalfe’s Law for data sets, where the value of a data set increases nonlinearly with each additional observation. For example, a single web page doesn’t have very much value, but 128 billion web pages can be used to build a search engine. A DNA fragment in isolation isn’t very useful, but millions of them can be combined to sequence a genome. A single adverse drug event could have any number of explanations, but millions of them can be processed to detect suspicious drug interactions. In each of these examples, the individual records have rich relationships that enhance the value of the data set as a whole. Open-source software tools with an emphasis on data visualization.�One indicator that a research area is full of data scientists is an active community of open source developers. The R Project is a widely known and used toolset that cuts across a variety of disciplines, and has even been used as a basis for specialized projects like Bioconductor. Astronomers have been using tools like AIPS�for processing data from radio telescopes�and IRAF�for data from optical telescopes for more than 30 years.�Bowtie is an open source project for performing very fast DNA sequence alignment, and the Crossbow Project combines Bowtie with Apache Hadoop for distributed sequence alignment processing. We can use the term “data scientist” as a specialization of “scientist” in the same way that we use the term “theoretical physicist” as a specialization of “physicist.” Just as there are theoretical physicists that work within the various subdomains of physics, such as cosmology, optics, or particle physics, there are data scientists at work within every branch of science. Data Scientists Who Find Oil: Reflection Seismology Reflection seismology is a set of techniques for solving a classic inverse problem: given a collection of seismograms and associated metadata, generate an image of the subsurface of the Earth that generated those seismograms. These techniques are primarily used by exploration and production companies in order to locate oil and natural gas deposits, although they were also used to identify the location of the Chicxulub Crater�that has been linked to the extinction of the dinosaurs. Marine Seismic Survey Seismic data is collected by surveying an area that is suspected to contain oil or gas deposits. Seismic waves are generated from a source,�which is usually an air gun in marine surveys or a machine called a Vibroseis for land-based surveys. The seismic waves reflect back to the surface at the interfaces between rock layers, where an array of receivers record the amplitude and arrival times of the reflected waves as a time series, which is called a trace. The data that is generated from a single source is called a shot�or shot record,�and a modern seismic survey may consist of tens of thousands of shots and multiple terabytes of trace data. Common Depth Point (CDP) Gather In order to solve the inversion problem, we take advantage of the geometric relationships between traces that have different source and receiver locations but a common depth point (also known as a common midpoint). By comparing the time it took for the seismic waves to travel from the different source and receiver locations and experimenting with different velocity models for the waves moving through the rock, we can estimate the depth of the common subsurface point that the waves reflected off of. By aggregating a large number of these estimates, we can construct a complete image of the subsurface. As we increase the density and the number of traces, we can create higher quality images that improve our understanding of the subsurface geology. A 3D seismic image of Japan's southeastern margin Additionally, seismic data processing has a long history of using open-source software tools that were initially developed in academia and were then adopted and enhanced by private companies. Both the Seismic Unix project, from the Colorado School of Mines, and SEPlib, from Stanford University, have their roots in tools created by graduate students in the late 1970s and early 1980s. Even the most popular commercial toolkit for seismic data processing, SeisSpace, is built on top of an open source foundation, the JavaSeis project. Hadoop and Seismic Data Processing Geophysicists have been pushing the limits of high-performance computing for more than three decades; they were early�adopters�of�the first�Cray supercomputers as well as the massively parallel Connection Machine. Today, the most challenging seismic data processing tasks are performed on custom compute clusters that take advantage of multiple GPUs per node, high-performance networking and storage systems for fast data access. The data volume of modern seismic surveys and the performance requirements of the compute clusters means that data from seismic surveys that are not undergoing active processing are often stored offsite on tape. If a geophysicist wants to re-examine an older survey, or study the effectiveness of a new processing technique, he must file a request to move the data into active storage and then consume precious cluster resources in order to process the data. Fortunately, Apache Hadoop has emerged as a cheap and reliable online storage system for petabyte-scale data sets. Even better, we can export many of the most I/O intensive steps in the seismic data processing into the Hadoop cluster itself, thus freeing precious resources in the supercomputer cluster for the most difficult and urgent processing tasks. Seismic Hadoop is a project that we developed at Cloudera to demonstrate how to store and process seismic data in a Hadoop cluster. It combines Seismic Unix with Crunch, the Java library we developed for creating MapReduce pipelines. Seismic Unix gets its name from the fact that it makes extensive use of Unix pipes in order to construct complex data processing tasks from a set of simple procedures. For example, we might build a pipeline in Seismic Unix that first applies a filter to the trace data, then edits some metadata associated with each trace, and finally sorts the traces by the metadata that we just edited: sufilter f=10,20,30,40 | suchw key1=gx,cdp key2=offset,gx key3=sx,sx b=1,1 c=1,1 d=1,2 | susort cdp gx Seismic Hadoop takes this same command and builds a Crunch pipeline that performs the same operations on a data set stored in a Hadoop cluster, replacing the local susort command with a distributed sort across the cluster using MapReduce. Crunch takes care of figuring out how many MapReduce jobs to run and which processing steps are assigned to the map phase and which are assigned to the reduce phase. Seismic Hadoop also takes advantage of Crunch’s support for streaming the output of a MapReduce pipeline back to the client in order to run the utilities for visualizing data that come with Seismic Unix. Challenges to Solve Together Talking to a geophysicist is a little bit like seeing into the future: the challenges they face today are the challenges that data scientists in other fields will be facing five years from now. There are two challenges in particular that I would like the broader community of data scientists and Hadoop developers to be thinking about: Reproducibility.� Geophysicists have developed tools that make it easy to understand and reproduce the entire history of analyses performed on a particular data set. One of the most popular open source seismic processing toolkits, Madagascar, even chose reproducibility.org as its home page. Reproducible research has enormous benefits in terms of data quality, transparency, and education, and all of the tools we develop should be built with reproducibility in mind. Dynamic and resource-aware scheduling of jobs on heterogeneous clusters.�MR2 and YARN will unleash a Cambrian explosion of data-intensive processing jobs on Hadoop clusters. What was once only MapReduce jobs will now include MPI jobs, Spark queries, and BSP-style computations. Different jobs will have radically different resource requirements in terms of CPU, memory, disk, and network utilization, and we will need fine-grained resource controls, intelligent defaults, and robust mechanisms for recovering from task failures across all job types. It is an unbelievably exciting time to be working on these big data problems. Join us and be part of the solution!</snippet></document><document id="461"><title>Apache HBase 0.92.0 has been released</title><url>http://blog.cloudera.com/blog/2012/01/apache-hbase-0-92-0-has-been-released/</url><snippet>Today the Apache HBase community has proudly released Apache HBase 0.92.0, a major new version of the scalable distributed data�store inspired by Google’s BigTable. �Over 670 issues were addressed, so in this post I’ll highlight some of the major features�and enhancements and describe what they mean for HBase users, admins, and�developers. User Features While the most visible change to the project is the�new project logo, the most�important changes for users are the performance and robustness improvements to HBase’s core functionality.�On the performance side, there are a few major highlights: HFile v2, a new more efficient storage format Faster recovery via distributed log splitting Lower latency region-server operations via new multi-threaded and asynchronous implementations. HFile v2 is a series of patches that change the internal format that HBase uses to store region data�on HDFS. �The key innovation is the implementation of multi-level file indexes. This improves the general�performance of HBase reads by�making I/O more granular. �The mechanism reduces latency spikes and reduces the memory�footprint required when HBase’s internal data files are loaded. �This also frees memory for more caching and�allows for significantly larger region files. Although HBase is fault tolerant, some recovery�scenarios take significant time. �One recovery procedure, log splitting, occurs when�regions are reassigned after a region server goes down. �In HBase�0.90.x, the write-ahead logs (HLogs) from each downed region server would be processed serially by�a single master. HBase 0.92.0 parallelizes this HLog processing by performing distributed log splitting concurrently on several region servers instead of on a single machine. �The resulting speedup reduces recovery times and thus significantly reduces region unavailability time if many region servers fail simultaneously or if a cluster is restarted ungracefully. Finally, several operations such as compaction, table creation, and bulk loading have new multi-threaded or�asynchronous�implementations. �By using thread pooling and pipelining tasks, many of these�occasional�operations can happen�concurrently and complete more quickly. Operator Features HBase 0.92.0 improves�supportability and�simplifies operations�by introducing features that aid in diagnosing HBase’s state and aid in repairing a corrupted HBase. �If HBase gets slow or seems stuck, the HBase 0.90.x releases unfortunately require shell access and�developers to diagnose even simple problems. �In more serious cases, expert HBase developers would be required for diagnosis, repair, and recovery. � HBase 0.92.0 exposes new features that help operators pinpoint or repair problems more quickly. �Highlights include: An enhanced web UI that exposes more internal state Improved logging for identifying slow queries Improved corruption detection and repair tools HBase 0.92.0′s enhanced master UI provides more detailed state information which can hasten problem diagnosis. Specifically, this�version now exposes regions-in-transition states, and also includes a�centralized cluster debug page that displays detailed cluster state information�in a single web page. This can help identify problems and pinpoint which machines to focus on. Sometimes a slow machine is an indication that a machine is on the verge of failing. �HBase 0.92.0′s region servers now provides a ‘show�processlist’-like JSON encoded thread dump on its web interface. �It was also�augmented to expose and log slow query metrics. This�allows administrators to determine if particular operations are�inefficient or to forensically correlate slow queries with region server�operations such as flushes, compaction, GC’s, or splits. Finally, HBase 0.92.0 includes improved utilities for dealing with�the rare occasions where data corruption has affected the integrity and�consistency of HBase’s internal data. �Previously, repairs for�these potentially disasterous situations required�time-consuming manual analysis and intimate knowledge of HBase’s internals. �HBase 0.92.0 improves hbck, a tool for detecting and fixing�transient corruptions, by providing an accurate and detailed summary�of region layout. �This version also introduces another tool, OfflineMetaRepair, enables administrators to rebuild the META table of a severely corrupted HBase. Developer Features Last but not least, HBase 0.92.0 adds several advanced features that�improves HBase’s extensibility, flexiblity, and performance.�Highlights include: Coprocessors Build support for Hadoop 0.20.20x, 0.22, 0.23. Experimental: offheap slab cache and online table schema change For developers, the biggest news is the coprocessor framework. �Coprocessors are a powerful extension interface akin to database�triggers or kernel modules. �With coprocessors, custom coprocessors�can be plugged-in to HBase-specific operations such as data access such as gets and puts, as well as table modification, master�transitions, and HLog operations. �This version includes an HBase security coprocessor that implements mechanisms for�column-level access control and authorization. During this release cycle, the Apache Hadoop folks have official�releases that support hflush/sync/append. �The new HBase versions now�build against Hadoop 1.0.0 and have additions to build and test�against the 0.22 and 0.23 beta versions of Hadoop as well. Finally, I’ll highlight some experimental but promising new features. �An offheap slab cache implementation which provides a manual memory�management mechanism that is used to avoid object creation overhead, memory fragmentation, and other woes associated with depending upon GC. � Also included is an initial implementation of an�online table schema change mechanism that�allows for adding and removing column families without disabling a table. Summary HBase 0.92.0 is significant major new version and is truly a community effort with major contributions�from companies including Facebook, eBay, TrendMicro, StumbleUpon,�Salesforce, Explorys, and Cloudera. �Although this version is not�currently available in Cloudera’s Distribution including Apache Hadoop�(CDH) yet, several features have already made it into CDH3 HBase�updates and will be the basis of a future CDH4 release.��</snippet></document><document id="462"><title>Hadoop World 2011 Videos and Slides Available</title><url>http://blog.cloudera.com/blog/2012/01/hadoop-world-2011-videos-and-slides-available/</url><snippet>Last November in New York City, Hadoop World, the largest conference of Apache Hadoop practitioners, developers, business executives, industry luminaries and innovative companies took place. The enthusiasm for the possibilities in Big Data management and analytics with Hadoop was palpable across the conference. Cloudera CEO, Mike Olson, summarizes Hadoop World 2011 in these final remarks. Those who attended Hadoop World know how difficult navigating a route between two days of five parallel tracks of compelling content can be�particularly since Hadoop World 2011 consisted of sixty-five informative sessions about Hadoop. Understanding that it is nearly impossible to obtain and/or retain all the valuable information shared live at the event, we have compiled all the Hadoop World presentation slides and videos for perusing, sharing and for reference at your convenience. You can turn to these resources for technical Hadoop help and real-world production Hadoop examples, as well as information about advanced data science analytics. I�d like to take this opportunity to again thank all who participated in making Hadoop World 2011 a success: sponsors, attendees, the Sheraton New York Hotel &amp; Towers and the Hadoop World production team. Hadoop World Resource Links Hadoop World Agenda: http://www.hadoopworld.com/agenda/ The Hadoop World Agenda has both Video and Slide (PPT) links for each presentation located in the presentations designated time slot. Place your cursor over a time slot to learn more about the presentations content. Hadoop World Resources on Cloudera.com: http://www.cloudera.com/resources/Hadoop+World/ Each Hadoop World presentation video and slide deck is listed in the Resources section of the Cloudera web site. Scroll the list to find a presentation of interest to you. Notes Larry Feinsmith�s keynote presentation will not be listed in conjunction with JPMorgan &amp; Chase�s wishes. The session, �Life in Hadoop Ops � Tales From the Trenches� does not have a corresponding slide deck as this session was a free flowing discussion panel. Hadoop World 2012 will take place in the fall. Specific location and dates will be disclosed in the future.</snippet></document><document id="463"><title>Apache Sqoop: Highlights of Sqoop 2</title><url>http://blog.cloudera.com/blog/2012/01/apache-sqoop-highlights-of-sqoop-2/</url><snippet>This blog was originally posted on the Apache Blog: https://blogs.apache.org/sqoop/entry/apache_sqoop_highlights_of_sqoop Apache Sqoop (incubating) was created to efficiently transfer bulk data between Hadoop and external structured datastores, such as RDBMS and data warehouses, because databases are not easily accessible by Hadoop. Sqoop is currently undergoing incubation at The Apache Software Foundation. More information on this project can be found at http://incubator.apache.org/sqoop. The popularity of Sqoop in enterprise systems confirms that Sqoop does bulk transfer admirably. That said, to enhance its functionality, Sqoop needs to fulfill data integration use-cases as well as become easier to manage and operate. What is Sqoop? As described in a previous blog post, Sqoop is a bulk data transfer tool that allows easy import/export of data from structured datastores such as relational databases, enterprise data warehouses, and NoSQL systems. Using Sqoop, you can provision the data from an external system into HDFS, as well as populate tables in Hive and HBase. Similarly, Sqoop integrates with the workflow coordinator Apache Oozie (incubating), allowing you to schedule and automate import/export tasks. Sqoop uses a connector-based architecture which supports plugins that provide connectivity to additional external systems. Figure 1: Sqoop 1.4.0-incubating Architecture Sqoop’s Challenges Sqoop has enjoyed enterprise adoption, and our experiences have exposed some recurring ease-of-use challenges, extensibility limitations, and security concerns that are difficult to support in the original design: Cryptic and contextual command line arguments can lead to error-prone connector matching, resulting in user errors Due to tight coupling between data transfer and the serialization format, some connectors may support a certain data format that others don’t (e.g. direct MySQL connector can’t support sequence files) There are security concerns with openly shared credentials By requiring root privileges, local configuration and installation are not easy to manage Debugging the map job is limited to turning on the verbose flag Connectors are forced to follow the JDBC model and are required to use common JDBC vocabulary (URL, database, table, etc), regardless if it is applicable These challenges have motivated the design of Sqoop 2, which is the subject of this post. That said, Sqoop 2 is a work in progress whose design is subject to change. Sqoop 2 will continue its strong support for command line interaction, while adding a web-based GUI that exposes a simple user interface. Using this interface, a user can walk through an import/export setup via UI cues that eliminate redundant options. Various connectors are added in the application in one place and the user is not tasked with installing or configuring connectors in their own sandbox. These connectors expose their necessary options to the Sqoop framework which then translates them to the UI. The UI is built on top of a REST API that can be used by a command line client exposing similar functionality. The introduction of Admin and Operator roles in Sqoop 2 will restrict ‘create’ access for Connections to Admins and ‘execute’ access to Operators. This model will allow integration with platform security and restrict the end user view to only operations applicable to end users. Figure 2: Sqoop 2 Architecture Ease of Use Whereas Sqoop requires client-side installation and configuration, Sqoop 2 will be installed and configured server-side. This means that connectors will be configured in one place, managed by the Admin role and run by the Operator role. Likewise, JDBC drivers will be in one place and database connectivity will only be needed on the server. Sqoop 2 will be a web-based service: front-ended by a Command Line Interface (CLI) and browser and back-ended by a metadata repository. Moreover, Sqoop 2′s service level integration with Hive and HBase will be on the server-side. Oozie will manage Sqoop tasks through the REST API. This decouples Sqoop internals from Oozie, i.e. if you install a new Sqoop connector then you won’t need to install it in Oozie also. Ease of Extension In Sqoop 2, connectors will no longer be restricted to the JDBC model, but can rather define their own vocabulary, e.g. Couchbase no longer needs to specify a table name, only to overload it as a backfill or dump operation. Common functionality will be abstracted out of connectors, holding them responsible only for data transport. The reduce phase will implement common functionality, ensuring that connectors benefit from future development of functionality. Sqoop 2′s interactive web-based UI will walk users through import/export setup, eliminating redundant steps and omitting incorrect options. Connectors will be added in one place, with the connectors exposing necessary options to the Sqoop framework. Thus, users will only need to provide information relevant to their use-case. With the user making an explicit connector choice in Sqoop 2, it will be less error-prone and more predictable. In the same way, the user will not need to be aware of the functionality of all connectors. As a result, connectors no longer need to provide downstream functionality, transformations, and integration with other systems. Hence, the connector developer no longer has the burden of understanding all the features that Sqoop supports. Security Currently, Sqoop operates as the user that runs the ‘sqoop’ command. The security principal used by a Sqoop job is determined by what credentials the users have when they launch Sqoop. Going forward, Sqoop 2 will operate as a server based application with support for securing access to external systems via role-based access to Connection objects. For additional security, Sqoop 2 will no longer allow code generation, require direct access to Hive and HBase, nor open up access to all clients to execute jobs. Sqoop 2 will introduce Connections as First-Class Objects. Connections, which will encompass credentials, will be created once and then used many times for various import/export jobs. Connections will be created by the Admin and used by the Operator, thus preventing credential abuse by the end user. Furthermore, Connections can be restricted based on operation (import/export). By limiting the total number of physical Connections open at one time and with an option to disable Connections, resources can be managed. Summary As detailed in this presentation, Sqoop 2 will enable users to use Sqoop effectively with a minimal understanding of its details by having a web-application run Sqoop, which allows Sqoop to be installed once and used from anywhere. In addition, having a REST API for operation and management will help Sqoop integrate better with external systems such as Oozie. Also, introducing a reduce phase allows connectors to be focused only on connectivity and ensures that Sqoop functionality is uniformly available for all connectors. This facilitates ease of development of connectors. We encourage you to participate in and contribute to Sqoop 2′s Design and Development (SQOOP-365).</snippet></document><document id="464"><title>Capacity Planning with Cloudera Manager</title><url>http://blog.cloudera.com/blog/2012/01/capacity-planning-with-cloudera-manager/</url><snippet>If you’re like a myriad of other systems administrators out there, you may be running a production Hadoop cluster, spec’ing one out, or just starting to investigate the possibility of bringing Hadoop into your workplace. As any of these folks will be able to tell you, one of the most important tasks you’ll encounter is capacity planning. With the release of Cloudera Manager 3.7, we’re bringing you a new set of tools to aid you in this process. In this post, we’ll take a look at how you can leverage Cloudera Manager to deal with some common scenarios that you might run into while planning out a Hadoop cluster. Questions and Patterns How is my disk usage growing over time? One very interesting disk usage pattern can be seen in Josh’s recent blog post on his analysis of drug interactions. Josh started with a relatively small data set, containing about one million records. However, during one of the stages of his analytic process, the number of records was blown up from one million to three trillion. Many types of analyses can result in very large intermediate data sets, while the final output may just be a fraction of the intermediate data. The consequence is that there are temporary spikes in disk usage, which need to be understood, in order to appropriately plan out a Hadoop deployment. Maybe you want to understand the rate at which your data is growing. Perhaps a Flume installation is constantly streaming new files to HDFS, additional business units have expressed an interest in getting data into the cluster, or more users are running more jobs, resulting in more data landing on disk. It’s useful to be able to characterize the growth rate of your data within the cluster. Using Cloudera Manager, you can view historical disk usage reports. A local maximum like the one Josh experienced is shown below, as well as data growing over time into a global maximum. Being able to visualize this growth makes it easy to determine how long your free disk space will last. A historical disk usage report generated by Cloudera Manager What does my disk usage look like right now? A simpler, more common question that a sysadmin might ask is: “How much disk are we using, and who’s to blame?” Cloudera Manager provides a set of operational reports to look at the current state of your Hadoop cluster to see where disk space is going in easy-to-digest bar charts. All it takes is a click of the mouse to pull up a report on how much disk space each user, group, or directory is using. Cloudera Manager provides snapshots of the current disk usage for a cluster. Admins can see how much data is being used, in terms of bytes in HDFS, the raw bytes on physical disk (accounting for replication factors), and file counts. On the right-most chart, we can see that one of the users owns a very large amount of files, which could potentially bog down MapReduce jobs. Using this chart, it’s a simple task to identify the largest consumers of disk resources. A report of current disk usage from Cloudera Manager How is MapReduce being used? At the other end of the spectrum, it’s important to understand the types of jobs that users are running, and which users are using more than their share of cluster resources for executing jobs. By looking at a chart like the one below, Hadoop administrators can get a quick view of which users are utilizing the cluster, how much reading and writing their jobs are doing, how many map and reduce slots their jobs are using, and how long their jobs have been running on the cluster, to name a few useful metrics. A MapReduce usage report from Cloudera Manager Cloudera Manager Will Help You Using Cloudera Manager, you’ll be able to get snapshots of the file system, identify trends in data growth, and aggregate that information by users, groups, or directories that are interesting to you, in order to quickly identify the biggest consumers of resources within the cluster. You’ll be able to discover how individual users are utilizing the cluster by looking at aggregated MapReduce usage statistics, and if you need to do further number crunching, all it takes is a click to export your reports to CSV or Excel spreadsheets, making your data portable and easy to manipulate. If you find yourself needing to ask questions like the ones outlined above, Cloudera Manager will help you be successful in building your cluster.</snippet></document><document id="465"><title>Cloudera Manager – Thank You Customers!</title><url>http://blog.cloudera.com/blog/2012/01/cloudera-manager-thank-you-customers/</url><snippet>Bala Venkatrao is the Director of Product Management at Cloudera. As many of you know, we recently launched Cloudera Enterprise 3.7. Here’s the link to the press release This release marked a transition from Cloudera Management Suite (CMS) to Cloudera Manager (CM), the industry’s first and most comprehensive management application for Apache Hadoop. Over the last month we have received very positive feedback from our customers. I want to thank again all the Clouderans who spent countless hours bringing this product to market. I also want to take this opportunity to thank our customers for helping us get here, as many of them helped us to prioritize the key features for this release. Several customers have also shared the challenges/use cases from their Hadoop deployments and the need for specific features (more later) in Cloudera Manager. Many customers were actively involved in usability testing sessions for Cloudera Manager, which were immensely helpful! At Cloudera, we strive hard to listen to our customers and help build products to address their needs.  We hold regular meetings with customers, sharing early design prototypes and feature ideas and then quickly iterate on the feedback we receive. Cloudera Manager has been a result of this amazing collaboration with our customers and we look forward to this continued partnership as we build on our vision to make it even easier for our customers to manage their Hadoop environments. Here are a few examples of features in Cloudera Manager and how they address some of the pressing needs for our customers: Log Management Hadoop generates a ton of log data and important messages are buried in these log files. However, prior to Cloudera Manager, customers had to write their own scripts to search through these distributed log files attempting to identify which logs contained important messages.  Extracting and presenting this information in an actionable manner is even more challenging. With Cloudera Manager, we made it extremely easy for customers to search through numerous log files for important information. Based on the time range selected and the service type selected, CM will enable customers to search for key messages (filtered by severity or key word) across their Hadoop clusters. A single view presents the consolidated results with key summary information. In addition, CM also provides access to the full log files for further review and analysis. Customers typically start reviewing the log files when Hadoop clusters are not behaving correctly. With the consolidated log management feature in CM, it is now easier and much more efficient to mine and extract useful information from these log files. Proactive Management For the most part, Hadoop administrators work in a reactionary mode. They tend to address issues only when things have failed (jobs failed, slow running jobs, services stopped etc.). A constant request from customers is to be more proactive. What this translates to is the need for more actionable insights and recommendations from Cloudera Manager. To this extent, we have added a new feature in Cloudera Manager called Events. There are several activities (messages, checks, actions etc.) happening in the Hadoop cluster that operators may benefit keeping an eye on. We have synthesized all of this relevant information into Events and made it available in a format that is easy to consume and take action on. Events in CM constitute the following: Health events – These events indicate the occurrence of certain health check activities, or when health check results have met specific conditions (thresholds). For example the overall health of a HDFS service changes from good/green to bad/red or the number of healthy datanodes falls below 95% (set threshold). Log Message events – These events are generated when a log message matches a certain set of pre-defined rules of interest. The default set of rules is based on Cloudera’s experience in supporting several Hadoop clusters. We have mined through the various support tickets to identify patterns/messages in log files that matter the most and are worthwhile paying attention to. We are constantly adding to this list as we learn more from the various Hadoop clusters we help manage. Activity events – These are events generated by the Activity Monitor; specifically, for jobs that fail, or that run slowly (as determined by comparison with duration limits).The time duration for these checks is configurable. Audit events – These are events generated by actions taken through Cloudera Manager, such as creating, deleting, starting, or stopping services or roles. Configuration changes to a service or role also generate audit events that can be tracked. Many of the above Events can be setup as Alerts and forwarded to the right operator as email notifications. We hope the combination of Events + Alerts feature in CM will enable the Hadoop Operator to be more proactive in managing their Hadoop clusters. Support Integration A core value proposition of the Cloudera Enterprise subscription is Support. We employ some of the key contributors on the various Hadoop projects and provide their expertise to address some of the pressing needs of our customer’s Hadoop problems. From talking to the support team and customers, we have seen that on many occasions, problem resolution gets delayed primarily due to lack of information about the problem and the customer’s Hadoop environment. Typically, it takes several iterations to get all the pieces of information of the customer’s Hadoop clusters (configuration files, log files etc.) to triage to the root cause. With Cloudera Manager, we have greatly simplified this process. Support integration is built-in as part of this offering. What this enables is through a single click, customers can send all the relevant information about their cluster back to Cloudera support. This way, the support team and the customers are literally on the same page when working together to trouble shoot a problem and identify the right solution. The above features highlight only a subset of the features in Cloudera Manager. In the coming weeks, we will have blog posts/videos that provide more details on the various features and how they help customer to better manage their Hadoop clusters. For our existing customers – I hope you have had the opportunity to try out Cloudera Manager. If not, you can download it from: https://ccp.cloudera.com/display/SUPPORT/Downloads We are always eager to listen to your feedback as we are constantly looking for ways to further improve the product. Thanks again for all your support in 2011 and look forward to your continued collaboration going forward. If you are interested in participating in our monthly product meetings or for any other feedback regarding Cloudera Manager, please contact me at bala@cloudera.com For prospective customers – A free version of Cloudera Manager is available for download. More information available at http://www.cloudera.com/products-services/tools/ We welcome your feedback at scm-users@cloudera.org Thanks, Bala</snippet></document><document id="466"><title>Oracle selects CDH and Cloudera Manager as the Apache Hadoop Platform for the Oracle Big Data Appliance</title><url>http://blog.cloudera.com/blog/2012/01/oracle-selects-cdh-and-cloudera-manager-as-the-apache-hadoop-platform-for-the-oracle-big-data-appliance/</url><snippet>Cloudera users gain more choice, tighter Oracle integration. Cloudera partners gain increased validation of their platform choice. Ed Albanese Ed leads business development for Cloudera. He is responsible for identifying new markets, revenue opportunities and strategic alliances for the company. Summary: Oracle has selected Cloudera’s Distribution Including Apache Hadoop (CDH) and Cloudera Manager software as core technologies on the Oracle Big Data Appliance, a high performance “engineered system.” Oracle and Cloudera announced a multiyear agreement to provide CDH, Cloudera Manager, and support services in conjunction with Oracle Support for use on the Oracle Big Data Appliance. Announced at Oracle Open World in October 2011, the Big Data Appliance was received with significant market interest. Oracle reported then that it would be released in the first half of 2012. Just 10 days into that period, Oracle has announced that the Big Data Appliance is available immediately. The product itself is noteworthy. Oracle has combined Oracle hardware and software innovations with Cloudera technology to deliver what it calls an “engineered system.” Oracle has created several such systems over the past few years, including the Exadata, Exalogic, and Exalytics products. The Big Data Appliance combines Apache Hadoop with a purpose-built hardware platform and software that includes platform components such as Linux and Java, as well as data management technologies such as the Oracle NoSql database and Oracle integration software. We look forward to all that will be written about the features and performance of the Oracle Big Data Appliance. I expect most of it will be positive and deservedly so. I also would like to take this opportunity to share my thoughts on the market significance of this product for readers of this blog: The Oracle Big Data Appliance is a tremendous validation of Apache Hadoop. The leading data management company in the world has released a highly engineered Apache Hadoop solution for global distribution. CDH users everywhere will benefit from increased choice. The breadth and quality of tools, applications, systems and services that support the CDH platform will undoubtedly grow as the Oracle ecosystem moves to support the Oracle Big Data Appliance. Oracle has made a decision to keep its entry into the Apache Hadoop market open source. Oracle customers who buy the Big Data Appliance get CDH bit for bit, unaltered and with the same license it currently ships with from cloudera.com – Apache v2. Cloudera customers who use Oracle technologies benefit immediately from tighter integration with more Oracle products. The Big Data Appliance is engineered to work with Oracle Exadata Database Machine, Oracle ExaLogic Elastic Cloud, Oracle Exalytics In-Memory Machine, Oracle NoSQL database and Oracle 11. 2011 was an excellent year for Cloudera partners and customers. Cloudera further cemented its reputation as platform vendor that delivers reliable, production-ready Apache Hadoop through a growing number of partners globally. We executed on a strategy to empower any vendor – big or small – to deliver competitively differentiated solutions based on Apache Hadoop to a fast growing user population that appreciates choice. In 2011, Dell launched an excellent set of products ready to be used in both large and small clusters. SGI delivered a performance-oriented architecture and became a global reseller of Cloudera technologies. Storage leader NetApp delivered a novel solution that allows for high flexibility in separating storage from processing. The list of software vendors testing and certifying their products with CDH is big and growing quickly. 2012 promises to be an even better year for CDH users, if variety of solutions, breadth of distribution and supported integrations are the measurement criteria. Open source has always been about choice and we have always taken that commitment to choice for our customers and partners very seriously. The Oracle Big Data Appliance represents a unique solution from a market leader in data management software and demonstrates that Oracle is committed to delivering a world-class product that is competitively differentiated and compelling. This is great news for both users of CDH and all of the partners with whom Cloudera works. Oracle’s selection of CDH further ensures that Cloudera customers and partners are using the most reliable, most widely distributed Apache Hadoop technology stack on the planet.</snippet></document><document id="467"><title>Apache Hadoop Selected for the InfoWorld 2012 Technology of the Year Award</title><url>http://blog.cloudera.com/blog/2012/01/hadoop-selected-for-the-infoworld-2012-technology-of-the-year-award/</url><snippet>Great news! The InfoWorld Tech Center has chosen Apache Hadoop for a 2012 Technology of the Year Award. Judged by InfoWorld Test Center editors and reviewers, the annual awards identify the best and most innovative products on the IT landscape. Winners are drawn from all of the products tested during the past year, with the final selections made by InfoWorld�s Test Center staff. All products reviewed by the Test Center are eligible to win, and we at Cloudera are very excited that Hadoop was named among the finalists. I joined Cloudera in 2011 and it�s been very exciting for me to join the Hadoop community and participate in what, by all accounts, was a landmark year. It�s been fantastic to see how Hadoop has empowered companies of every size and in every industry to do new and interesting things with their data. And this is just the beginning. 2012 promises to bring even more innovation and great use cases for this game-changing platform. The impact of Hadoop is not just on the financial statements of corporate America. In addition to opening up new revenue streams for companies and helping them achieve ever greater levels of operational efficiency, I am particularly impressed with how Hadoop has enabled the use of data for the betterment of society and the human experience. There are so many ways Hadoop has already made a difference in our lives. Here are some of my favorite examples: Hadoop helps us save lives Perhaps the best way to make a life better� is to save it! Hadoop has been used in healthcare and biopharma to collect, standardize, and aggregate large-scale clinical data � enabling healthcare leaders to identify and measure outcomes, correlations, trends, and metrics that lead to better and faster delivery of care as well as prevention of Adverse Drug Events (ADEs). Hadoop keeps us honest Bad guys beware. Financial institutions leverage Hadoop to prevent fraud through pattern recognition. The predictive algorithms run in Hadoop detect anomalies that indicate or result in harmful behavior. Hadoop keeps our critical systems running We depend on technology for everything from national defense to global economics. Hadoop helps us prevent downtime by parsing through vast quantities of machine logs to identify leading indicators of system failure. Hadoop helps us mine the resources we depend on Hadoop helps us aggregate and process data from multiple sources to predict the likely locations of oil and other valuable natural resources. Hadoop enhances our online experience Powering everything from personalized web browsing to the content and messaging platforms that have completely revolutionized the way we interact with each other, Hadoop has been a boon to the online experience. The web would not be what it is today without it � plain and simple. The impact that Hadoop has already had on our world is measurable and vast, and the journey has just begun. Speaking for all of us here at Cloudera, it has been an honor and a privilege to take part in a dynamic, gifted community that creates such a powerful and useful product. Hadoop is well-deserving of this award. Congratulations to the Apache Software Foundation and everyone who participates in making Hadoop the transformative technology that it is!</snippet></document><document id="468"><title>Apache Hadoop in 2011</title><url>http://blog.cloudera.com/blog/2012/01/hadoop-in-2011/</url><snippet>2011 was a breakthrough year for Apache Hadoop as many more mainstream organizations large and small turned to Hadoop to manage and process Big Data, while enterprise software and hardware vendors have also made Hadoop a prominent part of their offerings. Big Data and Hadoop became synonymous in much of the enterprise discourse, and Big Data interest is not restricted to Big Companies. Apache Hadoop Releases Hadoop had three major releases in 2011: 1.0 (AKA 0.20.205.x), 0.22, and 0.23. 1.0.0 adds HDFS support for HBase, Webhdfs, and HDFS performance improvements 0.23 includes performance improvements, Name Node federation, and support for job scheduling and execution models other than MapReduce; it is not yet ready for production use 0.22 is a branch release based on 0.21 with some of the features of 1.0.0 Hadoop Family Releases ZooKeeper (distributed lock manager) provided an update to version 3.3 and released version 3.4 which includes Kerberos client authentication and multiupdate support. HBase (distributed key-value store on HDFS) saw updates to 0.90; 0.92 didn’t quite make it out by the end of the year but immediately after with coprocessors for adding custom code around queries, security, a new file format, and distributed log splitting. Avro (data serialization format) did 1.5 and 1.6 releases, including�a .NET implementation, support for Snappy compression, a builder API, asynchronous RPC, reading and writing Protobuf and Thrift data structures in Avro data files, and performance and schema resolution improvements. Hive (SQL-like interface to Hadoop) had an update release 0.7.1 and a major release 0.8 including bitmap indexes, the TIMESTAMP data type, a plugin developer kit, and JDBC driver improvements. Mahout (machine learning with Hadoop) did a major release 0.5 with improved Lanczos solver, LDA improvements, a Stochastic Singular Value Decomposition implementation, an incremental SVD implementation, and an Alternating Least Squares with Weighted Regularization collaborative filtering implementation. Pig (data flow language for MapReduce) had an update to 0.8 and a major release, 0.9 which�introduced control structures, changed the query parser, and performed semantic cleanup. Cassandra (distributed key-value store) released 0.7, 0.8, and 1.0.6, including�data compression and increased read and write performance. Hama (bulk synchronous parallel computing for e.g. matrix, graph, and network algorithms) is an incubator project that did two major releases: 0.2 and 0.3. Whirr (a set of libraries for running cloud services, with an emphasis on Hadoop-related services) graduated to a TLP (top-level project) and made four releases, including support for ElasticSearch, Voldemort, BYON, and HBase. Projects that joined the Apache incubator in 2011 Flume (streaming data into HDFS); the Flume NG (next generation) project was launched to provide increased robustness. Accumulo (distributed key-value store on HDFS). Bigtop (packaging, deployment, and test framework for Hadoop) made two releases and helped with testing Hadoop releases 1.0, 0.22, and 0.23 in combination with other ecosystem components. Bigtop was used to help validate and test all the new major Hadoop releases. Giraph (graph processing with Hadoop following the bulk-synchronous parallel model relative to graphs where vertices can send messages to other vertices during a given superstep). HCatalog (extension of Hive metadata store) released version 0.2 providing read and write capability for Pig and Hadoop, and read capability for Hive. Kafka (a distributed publish-subscribe messaging system) released versions 0.6 and 0.7. Oozie (workflow management for MapReduce, Pig, and Hive jobs); it released versions 3.0 and 3.1 which added support for “bundles” – sets of workflows managed together. S4 (streaming data processing) released version 0.3. Sqoop (data transfer between HDFS and relational databases) released version�1.4 with among other things customized type mapping. Hadoop-related conferences Besides the very well-attended Hadoop-specific conferences�Hadoop Summit and�Hadoop World, many conferences had significant Hadoop sections or sessions in 2011, including ApacheCon,�Strata,�Cloud Computing Expo,�Chicago Data Summit,�Structure Big Data 2011, and�Oscon. Vendor support Microsoft dropped Dryad and will be supporting Hadoop in 2012. Many other large companies announced major Hadoop initiatives during the year, including�Oracle, Dell, HP, IBM, Informatica, and NetApp. Other Hadoop indicators In December 2011 the percentage of all job postings analyzed by Indeed that mentioned Hadoop; was twice what it was in December 2010. The “Powered By” list at http://wiki.apache.org/hadoop/PoweredBy went from 108 to 157 entries during the year. The Hadoop family mailing lists saw about 72k messages in 2010, 101k messages in 2011 (going by markmail.org). Ten new Hadoop committers were recognized by the community. Predictions for 2012 In 2012 we’ll see HDFS become highly available. The Name Node will be shadowed with a hot standby (most of this work was done in 2011). MapReduce 2 / yarn will stabilize and support clusters larger than 4,000 nodes. More sophisticated job scheduling will allow adherence to strict SLAs (service level agreements) on start and completion while better utilizing cluster resources. Hadoop will be used more extensively where there are real-time access requirements through HBase and other specialized interfaces. HBase 0.92 with support for coprocessors (triggers or custom code executed around a query) will be released and there will be a flurry of coprocessor contrib packages to support secondary indexes, efficient aggregations, and many other optimizations. It will become easier to set up and automate the flow of data into and out of HDFS without significant administrative overhead. The number of BI offerings for Hadoop and their adoption will increase markedly. Close attention will be paid to Hadoop and HBase uptime as they are deployed more widely in mission-critical roles.</snippet></document><document id="469"><title>An update on Apache Hadoop 1.0</title><url>http://blog.cloudera.com/blog/2012/01/an-update-on-apache-hadoop-1-0/</url><snippet>Some users &amp; customers have asked about the most recent release of Apache Hadoop, v1.0: what’s in it, what it followed and what it preceded.  To explain this we should start with some basics of how Apache projects release software: By and large, in Apache projects new features are developed on a main codeline known as “trunk.”  Occasionally very large features are developed on their own branches with the expectation they’ll later merge into trunk.  While new features usually land in trunk before they reach a release, there is not much expectation of quality or stability.  Periodically, candidate releases are branched from trunk.  Once a candidate release is branched it usually stops getting new features.   Bugs are fixed and after a vote, a release is declared for that particular branch.  Any member of the community can create a branch for a release and name it whatever they like. This diagram illustrates the history of the various Apache Hadoop releases and their origins.  There are 3 occasions where community releases from the Apache Hadoop project broke with what would be a more traditional release &amp; branch convention.  These occasions are usually the source of confusion for users. More than a year after Apache Hadoop 0.20 branched, significant feature development continued on just that branch and not on trunk.  Two major features were added to branches off 0.20.2.  One feature was authentication, enabling strong security for core Hadoop.  The other major feature was append, enabling users to run Apache HBase without risk of data loss.  The security branch was later released as 0.20.203.  These branches and their subsequent release have been the largest source of confusion for users because since that time, releases off of the 0.20 branches had features that releases off of trunk did not have and vice versa. Apache Hadoop .22 released chronologically after Apache Hadoop 0.23.  In actuality Apache Hadoop 0.23 is a strict superset of features over 0.22 but it actually released a month before 0.22. A few weeks after 0.23 released, the 0.20 branch formerly known as 0.20.205 was renumbered 1.0.  There is next to no functional difference between 0.20.205 and 1.0.  This is just a renumbering. Because of issue #1, there has been an 18 month period where there has been no one Apache release that had all the committed features of Apache Hadoop.  This table illustrates the point: As members of the Apache Hadoop community, Cloudera engineers have focused their efforts on getting back to releases that are strict superset of all of the features of any past releases so as to avoid having to make the unpleasant choice of picking one feature set over another.  The good news is minus the confusion over the 1.0 numbering, we are basically there.  There have been two good recent releases off of trunk (0.22 and 0.23) one of which (0.23) does have all of the features of any past release.  It’s very possible these new releases will get renumbered to 2.0 or 3.0 or some other number to indicate they are functional supersets of 1.0 but this remains to be decided. Many of you are CDH users and by now you’re wondering what Apache Hadoop you are running today and what Apache Hadoop you’ll be running in the future.  This diagram shows the CDH releases and the Apache Hadoop releases they draw from. The CDH1 distribution incorporated the 0.18.3 Apache Hadoop release.  The CDH2 distribution incorporated the 0.20.1 Apache Hadoop release.  The CDH3 distribution incorporated the 0.20.2 Apache Hadoop release plus the features of the 0.20.append and 0.20.security branches that collectively are now known as “1.0.”  The Apache Hadoop in CDH3 has been the equivalent of the recently announced Apache Hadoop 1.0 for approximately a year now.  The CDH4 distribution will likely incorporate a release from the 0.23.x series.  We also do quarterly updates for CDH releases.  These updates typically include backports from trunk that fix bugs or improve performance &amp; stability, not new component releases.  In some cases when it is not destabilizing or compatibility breaking, a CDH update will include an incrementally new component version.  For example CDH3U0 uses HBase 0.90.0 whereas CDH3U2 uses HBase 0.90.4. Cloudera’s Distribution including Apache Hadoop currently incorporates and integrates 13 different open source components to create a single open source Apache Hadoop based data management platform.  11 of the 13 components come from Apache projects, Apache Hadoop being one of them.  All of these projects have their own branch and release quirks because each project is a different collection of individuals with different motivations and preferences.  This is a feature, not a bug of the Apache community process.  By creating an environment where individuals with disparate motivations can all contribute, projects attract more contributors and more innovation. CDH has a multi-year history of annual releases, quarterly updates, clear upgrade paths and strong policies around maintaining compatibility and stability across updates.  This has only been possible because the CDH engineering team is comprised of more than 20 engineers that are committers and PMC members of the various Apache projects who can shape the innovation of the extended community into a single coherent system.  It is why we believe demonstrated leadership in open source contribution is the only way to harness the open innovation of the Apache Hadoop ecosystem. The most current GA release of CDH is CDH3, update 2.  Find out more about it here.</snippet></document><document id="470"><title>Caching in Apache HBase: SlabCache</title><url>http://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/</url><snippet>This was my summer internship project at Cloudera, and I’m very thankful for the level of support and mentorship I’ve�received�from the Apache HBase community. I started off in June with a very limited knowledge of both HBase and distributed systems in general, and by September, managed to get this patch committed to HBase trunk. I couldn’t have done this without a phenomenal�amount of help from Cloudera and the greater HBase community. Background The amount of memory available on a commodity server has increased drastically in tune with Moore�s law. Today, its very feasible to have up to 96 gigabytes of RAM on a mid-end, commodity server. This extra memory is good for databases such as HBase which rely on in memory caching to boost read performance. However, despite the�availability�of high memory servers, the garbage collection algorithms available on production quality JDK’s have not caught up. Attempting to use large amounts of heap will result in the occasional stop-the-world pause that is long enough to cause stalled requests and timeouts, thus noticeably�disrupting latency sensitive user applications. Garbage Collection The below is meant to be a quick summary of an immensely complex topic, if you would like a more detailed explanation of garbage collection, check out this post. HBase, along with the rest of the Apache Hadoop ecosystem, is built in Java. This gives us access to an incredibly well-optimized virtual machine and an excellent mostly-concurrent garbage collector in the form of Concurrent-Mark-Sweep (CMS). However, large heaps remain a weakness, as CMS collects garbage without moving it around, potentially causing the free space to be spread throughout�the heap instead of in a large contiguous chunk. Given enough time, fragmentation will require a full, stop the world, garbage collection with a copying collector capable of relocating objects. This results in a potentially long stop-the-world pause, and�acts as a practical limit to the size of our heap. Garbage collectors which do not require massive stop the world�compactions�do exist, but are not presently suitable for use with HBase at the moment. The Garbage-First (G1) collector included in recent versions of the JVM, is one promising example, but early testing still indicates that it exhibits some flaws. JVMs from other (non-Oracle) vendors which offer low-pause concurrent garbage collectors also exist, but they are not in widespread use by the HBase and Hadoop�communities. The Status Quo Currently, in order to utilize all available memory, we allocate a smaller heap and let the OS utilize the rest of the memory. In this case, the memory isn�t wasted – it�s used by the filesystem cache. While this does give a noticable performance improvement, it has its drawbacks. Data in the FS cache is also treated as a file, requiring us to checksum, and verify the file. �We also have no guarantee what the FileSystem cache will do and have only limited control over the eviction policy of this cache. While the Linux FS cache is nominally a LRU cache, other processes or jobs running on the system may flush our cache, adversely impacting performance. The FS cache is better than putting the memory to waste, but it’s neither the most�efficient, nor the most consistent solution. Enter SlabCache Another option would be to manually manage the cache within Java via Slab Allocation – opting to avoid garbage collection all together. This is the approach I implemented in HBASE-4027. SlabCache operates by allocating a large quantity of contiguous memory, and then performing Slab Allocation within that block of memory. Buffers of likely sizes of cached objects are first allocated in advance – objects are fit into the smallest buffer available that can contain them upon caching. �Effectively, any fragmentation issues are internalized by the cache, trading off some space in order to avoid any external fragmentation issues. As blocks generally converge around a single size, this method can still be quite space efficient. Implementation While slab allocation does not create fragmentation, other parts of HBase still can. With Slab Allocation, the frequency of stop-the-world(STW) pauses may be reduced, but the worst case maximum pause time isn’t – The JVM can still decide to move our entire slab around if we happen to be really unlucky, contributing again to�significant�pauses. In order to prevent this, SlabCache allocates its memory using direct ByteBuffers. Direct ByteByffers, available in the java.nio package, are allocated�outside of the normal Java heap — just like using malloc() in a C�program.�The garbage collector will not move memory allocated in this fashion – guaranteeing that a direct ByteBuffer will never contribute to the maximum garbage collection time. The ByteBuffer �wrapper� is then registered as an object, which when collected, is released back into the system using free. Reads are performed using a copy-on-read approach. Every time HBase does a read from SlabCache, the data is copied out of the SlabCache and onto the heap. While passing by reference would have been the more performant solution, that would have required some way of carefully tracking references to these objects. I decided against reference counting, as reference counting opens up the potential for an entirely new class of bugs, making�continuing�work on HBase more difficult. �Solutions involving finalizers or reference queues, were also discarded, as neither of them guarantee timely operation. In the future, I may decide to revisit reference counting if necessary to increase read speed. SlabCache operates as an L2 cache, replacing the FS cache in this role. The on-heap cache is maintained as the L1 cache. This solution allows us to use large amounts of memory with a substantial speed and consistency performance over the status quo, while at the same time ameliorating the downsides of the copy-on-read approach. Because the vast majority of our hits will come from the on-heap L1 cache, we do a minimum of copying data and creating new objects. Performance SlabCache operates at around 3-4x the performance of the file system cache, and also provides more consistent performance. Performance comparisons of the 3 caches as followed. In each test, each cache was configured so that it was the primary (L1), and only cache of HBase. YCSB was then run against HBase-trunk. HBase in all cases was running in Standalone mode, compiled against 0.20-append branch. As HDFS has gotten faster since the last release, I’ve also provided tests with the RawLocalFS, in order to isolate the difference between accessing the Slab cache and accessing the FS cache by removing HDFS from the equation.�In this mode, CRC is turned off, and the local filesystem (ext3) is used directly. Even given these optimal conditions, SlabCache still nets a considerable performance gain. If you’d like to try out this code in trunk, simply set MaxDirectMemorySize in hbase-env.sh. This will automatically configure configure the cache to use 95% of the MaxDirectMemorySize, and set reasonable defaults for the Slab Allocator. If finer control is desired, you are free to change the SlabCache settings in hbase-site.xml, which will allow you to have finer control over off-heap memory usage and slab�allocation�sizing.? Conclusion If you’re running into read performance walls with HBase, and have extra memory to spare, then please give this feature a try! This is due to be released in HBase-0.92 as an experimental feature, and will hopefully enable the more efficient usage of memory. I had an amazing summer working on this project, and as an intern, I’m awed to see this feature work and be released�publicly. If you found this post interesting, and would like to work on problems like this, check out the careers page.</snippet></document><document id="471"><title>Cloudera Connector for Teradata 1.0.0</title><url>http://blog.cloudera.com/blog/2012/01/cloudera-connector-for-teradata-1-0-0/</url><snippet>Apache Sqoop (incubating) provides an efficient approach for transferring big data between Hadoop related systems (such as HDFS, Hive, and HBase) and structured data stores (such as relational databases, data warehouses, and NoSQL systems). The extensible architecture used by Sqoop allows support for a data store to be added as a so-called connector. By default, Sqoop comes with connectors for a variety of databases such as MySQL, PostgreSQL, Oracle, SQL Server, and DB2. In addition, there are also third-party connectors available separately from various vendors for several other data stores, such Couchbase, VoltDB, and Netezza. This post will take a brief look at the newly introduced Cloudera Connector for Teradata 1.0.0. Features A key feature of the connector is that it uses temporary tables to provide atomicity on data transfer. This feature ensures that either all or none of the data are transferred during import and export operations. Moreover, the connector opens JDBC connection against Teradata for fetching and inserting data, and it automatically injects appropriate parameter underneath to use the FastExport/FastLoad feature of Teradata for fast performance. Installation The first thing you will need is to install Sqoop. CDH3 documentation serves as a good reference on how to do this. You also need the Teradata JDBC JAR files (terajdbc4.jar and tdgssconfig.jar), and they can be put under the lib directory of your Sqoop installation (so Sqoop can pick them up at run time). One last thing is to enable Sqoop to process the Teradata JDBC URL syntax with the specialized Teradata manager factory. To do this, you can add the following inside a sqoop-site.xml file within the configuration directory of your Sqoop installation: &lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;sqoop.connection.factories&lt;/name&gt;
    &lt;value&gt;com.cloudera.sqoop.manager.TeradataManagerFactory&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
  Once you have finished the above steps, you are ready to download the connector and give it a try. Data Import Using this connector is similar to using other connectors with a difference on the JDBC URL syntax, which looks like jdbc:teradata:///DATABASE=. For example, to import all data from a table named MY_DATA on a database named MY_BASE into HDFS: sqoop import
  --connect jdbc:teradata://hostname/DATABASE=MY_BASE
  --username sqooptest  --password xxxxx
  --table MY_DATA --num-mappers 3
  You can import the data into Hive as well by modifying the above example slightly as: sqoop import
  --connect jdbc:teradata://hostname/DATABASE=MY_BASE
  --username sqooptest  --password xxxxx
  --table MY_DATA --num-mappers 3
  --hive-import
  Data Export Similarly, to export all data under a HDFS directory named MY_DATA to a table with the same name on a Teradata database named MY_BASE: sqoop export
  --connect jdbc:teradata://localhost/DATABASE=MY_BASE
  --username sqooptest --password xxxxx
  --table MY_DATA --num-mappers 3
  --export-dir /user/johndoe/MY_DATA
  Temporary Tables By default, temporary tables are used to ensure atomicity. During import operation, a temporary table is created with a copy of the original data divided into a number of partitions. The number of partitions in the temporary table is the same as the number of mappers in Hadoop (as specified by the user). Then each individual mapper pulls data from one single partition, and multiple mappers run in parallel to one another. After data are successfully imported, the temporary table is deleted. In the opposite direction, during export operation, each individual mapper creates a temporary table to load data into. Similar to import operation, multiple mappers runs in parallel to one another. At the end, data from all temporary tables are merged into the target table, and then the temporary tables are deleted as well. If for any reason export operation cannot be completed successfully, the target table will remain intact. Disabling the temporary tables While temporary tables provide the benefit of atomicity, it also takes up additional time and space. Hence, there is a switch available to turn off the temporary table usage as needed depending on the use case scenario. To disable the temporary table usage during import operation, for example, sqoop import
  -D sqoop.teradata.import.use.temporary.table=false
  --connect jdbc:teradata://hostname/DATABASE=MY_BASE
  --username sqooptest  --password xxxxx
  --table MY_TABLE --num-mappers 3
  In this case, all mappers pull data from the source table MY_TABLE directly at the same time. Note that temporary tables cannot be disabled during export operation due to the technical constraint of using the FastLoad feature. Choosing a different location The temporary table is created under the database that is currently connected. sqoop import
  --connect jdbc:teradata://hostname/DATABASE=MY_BASE
  --username sqooptest  --password xxxxx
  --table MY_DATA --num-mappers 3
  In the above example, the temporary table is created under MY_BASE, which happens to be the same database where the source table MY_DATA is located. This can be changed by providing a qualified table name under a database different from the one that is currently connected. For example, sqoop import
  --connect jdbc:teradata://hostname/DATABASE=TEMP_BASE
  --username sqooptest  --password xxxxx
  --table MY_BASE.MY_DATA --num-mappers 3
  In this case, the temporary table is created under TEMP_BASE while the source table MY_DATA is located under MY_BASE. As a result, you may have a dedicated temporary space for all temporary data if needed. Performance Consideration From Hadoop perspective, you can increase the number of mappers for better performance in general. The number of mappers in parallel may be up to the number of task trackers times 2 (by default). However, you will also need to take your Teradata settings into consideration. In other words, parallelism may also be limited by the maximum number of FastExport/FastLoad processes allowed in Teradata at the mean time (see MaxLoadTasks and MaxLoadAWT settings in Teradata for further details). This connector takes advantage of the FastExport/FastLoad feature of the underlying JDBC connection for fast performance. This feature in Teradata is recommended for dealing with a large amounts of data, and thus the connector is best used with such situation. Where to Get More This post only provides an overview on the Cloudera Connector for Teradata. If you are interested in learning more, a full user guide is available here.</snippet></document><document id="472"><title>Apache Hadoop for Archiving Email – Part 2</title><url>http://blog.cloudera.com/blog/2012/01/hadoop-for-archiving-email-part-2/</url><snippet>Part 1 of this post covered how to convert and store email messages for archival purposes using Apache Hadoop, and outlined how to perform a rudimentary search through those archives. But, let�s face it: for search to be of any real value, you need robust features and a fast response time. To accomplish this we use Solr/Lucene-type indexing capabilities on top of HDFS and MapReduce. Before getting into indexing within Hadoop, let us review the features of Lucene and Solr: Apache Lucene and Apache Solr Apache Lucene is a mature, high performance, full-featured Java API used for indexing and searching that has been around since the late nineties — it supports field-specific indexing and searching, sorting, highlighting, and wildcard searches, to name only a few. Everything in Lucene boils down to creating a document using artifacts such as email messages, HTML, PDF, XML, Word, Excel, etc, the contents of which will end up being parsed and added to Lucene documents as name/value pairs.� There are a number of libraries available for extracting actual content, depending on what the artifact is. When extracting content from .msg email files, for instance, TIKA and POI are some useful libraries. Once you have added name/value pairs from the email content to the document, the index portion is taken care of. We can then use IndexSearcher to search through the indexed contents as illustrated below, in Figure 1: Figure 1: Indexing and Searching using Lucene. Apache Solr, on the other hand, is a Lucene-based full text search server with XML, JSON, and HTTP APIs, which has a web admin interface and provides extensive caching, replication, search distribution, as well as the ability to add customized plugins. Solr already includes various parsing libraries, including Tika, POI, and TagSoup, among others. Figure 2 below illustrates the Solr components and deployment architecture: Figure 2: Solr Components and Deployment Architecture. Next, let�s explore how you can use both Solr and Lucene within the Hadoop environment for indexing and searching massive amounts of data: First, you need to get data into HDFS, as covered in Hadoop for Archiving Email – Part 1. Once the data is there, you can start to run MapReduce to create indexes in parallel that can then be dumped into HDFS or into a Local File System. If an index is stored within a Local File System, simply serve it from there by pointing Solr to it. It can be run either in single, replicated or distributed mode, depending on the size of the index to serve.� However, if you need to make search available for only for a small number of users, you can simply store data directly in� HDFS and provide an interface for your users to access it directly. One tool that I find very handy for providing such an interface is Luke. Luke was built for development and diagnostic purposes, and can be used to search, display and browse the results of a Lucene index. With Luke, you can view documents, analyze results, copy and delete them, or optimize indexes that have already been built. The best part: you can easily operate on multipart indexes stored directly in HDFS, as illustrated in Figure 3: Figure 3: Indexing and Searching within HDFS or Local Filesystem. Having discussed design at a high level, let�s now dive deeper into the details of MapReduce for creating an index. Here is how the configure portion of each mapper could look: //initialize indexWriter..
Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_33);
IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_33, analyzer);
//if we are writing to hdfs, then use RAMDirectory
if (toHDFS){
� � � iwc.setOpenMode(OpenMode.CREATE);
� � � idx =� new RAMDirectory();
� � � writer = new IndexWriter(idx, iwc);
} else {
� � �//use CREATE_OR_APPEND so if index already exists it will simply be appended to
� � �iwc.setOpenMode(OpenMode.CREATE_OR_APPEND);
� � �idx = FSDirectory.open(new File(outputDir));
� � �writer = new IndexWriter(idx, iwc);
} Initialize the analyzer and index writer config. If writing to HDFS, you can use RAMDirectory to hold the indexes created; and once complete, flush to HDFS. If writing to a local file system, simply create FSDirectory with the location. Having configured the mapper, let�s look at the map method: public void map(LongWritable key, Text value, OutputCollector output,
Reporter reporter) throws IOException {
try {
� � �Document doc = new Document();
� � �//add email file path
� � �String path = key.toString();
� � �Fieldable field = new Field("path", path,
� � �Field.Store.YES, Field.Index.ANALYZED);
� � �doc.add(field);

� � �//convert content into MapiMessage
� � �InputStream input = new
� � �ByteArrayInputStream(value.getBytes());
� � �MAPIMessage msg = new MAPIMessage(input);
� � �//add recipient as stored and analyzed field so we can
� � �//search based on recipient and display recipient name in the results
� � �String recipient = msg.getRecipientEmailAddress();
� � �field = new Field("receipient",
� � �recipient,Field.Store.YES, Field.Index.ANALYZED );
� � �doc.add(field);

� � �String subject = msg.getSubject();
� � �field = new Field("subject", subject, Field.Store.YES,
� � �Field.Index.ANALYZED );
� � �doc.add(field);

� � �String content = msg.getTextBody();
� � �field = new Field("content", content, Field.Store.YES,
� � �Field.Index.ANALYZED );
� � �doc.add(field);

� � �//add more fine grained fields based on search criteria needed
� � �...

� � �writer.addDocument(doc);
} catch (Exception e) {
� � �e.printStackTrace();
}
}
 Once it�s configured within each map, the exercise boils down to parsing the content and adding it to the writer. The above code does the following: Create a Lucene document. Parse email content passed into MAPIMessage. Extract necessary fields and add it to the index. (In this example I only extract recipient, subject and content. You can add more fields as necessary and use Hbase to store the content if needed.) At this point, the index has been created � either in memory or the Local File System. The final task is to close the index to make it searchable, which can be done within the close method of the mapper, as demonstrated below: public void close() {
try {
writer.optimize();
writer.close();
if (toHDFS) {
� � �String files[]= idx.listAll();
� � �FileSystem dfs = FileSystem.get(localConf);
� � �for (int i = 0; i &lt; files.length; i++){
� � �� � �//Read data into byte array, create file in HDFS, write bytes to that file
� � �� � � ....
� � �� � �fs.write(array);
� � �� � �fs.close();
� � �}
}
} catch (CorruptIndexException e) {
� � �e.printStackTrace();
} catch (IOException e) {
� � �e.printStackTrace();
}
}
 Close the created index. In the case of HDFS, walk through the index in memory and write it to HDFS. The index should now have been created, either in the Local File System of each of the DataNodes, or in HDFS directly. If it is in the Local File System, you can opt to make the directory part of the �www� directory and enable Solr to serve it from there. If it is in HDFS, one could load the index in RAMDirectory within each mapper and search, use a tool like Luke to provide a search interface, or put a mechanism in place to copy it to the Local File System to point Solr at it. Appending Appending to an existing index can be a bit tricky. If the index sits in a Local File System, this can be accomplished by setting the index writer to APPEND mode and adding new documents. This can get a bit more complicated, however, when the index is in HDFS. One option would be to write an index to a new directory in HDFS, then merge with the existing index. SolrCloud and Katta Since we are discussing fast search options, it also makes sense to touch on components like SolrCloud and Katta. SolrCloud enables clusters of Solr instances to be created, with a central configuration, automatic load balancing, resizing, rebalancing and fail-over. Katta serves indexes in a distributed manner similar to HDFS. It has built in replication for fail-over and performance, is easy to integrate with Hadoop clusters and has master fail-over. However, it does not provide real-time updates, nor is it an indexer � it is simply a serving tool for Lucene indexes. In Part 3 of this series, I will cover ways to ingest such email messages and ways to put the steps involved in a workflow. In the meantime, drop us a line if you have any questions on storing email message in Hadoop and index and search them using Solr and Lucene.</snippet></document><document id="473"><title>What’s New in Apache Sqoop 1.4.0-incubating</title><url>http://blog.cloudera.com/blog/2012/01/whats-new-in-apache-sqoop-1-4-0-incubating/</url><snippet>This blog was originally posted on the Apache Blog. Apache Sqoop recently celebrates its first incubator release, version 1.4.0-incubating.  There are several new features and improvements added in this release.  This post will cover some of those interesting changes.  Sqoop is currently undergoing incubation at The Apache Software Foundation.  More information on this project can be found at http://incubator.apache.org/sqoop. Customized Type Mapping (SQOOP-342) Sqoop is equipped with a default mapping from most SQL types to appropriate Java or Hive counterparts during import.  Even though, this one-mapping-fits-all approach might not be ideal in all scenarios considering a wide variety of data stores available today, not to mention there are certain vendor-specific SQL types that may not be covered by the default mapping. To allow customized type mapping, two new arguments, map-column-java and map-column-hive, are introduced for changing mapping to Java and Hive, respectively.  The list of mapping is expected in the form of &lt;column name&gt;=&lt;target type&gt;, such as $ sqoop import ... --map-column-java id=Integer,name=String For the above example, the columns id and name will be mapped to Java Integer and String, respectively. Boundary Query Support (SQOOP-331) Sqoop uses a canned query (select min(&lt;split column&gt;), max(&lt;split column&gt;) from &lt;table name&gt;) to determine boundaries for creating splits in all cases by default.  This query may not always be the most optimal one however.  Hence, to provide flexibility for using different queries based on distinct usages, a new boundary-query argument is provided to take any arbitrary query returning two numeric columns for the same purpose of creating splits. Date/Time Incremental Append (SQOOP-321) Incremental import in Sqoop can be used to only retrieve those rows with the value of a check column beyond a certain threshold.  The threshold needs to be the maximum value of the check column (in append mode) or the timestamp (in lastmodified mode) at the end of last import. Previously, in append mode, the check column has to be in numeric type.  If a date/time type is desired, the user has to manually select the maximum value out of the date/time column and then specify that value as the last-value argument in lastmodified mode instead.  As part of this release, now the check column can be in date/time type as well. Composite Key Update (SQOOP-313) By default, Sqoop export adds new records into a table by INSERT statements.  However, if any record is in conflict with an existing one due to table constraints (such as a unique key), the underlying INSERT statement will fail and the export process will fail.  If an existing record needs to be modified, the update-key argument can be specified and UPDATE statements will be used instead underneath. Before this release, only a single column name can be specified in the update-key argument.  This column name will be used to determine the matching record(s) for update.  However, in many real world situations, multiple columns are required to identify the matching record(s).  Thus, starting from this release, a comma separated list of column names can be given as the update-key argument. Mixed Update/Insert Export (SQOOP-327) As mentioned, Sqoop export can only either insert (by default) or update (with the update-key argument) records into a table.  As a result, one issue is that if data are being inserted, they may cause constraint violations when they exist already.  Another issue is that if data are being updated, they may be silently ignored when there are no matching update keys found.  It lacks the functionality to both update those data with matching update keys and insert those without. A new update-mode argument is introduced to resolve the above issues.  Its value can be either updateonly or allowinsert.  As the name suggests, the difference is those records without matching update keys are simply dropped when the value is updateonly or are inserted when the value is allowinsert.  Note that this feature is currently provided only for built-in Oracle connector. IBM DB2 Support (SQOOP-329) The extensible architecture used by Sqoop allows support for a data store to be added as a so-called connector.  By default, Sqoop comes with connectors for a variety of databases such as MySQL, PostgreSQL, Oracle, and SQL Server.  In addition, there are also third-party connectors available separately from various vendors for several other data stores, such Couchbase, VoltDB, and Netezza.  As part of this release, a new connector is provided to import and export data against IBM DB2 database. The Final Chapter If you are interested in learning more about the changes, a complete list for Sqoop 1.4.0-incubating can be found here.  You are also encouraged to give this new release a try.  Any help and feedback is more than welcome. For more information on how to report problems and to get involved, visit the Sqoop project website at http://incubator.apache.org/sqoop/.</snippet></document><document id="474"><title>Apache ZooKeeper 3.4.2 has been released</title><url>http://blog.cloudera.com/blog/2011/12/apache-zookeeper-3-4-2-has-been-released/</url><snippet>Apache ZooKeeper release 3.4.2 is now available. This is a bug fix release covering 2 issues, one of which was considered a blocker. ZOOKEEPER-1333 is the most serious of the issues addressed, it could cause a server to fail to rejoin the quorum on restart: ZOOKEEPER-1333 NPE in FileTxnSnapLog when restarting a cluster If you are running 3.4.1 or earlier be sure to upgrade immediately. See my earlier post for details on what’s new in 3.4. Stability, Compatibility and Testing It is important to note that 3.4.2 is not yet ready for production. It is an early release that users can start testing so that we can stabilize later 3.4.x releases.  We expect a dot release to be production-ready soon, and to be incorporated into CDH4. Getting Involved The Apache ZooKeeper project is working on a number of new features. Our How To Contribute page is a great place to start if you’re interested in getting involved as a developer. You can also follow me on twitter. Acknowledgements A special thanks to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc).</snippet></document><document id="475"><title>Apache HBase 0.90.5 is now available</title><url>http://blog.cloudera.com/blog/2011/12/apache-hbase-0-90-5-is-now-available/</url><snippet>Apache HBase 0.90.5 is now available.  This release of the scalable distributed data store inspired by Google’s BigTable is a fix release that covers 81 issues, including 5 considered blockers, and 11 considered critical.  The release addresses several robustness and resource leakage issues, fixes rare data-loss scenarios having to do with splits and replication, and improves the atomicity of bulk loads.  This version includes some new supporting features including improvements to hbck and an offline meta-rebuild disaster recovery mechanism. The 0.90.5 release is backward compatible with 0.90.4. Many of the fixes in this release will be included as part of CDH3u3. Acknowledgments A special thanks to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc).</snippet></document><document id="476"><title>How I found Apache Hadoop</title><url>http://blog.cloudera.com/blog/2011/12/how-i-found-hadoop/</url><snippet>This is a guest post contributed by Loren Siebert. Loren is a San Francisco entrepreneur and software developer, and is currently the technical lead for the USASearch program. A year ago I rolled my first Apache Hadoop system into production. Since then, I’ve spoken to quite a few people who are eager to try Hadoop themselves in order to solve their own big data problems. Despite having similar backgrounds and data problems, few of these people have sunk their teeth into Hadoop. When I go to Hadoop Meetups in San Francisco, I often meet new people who are evaluating Hadoop and have yet to launch a cluster. Based on my own background and experience, I have some ideas on why this is the case. I studied computer science in school and have worked on a wide variety of computer systems in my career, with a lot of focus on server-side Java. I learned a bit about building distributed systems and working with large amounts of data when I built a pay-per-click (PPC) ad network in 2004. The system is still in operation and at one point was handling several thousand searches per second. As the sole technical resource on the system, I had to educate myself very quickly about how to scale up. As I contemplated how doomed I would be should traffic levels increase much more, I remember wondering to myself, “How does Google deal with all that data?” The answer came to me in the form of the Google File System (GFS) paper and later the MapReduce paper, both from Google. It dawned on me that because Google was forced to solve a much larger problem, they had come up with an elegant solution for a whole range of more modest data problems running on commodity hardware. But it wouldn’t be until 2010 that I would get to work with this technology firsthand. As I wrote in an earlier article, I started re-architecting USASearch, the U.S. government�s search system, in 2009 based on a solution stack of free, open source software including Ruby on Rails, Solr, and MySQL. A wave of d�ja vu hit me as I started worrying about what to do with the growing mountain of data piling up in MySQL and our increasing need to analyze it in different ways. I had heard that a new company called Cloudera, founded by some big data people from Yahoo!, Google, and Facebook, was making Hadoop available for the masses in a reliable distribution, much in the same way that RedHat did for Linux. Curiosity got the best of me and I bought the newly minted Hadoop: The Definitive Guide from O’Reilly. The most insightful part of the book to me was the very first sentence. It’s a quote from Grace Hopper: “In pioneer days, they used oxen for heavy pulling, and when one ox couldn�t budge a log, they didn�t try to grow a bigger ox.” I didn’t want to grow a bigger server; I wanted to harness a bunch of small servers together to work in unison. The more I learned the more curious I got, so I started reading more. And that’s when I hit my first roadblock. I think people who have been working with Hadoop technologies for years and years sometimes forget just how rich and diverse the big data software ecosystem has become, and how daunting it can be to folks approaching it for the first time. When people at the Meetups say they are evaluating solutions to their data scaling problem, the answers they hear sound something like this: “Just use Hadoop Hive Pig Mahout Avro HBase Cassandra Oozie Sqoop Flume ZooKeeper Cascading NoSQL RCFile. Oh, almost forgot�cloud.” The thought of wading through all of that just to learn about what I needed to learn about was a bit too overwhelming for me, so I put the whole matter aside for a few months. Over time, I started to dive into each of these projects to understand the primary use case, how active the developer community was and which organizations were using it in production. I converged on the idea of using Hive as a warehouse for our data. I opted for Cloudera’s distribution since I wanted to reduce the risk of running into compatibility issues between all the various subsystems. Having tracked down anomalies in a highly multi-threaded and contentious distributed Java system before, I liked the idea of someone else taking on that problem for me. At some point, I had read everything I could read and grew impatient to get my hands dirty, so I decided to just download CDH3 on my laptop and give it a try. The tutorial instructions for the standalone version worked, and I successfully computed more digits of pi than I ever thought I’d need. After creating some sample data in Hive and running a few queries, I felt pretty confident that Hive would be the right tool for the job. I just needed to find somewhere to install and run HDFS (namenode, secondary namenode, and data nodes), Hadoop (jobtracker and tasktracker nodes), Hive, and Hue for a nice front end to it all. I knew from my past experience how to stretch the limits of CPU, disk, IO, and memory on commodity servers, and I identified a few potential servers at our primary datacenter with resources I figured I could leverage. Once again I followed the tutorial instructions, this time for the fully distributed version of CDH3, and once again I started to compute pi. And that�s when I hit my second roadblock. It took me a few days�to figure out that I had a problem with DNS. Each machine needs to be able to resolve every other machine�s name and IP in the cluster. Whether you do that via /etc/hosts or a local DNS server is up to you, but it needs to happen or the whole thing gets wedged. Once I got that sorted out, everything just started falling into place and I had Hive working in production within a few days. A week later, I started pulling out the MySQL jobs and deleting big tables, and that�s been the trend ever since. Over time, I�ve gone on to learn about using custom Ruby mappers in Hive, moving data back and forth between MySQL and Hive with Sqoop, and getting the data into HDFS in real-time with Flume. All of these components from the Cloudera distribution are working nicely in our production environment now, and I sleep well at night knowing I have such a solid, deliberate plan for growth. My initial investment in learning about the Hadoop ecosystem is really paying dividends, but when I think about all those people at the Meetups stuck in evaluation mode, I feel their pain. Does it have to be such a struggle? The big challenge in my opinion is not that any one piece of the puzzle is too difficult. Any reasonably smart (or in my case stubborn) engineer can set themselves on the task of learning about a new technology once they know that it needs to be learned. The challenge with the Hadoop ecosystem is that it presents the newbie with the meta-problem of figuring out which of these tools are appropriate for their use case at all, and whether or not to even consider the problem today versus deferring it until later. In a way Facebook has it easy, because when you are adding 15TB of data per day, that decision is pretty much made for you. For all the companies sitting in the twilight between the gigabyte and the petabyte who don�t have Hadoop expertise in-house, there is a collection of free information to help guide people to the right solution space (Hadoop Tutorial, White Papers). These days, when I talk to people who are evaluating solutions to their big data problems, my advice to them is to break down their problems into a few discrete use cases and then work on ferreting out the technologies that are designed for that use case. Get a proof of concept to demonstrate that the technology can address your use case and convince yourself and others that you�re on the right track. Work toward putting something simple into production. Lather, rinse, and repeat. I am still in that cycle myself, as these days I�m exploring HBase and OpenTSDB to give me low-latency access to time series data and Mahout to do frequent item set mining, but that�s another article for another day.</snippet></document><document id="477"><title>Apache Whirr 0.7.0 has been released</title><url>http://blog.cloudera.com/blog/2011/12/apache-whirr-0-7-0-has-been-released/</url><snippet>Apache Whirr release 0.7.0 is now available. It includes changes covering over 50 issues, four of which were considered blockers. Whirr is a tool for quickly starting and managing clusters running on cloud services like Amazon EC2. This is the first Whirr release as a top level Apache project (previously releases were under the auspices of the Incubator). In addition to improving overall stability some of the highlights are described below: Support for Apache Mahout as a deployable component is new in 0.7.0. Mahout is a scalable machine learning library implemented on top of Apache Hadoop. WHIRR-384 – Add Mahout as a service WHIRR-49 – Allow Whirr to use Chef for configuration management WHIRR-258 – Add Ganglia as a service WHIRR-385 – Implement support for using nodeless, masterless Puppet to provision and run scripts Whirr 0.7.0 will be included in a scheduled update to CDH4. Getting Involved The Apache Whirr project is working on a number of new features. The How To Contribute page is a great place to start if you’re interested in getting involved as a developer. Acknowledgements A special thanks to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc), and big thank you to our tireless release manager Andrei Savu.</snippet></document><document id="478"><title>Apache Avro at RichRelevance</title><url>http://blog.cloudera.com/blog/2011/12/apache-avro-at-richrelevance/</url><snippet>This is a guest post from RichRelevance Principal Architect and Apache Avro PMC Chair Scott Carey. In Early 2010 at RichRelevance, we were searching for a new way to store our long lived data that was compact, efficient, and maintainable over time. We had been using Hadoop for about a year, and started with the basics – text formats and SequenceFiles. Neither of these were sufficient. Text formats are not compact enough, and can be painful to maintain over time. A basic binary format may be more compact, but it has the same maintenance issues as text. Furthermore, we needed rich data types including lists and nested records. After analysis similar to Doug Cutting’s blog post, we chose Apache Avro. As a result we were able to eliminate manual version management, reduce joins during data processing, and adopt a new vision for what data belongs in our event logs. On Cyber Monday 2011, we logged 343 million page view events, and nearly 100 million other events into Avro data files. Avoiding Version Management Baggage Have you ever seen code for manual serialization version management like the below? 
int version = input.readInt();
this.name = input.readName();
this.age = input.readInt();
if (version &gt;= 2) {
  this.favoriteColor = input.readString();
} else {
  this.favoriteColor = "";
} Manual version management is painful. If you evolve what data you store continuously, it does not take long to end up with dozens of versions. In order to read every version that has been written and stored, your code has to carry a lot of baggage. With Avro, you can avoid writing code like the above. The concept is simple. Store the schema used to write your data along with your data, and use it to make the written data conform to the schema that the reader expects. If a field is missing, use the default. If has been removed or moved, handle it. Over the last two years, we have doubled the complexity of our our page view schema across about 15 schema versions. There is not one line of code that deals with version management, and the current code can read any of the data written over that time. What may be a surprise to some is that our old code can read newly written data as well. The data is both forward and backward compatible, within the rules in the Avro Specification. Leveraging Complex Data Types Avro supports complex data types such as arrays, maps, enumerations, and nested records. The Avro data model makes it possible to serialize any non-recursive data structure, including trees and heterogeneous lists. We use this property to describe our events using Avro Schemas that map to natural object representations on our front end servers. For example, one of the elements in a page view is an array of product recommendation sets, each set containing a list of products displayed. Another element in a page view is what we call a page context – each type of page on a merchant’s site has a unique context that differs from other page types. A product page context is the product being displayed. A search page context is the search terms in the search query. There are about 30 page context types, and we represent the range of page context possibilities using an Avro union, so that all of these different event variations can be written in the same format and to the same log. With a simpler data model, one might have had to log each context type separately, making it harder to get a full picture of what happened in a single request during analysis. A New Vision for an Event Log With the above properties of Avro, we were able to formulate a new vision for what an event log should be. The new model has the following properties: A singe HTTP request creates a single, atomic log event defined by an Avro schema The event contains all of the resolved request inputs The event contains the result of any decisions made during the request Together, these imply that it is never necessary to join different sets of data together to reconstruct what happened in an individual request during analysis. This also significantly reduces the value of data contained in raw HTTP logs, since the Avro based logs become the origin for all major processing. Since raw HTTP logs are significantly larger than compressed binary structured data, this significantly reduces the size of data we must keep for long periods of time. More Avro at RichRelevance We have built Hive and Pig adapters to map our Avro data into these tools for ad-hoc queries and automated tasks. Additionally, we leverage the same Avro schemas from our log files to store click streams in HBase. We also use Avro to store data compactly in key-value stores. The log file example is what I call a schema first use case of Avro, where we define a schema for log events that can be used across different systems over a long period of time. An alternative usage style is what I call code first, where you start with code and bind that to a serialization with a less schema-centric view. I feel that the code first usage style is more applicable for data that lives for short or medium time scales, such as with RPC or MapReduce intermediates. We will be deepening our investment in Avro and using it with code first use cases in the future, in the process working with the community to improve the developer experience for those use cases. Avro is a growing, evolving project that I see as more broad than a serialization framework. At heart Avro is about applying a schema to data, in order to manipulate that data in well defined ways. Serialization, validation, and transformation are only some of the operations you can apply to data that conforms to a known schema. Over time the project will grow to have more and more functionality centered around operations you can apply to data that conforms to an Avro Schema. I look forward to working with the Avro community as the project continues to evolve!</snippet></document><document id="479"><title>Notes from the Flume NG Hackathon</title><url>http://blog.cloudera.com/blog/2011/12/notes-from-the-flume-ng-hackathon/</url><snippet>This blog was originally posted on the Apache Blog: https://blogs.apache.org/flume/entry/apache_flume_hackathon. Apache Flume is currently undergoing incubation at The Apache Software Foundation. �More information on this project can be found at http://incubator.apache.org/flume. The next generation of Apache�s log ingestion framework, Apache Flume NG, has bolted out of the starting blocks. �Last Friday, employees from a diversity of enterprises packed Cloudera Headquarters to learn more and to contribute to the project themselves. �With over 40 folks attending the hackathon, what�s clear is that major enterprises value reliable, performant data ingestion, and that Flume�s new branch is well-positioned to make that kind of ingest a reality. �During the session, 10 JIRAs were filed, with one resolved, one code review in progress, and four patches in the works. Cloudera captured the morning session on video and has made it�available for members of the flume user and dev lists and for the community at large. �During the morning Teach-In, Flume committers covered NG�s design, the mechanics of contributing to the Apache project, and more.   Get a better understanding of NG�s design by reading Arvind Prabhakar�s recent post, which does a nice job covering the ins and outs of the new framework:https://blogs.apache.org/flume/entry/flume_ng_architecture. Apache Flume Committers share the new design with the community Guest Post by Basier Aziz. Photo by Kate Ting, Video by Cloudera</snippet></document><document id="480"><title>My Internship at Cloudera</title><url>http://blog.cloudera.com/blog/2011/12/my-internship-at-cloudera/</url><snippet>David joined us as part of our intern program, and built the prototype for the distributed log search functionality that’s available as part of Cloudera Manager 3.7. He did an awesome job, and wrote the following blog post which, now that CM3.7 has been released, we’re pleased to publish. The project My intern project was to build a log searching tool, specialized for Apache Hadoop. My mini-app allows Hadoop cluster admins and operators to search their error logs across many machines, filter by time range, text in the log message, and find the namenode machine, for example. The results are then ordered by time, and shown to the user. This project was inspired by the extreme wizardry required to search logs with traditional tools, such as grep and ssh (or parallel ssh), especially since these tools do not order the results by time. Ordering by time is very important, as it allows one to triage the sources of failures across your cluster, and figure out where it all started. How do I feel about my project in retrospect? I had a ton of independence when it came to building my app. As part of the Enterprise Team, which wrote Cloudera Manager allowing one to easily deploy Hadoop to a whole cluster within a few clicks, I wrote the REST API available on the individual machines of the cluster. I wrote the master server code that makes requests in parallel to each of the cluster machines, asking for their search results. I designed and wrote the UI for the app, in addition to conceiving ways to make life easier for users who interact with it (with some help from user-testing, of course). A couple of the niceties I added to the search page include: Search without page refresh. Saves the current search’s options to the URL bar, so that if you send the URL to a fellow admin, they can run the exact same search and see exactly what you’re seeing. Or you can save the URL and re-run the search at a later date. A context view that does the same thing as the above, but lets you browse a single log file, with pagination (a single day’s log file can get as big as 1GB, so it wouldn’t be a great idea to send it all to the client at the same time). In this process I learned more about python, the difficulty that python’s built-in date capabilities can cause, and how it can be quite fun to run code distributed across hundreds or thousands of machines. I also spent some time profiling the internal Cloudera log searching library (written by Adam Warrington) which is the workhorse of the REST API (the master server communicates with its minions over HTTP). We were able to cut the worst-case run time on sample data by ~88%, which made me happy. During the process, I learned when possible, it’s best to meta-program other people’s code by asking them to make it faster, as the process of learning and reading all the code they’ve written can take some time, especially when you only need to make what you hope is a small change. It’s really great to arrive at work and hear that someone else has just finished coding up the optimizations your app needed. Technical Portion: how the log search feature works. Whenever you run a search, the main page of the search UI makes a request to a JSON endpoint, asking for log search results from, say, yesterday on all datanodes in the cluster. This request reaches the master server (SCM), which knows all about the machines in the cluster. The master server has a number of threads which make requests in parallel to each of the applicable cluster machines, each of which exposes a JSON endpoint. Each individual cluster machine then runs some python code that searches the relevant log files and returns the result as JSON. The master server collates the results from each cluster machine, and returns this to the browser. The results are then displayed to the user in what can sometimes be a very long list. We decided it would be best not to maintain an index on the log files, as there can be many terabytes of data to sift through. For this reason, searches are done on demand by each individual cluster machine. Searches which include a time range are quite fast, as binary search is used to find the relevant time range, and then the first 20 results are returned. We also made an effort to optimize searches similar to grep helloworld that filter out certain words when we scan the particular line for the word, and skip the line without parsing it into an event if that line does not contain helloworld. We made this optimization because parsing each log event into date, message, and source was quite slow when searching large files. Because I wrote the three components that make the search work (UI, JSON route on master server, JSON route on cluster machines), I got a good overview of many aspects of the code base. A brief overview of the indirectly-dev related skills I learned a Cloudera I’ve learned git really well, which I totally love now. I can rebase, cherry-pick, :/search, and reflog like the best of them. My git skills could be considered quite fetching among certain branches of society. While here I’ve also had the opportunity to really flesh out my dotfiles, especially my .gitconfig and my .profile (aka .bashrc). I also got a real feel for Dojo while I’ve been here, and I’d say that my next choice of javascript toolkit will be much better informed because of this. Code reviews kept my code quality up, helping me catch little things like comments that were no longer relevant. I have yet to master the art of spotting and veto-ing changes that will break what I’ve written. Maybe next year! Dev environment From the web developer’s point of view, there’s are a couple times when you’d normally need to do a couple extra alt-tabs and/or refreshes. Of course, I’m pampered because I’ve never had to build anything that goes into production in a massive and/or distributable way, as we do here at Cloudera with our management tools. Also, I’ve traditionally done my work with web-servers that don’t require one to compile source code. I found the following things quite annoying about the dev environment: static files, when changed, would not show their changes upon refresh .less files, an advanced and superior alternative to CSS, would not auto-recompile upon change A solution to the first one was found by a coworker. I fixed the second problem by writing a little node script that watches less files and recompiles when any of them change. While I was at it, I also made it easy for devs to use vogue, which reloads stylesheets whenever they’ve changed, without requiring a page refresh. This further improves development, as pages can get quite heavy when in development mode, where every javascript file is loaded individually, and it’s nice to have CSS changes automatically reflected in the UI. Thanks Cloudera! That’s about all I have on my mind when it comes to my internship. I learned a ton, enjoyed the free lunches a lot, as well as the 30 inch monitors. These things make a big difference, and also make me feel way cooler than some of my friends who don’t get these things. So long Cloudera and thanks for all the fish! Now I’m off to another planet for a year in the world of academia! David Trejo Software Engineer Intern Cloudera Summer ‘11 Enterprise Team Brown University Computer Science ‘13 Go to Cloudera Careers &gt;</snippet></document><document id="481"><title>Apache ZooKeeper 3.4.1 has been released</title><url>http://blog.cloudera.com/blog/2011/12/apache-zookeeper-3-4-1-has-been-released/</url><snippet>Apache ZooKeeper release 3.4.1 is now available: this is a fix release covering 7 issues, 2 of which were considered blockers. ZOOKEEPER-1319 is the most serious of the issues addressed, it could cause an inconsistent state in the cluster. If you are running 3.4.0 be sure to upgrade immediately: ZOOKEEPER-1319 Missing data after restarting+expanding a cluster Other issues ZOOKEEPER-1269 Multi deserialization issues ZOOKEEPER-1305 zookeeper.c:prepend_string func can dereference null ptr ZOOKEEPER-1311 ZooKeeper test jar is broken ZOOKEEPER-1315 zookeeper_init always reports sessionPasswd=&lt;hidden&gt; ZOOKEEPER-1316 zookeeper_init leaks memory if chroot is just ‘/’ ZOOKEEPER-1317 Possible segfault in zookeeper_init Stability, Compatibility and Testing It is important to note that 3.4.1 is not yet ready for production. It is an early release that users can start testing so that we can stabilize later 3.4.x releases.  We expect a later dot release to be production-ready soon, and will be incorporated into CDH4. Getting Involved The Apache ZooKeeper project is working on a number of new features. Our How To Contribute page is a great place to start if you’re interested in getting involved as a developer. You can also follow me on twitter. Acknowledgements A special thanks to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc).</snippet></document><document id="482"><title>Cloudera Manager 3.7 released</title><url>http://blog.cloudera.com/blog/2011/12/cloudera-manager-3-7-released/</url><snippet>Aparna Ramani is the Director of Engineering for Cloudera Enterprise. Cloudera Manager 3.7, a major new version of Cloudera’s Management applications for Apache Hadoop, is now available. Cloudera Manager Free Edition is a free download, and the Enterprise edition of Cloudera Manager is available as part of the Cloudera Enterprise subscription. Cloudera Manager 3.7 includes several new features and enhancements: Automated Hadoop Deployment – Cloudera Manager 3.7 allows you to install the complete Hadoop stack in minutes.  We’ve now upgraded Cloudera Manager with the easy installation we first introduced in version 3.6 of SCM Express. (SCM Express is now replaced by Cloudera Manager Free Edition.). Centralized Management UI - Version 3.5 of the Cloudera Management Suite included distinct modules for Resource Management, Activity Monitoring and Service and Configuration Management. In Cloudera Manager 3.7, all of these feature sets are now integrated into one centralized browser-based administration console. Service &amp; Configuration Management - We added several new configuration wizards to guide you in properly configuring HDFS and HBase host deployments, adding new hosts on demand, and adding/restarting services as needed. Cloudera Manager 3.7 now also manages Oozie and Hue. Service Monitoring – Cloudera Manager monitors the health of your key Hadoop services—HDFS, HBase, MapReduce—and displays alerts on suspicious or bad health. For example, to determine the health of HDFS, Cloudera Manager measures the percentage of corrupt, missing, or under-replicated blocks. Cloudera Manager also checks if the NameNode is swapping memory or spending too much time in Garbage Collection, and whether HDFS has enough free space. Trends in relevant metrics can be visualized through time-series charts. Log Search – You can search through all logs for Hadoop services across the whole cluster. You can also view results filtered by service, role, host, search phrase and log event severity. Events and Alerts – Cloudera Manager proactively reports on important events such as the change in a service’s health, detection of a log message of appropriate severity, or the slowness (or failure) of a job. Cloudera Manager aggregates the events for easy filtering and viewing, and you can configure Cloudera Manager to send email alerts. Global Time Control – You can view the state of your system for any time period in the past. Combined with health state, events and log information, this feature serves as a powerful diagnostic tool. Role-based Administration - Cloudera Manager 3.7 supports two types of users: admin users, who can change configs and execute commands and workflows; and read-only users, who can only monitor the system. Configuration versioning and Audit trails – You can view a complete history of configuration changes with user annotations. You can roll-back to previous configuration states. Activity Monitoring – The Activity Monitoring feature includes several performance and scale improvements. Operational Reports – The ‘Resource Manager’ feature in the Cloudera Management Suite 3.5 is now in Cloudera Manager’s ‘Reports’ feature. You can visualize disk usage by user, group, and directory; you can track MapReduce activity on the cluster by job, or by user. Support Integration – We’ve improved the Cloudera support experience by adding a feature that lets you send a snapshot of your cluster state to our support team for expedited resolution. Cloudera Manager Free Edition and 1-click Upgrade – The Free Edition of Cloudera Manager includes a subset of the features described above. After you install Cloudera Manager Free Edition, you can easily upgrade to the Enterprise edition by entering a license key. Your data will be preserved as the Cloudera Manager wizard guides you through the upgrade. You can download the new Cloudera Manager 3.7 at: https://ccp.cloudera.com/display/SUPPORT/Downloads . Check it out. We look forward to your feedback. P.S. : We’re hiring! Visit: http://www.cloudera.com/company/careers</snippet></document><document id="483"><title>Apache Flume – Architecture of Flume NG</title><url>http://blog.cloudera.com/blog/2011/12/apache-flume-architecture-of-flume-ng-2/</url><snippet>This blog was originally posted on the Apache Blog: https://blogs.apache.org/flume/entry/flume_ng_architecture Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large amounts of log data from many different sources to a centralized data store. Flume is currently undergoing incubation at The Apache Software Foundation. More information on this project can be found at http://incubator.apache.org/flume. Flume NG is work related to new major revision of Flume and is the subject of this post. Prior to entering the incubator, Flume saw incremental releases leading up to version 0.9.4. As Flume became adopted it became clear that certain design choices would need to be reworked in order to address problems reported in the field. The work necessary to make this change began a few months ago under the JIRA issue FLUME-728. This work currently resides on a separate branch by the name flume-728, and is informally referred to as Flume NG. At the time of writing this post Flume NG had gone through two internal milestones – NG Alpha 1, and NG Alpha 2 and a formal incubator release of Flume NG is in the works. At a high-level, Flume NG uses a single-hop message delivery guarantee semantics to provide end-to-end reliability for the system. To accomplish this, certain new concepts have been incorporated into its design, while certain other existing concepts have been either redefined, reused or dropped completely. In this blog post, I will describe the fundamental concepts incorporated in Flume NG and talk about it’s high-level architecture. This is a first in a series of blog posts by Flume team that will go into further details of it’s design and implementation. Core Concepts The purpose of Flume is to provide a distributed, reliable, and available system for efficiently collecting, aggregating and moving large amounts of log data from many different sources to a centralized data store. The architecture of Flume NG is based on a few concepts that together help achieve this objective. Some of these concepts have existed in the past implementation, but have changed drastically. Here is a summary of concepts that Flume NG introduces, redefines, or reuses from earlier implementation: Event: A byte payload with optional string headers that represent the unit of data that Flume can transport from it’s point of origination to it’s final destination. Flow: Movement of events from the point of origin to their final destination is considered a data flow, or simply flow. This is not a rigorous definition and is used only at a high level for description purposes. Client: An interface implementation that operates at the point of origin of events and delivers them to a Flume agent. Clients typically operate in the process space of the application they are consuming data from. For example, Flume Log4j Appender is a client. Agent: An independent process that hosts flume components such as sources, channels and sinks, and thus has the ability to receive, store and forward events to their next-hop destination. Source: An interface implementation that can consume events delivered to it via a specific mechanism. For example, an Avro source is a source implementation that can be used to receive Avro events from clients or other agents in the flow. When a source receives an event, it hands it over to one or more channels. Channel: A transient store for events, where events are delivered to the channel via sources operating within the agent. An event put in a channel stays in that channel until a sink removes it for further transport. An example of channel is the JDBC channel that uses a file-system backed embedded database to persist the events until they are removed by a sink. Channels play an important role in ensuring durability of the flows. Sink: An interface implementation that can remove events from a channel and transmit them to the next agent in the flow, or to the event’s final destination. Sinks that transmit the event to it’s final destination are also known as terminal sinks. The Flume HDFS sink is an example of a terminal sink. Whereas the Flume Avro sink is an example of a regular sink that can transmit messages to other agents that are running an Avro source. These concepts help in simplifying the architecture, implementation, configuration and deployment of Flume. Flow Pipeline A flow in Flume NG starts from the client. The client transmits the event to it’s next hop destination. This destination is an agent. More precisely, the destination is a source operating within the agent. The source receiving this event will then deliver it to one or more channels. The channels that receive the event are drained by one or more sinks operating within the same agent. If the sink is a regular sink, it will forward the event to it’s next-hop destination which will be another agent. If instead it is a terminal sink, it will forward the event to it’s final destination. Channels allow the decoupling of sources from sinks using the familiar producer-consumer model of data exchange. This allows sources and sinks to have different performance and runtime characteristics and yet be able to effectively use the physical resources available to the system. Figure 1 below shows how the various components interact with each other within a flow pipeline. Figure 1: Schematic showing logical components in a flow. The arrows represent the direction in which events travel across the system. This also illustrates how flows can fan-out by having one source write the event out to multiple channels. By configuring a source to deliver the event to more than one channel, flows can fan-out to more than one destination. This is illustrated in Figure 1 where the source within the operating Agent writes the event out to two channels – Channel 1 and Channel 2. Conversely, flows can be converged by having multiple sources operating within the same agent write to the same channel. A example of the physical layout of a converging flow is show in Figure 2 below. Figure 2: A simple converging flow on Flume NG. Reliability and Failure Handling Flume NG uses channel-based transactions to guarantee reliable message delivery. When a message moves from one agent to another, two transactions are started, one on the agent that delivers the event and the other on the agent that receives the event. In order for the sending agent to commit it’s transaction, it must receive success indication from the receiving agent. The receiving agent only returns a success indication if it’s own transaction commits properly first. This ensures guaranteed delivery semantics between the hops that the flow makes. Figure 3 below shows a sequence diagram that illustrates the relative scope and duration of the transactions operating within the two interacting agents. Figure 3: Transactional exchange of events between agents. This mechanism also forms the basis for failure handling in Flume NG. When a flow that passes through many different agents encounters a communication failure on any leg of the flow, the affected events start getting buffered at the last unaffected agent in the flow. If the failure is not resolved on time, this may lead to the failure of the last unaffected agent, which then would force the agent before it to start buffering the events. Eventually if the failure occurs when the client transmits the event to its first-hop destination, the failure will be reported back to the client which can then allow the application generating the events to take appropriate action. On the other hand, if the failure is resolved before the first-hop agent fails, the buffered events in various agents downstream will start draining towards their destination. Eventually the flow will be restored to its original characteristic throughput levels. Figure 4 below illustrates a scenario where a flow comprising of two intermediary agents between the client and the central store go through a transient failure. The failure occurs between agent 2 and the central store, resulting in the events getting buffered at the agent 2 itself. Once the failing link has been restored to normal, the buffered events drain out to the central store and the flow is restored to its original throughput characteristics. Figure 4: Failure handling in flows. In (a) the flow is normal and events can travel from the client to the central store. In (b) a communication failure occurs between Agent 2 and the event store resulting in events being buffered on Agent 2. In (c) the cause of failure was addressed and the flow was restored and any events buffered in Agent 2 were drained to the store. Wrapping up In this post I described the various concepts that are a part of Flume NG and its high-level architecture. This is first of a series of posts from the Flume team that will highlight the design and implementation of this system. In the meantime, if you need anymore information, please feel free to drop an email on the project’s user or developer lists, or alternatively file the appropriate JIRA issues. Your contribution in any form is welcome on the project. Links: Project Website: http://incubator.apache.org/flume/ Flume NG Getting Started Guide: https://cwiki.apache.org/confluence/display/FLUME/Getting+Started Mailing Lists: http://incubator.apache.org/flume/mail-lists.html Issue Tracking: https://issues.apache.org/jira/browse/FLUME IRC Channel: #flume on irc.freenode.net</snippet></document><document id="484"><title>Crunch for Dummies</title><url>http://blog.cloudera.com/blog/2011/12/crunch-for-dummies/</url><snippet>This guide is intended to be an introduction to Crunch. Introduction Crunch is used for processing data. Crunch builds on top of Apache Hadoop to provide a simpler interface for Java programmers to process data. In Crunch you create pipelines, not unlike Unix pipelines, such as the command below: grep "ERROR" log | sort | uniq -c
 Crunch pipelines consist of a series of functions you apply to the input data. Let’s say you have raw Apache HTTPD server logs and that you want to know the total amount of data downloaded by ip address. The raw input consists of lines like so: 96.7.4.14 - - [24/Apr/2011:04:20:11 -0400] "GET /cat.jpg HTTP/1.1" 200 12433 "http://google.com" "Firefox/3.6.16"
88.1.4.5 - - [24/Apr/2011:06:21:41 -0400] "GET /cat.jpg HTTP/1.1" 200 12433 "http://google.com" "Firefox/3.6.16"
96.7.4.14 - - [24/Apr/2011:07:33:59 -0400] "GET /dog.jpg HTTP/1.1" 200 42431 "http://google.com" "Firefox/3.6.16"
 The ip address is field one and the number of bytes transferred is field seven. The steps you would follow: Parse each input record outputting (emitting) field one and seven, e.g. 96.7.4.14 and 12433 for the first entry. Group all the (ip, response size) records by ip address. Foreach ip, sum up the response sizes. The flow of this would look as follows. Parse the input data, outputting ip address and response size: 96.7.4.14, 12433
88.1.4.5, 12433
96.7.4.14, 42431
 Group by ip: 88.1.4.5, [ 12433 ]
96.7.4.14, [ 42431, 12433 ]
 Calculate the sum: 88.1.4.5, 12433
96.7.4.14, 54864
 Apache Hadoop If the dataset was large enough such that we could not process it easily on a single server, we might consider Apache Hadoop. However, using Hadoop would require learning a complex process called MapReduce or a higher level language such as Apache Hive or Apache Pig. Crunch Using Crunch, a Java programmer with limited knowledge of Hadoop and MapReduce can utilize the Hadoop cluster. The program is written in pure Java and does not require the use of MapReduce specific constructs such as writing a Mapper, Reducer, or using Writable objects to wrap Java primitives. In Crunch a Pipeline object represents the list of functions (parsing, counting, etc) you would like to take on some input data. Crunch programs will generally proceed as follows: Create a Pipeline object Read input (e.g. text file) Execute various functions on input data Write result to some destination (e.g. text file) Example Let’s look at the code to execute the work we described above, calculating the total number of bytes transfered for a particular IP address. Creating the Pipeline object:  Pipeline pipeline = new MRPipeline(TotalBytesByIP.class, getConf());
 In the MRPipeline constructor we pass the class containing our code. We also pass the configuration object which was given to our class at runtime. Reading input:  PCollection&lt;String&gt; lines = pipeline.readTextFile(args[0]);
 There are a couple of things to note above. First, in Crunch all actions are delayed until we call pipeline.run() or pipeline.done(). Second, because all actions are delayed, we will not store the input data in a java.util.Collection but in a Crunch PCollection. There is support for converting a Crunch PCollection to a java.util.Collection. Processing of the PCollection will be done with functions which are a subclass of DoFn. The DoFn class requires you implement one method, process() which is the method which will be called for each line of the text file. Note the text file is represented by the PCollection at this point. The function below, based on a regular expression, parses each Apache HTTP log line, outputting (emitting) the ip address and the response size. // Input: 55.1.3.2  ...... 200 512 ....
// Output: (55.1.3.2, 512)
DoFn&lt;String, Pair&lt;String, Long&gt;&gt; extractIPResponseSize = new DoFn&lt;String, Pair&lt;String, Long&gt;&gt;() {
  transient Pattern pattern;
  public void initialize() {
    pattern = Pattern.compile(logRegex);
  }
  public void process(String line, Emitter&lt;Pair&lt;String, Long&gt;&gt; emitter) {
    Matcher matcher = pattern.matcher(line);
    if(matcher.matches()) {
      try {
        Long responseSize = Long.parseLong(matcher.group(7));
        String remoteAddr = matcher.group(1);
        emitter.emit(Pair.of(remoteAddr, responseSize));
      } catch (NumberFormatException e) {
        // corrupt line, we should increment a counter
      }
    }
  }
};
 Next we need to apply the extractIPResponseSize function to each of the lines in the input, represented by the PCollection lines:   lines.parallelDo(extractIPResponseSize
 Note however, that the above we have not assigned the results of the lines.parallelDo to another variable. In order to do this, we are required to inform the parallelDo method the type of data which will be returned. In our case, we want to group by ip address (key) and as such we will return a table (basically a map) of Strings (ip addresses) and longs (response sizes): PTable&lt;String, Long&gt; ipAddrResponseSize =
  lines.parallelDo(extractIPResponseSize,
    Writables.tableOf(Writables.strings(),Writables.longs()))
 The above code is nearly complete, but we still have not grouped the data by ip address (key) nor summed up the responses sizes. We need to create what is known as a combiner to do the sum: // Combiner used for summing up response size
CombineFn&lt;String, Long&gt; longSumCombiner = CombineFn.SUM_LONGS();
 Then execute the group by key (ip address) and sum operation: // Table of (ip, sum(response size))
PTable&lt;String, Long&gt; ipAddrResponseSize =
  lines.parallelDo(extractIPResponseSize,
    Writables.tableOf(Writables.strings(),Writables.longs()))
        .groupByKey()
        .combineValues(longSumCombiner);
 You will be tempted to try and iterate over ipAddrResponseSize in order to see the results. However, remember that everything in Crunch is delayed until pipeline.run() or pipeline.done() is called. Additionally, in this tutorial, we won’t cover processing the results of a pipeline in memory. We simply write the results to a file. Below we write the results to a text file and then execute the pipeline: pipeline.writeTextFile(ipAddrResponseSize, args[1]);
// Execute the pipeline as a MapReduce.
pipeline.done();
 If you have made it this far, congratulations! To start using Crunch, either: Look at the Crunch Examples Read the in depth Crunch README</snippet></document><document id="485"><title>FoneDoktor, A WibiData Application</title><url>http://blog.cloudera.com/blog/2011/12/fonedoktor-a-wibidata-application/</url><snippet>This guest blog post is from Alex Loddengaard, creator of FoneDoktor, an Android app that monitors phone usage and recommends performance and battery life improvements. FoneDoktor uses WibiData, a data platform built on Apache HBase from Cloudera’s Distribution including Apache Hadoop, to store and analyze Android usage data. In this post, Alex will discuss FoneDoktor’s implementation and discuss why WibiData was a good data solution. A version of this post originally appeared at the WibiData blog. At last month’s Hadoop World, one of the sessions spotlighted FoneDoktor, an Android app that collects data about device performance and app resource usage to offer personalized battery and performance improvement recommendations directly to users. In this post, I’ll talk about how I used WibiData — a system built on Apache HBase from CDH — as FoneDoktor’s primary data storage, access, and analysis system. WibiData is an integrated system for managing, analyzing and serving complex user data in support of investigative and operational analytic workloads. It leverages HBase to combine batch analysis and real time access within the same system, and integrates with existing BI, reporting and analysis tools. Having used Hadoop for over four years now, I was insanely impressed with the simplicity that WibiData brings to apps that need to store, access, and analyze massive amounts of user data. Read on for how I used it to build FoneDoktor. What is FoneDoktor? FoneDoktor is an Android app that monitors phone usage and recommends usage improvements to better Android phone performance and battery life. Usage information such as average screen brightness, average signal strength, wifi connectivity, power cycles, and more is collected throughout the day and sent to a WibiData cluster. Data is only sent when the phone is connected to a power source, to avoid using battery to send data upstream. Once FoneDoktor has been running for a few weeks on a phone, it starts analyzing the usage data and makes recommendations in the form of Android push notifications. A notification might suggest that you should turn on auto screen brightness, or start using wifi when your signal is low, if available of course. FoneDoktor has several more notification types, too. How Does FoneDoktor Work? A typical usage record, stored as an Apache Avro record, might look something like this: {"datetime": "2011-11-12 03:24:08.111", "seconds_on": 477,
 "avg_brightness": 255, "is_on_power": true, "is_on_wifi":
 true, "is_on_3g": true, "signal_strength": 7, "device_id":
 "ABCD1234", "is_auto_brightness": true} In this case this record is saying that the screen was at full brightness (255) for 477 seconds, connected to power, with wifi and 3g on, and a signal strength of 7. This particular record has a unique device ID, which is what’s used as the WibiData key. On any given day, FoneDoktor will collect about 100 records from each phone. These records are stored in a WibiData column. Each WibiData column stores a specific type of record. For example, one column exists for WiFi-specific records, another column for screen brightness records, etc. Since WibiData timestamps each record as it’s stored, every record is accessible by its key (device ID), column (record type), and timestamp (when the record was created). WibiData also makes it easy to scan both rows and values (by timestamp) in a particular column. Data Storage and Access with WibiData The write path (outlined in an architecture diagram in the conclusion section below) for a record starts at the phone. The record is cached on the phone if it’s not connected to power. Then, once the phone is connected to power, the record is sent upstream as JSON to a web server implemented in Python/Django. The web server creates an Avro record and sends a Thrift RPC to the WibiData data access server, which writes the record into WibiData. FoneDoktor’s read path is as straightforward as its write path. The phone periodically queries the web server to see if any new notifications or summary data is available. The web server fires off a Thrift RPC to the WibiData data access server, which queries WibiData and returns an Avro record and is serialized into JSON in Python/Django before being sent to FoneDoktor. WibiData is implemented on top of HBase, which means clients don’t need to worry about caches or indexes when reading and writing data. WibiData scales out of the box. User Analysis in WibiData Without WibiData, MapReduce would power FoneDoktor’s data analysis over a real-time storage system such as HBase. With WibiData, the analysis APIs are very obvious and far more simple than MapReduce. FoneDoktor has two different types of analysis. First, some analysis only looks at a single phone — for example to do battery calculation, summary information about usage, etc. This type of analysis is done in WibiData with producers. All other forms of analysis are done on the entire data set, looking at how all phones are used and creating correlations between usage and performance. This type of analysis is done by gatherers. Producers The producer API is dead simple. You specify which tables and columns your data comes from, where output will be written to in WibiData, and a method for processing a single row at a time. WibiData handles buffering, reading from HBase, writing back to HBase, and everything else. No MapReduce. No input/output complexity. In FoneDoktor’s case, a producer might look at the set of screen brightness records for a given phone and output an aggregate screen brightness average, which can be used for further analysis later. Gatherers The gatherer API is slightly more complex than the producer API, but it’s still far more simple than a traditional MapReduce job. Just like the producer API, you specify which data you want to read in, and where output should go. You then write a method for processing individual rows, where the output data of this method is used as input data in a reducer. The reducer is not a traditional MapReduce reducer, but it works very similar to one. It takes aggregated keys and their respective lists of values and outputs data to a WibiData cell. Again, there’s no need for ETL and complex input/output strategies. In FoneDoktor’s case, a gatherer might look at which devices perform worse than others, and dig into the usage data to learn why. For example, it may find that two users with the same device have drastically different battery life. It will then look at the usage differences and make suggestions to each respective user for improving their battery life. Conclusions – Why WibiData Almost every use of Hadoop requires a sibling real-time data storage and access solution (OLTP – online transaction processing) for serving a website, OLAP dashboard, mobile app, or any other real-time piece of software. In practice Hadoop works alongside data solutions such as MySQL, Vertica, Oracle, Teradata, HBase, and lots of other alternatives to power these real-time applications. Hadoop does batch, background processing and these other systems serve and store the real-time data. Without WibiData, FoneDoktor’s architecture would look something like this: Architecture Without WibiData Hadoop, during the four years I’ve worked with it, has started to play much more nicely with sibling real-time data storage solutions, especially with Cloudera’s partnerships and connectors. With WibiData, the process of using Hadoop alongside a real-time solution is easier than ever because it provides one solution for serving, storing and analyzing data. Analysis, just like in Hadoop, is done in batch, in the background. Serving and storage, like HBase, is done in real-time, at massive scale. The difference is WibiData comes with great abstractions to make using it much more simple than the alternative. Furthermore, WibiData comes with lots of built-in libraries to do most of the needed analysis work for you for complex machine learning and data mining work. With WibiData, FoneDoktor’s architecture looks like this: Architecture With WibiData Building FoneDoktor was lots of fun – it was my first Android app and my first usage of WibiData. I’m very impressed with WibiData for the reasons I’ve stated here. Android was great, too. Use WibiData if you’re tired of moving user data from system to system, to either analyze it or store and access it. WibiData just works, it scales, and most importantly, it’s simple in an otherwise complex technology stack.</snippet></document><document id="486"><title>Apache HBase Pow-wow Summary 11/29/2011</title><url>http://blog.cloudera.com/blog/2011/12/apache-hbase-pow-wow-summary-11292011/</url><snippet>San Francisco, Salesforce.com HQ - Recently there was an Apache HBase Pow-wow where project contributors gathered to discuss the directions of future releases of HBase in person.   This group included a quorum of the core committers from Facebook, StumbleUpon, Salesforce, eBay, and Cloudera as well as many contributors and users from other companies.  This was an open discussion, and in compliance with Apache Software Foundation policies, the agenda and  detailed minutes were shared with the community at large so that everyone can chime in before any final decisions are made. We summarize some of the high-level discussion topics: Stability/Testing What is HBase? (and what isn’t it) Releases, branching, and versioning Goals for operability Stability is a main focus for future releases and there was a bit of discussion about test tools and methods to ensure stability.  One point of discussion was the availability of Nicholas Keywal’s new testing categorization and testing speed improvements.  This was followed by approaches for integration-level testing — Roman Shaposhnik from Cloudera explained Apache Bigtop’s goals (a recent Bigtop presentation), and Mikhail Boutin presented a deck about the levels of testing done with patches internally at Facebook.  This later broke out into a “testing mini-summit” between some of the Facebook, Cloudera, and StumbleUpon folks focused on system and integration testing. The next major discussion was about what HBase is trying to be.  After some discussions, the consensus seemed to be simply: “HBase is super-fast reliable big data store”.  This in effect means a focus on tightening up the core and encouraging additions (specific coprocessors for example) to remain as separate projects. With a 0.92 release candidate just out, versions and branches for 0.94 was the next topic of discussion. The first half boiled down to a debate between time-based and feature-based releases. The consensus was to defer branching 0.94 immediately, to be stricter with trunk, to use feature branches for larger contributions, and to have a maximum elapsed time between releases so that there can be multiple release per year.  The precondition for branching 0.94 would be either the merging of a potentially destabilizing change or some minimum elapsed timed.  A follow-on discussion was about naming a release the 1.0 release.  We generally agreed that 1.0 would be reserved for when we achieved BigTable parity and at least a release or so after 0.94. Operability and training new HBase contributors and operators was the next focus. Today, it seems that most shops have folks in a developer role and a folks in an operational DBA-like role.  The goal of this thrust is to improve tools that enable someone with black-box knowledge to repair and operate the HBase system.  This means improving tools like hbck and to improve logging to provide more actionable messages. Next, Nicholas Spiegelberg from Facebook pitched Phabricator and Arc — tools for core developers that simplify the patch review and submission process. The scheduled discussions wrapped up with some wishful blue sky discussion, and then we broke out into smaller discussions accompanied with pizza and brews. Acknowledgements: Thanks for Lars Hofhansl, Chris Trezzo and the Salesforce.com crew for hosting!</snippet></document><document id="487"><title>Recommendation with Apache Mahout in CDH3</title><url>http://blog.cloudera.com/blog/2011/11/recommendation-with-apache-mahout-in-cdh3/</url><snippet>Intro The amount of information we are exposed to on a daily basis is far outstripping our ability to consume it, leaving many of us overwhelmed by the amount of new content available. Ideally we’d like machines and algorithms to help find the more interesting—to individual preference—items so we can easily focus our attention on these items of relevance. Have you ever been recommended a friend on Facebook? Or an item you might be interested in on Amazon? If so then you’ve benefitted from the value of recommendation systems. Recommendation systems apply knowledge discovery techniques to the problem of making recommendations that are personalized for each user. Recommendation systems are one way we can use algorithms to help us sort through the masses of information to find the “good stuff” in a very personalized way. Due to the explosion of web traffic and users the scale of recommendation poses new challenges for recommendation systems.  These systems face the dual challenge of producing high quality recommendations and calculating these personalized recommendations for millions of users. In recent years, collaborative filtering (CF) has become a popular way to effectively meet these challenges.  CF techniques start by analyzing the user-item matrix to identify relationships between different users or items, and use that information to produce recommendations for each user. Quick CF Recommendation Overview From an algorithmic standpoint, the recommendation systems we’ll talk about today are considered in the k-nearest neighbor family of problems (another type would be a SVD-based recommender). We want to predict the estimated preference of a user towards an item they have never seen before. We also want to generate a ranked (by preference score) list of items the user might be most interested in. Two well-known styles of recommendation algorithms are item-based recommenders and user-based recommenders. Both types rely on the concept of a similarity function/metric (ex: Euclidean distance, log likelihood), whether it is for users or items. With user-based recommendation algorithms we’re looking at the notion of similarity between users based on preferences/actions/ratings of those users so we can recommend the same things to similar users. We start out by finding out which users are more similar (as opposed to finding more similar items first). With item-based CF we’re looking at how users interact with items (books, videos, news, users …) and from that information we look at which items are more similar. In both types of CF we start with deriving some idea of the user-item preferences and then either look at user-user or item-item similarity.  We get these interactions from explicit (e.g.  a user rating something by number of stars) and implicit (e.g. purchasing a book) ratings. Given the past actions of a user, the then attempt to predict the future in a “content agnostic” fashion that doesn’t use  the properties of the items themselves, just how user’s interact with the items. Typically we’re interested in item-based recommendation systems when the number of items is very low compared to the number of users. This scenario can provide a good performance gain versus a user-based setup. Another aspect to consider is that the items are going to change far less frequently than the users. Mathematically speaking we’re creating a user-item-matrix from the user preference data and then predicting the missing entries by finding patterns from the user preference information we know about. In the most simple case, we’re computing the number of times each pair of items occurs together in a user’s list of preferences/ratings and given that grouping (for a given pair) across all users we then feed these “co-occurrences” to a similarity function like “log likelihood”. Mahout’s implementation of this algorithm is also a great example of how an existing concept is rebuilt for MapReduce. A difference to note with the MapReduce version is that Mahout uses a “Co-Occurrence” matrix for the concept of “item similarity”, which provides some degree of similarity between items (the idea being that the more two items co-occur, the more similar they likely are). This means we are calculating the number of times a pair of items occurs together per user. With this job we’re able to calculate a lot of co-occurrences in parallel which highlights the power of MapReduce and the “out of the box” functionality offered with Mahout. Once we know a bit about how similar our items are (here, how often they co-occur) we can then predict our user’s preference towards a small subset of other items the user has not seen before. We want to predict a rating for an item a user has not seen before based on the information gathered from other users . To do this we look at all items that are similar to an item we’d like to predict a rating on (for the user) and have been rated by this user. In the case of Mahout’s distributed recommender we multiply the user vector (as column vector) vs the co-occurrence matrix. This means we multiply the user column vector times each item row vector. The sum of each of those row time column multiplications creates a rating for each item relative to the user. We can then select the top N most highly rated items (minus the items the user has already rated) to recommend to the user. Install Mahout with CDH3u3 Mahout has been integrated with CDH3 since CDH3u2. Let’s start out by getting a cluster going in either Pseudo-Distributed Mode or on a full cluster (If CDH3u2 is already running, please skip this part, otherwise go ahead and install CDH3u3). Click following link to get started: CDH3 Quick Start Guide Make sure $JAVA_HOME is set or Mahout will complain. Click the link to learn about Mahout Installation. Now that we have Mahout installed, let’s take a look at how we interact with it from the command line. Working with Mahout Running Mahout is fairly simple and acts a lot like Hadoop from the command line. For instance, if we type “mahout” on the command line after the install we should get: mahout
 Running on hadoop, using HADOOP_HOME=/usr/lib/hadoop-0.20
 HADOOP_CONF_DIR=/etc/hadoop-0.20/conf
 MAHOUT-JOB: /usr/lib/mahout/mahout-examples-0.5-cdh3u2-SNAPSHOT-job.jar
 An example program must be given as the first argument.
 Valid program names are:
 arff.vector: : Generate Vectors from an ARFF file or directory
 canopy: : Canopy clustering
 cat: : Print a file or resource as the logistic regression models would see it
 cleansvd: : Cleanup and verification of SVD output
 clusterdump: : Dump cluster output to text
 dirichlet: : Dirichlet Clustering
 eigencuts: : Eigencuts spectral clustering
 evaluateFactorization: : compute RMSE of a rating matrix
 factorization against probes in memory
 evaluateFactorizationParallel: : compute RMSE of a rating matrix
 factorization against probes
 fkmeans: : Fuzzy K-means clustering
 fpg: : Frequent Pattern Growth
 itemsimilarity: : Compute the item-item-similarities for item-based
 collaborative filtering
 kmeans: : K-means clustering
 lda: : Latent Dirchlet Allocation
 ldatopics: : LDA Print Topics
 lucene.vector: : Generate Vectors from a Lucene index
 matrixmult: : Take the product of two matrices
 meanshift: : Mean Shift clustering
 parallelALS: : ALS-WR factorization of a rating matrix
 predictFromFactorization: : predict preferences from a factorization
 of a rating matrix
 prepare20newsgroups: : Reformat 20 newsgroups data
 recommenditembased: : Compute recommendations using item-based
 collaborative filtering
 rowid: : Map SequenceFile to
 {SequenceFile,
 SequenceFile}
 rowsimilarity: : Compute the pairwise similarities of the rows of a matrix
 runlogistic: : Run a logistic regression model against CSV data
 seq2sparse: : Sparse Vector generation from Text sequence files
 seqdirectory: : Generate sequence files (of Text) from a directory
 seqdumper: : Generic Sequence File dumper
 seqwiki: : Wikipedia xml dump to sequence file
 spectralkmeans: : Spectral k-means clustering
 splitDataset: : split a rating dataset into training and probe parts
 ssvd: : Stochastic SVD
 svd: : Lanczos Singular Value Decomposition
 testclassifier: : Test Bayes Classifier
 trainclassifier: : Train Bayes Classifier
 trainlogistic: : Train a logistic regression using stochastic gradient descent
 transpose: : Take the transpose of a matrix
 vectordump: : Dump vectors from a sequence file to text
 wikipediaDataSetCreator: : Splits data set of wikipedia wrt feature
 like country
 wikipediaXMLSplitter: : Reads wikipedia data and creates ch
 If your JAVA_HOME env variable is set correctly you should see the above output. Now that we have Mahout installed and working with CDH let’s move on to working with some data. GroupLens Movie Data The input data for this demo is based on 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users, which you can download from the www.grouplens.org site. The zip file contains four files: movies.dat (movie ids with title and category) ratings.dat (ratings of movies) README users.dat (user information) Ratings File Description The ratings file is most interesting to us since it’s the main input to our recommendation job. Each line has the format: UserID::MovieID::Rating::Timestamp where UsersIDs are integers MovieIDs are integers Ratings are 1 through 5 “stars” (integers) Time stamp is seconds since the epoch Each user has at least 20 ratings This file isn’t exactly how Mahout prefers, but this is an easy fix. Mahout is looking for a CSV file with lines of the form: userID, itemID, value So let’s adjust our input file to match what we need to run our job. First download the file and unzip it locally from: http://www.grouplens.org/system/files/ml-1m.zip Next we’ll run the command: tr -s ':' ',' &lt; ratings.dat | cut -f1-3 -d, &gt; ratings.csv This produces the output format we’ll use in the next section when we run our “Itembased Collaborative Filtering” job. While this is an extra step it gives us good practice preparing data for input into our recommendation jobs. Running Mahout’s Recommendation System The core workflow in mahout is based around the class org.apache.mahout.cf.taste.hadoop.item.RecommenderJob and is a completely distributed item-based recommender. The input to this job is going to be the “ratings.csv” file we generated above (after we converted the ratings.dat file to the ratings.csv file). The format of this file looks like: userID, itemID, value and the output will be another CSV file with the layout of: userID [ itemID, score, ... ] which represents the userIDs with their recommended itemIDs along with the preference scores. This algorithm is also implemented in MapReduce and the following table has some of the options we can specify for the job: Flag Description --input (-i) Path to the job input directory in HDFS (-i shortcut flag is broken in 0.5, MAHOUT-842) --output (-o) Path to the output dir in HDFS --usersFile File listing users to generate recommendations for --tempDir Intermediary output directory To set things up we need to setup the list of users we want to produce a recommendation for.  Put the number “6040″ (a user ID in our source data) on a line by itself in a file and put that in HDFS. Use the hadoop command line tools to copy this file to HDFS: hadoop fs -put [my_local_file] [user_file_location_in_hdfs] With our user list in hdfs we can now run the Mahout  recommendation job with a command in the form of: mahout recommenditembased --input [input-hdfs-path] --output [output-hdfs-path] --tempDir [tmp-hdfs-path] --usersFile [user_file_location_in_hdfs] which will run for a while (a chain of 10 MapReduce jobs) and then write out the item recommendations into HDFS we can now take a look at.  If we tail the output from the RecommenderJob with the command: hadoop fs -cat [output-hdfs-path]/part-r-00000 it should look like: 6040     [1941:5.0,1904:5.0,2859:5.0,3811:5.0,3814:5.0,14:5.0,17:5.0,3795:5.0,3794:5.0,3793:5.0] where each line represents a userID with associated recommended itemIDs and their scores. Conclusion This concludes the introduction to Mahout’s implementation of Collaborative Filtering which is now included in CDH3u2. CF is but one of the many techniques included in the Mahout project and the reader should now be ready to not only further explore CF but tackle some of the other techniques as well. Special thanks to Sean Owen for review help on this article. References S. Owen, R. Anil, T. Dunning, E. Friedman: Mahout in Action Sarwar et al.: Item-Based Collaborative Filtering Recommendation Algorithms Apache Mahout Wiki: http://mahout.apache.org/</snippet></document><document id="488"><title>Apache ZooKeeper 3.3.4 has been released</title><url>http://blog.cloudera.com/blog/2011/11/apache-zookeeper-3-3-4-has-been-released/</url><snippet>Apache ZooKeeper release 3.3.4 is now available: this is a fix release covering 22 issues, 9 of which were considered blockers. Some of the more serious issues include: ZOOKEEPER-1208 Ephemeral nodes may not be removed after the client session is invalidated ZOOKEEPER-961 Watch recovery fails after disconnection when using chroot connection option ZOOKEEPER-1049 Session expire/close flooding renders heartbeats to delay significantly ZOOKEEPER-1156 Log truncation truncating log too much – can cause data loss ZOOKEEPER-1046 Creating a new sequential node incorrectly results in a ZNODEEXISTS error ZOOKEEPER-1097 Quota is not correctly rehydrated on snapshot reload ZOOKEEPER-1117 zookeeper 3.3.3 fails to build with gcc &gt;= 4.6.1 on Debian/Ubuntu Stability, Compatibility and Testing 3.3.4 is a stable release that’s fully backward compatible with 3.3.3. Only bug fixes relative to 3.3.3 have been applied. Version 3.3.4 will be incorporated into the upcoming CDH3U3 release. Note that these changes are included in the recent 3.4.0 release which I detailed earlier. Getting Involved The Apache ZooKeeper project is working on a number of new features. Our How To Contribute page is a great place to start if you’re interested in getting involved as a developer. You can also follow me on twitter. Acknowledgements A special thanks to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc).</snippet></document><document id="489"><title>Apache ZooKeeper 3.4.0 has been released</title><url>http://blog.cloudera.com/blog/2011/11/apache-zookeeper-3-4-0-has-been-released/</url><snippet>Apache ZooKeeper release 3.4.0 is now available: it includes changes covering over 150 issues, 27 of which were considered blockers. ZooKeeper 3.3.3 clients are compatible with 3.4.0 servers, enabling a seamless upgrade path (3.4.0 clients with 3.3.3 servers has also been tested successfully). In addition to improving overall stability some of the highlights are described below: Things of interest to developers implementing ZooKeeper clients: Native Windows version of C client Support Kerberos authentication of clients Multi-update client API Improved REST Interface The native windows support allows visual studio users to now compile a native client. This will increase ZooKeeper use substantially on that platform, in particular enabling client bindings other than just Java (on Unix the c binding is used to provide a number of bindings to various languages such as python, perl, ruby, go, etc…). Kerberos authentication was initially developed in ZooKeeper to support the efforts going on in HBase to enable security for those users. However Kerberos client based authentication is now available to anyone using ZooKeeper. Multi-update is one of the only changes in recent history to extend the ZooKeeper client interface. It allows for multiple operations to be “batched” together as a single atomic operation that either succeeds or fails in its entirety. This feature will greatly simplify implementation of certain domain specific (i.e. client side) business logic. Improved Operational support: Operations loves ZooKeeper because of its resilience to failure and self-recovery features, here are some of the new features available in 3.4.0: Existing monitoring support has been extended through the introduction of a new ‘mntr’ 4 letter word Add tools and recipes for monitoring as a contrib Web-based Administrative Interface Automating log and snapshot cleaning Add logging/stats to identify production deployment issues Support for building RPM and DEB packages A number of contributions were accepted around enabling improved monitoring of the system. In particular the new monitoring 4 letter word and the Ganglia/Nagios integration that now ship with the release will go a long way in helping operations support ZooKeeper in production. Stability, Compatibility and Testing It is important to note that 3.4.0 is not yet ready for production. It is an early release that users can start testing so that we can stabilize later 3.4.x releases.  We expect a later dot release to be production-ready soon, and will be incorporated into CDH4. Getting Involved The Apache ZooKeeper project is working on a number of new features. Our How To Contribute page is a great place to start if you’re interested in getting involved as a developer. You can also follow me on twitter. Acknowledgements A special thanks to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc), and big thank you to our amazing release manager Mahadev Konar.</snippet></document><document id="490"><title>Inaugural Sqoop Meetup</title><url>http://blog.cloudera.com/blog/2011/11/inaugural-sqoop-meetup/</url><snippet>This blog was originally posted on the Apache Blog:  https://blogs.apache.org/sqoop/entry/inaugural_sqoop_meetup Over 30 people attended the inaugural Sqoop Meetup on the eve of Hadoop World in NYC. Faces were put to names, troubleshooting tips were swapped, and stories were topped – with the table-to-end-all-tables weighing in at 28 billion rows. I started off the scheduled talks by discussing “Habits of Effective Sqoop Users.” One tip to make your next debugging session more effective was to provide more information up front on the mailing list such as versions used and running with the –verbose flag enabled. Also, I pointed out workarounds to common MySQL and Oracle errors. Next up was Eric Hernandez’s “Sqooping 50 Million Rows a Day from MySQL,” where he displayed battle scars from creating a single data source for analysts to mine. Key lessons learned were: (1.) Develop an incremental import when sqooping in large active tables. (2.) Limit the amount of parts that data will be stored in HDFS. (3.) Compress data in HDFS. The final talk of the night was given by Joey Echeverria on “Scratching Your Own Itch.” Joey methodically stepped future Sqoop committers through the science from finding a Sqoop bug, filing a jira, coding a patch, submitting it for review, revising accordingly, and finally to ship it ‘+1′ approval. With the conclusion of the scheduled talks, the hallway talks commenced and went well into the night. Sqoop Committer Aaron Kimball was even rumored to have shed a tear over the healthy turnout and impending momentum barreling towards the next Sqoop Meetup on the Left Coast. See you there! Photos from Masatake Iwasaki and Kate Ting.</snippet></document><document id="491"><title>Coming Attractions: Apache Hive 0.8.0</title><url>http://blog.cloudera.com/blog/2011/11/coming-attractions-apache-hive-0-8-0/</url><snippet>The Apache Hive team is hard at work putting the finishing touches on the 0.8.0 release. While the release hasn’t reached the GA milestone yet, I think now would be a good time to start highlighting some of the new features and improvements that users can expect to find in this important update: Bitmap Indexes The infrastructure required to support table indexes was originally added in the 0.7.0 release, but at the time no viable indexing plugin was provided. Project contributors have remedied this situation in the 0.8.0 release with the inclusion of support for bitmap indexes. This is a very important addition to Hive since it promises to significantly increase the performance of queries on indexed tables. More information about Hive Table Indexes can be found in the original design document, as well as in the comments that accompany the Bitmap Index JIRA ticket. TIMESTAMP datatype In response to frequent requests from users, Hive 0.8.0 will include support for the SQL TIMESTAMP datatype. We anticipate that this addition will make it much easier to integrate third-party ETL and BI tools with Hive. More information about the TIMESTAMP type can be found in the original JIRA ticket as well as in the Hive Language Manual. Plugin Developer Kit From the start, extensibility has been one of the key design goals of Hive; and the project has consistently delivered on this goal by providing a rich variety of extension points including User Defined Functions (UDFs), Serialization/Deserialization libraries (SerDes), StorageHandlers, and IndexHandlers. Up to this point one of the big inconveniences facing extension writers has been the requirement that they have access to a complete Hive source build. The new Hive Plugin Developer Kit seeks to relax this requirement by allowing developers to build and test extensions directly against a specific binary release of Hive. Currently the PDK is targeted only at UDFs, but there are plans to eventually extend this to support the other extension points including SerDes and StorageHandlers. More information about the Plugin Developer Kit can be found on the PDK page on the Hive Wiki. JDBC Driver Improvements The 0.8.0 release will also include several significant bug fixes and enhancements for the Hive JDBC driver. This module has received a lot of interest from the Hive user community due to the critical role it plays in enabling integrations between Hive and third-party ETL and BI tools. I hope this quick overview has increased your interest in the next release of Hive. We expect the GA release of Hive 0.8.0 to drop sometime in the next couple of weeks, and look forward to getting feedback from the community about the new features described above.</snippet></document><document id="492"><title>Using Apache Hadoop to Find Signal in the Noise: Analyzing Adverse Drug Events</title><url>http://blog.cloudera.com/blog/2011/11/using-hadoop-to-analyze-adverse-drug-events/</url><snippet>Last month at the Web 2.0 Summit in San Francisco, Cloudera CEO Mike Olson presented some work the Cloudera Data Science Team did to analyze adverse drug events. We decided to share more detail about this project because it demonstrates how to use a variety of open-source tools – R, Gephi, and Cloudera’s Distribution Including Apache Hadoop (CDH) – to solve an old problem in a new way. Background: Adverse Drug Events An adverse drug event (ADE) is an unwanted or unintended reaction that results from the normal use of one or more medications. The consequences of ADEs range from mild allergic reactions to death, with one study estimating that 9.7% of adverse drug events lead to permanent disability. Another study showed that each patient who experiences an ADE remains hospitalized for an additional 1-5 days and costs the hospital up to $9,000. Some adverse drug events are caused by drug interactions, where two or more prescription or over-the-counter (OTC) drugs taken together leads to an unexpected outcome. As the population ages and more patients are treated for multiple health conditions, the risk of ADEs from drug interactions increases. In the United States, roughly 4% of adults older than 55 are at risk for a major drug interaction. Because clinical trials study a relatively small number of patients, both regulatory agencies and pharmaceutical companies maintain databases in order to track adverse events that occur after drugs have been approved for market. In the United States, the FDA uses the Adverse Event Reporting System (AERS), where healthcare professionals and consumers may report the details of ADEs they experienced.  The FDA makes a well-formatted sample of the reports available for download from their website, to the benefit of data scientists everywhere. Methodology Identifying ADEs is primarily a signal detection problem: we have a collection of events, where each event has multiple attributes (in this case, the drugs the patient was taking) and multiple outcomes (the adverse reactions that the patient experienced), and we would like to understand how the attributes correlate with the outcomes. One simple technique for analyzing these relationships is a 2×2 contingency table: For All Drugs/Reactions: Reaction = Rj Reaction != Rj Total Drug = Di A B A + B Drug != Di C D C + D Total A + C B + D A + B + C + D   Based on the values in the cells of the tables, we can compute various measures of disproportionality to find drug-reaction pairs that occur more frequently than we would expect if they were independent. For this project, we analyzed interactions involving multiple drugs, using a generalization of the contingency table method that is described in the paper, “Empirical bayes screening for multi-item associations” by DuMouchel and Pregibon. Their model computes a Multi-Item Gamma-Poisson Shrinkage (MGPS) estimator for each combination of drugs and outcomes, and gives us a statistically sound measure of disproportionality even if we only have a handful of observations for a particular combination of drugs. The MGPS model has been used for a variety of signal detection problems across multiple industries, such as identifying fraudulent phone calls, performing market basket analyses and analyzing defects in automobiles. Solving the Hard Problem with Apache Hadoop At first glance, it doesn’t seem like we would need anything beyond a laptop to analyze ADEs, since the FDA only receives about one million reports a year. But when we begin to examine these reports, we discover a problem that is similar to what happens when we attempt to teach computers to play chess: a combinatorial explosion in the number of possible drug interactions we must consider. Even restricting ourselves to analyzing pairs of drugs, there are more than 3 trillion potential drug-drug-reaction triples in the AERS dataset, and tens of millions of triples that we actually see in the data. Even including the iterative Expectation Maximization algorithm that we use to fit the MGPS model, the total runtime of our analysis is dominated by the process of counting how often the various interactions occur. The good news is that MapReduce running on a Hadoop cluster is ideal for this problem. By creating a pipeline of MapReduce jobs to clean, aggregate, and join our data, we can parallelize the counting problem across multiple machines to achieve a linear speedup in our overall runtime. The faster runtime for each individual analysis allows us to iterate rapidly on smaller models and tackle larger problems involving more drug interactions than anyone has ever looked at before. Visualizing the Results The output of our analysis is a collection of drug-drug-reaction triples that have very large disproportionality scores. But as we all know, correlation is not causation. The output of our analysis provides us with useful information that should be filtered and evaluated by domain experts and used as the basis for further study using controlled experiments. With that caveat in mind, our analysis revealed a few drug pairs with surprisingly high correlations with adverse events that did not show up in a search of the academic literature: gabapentin (a seizure medication) taken in conjunction with hydrocodone/paracetamol was correlated with memory impairment, and haloperidol in conjunction with lorazepam was correlated with the patient entering into a coma. Even with restrictive filters applied to the drug-drug-reaction triples, we still end up with tens of thousands of triples that score high enough to merit further investigation. In addition to looking at individual triples, we can also use graph visualization tools like Gephi to explore the macro-level structure of the data. Gephi has a number of powerful layout algorithms and filtering tools that allow us to impose structure on an undifferentiated mass of data points. Here is a graph in which the vertices are drugs and the thickness of the edges represent the number of high scoring adverse reactions that feature each pair of drugs: We can also pan and zoom to different regions of the graph and highlight clusters of drug interactions. Here is a cluster of drugs that are used in treating HIV: And here is a cluster of drugs that are used to fight cancer: The combination of Apache Hadoop, R, and Gephi changes the way we think about analyzing adverse drug events. Instead of focusing on a handful of outcomes, we can process all of the events in the data set at the same time. We can try out hundreds of different strategies for cleaning records, stratifying observations into clusters, and scoring drug-reaction tuples, run everything in parallel, and analyze the data at a fraction of the cost of a traditional supercomputer. We can render the results of our analyses using visualization tools that can be used by domain experts to explore relationships within our data that they might never have thought to look for. By dramatically reducing the costs of exploration and experimentation, we foster an environment that enables innovation and discovery. Open Data, Open Analysis This project was possible because the FDA’s Center for Drug Evaluation and Research makes a portion of their data open and available to anyone who wants to download it. In turn, we are releasing a well-commented version of the code we used to analyze that data – a mixture of Java, Pig, R, and Python – on the Cloudera github repository under the Apache License. We also contributed the most useful Pig function developed for this project, which computes approximate quantiles for a stream of numbers, to LinkedIn’s datafu library. We hope to collaborate with the community to improve the models over time and create a resource for students, researchers, and fellow data scientists.</snippet></document><document id="493"><title>Hadoop World 2011 Final Remarks</title><url>http://blog.cloudera.com/blog/2011/11/hadoop-world-2011-final-remarks/</url><snippet>The third annual Hadoop World conference has come and gone. The two days of conference keynotes and sessions were surrounded by receptions, meetups and training classes, and marked by plenty of time for networking in hallways and the exhibit hall.  The energy exhibited at the conference was infectious and exchange of ideas outstanding. Nearly 1,500 people – almost double last year’s number – attended. They came from 580 companies, 27 countries and 40 of the United States.  Big data is clearly a global phenomenon. I want to extend a sincere “thank you” to all the Hadoop World sponsors and presenters. As an important part of the Apache Hadoop ecosystem, you continue to drive innovation and adoption in this space.  Your support and sharing of your knowledge helped to make this conference the most comprehensive forum for Apache Hadoop. We know that Hadoop World can be quite the data overload. The conference was packed with a tremendous amount of valuable information and insight. With 60 breakout sessions, many of you are interested in viewing the presentations you could not attend. We are working as quickly as possible to get the presentations online and available.  Slide decks from the presentations have been posted and video recordings will be available shortly. Please visit the Hadoop World 2011 agenda where you will find links to the session slide decks. Once again, thank you for your interest and participation in Hadoop World 2011. We look forward to seeing you in 2012*! *Dates and location to be determined.</snippet></document><document id="494"><title>Building and Deploying MR2</title><url>http://blog.cloudera.com/blog/2011/11/building-and-deploying-mr2/</url><snippet>A number of architectural changes have been added to Hadoop MapReduce. The new MapReduce system is called MR2 (AKA MR.next). The first release version to include these changes will be Apache Hadoop 0.23. A key change in the new architecture is the disappearance of the centralized JobTracker service. Previously, the JobTracker was responsible for provisioning the resources across the whole cluster, in addition to managing the life cycle of all submitted MapReduce applications; this typically included starting, monitoring and retrying the applications individual tasks. Throughout the years and from a practical perspective, the Hadoop community has acknowledged the problems that inherently exist in this functionally aggregated design (See MAPREDUCE-279). In MR2, the JobTracker aggregated functionality is separated across two new components: Central Resource Manager (RM): Management of resources in the cluster. Application Master (AM): Management of the life cycle of an application and its tasks. Think of the AM as a per-application JobTracker. The new design enables scaling Hadoop to run on much larger clusters, in addition to the ability to run non-mapreduce applications on the same Hadoop cluster. For more architecture details, the interested reader may refer to the design document at: https://issues.apache.org/jira/secure/attachment/12486023/MapReduce_NextGen_Architecture.pdf. The objective of this blog is to outline the steps for building, configuring, deploying and running a single-node NextGen MR cluster. In the following steps, I’ve chosen to use ~/mr2 as my working directory. Inside this directory, we’ll create a source directory for the code we’ll soon checkout, and a deploy directory for our deployment. ahmed@ubuntu:~$ cd ~/mr2
ahmed@ubuntu:~/mr2$ mkdir source
ahmed@ubuntu:~/mr2$ mkdir deploy Make sure protbuf is in your library path or: ahmed@ubuntu:~/mr2$ export LD_LIBRARY_PATH=/usr/local/lib We’ll now checkout the source code from the apache git repository. ahmed@ubuntu:~/mr2$ cd source
ahmed@ubuntu:~/mr2/source$ git clone git://git.apache.org/hadoop-common.git
Cloning into hadoop-common...
.
.
ahmed@ubuntu:~/mr2/source$ cd hadoop-common/
ahmed@ubuntu:~/mr2/source/hadoop-common$ git branch
* trunk Create the deployment tar files. ahmed@ubuntu:~/mr2/source/hadoop-common$ mvn package -Pdist -Dtar -DskipTests Copy the created Hadoop tar file to our deploy directory and Untar it. ahmed@ubuntu:~/mr2/source/hadoop-common$ cp ./hadoop-dist/target/hadoop-0.24.0-SNAPSHOT.tar.gz ../../deploy/.
ahmed@ubuntu:~/mr2/deploy$ tar -xzvf hadoop-0.24.0-SNAPSHOT.tar.gz
ahmed@ubuntu:~/mr2/deploy$ cd hadoop-0.24.0-SNAPSHOT/ Export some needed environment variables. See the following listing: #!/bin/bash
export HADOOP_DEV_HOME=`pwd`
export HADOOP_MAPRED_HOME=${HADOOP_DEV_HOME}
export HADOOP_COMMON_HOME=${HADOOP_DEV_HOME}
export HADOOP_HDFS_HOME=${HADOOP_DEV_HOME}
export YARN_HOME=${HADOOP_DEV_HOME}
export HADOOP_CONF_DIR=${HADOOP_DEV_HOME}/conf/
export YARN_CONF_DIR=${HADOOP_DEV_HOME}/conf/ We’ll start our configuration; the configuration directory will contain the following files: core-site.xml, hdfs-site.xml, mapred-site.xml, slaves,� yarn-env.sh and yarn-site.xml. Make sure the contents of the xml files in the conf directory are as follows: ahmed@ubuntu:~/mr2/deploy/ hadoop-0.24.0-SNAPSHOT/conf$ cat yarn-site.xml 1 2 3 4 5 6 7 8 9 10 11 12 &lt;?xml version=”1.0″?&gt; &lt;configuration&gt; &lt;!– Site specific YARN configuration properties –&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce.shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; ahmed@ubuntu:~/mr2/deploy/hadoop-0.24.0-SNAPSHOT/conf$ cat core-site.xml 1 2 3 4 5 6 7 8 &lt;?xml version=”1.0″?&gt; &lt;?xml-stylesheet href=”configuration.xsl”?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; ahmed@ubuntu:~/mr2/deploy/hadoop-0.24.0-SNAPSHOT/conf$ cat mapred-site.xml 1 2 3 4 5 6 7 8 &lt;?xml version=”1.0″?&gt; &lt;?xml-stylesheet href=”configuration.xsl”?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt; mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; ahmed@ubuntu:~/mr2/deploy/hadoop-0.24.0-SNAPSHOT/conf$ cat hdfs-site.xml 1 2 3 4 5 6 7 8 9 10 11 12 &lt;?xml version=”1.0″?&gt; &lt;?xml-stylesheet href=”configuration.xsl”?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; Now we can format our HDFS namenode as usual: ahmed@ubuntu:~/mr2/deploy/hadoop-0.24.0-SNAPSHOT$ bin/hadoop namenode -format And start the HDFS services: ahmed@ubuntu:~/mr2/deploy/hadoop-0.24.0-SNAPSHOT$ sbin/hadoop-daemon.sh start namenode
ahmed@ubuntu:~/mr2/deploy/hadoop-0.24.0-SNAPSHOT$ sbin/hadoop-daemon.sh start datanode And the new MR2 services ahmed@ubuntu:~/mr2/deploy/hadoop-0.24.0-SNAPSHOT$ sbin/yarn-daemon.sh start resourcemanager
ahmed@ubuntu:~/mr2/deploy/hadoop-0.24.0-SNAPSHOT$ sbin/yarn-daemon.sh start nodemanager
ahmed@ubuntu:~/mr2/deploy/hadoop-0.24.0-SNAPSHOT$ sbin/mr-jobhistory-daemon.sh start historyserver Make sure all needed services are running: ahmed@ubuntu:~/mr2/deploy/hadoop-0.24.0-SNAPSHOT$ jps
7623 Jps
7433 ResourceManager
7587 JobHistoryServer
7495 NodeManager
7325 DataNode
7250 NameNode You can see the resource manager web console (shown below) using this address: http://localhost:8088 Our usual namenode web console should be also up: We can now start running some example jobs,�here is how we submit our first job; the randomwriter from our examples jar: ahmed@ubuntu:~/mr2/source/hadoop-common/hadoop-mapreduce-project$ cd $HADOOP_MAPRED_HOME
ahmed@ubuntu:~/mr2/deploy/hadoop-0.24.0-SNAPSHOT$ $HADOOP_COMMON_HOME/bin/hadoop jar $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-0.24.0-SNAPSHOT.jar randomwriter -Dmapreduce.randomwriter.bytespermap=10000 -Ddfs.blocksize=536870912 -Ddfs.block.size=536870912 output If everything goes fine, you’ll see the job console output and finally: 2011-09-30 14:47:47,688 INFO� mapreduce.Job (Job.java:monitorAndPrintJob(1245)) - Counters: 28
File System Counters
FILE: BYTES_READ=1200
FILE: BYTES_WRITTEN=437730
FILE: READ_OPS=0
FILE: LARGE_READ_OPS=0
FILE: WRITE_OPS=0
HDFS: BYTES_READ=1180
HDFS: BYTES_WRITTEN=150089
HDFS: READ_OPS=70
HDFS: LARGE_READ_OPS=0
HDFS: WRITE_OPS=40
org.apache.hadoop.mapreduce.JobCounter
TOTAL_LAUNCHED_MAPS=10
OTHER_LOCAL_MAPS=10
SLOTS_MILLIS_MAPS=138095
org.apache.hadoop.mapreduce.TaskCounter
MAP_INPUT_RECORDS=10
MAP_OUTPUT_RECORDS=20
SPLIT_RAW_BYTES=1180
SPILLED_RECORDS=0
FAILED_SHUFFLE=0
MERGED_MAP_OUTPUTS=0
GC_TIME_MILLIS=785
CPU_MILLISECONDS=4180
PHYSICAL_MEMORY_BYTES=491077632
VIRTUAL_MEMORY_BYTES=3847532544
COMMITTED_HEAP_BYTES=162529280
org.apache.hadoop.examples.RandomWriter$Counters
BYTES_WRITTEN=148669
RECORDS_WRITTEN=20
org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter
BYTES_READ=0
org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter
BYTES_WRITTEN=150089
Job ended: Fri Sep 30 14:47:47 PDT 2011
The job took 53 seconds. We’ll now run the conventional wordcount example job, to see a full map &amp; reduce job (as opposed to the randomwriter map-only job): ahmed@ubuntu:~/mr2/deploy/hadoop-0.24.0-SNAPSHOT$ $HADOOP_COMMON_HOME/bin/hadoop jar $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-0.24.0-SNAPSHOT.jar wordcount input output2 The job counters output after successful run: File System Counters
FILE: BYTES_READ=384
FILE: BYTES_WRITTEN=87759
FILE: READ_OPS=0
FILE: LARGE_READ_OPS=0
FILE: WRITE_OPS=0
HDFS: BYTES_READ=144
HDFS: BYTES_WRITTEN=46
HDFS: READ_OPS=9
HDFS: LARGE_READ_OPS=0
HDFS: WRITE_OPS=4
org.apache.hadoop.mapreduce.JobCounter
TOTAL_LAUNCHED_MAPS=1
TOTAL_LAUNCHED_REDUCES=1
DATA_LOCAL_MAPS=1
SLOTS_MILLIS_MAPS=2331
SLOTS_MILLIS_REDUCES=2353
org.apache.hadoop.mapreduce.TaskCounter
MAP_INPUT_RECORDS=5
MAP_OUTPUT_RECORDS=5
MAP_OUTPUT_BYTES=56
MAP_OUTPUT_MATERIALIZED_BYTES=72
SPLIT_RAW_BYTES=108
COMBINE_INPUT_RECORDS=5
COMBINE_OUTPUT_RECORDS=5
REDUCE_INPUT_GROUPS=5
REDUCE_SHUFFLE_BYTES=72
REDUCE_INPUT_RECORDS=5
REDUCE_OUTPUT_RECORDS=5
SPILLED_RECORDS=10
SHUFFLED_MAPS=1
FAILED_SHUFFLE=0
MERGED_MAP_OUTPUTS=1
GC_TIME_MILLIS=128
CPU_MILLISECONDS=1270
PHYSICAL_MEMORY_BYTES=212226048
VIRTUAL_MEMORY_BYTES=770711552
COMMITTED_HEAP_BYTES=137433088
Shuffle Errors
BAD_ID=0
CONNECTION=0
IO_ERROR=0
WRONG_LENGTH=0
WRONG_MAP=0
WRONG_REDUCE=0
org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter
BYTES_READ=36
org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter
BYTES_WRITTEN=46
 MR2 comes with new and updated web consoles that can be used to conveniently explore and monitor the system services. The Applications tab shows the submitted applications, the application�s id, user name, queue name, status, progress and other useful information. For example, here is the resource manager Applications snapshot when randomwriter and wordcount are running. The nodes tab shows the nodes of the cluster, their addresses, in addition to health and container information. For example, here is the nodes view for our single node cluster. The scheduler view shows useful scheduling information. In our example, we used the default FifoScheduler. And, as seen in the following snapshot, the view shows information like the queue minimum and maximum capacities, number of nodes, total and available capacities and other useful information. This snapshot was captured after running the randomwriter application but before submitting the wordcount application.</snippet></document><document id="495"><title>Apache Hadoop 0.23.0 has been released</title><url>http://blog.cloudera.com/blog/2011/11/apache-hadoop-0-23-0-has-been-released/</url><snippet>The Apache Hadoop PMC has voted to release Apache Hadoop 0.23.0. This release is significant since it is the first major release of Hadoop in over a year, and incorporates many new features and improvements over the 0.20 release series. The biggest new features are HDFS federation, and a new MapReduce framework. There is also a new build system (Maven), Kerberos HTTP SPNEGO support, as well as some significant performance improvements which we’ll be covering in future posts. Note, however, that 0.23.0 is not a production release, so please don’t install it on your production cluster. Features HDFS federation improves HDFS scalability by allowing multiple independent namenodes, each managing a portion of the namespace. Each datanode in the cluster can provide storage to all the namenodes (which means datanodes do not, for example, belong to a single namenode). Note that HDFS federation is not to be confused with HDFS High Availability, which will be coming in a future 0.23 release. MapReduce 2 (“next gen”) is a re-write of the the MapReduce runtime to overcome scalability bottlenecks in the jobtracker. It is based on a new framework called YARN for cluster resource management, and a MapReduce “application” which runs users’ jobs on YARN. In this design MapReduce becomes a user-space library, and also allows other parallel applications to run on Hadoop clusters, beside MapReduce applications. Be aware that Hadoop 0.23.0 does not come with the “classic” MapReduce runtime (MapReduce 1) which runs jobtrackers and tasktrackers. However, it does fully support both the old and new MapReduce user APIs (the old API is in the org.apache.hadoop.mapred package, the new one in org.apache.hadoop.mapreduce). In 0.23.0 the old API is deprecated and users are encouraged to move to the new API from this release onwards. Note that if you wish to use the classic runtime (or the old API) you can use a 0.20.x based release, such as the one included in CDH3. Stability, Compatibility and Testing It is important to stress that 0.23.0 is not ready for production use yet. It is an early release that users can start testing so that we can stabilize later 0.23 releases.  We expect a later dot release to be production-ready, and will be incorporated into CDH4. In terms of compatibility, in the vast majority of cases, programs written to use the public Hadoop APIs in 0.20.x should run correctly on 0.23.0, although they will need to be recompiled. You can find detailed notes on compatibility in HADOOP-7738. The process of updating Hadoop ecosystem projects to work with 0.23.0 is still underway. One of the goals of the Apache Bigtop project (incubating) is interoperability testing of Hadoop components, and the project is tracking the status of downstream builds that use Hadoop in BIGTOP-162. If you use Hadoop we encourage you to get involved in this testing effort by trying out your workloads and applications on Hadoop 0.23.0 and reporting any issues you find to the Hadoop project. Acknowledgements Thanks go to everyone who contributed to the release (reporting issues, fixing bugs, reviewing changes, writing documentation, etc), and especially to Arun C Murthy who did a fantastic job as release manager.</snippet></document><document id="496"><title>CDH3u2: Apache Mahout Integration</title><url>http://blog.cloudera.com/blog/2011/11/cdh3u2-apache-mahout-integration/</url><snippet>Cloudera believes that the flexibility and power of Apache Mahout (http://mahout.apache.org/) in conjunction with Hadoop is invaluable. Therefore, we have packaged the most recent stable release of Mahout (0.5) into CDH3u2, and we are very excited to work with the Mahout community becoming much more involved with the project as both Mahout &amp; Hadoop continue to grow. You can test our CDH with Mahout integration by downloading our most recent release: https://ccp.cloudera.com/display/DOC/Downloading+CDH+Releases Why we are packing Mahout with Hadoop? Machine learning is an entire field devoted to Information Retrieval, Statistics, Linear Algebra, Analysis of Algorithms, and many other subjects. This field allows us to examine things such as recommendation engines involving new friends, love interests, and new products. We can do incredibly advanced analysis around genetic sequencing and examination, distributed search and frequency pattern matching, as well mathematical analysis with vectors, matrices, and singular value decomposition (SVD). Apache Mahout is an open source project that is a part of the Apache Software Foundation, devoted to Machine Learning. Mahout can operate on top of Hadoop, which allows the user to apply the concept of Machine Learning via a selection of algorithms in Mahout to distributed computing via Hadoop. Mahout packages popular machine learning algorithms such as: Recommendation mining, takes users’ behavior and find items said specified user might like. Clustering, takes e.g. text documents and groups them based on related document topics. Classification, learns from existing categorized documents what specific category documents look like and is able to assign unlabeled documents to the appropriate category. Frequent item set mining, takes a set of item groups (e.g. terms in a query session, shopping cart content) and identifies, which individual items typically appear together. We are very excited to be working with the Apache Mahout community and highly encourage everyone who is using CDH currently to give Mahout a try! As always, we are open to any guests who would like to blog about their experience using Mahout with CDH.</snippet></document><document id="497"><title>Attend a Meetup Surrounding the Hadoop World Conference</title><url>http://blog.cloudera.com/blog/2011/10/attend-a-meetup-surrounding-the-hadoop-world-conference/</url><snippet>Several meetups for Apache Hadoop and Hadoop-related projects are scheduled for the evenings surrounding Hadoop World 2011. Make the most of your week in New York City by attending one or more of these meetups focusing on the Apache projects Hadoop, HBase, Sqoop, Hive and Flume. Food and beverages will be provided at each meetup. Join us to relax, get informed and network with your fellow conference attendees. Hadoop/HBase Meetup, NYC Edition November 7: 6pm – 8pm ET Attend this Apache Hadoop and Apache Hbase meetup the night before Hadoop World kicks off. Learn from a series of short 15-minute talks followed by interactive discussions that will be in an unconference format. Learn more Apache Sqoop (incubating) Meetup November 7: 7:30pm – 9pm ET Attend this unconference-formatted, Apache Sqoop-focused meetup to participate in and listen to discussions around key issues users encounter working with Sqoop. The meetup will also include discussions about popular feature requests and enhancements that could help define the next major feature for the Sqoop project. Learn more Apache Hive Meetup, NYC Edition November 9: 6:30pm ET After the commencement of Hadoop World, attend the Apache Hive meetup and listen to a series of short 15-minute talks that will be followed by an unconference-formatted discussion focused on several facets of the Apache Hive project. Learn more Apache Flume (incubating) Meetup November 9: 6:30pm – 8:30pm ET Attend this Apache Flume meetup following Hadoop World. There will be a short initial talk followed by an unconference-formatted discussion. Learn more If you have yet to register for Hadoop World, please click here or the green button bellow. Remember to book your lodging for Hadoop World before prices skyrocket. If you have trouble with the link to Sheraton, please call 212-581-1000 and mention Hadoop World. The conference group rate expires November 5.</snippet></document><document id="498"><title>CDH3 update 2 is released</title><url>http://blog.cloudera.com/blog/2011/10/cdh3-update-2-is-released/</url><snippet>Continuing with our practice from Cloudera’s Distribution Including Apache Hadoop v2 (CDH2), our goal is to provide regular (quarterly), predictable updates to the generally available release of our open source distribution.  For CDH3 the second such update is available today, approximately 3 months after update 1. For those of you who are recent Cloudera users, here is a refresh on our update policy: We will only include patches in updates that are non-compatibility breaking. We will only include patches in updates that are non-disruptive. You can skip updates without penalty – i.e., if you don’t find the contents of an update compelling, you can skip it and wait for a future update without having to do a delta upgrade. When it’s possible to pull features from our CDH4 roadmap into CDH3 updates in a non-disruptive way, we’ll take advantage of that opportunity. There are a number of improvements coming to CDH3 with update 2.  Among them are: 1. New features – Support for Apache Mahout (0.5).   Apache Mahout is a popular machine learning library that makes it easier for users to perform analyses like collaborative filtering and k-means clustering on Hadoop.   Also added in update 2 is expanded support for Apache Avro’s data file format.  Users can: load data into Avro data files in Hadoop via Sqoop or Flume run MapReduce, Pig or Hive workloads on Avro data files view the contents of Avro files from the Hue web client This gives users the ability to use all the major features of the Hadoop stack without having to switch file formats or default to text.  The Avro file format provides added benefits over text because it is faster and more compact. 2. Improvements (stability and performance) – HBase in particular has received a number of improvements that improve stability and recoverability.  All HBase users are encouraged to use update 2. 3. Bug fixes – 50+ bug fixes.  The enumerated fixes and their corresponding Apache project jiras are provided in the release notes. Update 2 is available in all the usual formats (RHEL, SLES, Ubuntu, Debian packages, tarballs, and SCM Express).  Check out the installation docs for instructions. If you’re running components from the Cloudera Management Suite they will not be impacted by moving to update 2. The next update (update 3) for CDH3 is planned for January, 2012. Thank you for supporting Apache Hadoop and thank you for supporting Cloudera.</snippet></document><document id="499"><title>Hadoop World 2011: A Glimpse into Operations</title><url>http://blog.cloudera.com/blog/2011/10/hadoop-world-2011-a-glimpse-into-operations/</url><snippet>Check out the Hadoop World 2011 conference agenda! Find sessions of interest and begin planning your Hadoop World experience among the sixty breakout sessions spread across five simultaneous tracks at http://www.hadoopworld.com/agenda/. Preview of Operations Track and Sessions The Operations track at Hadoop World 2011 focuses on the practices IT organizations employ to adopt and run Apache Hadoop with special emphasis on the people, processes and technology. Presentations will include initial deployment case studies, production scenarios or expansion scenarios with a focus on people, processes and technology. Speakers will discuss advances in reducing the cost of Hadoop deployment and increasing availability and performance. Unlocking the Value of Big Data with Oracle Jean-Pierre Dijcks, Oracle Analyzing new and diverse digital data streams can reveal new sources of economic value, provide fresh insights into customer behavior and identify market trends early on. But this influx of new data can create challenges for IT departments. To derive real business value from Big Data, you need the right tools to capture and organize a wide variety of data types from different sources, and to be able to easily analyze it within the context of all your enterprise data. Attend this session to learn how Oracle’s end-to-end value chain for Big Data can help you unlock the value of Big Data. Hadoop as a Service in Cloud Junping Du, VMware Hadoop framework is often built on native environment with commodity hardware as its original design. However, with growing tendency of cloud computing, there is stronger requirement to build Hadoop cluster on a public/private cloud in order for customers to benefit from virtualization and multi-tenancy. This session discusses how to address some the challenges of providing Hadoop service on virtualization platform such as: performance, rack awareness, job scheduling, memory over commitment, etc, and propose some solutions. Hadoop in a Mission Critical Environment Jim Haas, CBS Interactive Our need for better scalability in processing weblogs is illustrated by the change in requirements – processing 250 million vs. 1 billion web events a day (and growing). The Data Waregoup at CBSi has been transitioning core processes to re-architected Hadoop processes for two years. We will cover strategies used for successfully transitioning core ETL processes to big data capabilities and present a how-to guide of re-architecting a mission critical Data Warehouse environment while it’s running. Practical HBase Ravi Veeramchaneni, NAVTEQ Many developers have experience in working on relational databases using SQL. The transition to No-SQL data stores, however, is challenging and often time confusing. This session will share experiences of using HBase from Hardware selection/deployment to design, implementation and tuning of HBase. At the end of the session, audience will be in a better position to make right choices on Hardware selection, Schema design and tuning HBase to their needs. Hadoop Troubleshooting 101 Kate Ting, Cloudera Attend this session and walk away armed with solutions to the most common customer problems. Learn proactive configuration tweaks and best practices to keep your cluster free of fetch failures, job tracker hangs, and other common issues. I Want to Be BIG – Lessons Learned at Scale David “Sunny” Sundstrom, SGI SGI has been a leading commercial vendor of Hadoop clusters since 2008. Leveraging SGI’s experience with high performance clusters at scale, SGI has delivered individual Hadoop clusters of up to 4000 nodes. In this presentation, through the discussion of representative customer use cases, you’ll explore major design considerations for performance and power optimization, how integrated Hadoop solutions leveraging CDH, SGI Rackable clusters, and SGI Management Center best meet customer needs, and how SGI envisions the needs of enterprise customers evolving as Hadoop continues to move into mainstream adoption. There are several training classes and certification sessions provided surrounding the Hadoop World conference. Don’t forget to register and become Cloudera Certified in Apache Hadoop.</snippet></document><document id="500"><title>Nominations Are Open for the 2011 Government Big Data Solutions Award</title><url>http://blog.cloudera.com/blog/2011/10/nominations-are-open-for-the-2011-government-big-data-solutions-award/</url><snippet>This post was contributed by Bob Gourley, editor, CTOvision.com. The missions and data of governments make the government sector one of particular importance for Big Data solutions. Federal, State and Local governments have special abilities to focus research in areas like Health Sciences, Economics, Law Enforcement, Defense, Geographic Studies, Environmental Studies, Bioinformatics, Information Search/Discovery, and Computer Security. Government-Industry teams are working to field Big Data solutions in all these fields. The Government Big Data Solutions Award was established by the technology blog CTOvision.com to help facilitate the exchange of best practices, lessons learned and creative ideas for solutions to hard data challenges in the government sector. Nominations are being sought and your input would be most appreciated. Please nominate capabilities and solutions you know hold great potential for mission impact in the government sector. Award winners will be written up on CTOvision.com, and a presentation of awards will also be made at Hadoop World 2011. You may nominate industry solution providers with proven capabilities, government organizations who have built or implemented Big Data solutions or individuals who have played a direct role in establishing capability. The key criteria we are evaluating is the utility of the solution to serve government missions. Our judges include: Doug Cutting (creator of Lucene and Hadoop), Alan Wade (former CIA and IC CIO), Ryan Lasalle (Accenture Cyber R&amp;D), Ed Granstedt (QinetiQ Strategic Solution Center) and Chris Dorobek (Founder, editor and publisher of DorobekInsider.com). To nominate, please use our online survey form at: https://www.surveymonkey.com/s/CNHB5QZ</snippet></document><document id="501"><title>Hadoop World 2011: A Glimpse into Development</title><url>http://blog.cloudera.com/blog/2011/10/hadoop-world-2011-a-glimpse-into-development/</url><snippet>The Development track at Hadoop World is a technical deep dive dedicated to discussion about Apache Hadoop and application development for Apache Hadoop. You will hear committers, contributors and expert users from various Hadoop projects discuss the finer points of building applications with Hadoop and the related ecosystem. The sessions will touch on foundational topics such as HDFS, HBase, Pig, Hive, Flume and other related technologies. In addition, speakers will address key development areas including tools, performance, bringing the stack together and testing the stack. Sessions in this track are for developers of all levels who want to learn more about upcoming features and enhancements, new tools, advanced techniques and best practices. Preview of Development Track Sessions Building Web Analytics Processing on Hadoop at CBS Interactive Michael Sun, CBS Interactive Abstract: CBS Interactive successfully adopted Hadoop as the web analytics platform, processing one Billion weblogs daily from hundreds of web site properties that CBS Interactive oversees. After introducing Lumberjack—the Extraction, Transformation and Loading framework we built based on python and streaming, which is under review for Open-Source release—Michael will talk about web metrics processing on Hadoop, focusing on weblog harvesting, parsing, dimension look-up, sessionization, and loading into a database. Since migrating processing from a proprietary platform to Hadoop, CBS Interactive achieved robustness, fault-tolerance and scalability, and significant reduction of processing time to reach SLA (over six hours reduction so far). Gateway: Cluster Virtualization Framework Konstantin Shvachko, eBay Abstract: Access to Hadoop clusters through dedicated portal nodes (typically located behind firewalls and performing user authentication and authorization) can have several drawbacks — as shared multitenant resources they can create contention among users and increase the maintenance overhead for cluster administrators. This session will discuss the Gateway system, a cluster virtualization framework that provides multiple benefits: seamless access from users’ workplace computers through corporate firewalls; the ability to failover to active clusters for scheduled or unscheduled downtime, as well as the ability to redirect traffic to other clusters during upgrades; and user access to clusters running different versions of Hadoop. SHERPASURFING – Open Source Cyber Security Solution Wayne Wheeles, Novii Design Abstract: Every day billions of packets, both benign and some malicious, flow in and out of networks. Every day it is an essential task for the modern Defensive Cyber Security Organization to be able to reliably survive the sheer volume of data, bring the NETFLOW data to rest, enrich it, correlate it and perform. SHERPASURFING is an open source platform built on the proven Cloudera’s Distribution including Apache Hadoop that enables organizations to perform the Cyber Security mission and at scale at an affordable price point. This session will include an overview of the solution and components, followed by a demonstration of analytics. Integrating Hadoop with Enterprise RDBMS Using Apache SQOOP and Other Tools Arvind Prabhakar, Cloudera Guy Harrison, Quest Software Abstract: As Hadoop graduates from pilot project to a mission critical component of the enterprise IT infrastructure, integrating information held in Hadoop and in Enterprise RDBMS becomes imperative. We’ll look at key scenarios driving Hadoop and RDBMS integration and review technical options. In particular, we’ll deep dive into the Apache SQOOP project, which expedites data movement between Hadoop and any JDBC database, as well as providing an framework which allows developers and vendors to create connectors optimized for specific targets such as Oracle, Netezza etc. Next Generation Apache Hadoop MapReduce Mahadev Konar, Hortonworks Abstract: The Apache Hadoop MapReduce framework has hit a scalability limit around 4,000 machines. We are developing the next generation of Apache Hadoop MapReduce that factors the framework into a generic resource scheduler and a per-job, user-defined component that manages the application execution. Since downtime is more expensive at scale, high-availability is built-in from the beginning; as are security and multi-tenancy to support many users on the larger clusters. The new architecture will also increase innovation, agility and hardware utilization. We will be presenting the architecture and design of the next generation of map reduce and will delve into the details of the architecture that makes it much easier to innovate. We will also be presenting large scale and small scale comparisons on some benchmarks with MRV1.” There are several training classes and certification sessions provided surrounding the Hadoop World conference. Don’t forget to register and become Cloudera Certified in Apache Hadoop.</snippet></document><document id="502"><title>Introducing Crunch: Easy MapReduce Pipelines for Apache Hadoop</title><url>http://blog.cloudera.com/blog/2011/10/introducing-crunch/</url><snippet>As a data scientist at Cloudera, I work with customers across a wide range of industries that use Apache Hadoop to solve their business problems. Many of the solutions we create involve multi-stage pipelines of MapReduce jobs that join, clean, aggregate, and analyze enormous amounts of data. When working with log files or relational database tables, we use high-level tools like Apache�Pig and Apache�Hive for their convenient and powerful support for creating pipelines over structured and semi-structured records. As Hadoop has spread from web companies to other industries, the variety of data that is stored in HDFS has expanded dramatically. Hadoop clusters are being used to process satellite images, time series data, audio files, and seismograms. These formats are not a natural fit for the data schemas imposed by Pig and Hive, in the same way that structured binary data in a relational database can be a bit awkward to work with. For these use cases, we either end up writing large, custom libraries of user-defined functions in Pig or Hive, or simply give up on our high-level tools and go back to writing MapReduces in Java. Either of these options is a serious drain on developer productivity. Today, we�re pleased to introduce Crunch, a Java library that aims to make writing, testing, and running MapReduce pipelines easy, efficient, and even fun. Crunch�s design is modeled after Google�s FlumeJava, focusing on a small set of simple primitive operations and lightweight user-defined functions that can be combined to create complex, multi-stage pipelines. At runtime, Crunch compiles the pipeline into a sequence of MapReduce jobs and manages their execution. Example Let�s take a look at the classic WordCount MapReduce, written using Crunch: import com.cloudera.crunch.DoFn;
import com.cloudera.crunch.Emitter;
import com.cloudera.crunch.PCollection;
import com.cloudera.crunch.PTable;
import com.cloudera.crunch.Pipeline;
import com.cloudera.crunch.impl.mr.MRPipeline;
import com.cloudera.crunch.lib.Aggregate;
import com.cloudera.crunch.type.writable.Writables;

public class WordCount {
  public static void main(String[] args) throws Exception {
    // Create an object to coordinate pipeline creation and execution.
    Pipeline pipeline = new MRPipeline(WordCount.class);
    // Reference a given text file as a collection of Strings.
    PCollection&lt;String&gt; lines = pipeline.readTextFile(args[0]);

    // Define a function that splits each line in a PCollection of Strings into a
    // PCollection made up of the individual words in the file.
    PCollection&lt;String&gt; words = lines.parallelDo(new DoFn&lt;String, String&gt;() {
      public void process(String line, Emitter&lt;String&gt; emitter) {
        for (String word : line.split("\\s+")) {
          emitter.emit(word);
        }
      }
    }, Writables.strings()); // Indicates the serialization format

    // The Aggregate.count method applies a series of Crunch primitives and returns
    // a map of the unique words in the input PCollection to their counts.
    // Best of all, the count() function doesn't need to know anything about
    // the kind of data stored in the input PCollection.
    PTable&lt;String, Long&gt; counts = Aggregate.count(words);

    // Instruct the pipeline to write the resulting counts to a text file.
    pipeline.writeTextFile(counts, args[1]);
    // Execute the pipeline as a MapReduce.
    pipeline.done();
  }
}   Advantages It�s just Java. Crunch shares a core philosophical belief with Google�s FlumeJava: novelty is the enemy of adoption. For developers, learning a Java library requires much less up-front investment than learning a new programming language. Crunch provides full access to the power of Java for writing functions, managing pipeline execution, and dynamically constructing new pipelines, obviating the need to switch back and forth between a data flow language and a real programming language. Natural type system. Crunch supports reading and writing data that is stored using Hadoop�s Writable format or Apache Avro records. You do not need to write code that maps data stored in these formats into Crunch�s type system– they are supported natively. You can even mix and match Writable and Avro types within a single MapReduce: changing the Writables.strings() call to Avros.strings() in the WordCount example will run the MapReduce using Avro serialization instead of Writables. A modular library released under the Apache License. Experts in machine learning, text mining, and ETL can craft libraries using Crunch�s data model, and other developers can use those libraries to build custom pipelines that operate on their data. For example, Crunch can be used to create the glue code that converts raw data into the structured input that a machine learning algorithm expects, and Crunch will compile the glue code and the machine learning algorithm into a single MapReduce. Future Work We are releasing Crunch as a development project, not a product. We’re eager for developers to play with it and tell us what they like and what they dislike. You can get started with Crunch by downloading it from Cloudera�s github repository here. We have tested the library on a number of our use cases, but there will be bugs and rough edges that we will work out in the coming months. We gladly welcome contributions from the Hadoop ecosystem to help us improve Crunch as we prepare it for submission to the Apache Incubator, especially around: More efficient MapReduce compilation, including cost-based optimization, Support for HBase and HCatalog as data sources/targets, Tools and examples that build Crunch pipelines in other JVM languages, such as Scala, JRuby, Clojure, and Jython.</snippet></document><document id="503"><title>Apache Sqoop – Overview</title><url>http://blog.cloudera.com/blog/2011/10/apache-sqoop-overview/</url><snippet>This blog was originally posted on the Apache Blog: https://blogs.apache.org/sqoop/entry/apache_sqoop_overview Using Hadoop for analytics and data processing requires loading data into clusters and processing it in conjunction with other data that often resides in production databases across the enterprise. Loading bulk data into Hadoop from production systems or accessing it from map reduce applications running on large clusters can be a challenging task. Users must consider details like ensuring consistency of data, the consumption of production system resources, data preparation for provisioning downstream pipeline. Transferring data using scripts is inefficient and time consuming. Directly accessing data residing on external systems from within the map reduce applications complicates applications and exposes the production system to the risk of excessive load originating from cluster nodes. This is where Apache Sqoop fits in. Apache Sqoop is currently undergoing incubation at Apache Software Foundation. More information on this project can be found at http://incubator.apache.org/sqoop. Sqoop allows easy import and export of data from structured data stores such as relational databases, enterprise data warehouses, and NoSQL systems. Using Sqoop, you can provision the data from external system on to HDFS, and populate tables in Hive and HBase. Sqoop integrates with Oozie, allowing you to schedule and automate import and export tasks. Sqoop uses a connector based architecture which supports plugins that provide connectivity to new external systems. What happens underneath the covers when you run Sqoop is very straightforward. The dataset being transferred is sliced up into different partitions and a map-only job is launched with individual mappers responsible for transferring a slice of this dataset. Each record of the data is handled in a type safe manner since Sqoop uses the database metadata to infer the data types. In the rest of this post we will walk through an example that shows the various ways you can use Sqoop. The goal of this post is to give an overview of Sqoop operation without going into much detail or advanced functionality. Importing Data The following command is used to import all data from a table called ORDERS from a MySQL database: ---
$ sqoop import --connect jdbc:mysql://localhost/acmedb \
    --table ORDERS --username test --password ****
--- In this command the various options specified are as follows: import: This is the sub-command that instructs Sqoop to initiate an import. –connect &lt;connect string&gt;, –username &lt;user name&gt;, –password &lt;password&gt;: These are connection parameters that are used to connect with the database. This is no different from the connection parameters that you use when connecting to the database via a JDBC connection. –table &lt;table name&gt;: This parameter specifies the table which will be imported. The import is done in two steps as depicted in Figure 1 below. In the first Step Sqoop introspects the database to gather the necessary metadata for the data being imported. The second step is a map-only Hadoop job that Sqoop submits to the cluster. It is this job that does the actual data transfer using the metadata captured in the previous step. Figure 1: Sqoop Import Overview The imported data is saved in a directory on HDFS based on the table being imported. As is the case with most aspects of Sqoop operation, the user can specify any alternative directory where the files should be populated. By default these files contain comma delimited fields, with new lines separating different records. You can easily override the format in which data is copied over by explicitly specifying the field separator and record terminator characters. Sqoop also supports different data formats for importing data. For example, you can easily import data in Avro data format by simply specifying the option –as-avrodatafile with the import command. There are many other options that Sqoop provides which can be used to further tune the import operation to suit your specific requirements. Importing Data into Hive In most cases, importing data into Hive is the same as running the import task and then using Hive to create and load a certain table or partition. Doing this manually requires that you know the correct type mapping between the data and other details like the serialization format and delimiters. Sqoop takes care of populating the Hive metastore with the appropriate metadata for the table and also invokes the necessary commands to load the table or partition as the case may be. All of this is done by simply specifying the option –hive-import with the import command. ----
$ sqoop import --connect jdbc:mysql://localhost/acmedb \
      --table ORDERS --username test --password **** --hive-import
---- When you run a Hive import, Sqoop converts the data from the native datatypes within the external datastore into the corresponding types within Hive. Sqoop automatically chooses the native delimiter set used by Hive. If the data being imported has new line or other Hive delimiter characters in it, Sqoop allows you to remove such characters and get the data correctly populated for consumption in Hive. Once the import is complete, you can see and operate on the table just like any other table in Hive. Importing Data into HBase You can use Sqoop to populate data in a particular column family within the HBase table. Much like the Hive import, this can be done by specifying the additional options that relate to the HBase table and column family being populated. All data imported into HBase is converted to their string representation and inserted as UTF-8 bytes. ----
$ sqoop import --connect jdbc:mysql://localhost/acmedb \
        --table ORDERS --username test --password **** \
        --hbase-create-table --hbase-table ORDERS --column-family mysql
---- In this command the various options specified are as follows: –hbase-create-table: This option instructs Sqoop to create the HBase table. –hbase-table: This option specifies the table name to use. –column-family: This option specifies the column family name to use. The rest of the options are the same as that for regular import operation. Exporting Data In some cases data processed by Hadoop pipelines may be needed in production systems to help run additional critical business functions. Sqoop can be used to export such data into external datastores as necessary. Continuing our example from above – if data generated by the pipeline on Hadoop corresponded to the ORDERS table in a database somewhere, you could populate it using the following command: ----
$ sqoop export --connect jdbc:mysql://localhost/acmedb \
        --table ORDERS --username test --password **** \
        --export-dir /user/arvind/ORDERS
---- In this command the various options specified are as follows: export: This is the sub-command that instructs Sqoop to initiate an export. –connect &lt;connect string&gt;, –username &lt;user name&gt;, –password &lt;password&gt;: These are connection parameters that are used to connect with the database. This is no different from the connection parameters that you use when connecting to the database via a JDBC connection. –table &lt;table name&gt;: This parameter specifies the table which will be populated. –export-dir &lt;directory path&gt;: This is the directory from which data will be exported. Export is done in two steps as depicted in Figure 2. The first step is to introspect the database for metadata, followed by the second step of transferring the data. Sqoop divides the input dataset into splits and then uses individual map tasks to push the splits to the database. Each map task performs this transfer over many transactions in order to ensure optimal throughput and minimal resource utilization. Figure 2: Sqoop Export Overview Some connectors support staging tables that help isolate production tables from possible corruption in case of job failures due to any reason. Staging tables are first populated by the map tasks and then merged into the target table once all of the data has been delivered it. Sqoop Connectors Using specialized connectors, Sqoop can connect with external systems that have optimized import and export facilities, or do not support native JDBC. Connectors are plugin components based on Sqoop’s extension framework and can be added to any existing Sqoop installation. Once a connector is installed, Sqoop can use it to efficiently transfer data between Hadoop and the external store supported by the connector. By default Sqoop includes connectors for various popular databases such as MySQL, PostgreSQL, Oracle, SQL Server and DB2. It also includes fast-path connectors for MySQL and PostgreSQL databases. Fast-path connectors are specialized connectors that use database specific batch tools to transfer data with high throughput. Sqoop also includes a generic JDBC connector that can be used to connect to any database that is accessible via JDBC. Apart from the built-in connectors, many companies have developed their own connectors that can be plugged into Sqoop. These range from specialized connectors for enterprise data warehouse systems to NoSQL datastores. Wrapping Up In this post you saw how easy it is to transfer large datasets between Hadoop and external datastores such as relational databases. Beyond this, Sqoop offers many advance features such as different data formats, compression, working with queries instead of tables etc. We encourage you to try out Sqoop and give us your feedback. More information regarding Sqoop can be found at: Project Website: http://incubator.apache.org/sqoop Wiki: https://cwiki.apache.org/confluence/display/SQOOP Project Status:  http://incubator.apache.org/projects/sqoop.html Mailing Lists: https://cwiki.apache.org/confluence/display/SQOOP/Mailing+Lists</snippet></document><document id="504"><title>Hadoop World 2011: A Glimpse into Enterprise Architecture</title><url>http://blog.cloudera.com/blog/2011/10/hadoop-world-2011-a-glimpse-into-enterprise-architecture/</url><snippet>The Enterprise Architecture track at Hadoop World 2011 will provide insight into how Hadoop is powering today’s advanced data management ecosystems and how Hadoop fits into modern enterprise environments. Speakers will discuss architecture and models, demonstrating how Hadoop connects to surrounding platforms. Attendees of the Enterprise Architecture track will learn Hadoop deployment design patterns; enterprise models and system architecture; types of systems managing data that is transferred to Hadoop using Apache Sqoop and Apache Flume; and how to publish data via Apache Hive, Apache HBase and Apache Sqoop to systems that consume data from Hadoop. Preview of Enterprise Architecture Track Sessions Building Realtime Big Data Services at Facebook with Hadoop and HBase Jonathan Gray, Facebook, Inc. Abstract: Facebook has one of the largest Apache Hadoop data warehouses in the world, primarily queried through Apache Hive for offline data processing and analytics. However, the need for realtime analytics and end-user access has led to the development of several new systems built using Apache HBase. This talk will cover specific use cases and the work done at Facebook around building large scale, low latency and high throughput realtime services with Hadoop and HBase. This includes several significant contributions to existing projects as well as the release of new open source projects. Extending the Enterprise Data Warehouse with Hadoop Jonathan Seidman, Orbitz Worldwide Rob Lancaster, Orbitz worldwide Abstract: Hadoop provides the ability to extract business intelligence from extremely large, heterogeneous data sets that were previously impractical to store and process in traditional data warehouses. The challenge now is in bridging the gap between the data warehouse and Hadoop. In this talk we’ll discuss some steps that Orbitz has taken to bridge this gap, including examples of how Hadoop and Hive are used to aggregate data from large data sets, and how that data can be combined with relational data to create new reports that provide actionable intelligence to business users. Leveraging Hadoop for Legacy Systems Mathias Herberts, Crédit Mutuel Arkéa Abstract: Since many companies in the financial sector still relies on legacy systems for its daily operations, Hadoop can only be truly useful in those environments if it can fit nicely among COBOL, VSAM, MVS and other legacy technologies. In this session, we will detail how Crédit Mutuel Arkéa solved this challenge and successfully mixed the mainframe and Hadoop. Replacing RDB/DW with Hadoop and Hive for Telco Big Data Jason Han, NexR Inc. Abstract: This session will focus on the challenges of replacing existing Relational DataBase and Data Warehouse technologies with Open Source components. Jason Han will base his presentation on his experience migrating Korea Telecom (KT’s) CDR data from Oracle to Hadoop, which required converting many Oracle SQL queries to Hive HQL queries. He will cover the differences between SQL and HQL; the implementation of Oracle’s basic/analytics functions with MapReduce; the use of Sqoop for bulk loading RDB data into Hadoop; and the use of Apache Flume for collecting fast-streamed CDR data. He’ll also discuss Lucene and ElasticSearch for near-realtime distributed indexing and searching. You’ll learn tips for migrating existing enterprise big data to open source, and gain insight into whether this strategy is suitable for your own data. WibiData: Entity-centric Analysis with HBase Aaron Kimball, Odiago Abstract: WibiData is a collaborative data mining and predictive modeling platform for large-scale, multi-structured, user-centric data. It leverages HBase to combine batch analysis and real time access within the same system, and integrates with existing BI, reporting and analysis tools. WibiData offers a set of libraries for common user-centric analytic tasks, and more advanced data mining libraries for personalization, recommendation, and other predictive modeling applications. Developers can write re-usable libraries that are also accessible to data scientists and analysts alongside the WibiData libraries. In this talk, we will provide a technical overview of WibiData, and show how we used it to build FoneDoktor, a mobile app that collects data about device performance and app resource usage to offer personalized battery and performance improvement recommendations directly to users. Storing and Indexing Social Media Content in the Hadoop Ecosystem Lance Riedel, Jive Software Abstract: Jive is using Flume to deliver the content of a social web (250M messages/day) to HDFS and HBase. Flume’s flexible architecture allows us to stream data to our production data center as well as Amazon’s Web Services datacenter. We periodically build and merge Lucene indices with Hadoop jobs and deploy these to Katta to provide near real time search results. This talk will explore our infrastructure and decisions we’ve made to handle a fast growing set of real time data feeds. We will further explore other uses for Flume throughout Jive including log collection and our distributed event bus.</snippet></document><document id="505"><title>The Community Effect</title><url>http://blog.cloudera.com/blog/2011/10/the-community-effect/</url><snippet>Owen O’Malley recently collected and analyzed information in the Apache Hadoop project commit logs and its JIRA repository. That data describes the history of development for Hadoop and the contributions of the individuals who have worked on it. In the wake of his analysis, Owen wrote a blog post called The Yahoo! Effect. In it, he highlighted the huge amount of work that has gone into Hadoop since the project’s inception, and showed clearly how an early commitment to the project by Yahoo! had contributed to the growth of the platform and of the community. The crux of Owen’s analysis is captured in Figure 1, which comes from his post: Figure 1: Cumulative JIRA tickets closed for HADOOP, HDFS and MAPREDUCE The horizontal axis is a little bit off — Doug Cutting read the MapReduce paper from Google in 2004. Beginning that year, organizations including Overture, the Internet Archive and Yahoo! paid Doug as an independent contractor to add MapReduce and distributed storage to the Apache Nutch web crawler. It is fair to draw an arbitrary line, though: Yahoo! hired Doug as a full-time employee in early 2006 expressly to work on Hadoop, and assembled a team to collaborate with him on the project. That early investment was critical to building the platform that’s become the system of choice for analytical data management. With no disrespect to Yahoo!, however, the monolithic wall of green in Figure 1 tells a misleading story about the past, present and future of Apache Hadoop. It’s absolutely correct to note that Yahoo! covered the salaries of contributors in the early years. Five years is an eternity in the tech industry, however, and many of those developers moved on from Yahoo! between 2006 and 2011. If you look at where individual contributors work today — at the organizations that pay them, and at the different places in the industry where they have carried their expertise and their knowledge of Hadoop — the story is much more interesting. Figure 2: Cumulative patches contributed to core Hadoop: community members by current employer Figure 2 tells an encouraging story about the current state of Hadoop. Healthy open source projects bring together a diverse group of developers, looking at different problems and concentrating on a wide range of new features. Those developers create a community that collaborates to produce great software. The workloads that interest Microsoft, for example, will be different from those that interest Facebook. The fact that developers can draw on such a wide range of current requirements, and can make Hadoop better broadly — not for a single company, but for an entire industry — is critical. No single vendor can keep up with the large developer base and the broad adoption of a healthy open source project. That fact is critical. It separates Hadoop and Linux — projects with contributors from across the industry and around the world — from single vendor open source projects like MySQL, JBoss or Berkeley DB (the open source embedded database that my last company built and distributed). Each of these last three was the wholly-owned property of a single vendor. Each had a robust user community, who downloaded and used the software under an open source license. None, however, had a meaningful developer community, creating new features and contributing them to the project. The reason is simple: When a company owns a project, a developer is forced to donate the future commercial value of his or her work to the company in order to make a contribution. Unless you’re an employee of the company, that’s a huge disincentive to participation. Community projects like Hadoop, by contrast, have no single corporate owner. Individuals and companies are willing to share the cost of developing the software, since they can share in the commercial benefit that the project creates. The talent pool that contributes to Hadoop is both larger and deeper than any single organization could provide. There’s another important property of robust open source: It spawns complementary projects. In the early days, if you wanted to use Hadoop, you loaded data into the system by hand and coded up Java routines to run in the MapReduce framework. The broad community recognized these problems and invented new projects to address them — Apache Hive and Apache Pig for queries, Apache Flume and Apache Sqoop (both incubating) for data loading, Apache HBase for high-performance record storage and more. The contributions that Yahoo! made, as shown in Figure 1, represent a legacy of work on the core Apache Hadoop project, but not on the broader ecosystem of projects. That ecosystem has exploded in recent years, and most of the innovation around Hadoop is now happening in new projects. That’s not surprising — as Hadoop has matured, the core platform has stabilized, and the community has concentrated on easing adoption and simplifying use. Figure 3 shows the percent of new patches committed to the Core Hadoop project specifically, as a percentage of the contributions to the entire Hadoop ecosystem. Early, there was one project and it was the locus of all the work. Over time, the balance has shifted: Figure 3: Contributions to HADOOP, HDFS and MAPREDUCE as a percentage of total ecosystem contributions Of course, this work is also done by a diverse group of engineers working for different companies around the world. We can break down total lifetime contributions to the entire ecosystem by current employer: Figure 4: Lifetime patches contributed for all Hadoop-related projects: community members by current employer Figure 4 shares some shortcomings with Figure 1. It describes cumulative historical work, not necessarily recent or future contributions. People, not companies, do the work in open source. Over time those people move to new places to take on new challenges. They carry their expertise with them. Over time, too, individual companies increase or decrease their investment in open source projects as their requirements and business interests change. How are companies in the industry participating in sponsoring the development of the Hadoop ecosystem today? Figure 5 provides a snapshot of new development that’s happened so far in 2011, by the current employer of the developer doing the work: Figure 5: 2011 patches contributed to Hadoop and ecosystem projects: community member by current employer Clearly, the pace of innovation, and the breadth and depth of expertise that’s grown up around Hadoop, are excellent news for the core project. No one company sponsors more than a quarter of the new innovation in the Hadoop ecosystem and nearly half of all new patches are sponsored from a long tail of corporate benefactors and freelancers.  In fact, I expect this picture to get more interesting over time. Just since the beginning of 2011, established companies like IBM, EMC, Informatica, Oracle and Dell have announced plans to invest in the Hadoop ecosystem in various ways. A great deal of new money and talent will be directed toward Hadoop in the coming years. The community owes a deep debt of gratitude to Yahoo! for its early investment in Apache Hadoop. Certainly Cloudera does. Our employees and our customers benefit every day from Yahoo!’s decision to fund Doug’s early work and from its ongoing contributions to the platform. It is critical to remember, though, that the Hadoop community is much bigger than any single company. Bill Joy, founder of Sun Microsystems, has famously said, “Wherever you work, most of the smart people are somewhere else.” The genius of community-based open source is its ability to harness the insight, energy and enthusiasm of smart people across borders and company boundaries. As Cloudera’s CEO, I’m proud that we participate in a meaningful way in the work that the Apache Software Foundation oversees. The developers on my payroll do tremendous work on Hadoop and kindred projects. I am equally grateful, though, to the engineers at Hortonworks, Yahoo!, Facebook, StumbleUpon, Twitter, LinkedIn and the long list of other organizations that contribute to the ecosystem. It is a remarkable group and a vibrant community. We collected data for this post from https://issues.apache.org/jira/secure/IssueNavigator.jspa. Figures 2 through 5 depict patches committed as indicated by JIRAs in status “closed” or “resolved” as of the dates shown in the graph. All dates are based on the date when the patch was committed, not contributed. We chose to use commit dates as patches are often changed several times before they are eventually committed. The use of “date committed” creates spikes in Figure 2, due to unusual JIRA handling at the time of the Hadoop project split. The actual JIRA count is accurate, but the high commit activity is an an artifact of that split. Associations between contributors and current employer are based direct knowledge of the contributor, or on data from LinkedIn or elsewhere on the web. Contributors whose employers could not be determined are grouped under “everyone else.” Charts that refer to “Core Hadoop” are referring to patches committed to MapReduce, HDFS or Hadoop Common. Charts that refer to the Hadoop Ecosystem refer to patches committed to Core Hadoop as well as Pig, Hive, HBase, Whirr, HCatalog, Zookeeper, Mahout, Avro, Sqoop, Flume, Oozie, Bigtop and Hue.</snippet></document><document id="506"><title>My Summer Internship at Cloudera</title><url>http://blog.cloudera.com/blog/2011/10/my-summer-internship-at-cloudera/</url><snippet>This post was written by Daniel Jackoway following his internship at Cloudera during the summer of 2011. When I started my internship at Cloudera, I knew almost nothing about systems programming or Apache Hadoop, so I had no idea what to expect. The most important lesson I learned is that structured data is great as long as it is perfect, with the addendum that it is rarely perfect. My project was to develop a unified view of our customer data. The requirements were simple: pull in data from a variety of systems, group it by customer, and display it. The goal is that when someone at Cloudera needs to see all of the key information about our customers, it is available in one place. In addition, downloading and grouping data will make performing analysis much easier, allowing us to draw new insights about our business and our customers. I started by writing a script for each data source to download the necessary data and insert it into an HBase table in raw form. Next I wrote a script for each data source that grouped the data by customer, possibly transformed the data (filtering, sorting, inserting child objects within their parents, etc) and inserted it into a separate HBase table where each row corresponds to a single customer, with a column family for each data source. Finally, I exposed the data on an internal website using Django, integrating the different data sources as much as possible. One of the big challenges was turning the raw data into meaningful information. There were several discrepancies I needed to address with the data. As an example, companies have different names in different systems. Sometimes the difference is simply a matter of capitalization and/or spacing, but a greater challenge is that sometimes abbreviations were used in one system but not another, or one system ended the name with “, inc.” but another did not. I considered using fuzzy matching to solve all of these problems and realized that the Levenshtein distance between “Cloudera” and “Cloudera, inc.” is quite high, so I started looking at other forms of fuzzy matching and thinking of developing one that in particular favored long identical sub-strings. For example, I wanted my algorithm to see “Cloudera” and “Cloudera, inc.” as being highly similar for sharing the whole “Cloudera” part. As I contemplated embarking on a task to which I could have easily devoted the whole summer, I realized that I was heading down a rabbit hole. I took a step back and determined that trying to solve this problem in a fully automated way was not worth my time. It would have been time consuming, and it would have only made my problems worse; I still would have had to deal with names that should be merged but weren’t (since no scheme could perfectly determine if two names represent the same customer), but I also would have had to worry about names that shouldn’t have been merged but were. Why would I devote time to building a complex matching algorithm that doubled the number of problems I had to deal with? Instead, I created an alias table in HBase. The key is the customer name, with white-space removed and letters lower-cased to catch the easiest cases. One of the columns contains a UUID that is used as the key for that customer throughout the rest of the system. When my transform scripts move data from the raw table to the table where each row is a customer, they use the alias table to determine into which row to insert the transformed data. When my code merges two customers, it merges the current data and makes all alias entries that were pointing to either row point to the newly merged row, so that when the scripts next load new data into the table, they put it directly into the correct location. This approach does require manual intervention (in practice, all schemes were going to), but at least it was simple. This was an important lesson from my internship; I learned that some problems aren’t worth solving. Another major issue I had to tackle was cases where data was incorrect or incomplete. Our opportunity data, for example had various fields that were not used when the sytem was first configured. For example, contract terms were always a set period of time. Some fields such as the product quantity were changed so older records had a value but in different units than the units used in new records. For this tricky data rather than simply reading the values directly I wrote helper methods to return the value, sometimes trying 4 or 5 different ways to infer the actual value. For example, the contract end date, I had to base the value on close date about half the time. In this case one available helper method returned a tuple of the value that it was trying to infer (the end date) and a Boolean value representing whether the value returned was explicitly specified or inferred from another field. In the web view I used the explicit or inferred flag to add an annotation in the UI indicating when the value was approximated. Users could then look elsewhere if precision was important. This annotation also exposes where source data is missing fields, which can help us update the source data. Businesses are always changing, so the data that a business decided to keep track of two years ago may not be the same data that makes sense to keep track of today. Many of the values I was trying to infer from data were from fields that we added to our system. The old data still lacked meaningful values because no one had gone back and fixed all of the past data. Each data source having a “name” field seems great until you realize that none of the names are the same, and having a field representing the exact information you want is great until you realize that it’s null in 40% of cases. Another challenge was that I needed to interact with many different APIs, each with their own quirks, and similarly, I had to use various libraries to parse the different kinds of data and handle the transformations. I always expected APIs and libraries to be perfectly documented and to be designed with my usecase in mind. This was rarely the reality, so these tasks frequently took much longer than I expected. I knew from past experience that building good software always takes longer than planned, but it seemed even more true this summer. I realized that one factor contributing to this was that my whole project was centered around touching as many different “things” as possible. Each time I integrated with a new library, API, or existing codebase, there was always an additional cost of figuring out how to approach it. Additionally, there was always a chance that some aspect of the new process would not work as advertised or not provide a direct, optimized way for me to do what I needed. Significantly underestimating the difficulty of every single piece of my project helped me improve my ability to make those estimations. Overall, my internship at Cloudera was amazing. I got to work with very smart people building and shipping high quality software using HBase. I got to sit next to, eat lunch with, and hear internal talks by people working on an array of fascinating things, happy to share knowledge and advice. I saw how a software company operates and caught glimpses into how diverse companies—Cloudera’s customers—operate. At the beginning of my summer I’d thought my project was going to be a series of unexciting tasks, many of which I’ve done before. As it turned out I encountered some very interesting problems and a lot of good lessons to carry forward. Find available opportunities via theCloudera Careers web page.</snippet></document><document id="507"><title>Hadoop World 2011: A Glimpse into Business Solutions</title><url>http://blog.cloudera.com/blog/2011/09/hadoop-world-2011-a-glimpse-into-business-solutions/</url><snippet>Business Solutions is a Hadoop World 2011 track geared towards business strategists and decision makers. Sessions in this track focus on the motivations behind the rapidly increasing adoption of Apache Hadoop across a variety of industries. Speakers will present innovative Hadoop use cases and uncover how the technology fits into their existing data management environments. Attendees will learn how to leverage Hadoop to improve their own infrastructures and profit from increasing opportunities presented from using all of their data. Preview of Business Solutions Track Sessions Advancing Disney’s Data Infrastructure with Hadoop Matt Estes, Disney Connected and Advanced Technologies Gain insight into how Disney integrated Hadoop into their data infrastructure to solve some challenging use cases at Disney, ABC, and ESPN, while also considering cost effectiveness, scalability, data availability and providing Disney entities with new data driven business to consumer opportunities. From Big Data to Lives Saved: HBase in HeathCare Doug Meil, Explorys Learn how Explorys built a healthcare platform based on a massively parallel computing model centered around Hadoop and HBase that spans billions of anonymized clinical records. Subscribers can search and analyze patient populations, treatment protocols, and clinical outcomes with this uniquely powerful solution for accelerating lifesaving discovery. The Blind Men and the Elephant Matthew Aslett, The 451 Group Learn from The 451 Group’s latest research as to where Hadoop fits in the total data management landscape. This session will provide the insight into who is contributing to the Hadoop ecosystem, the vendors providing Hadoop solutions, how the expanding ecosystem is affecting the Apache Hadoop project, and what alternatives there are to Hadoop. Using Hadoop to Change Company Culture Amy O’Connor, Nokia In this talk, Amy O’Conner will highlight the journey Nokia is taking to evolve its culture from a manufacturing model to a data driven model. Learn how Nokia is building a platform for cultural evolution on top of Hadoop, how they administrate their data and how the company conducts the analysis that is enabling Nokia to compete—with data. More sessions in the Business Solutions track.. The Hadoop World agenda continues to grow and more breakout sessions are added daily. Remember to check regularly for updates and get registered for Hadoop World 2011.</snippet></document><document id="508"><title>Apache Hadoop for Archiving Email</title><url>http://blog.cloudera.com/blog/2011/09/hadoop-for-archiving-email/</url><snippet>This post will explore a specific use case for Apache Hadoop, one that is not commonly recognized, but is gaining interest behind the scenes. It has to do with converting, storing, and searching email messages using the Hadoop platform for archival purposes. Most of us in IT/Datacenters know the challenges behind storing years of corporate mailboxes and providing an interface for users to search them as necessary. �The sheer volume of messages, the content structure and its complexity, the migration processes, and the need to provide timely search results stand out as key points that must be addressed before embarking on an actual implementation. �For example, in some organizations all email messages are stored in production servers; others just create a backup dump and store them in tapes; and some organizations have proper archival processes that include search features. Regardless of the situation, it is essential to be able to store and search emails because of the critical information they hold as well as for legal compliance, investigation, etc. That said, let�s look at how Hadoop could help make this process somewhat simple, cost effective, manageable, and scalable. Big files are ideal for Hadoop. It can store them, provide fault tolerance, and process them in parallel when needed. As such, the first step in the journey to an email archival solution is to convert your email messages to large files. In this case we will convert them to flat files called sequence files, composed of binary key-value pairs. �One way to accomplish this is to: Put all the individual email message files into a folder in HDFS Use something like WholeInputFileFormat/WholeFileRecordReader (as described in Tom White�s book for small file conversion) to read the contents of the file as its value, and the file name as the key (see Figure 1. Message Files to Sequence File), Use IdentityReducer (outputs input values directly to output) to write it into sequence files. (Note: Depending on where your data is and bandwidth bottlenecks, simply spawning standalone JVMS to create sequence files directly in HDFS could also be an option. ) If you are dealing with millions of files, one way of sharing (partitioning them) would be to create sequence files by day/week/month, depending on how many email messages there are in your organization. This will limit the number of message files you need to put into HDFS to something that is more suitable, 1-2 million at a time given the NameNode memory footprint of each file. Once in HDFS, and converted into sequence files, you can delete the original files and proceed to the next batch.� Here is what a very basic map method in a Mapper class could look like; all it does is emit the filename as key and binary bytes as value. 1 public void map(NullWritable key, BytesWritable value,
       OutputCollector output, Reporter reporter)
       throws IOException {
      String filename = conf.get("map.input.file");
      output.collect(new Text(filename), value);
    }
 } Here is what the main driver looks like, 2 3 4 5 @Override
public int run(String[] args) throws IOException {
JobConf conf = new (SequenceFileCreator.class );
FileInputFormat.setInputPaths(jobConf,new Path(args[0]));
FileOutputFormat.setOutputPath(jobConf, new Path(args[1]));
conf.setInputFormat(WholeFileInputFormat.class);
 conf.setOutputFormat(SequenceFileOutputFormat.class);
//You could also compress the key value pairs using the following:
//SequenceFileOutputFormat.setOutputCompressionType(conf, //CompressionType.BLOCK);
//SequenceFileOutputFormat.setCompressOutput(conf, true);
//SequenceFileOutputFormat.setOutputCompressorClass(conf, GzipCodec.class); 

    conf.setOutputKeyClass(Text.class);
    conf.setOutputValueClass(BytesWritable.class);
    conf.setMapperClass(Mapper.class);
    conf.setReducerClass(IdentityReducer.class);
    JobClient.runJob(conf);
    return 0;
  } Code Walkthrough: The mapper emits the filename as the key and the file content BytesWritable as value. Sets the input path (where message files are stored) and output path (where output sequence file will be saved). WholeFileInputFormat is used as input format, which in turn uses WholeFileRecordReader to read the file content in its entirety. The content of the file gets sent to mapper as the value. This block is to enable compression. Using gzip, you could get a 10:1 ratio but the file would have to be processed as a whole. With lzo the compression is about 4:1 but files can be split and processed in parallel. This is where we set the output key for mapper as Text, output value for mapper as BytesWritable, the mapper class that we created and the reducer class as IdentityReducer. Since we do not need to sort, IdentityReducer works well in this case. Figure 1: Message Files to Sequence File With compression turned on, you can get I/O as well as storage benefits. So far, we have taken our message files and converted them into sequence files, taking care of the conversion as well as storage portion. �In terms of searching those files, if the email messages are Outlook messages, we can use Apache POI for Microsoft Documents to parse them to Java Objects, and search the contents of the messages to output the results, as needed. A quick example of code to perform the search and output the results looks like this, with the mapper first, followed by driver class: 1 2 3 public void map(Text key, BytesWritable value, OutputCollector output, Reporter reporter) throws IOException {
	String keyText = key.toString(); 		 	InputStream input = new
 	ByteArrayInputStream(value.getBytes());
 	MAPIMessage msg = new MAPIMessage(input);
	try {
		if (msg.getRecipientEmailAddress().contains("sales")) {
			//write the file directly to local file system
			//NFS mount if you want
			String fileName = keyText.toString();
			FileOutputStream fos = new
      			  FileOutputStream("/tmp/" + fileName);
			fos.write(value.getBytes());
	 		fos.close();
		}
	} catch (ChunkNotFoundException e) {
		e.printStackTrace();
	} catch (IOException ioe) {
		ioe.printStackTrace();
	}
} 	 		 	

public int run(String[] args) throws Exception {
	JobConf conf = new JobConf(SeqSearch.class); 		conf.setJobName("SearchEmails"); 		conf.setMapperClass(Map.class); 		conf.setInputFormat(SequenceFileInputFormat.class); 		conf.setOutputFormat(NullOutputFormat.class);
	FileInputFormat.setInputPaths(conf, new Path(args[0]));
	JobClient.runJob(conf);
	return 0;
} Code Walkthrough: BytesWritable comes in to the Mapper as the value from the sequence file, and is converted to MAPIMessage from Apache POI for MS Documents. The actual search is performed here. �In this example, it just searches for emails that contain �sales� in recipient email addresses, all of the sample emails have sales in recipient address so the results should have them all back. Of course, this would be where the bulk of the logic would go should one require extensive search features including parsing through various attachments. �In this example we write the results — the complete email message — directly to the Local Filesystem so that it can be viewed via messaging applications such as Outlook. This is the job configuration where the input format is set to SequenceFileInputFormat created above. In this case, since there is no mapper output we set the output format to NullOutputFormat. In this example, we write the complete email out to Local Filesystem one at a time. As an alternative, we could pass the results to reducers and write all the results as Text into HDFS as well. This would depend on what the need is. In this post I have described the conversion of email files into sequence files and store them using HDFS. I have looked at how to search through them to output results. Given the �simply add a node� scalability feature of Hadoop, it is very straightforward to add more storage as well as search capacity. Furthermore, given that Hadoop clusters are built using commodity hardware, that the software itself is open source, and that the framework makes it simple to implement specific use cases. This leads to an overall solution that is very cost effective compared to a number of existing software products that provide similar capabilities. The search portion of the solution, however, is very rudimentary. In part 2, I will look at using Lucene/Solr for indexing and searching in a more standard and robust way. (You can get a complete code sample at: https://github.com/cloudera/emailarchive)</snippet></document><document id="509"><title>Hadoop World 2011: A Glimpse of the Applications Track</title><url>http://blog.cloudera.com/blog/2011/09/a-glance-at-the-hadoop-world-2011-applications-track/</url><snippet>The Hadoop World train is approaching the station! Remember to mark November 8th and 9th in your calendars for Hadoop World 2011 in New York City. The Hadoop World agenda is beginning to take shape. View all scheduled sessions at hadoopworld.com/sessions, and check back regularly for updates. Hadoop World 2011 will feature five tracks to run in parallel across two days. The tracks and their intended audiences are Track 1: Business Solutions (Business Strategists and Decisions Makers) Track 2: Enterprise Architecture (Enterprise Architects) Track 3: Operations (IT/Operations Managers and Practitioners) Track 4: Applications (Data Scientists) Track 5: Development (Developers) In this blog post we provide a glimpse into several of the Applications track breakout sessions. Keep an eye out for future blog posts highlighting Hadoop World 2011 breakout tracks and their corresponding sessions. Preview of Applications Track Sessions Building a Model of Organic Link Traffic Michael Dewar, Data Scientist, bitly Gain insight to how bitly uses Python/Numpy, streaming Hadoop and machine learning to create a model of organic traffic patterns based on bitly’s click logs. LinkedIn: Products You May Like Abhishek Gupta, Software Engineer, Recommendation Engine, LinkedIn Learn the platform behind LinkedIn’s recommendation engines powered by Hadoop to help users find jobs, meet like-minded professionals, discover relevant professional groups or news articles and help organizations find similar profiles and utilize enhanced ad targeting. Leveraging Big Data in the Fight Against Spam and Other Security Threats Wade Chambers, Executive Vice President, Development/Operations, Proofpoint This presentation will explain radical new “spam anomalytics” techniques whereby billions of messages and message-related events are analyzed daily to find statistical norms (and consequently, the deviation from the norm) to detect and defend against new email threats as they merge. The war on spam. Data Mining in Hadoop, Making Sense Of It In Mahout! Michael Cutler, British Sky Broadcasting Michael will present a selection of methodologies, primarily using Mahout, that enable you to derive real insight into your data (mined in Hadoop) and build a recommendation engine focused on the implicit data collected from users. The Powerful Marriage of R and Hadoop David Champagne, CTO, Revolution Analytics Get an overview of R (a popular open source programming language), the ways in which R has been integrated with Hadoop, real-world use cases and a look at how enterprises can take advantage of both of these industry-leading technologies. More sessions in the Applications track.. The Hadoop World agenda continues to grow and more breakout sessions are added daily. Remember to check regularly for updates and get registered for Hadoop World 2011.</snippet></document><document id="510"><title>Apache Hadoop Applied</title><url>http://blog.cloudera.com/blog/2011/09/hadoop-applied/</url><snippet>BusinessWeek recently published a fascinating article on Apache Hadoop and Big Data, interviewing several Cloudera customers as well as our CEO Mike Olson. One of the things that has consistently exceeded our expectations is the diversity of industries that are adopting Hadoop to solve impressive business challenges and create real value for their organizations. Two distinct use cases that Hadoop is used to tackle have emerged across these industries. Though these have different names in each industry, the mechanics have clear parallels that cross domains. Data Processing: Data Processing is Hadoop�s original use case. By scaling out the amount of data that users could store and access in a single system then distributing the document and log processing used to index, and extract patterns from this data, Hadoop made a direct impact on the web and online advertising industries early on. Today, data processing means more than sessionization of click stream data, index construction or attribution for advertising. Hadoop is used to process data by commerce, media and telecommunications companies in order to measure engagement, and handle complex mediation. Retail and financial institutions use Hadoop to understand customer preferences, better target prices and reconcile trades. Most recently we�re seeing Hadoop used for time series and signal processing in the energy sector and genome mapping and alignment among life sciences organizations. Advanced Analytics: Today, Hadoop is not only used for data processing, but also advanced analytics. A slightly ambiguous term, we�ve found organizations speaking of advanced analytics as a way of referring to the types of analytics that are challenging or impossible using tools that are optimized for relational analysis. These new challenges include social network analysis and smarter targeting by web and advertising companies, content optimization by a wide variety of publishers and network analysis at media and telecommunications companies. Our retail customers look at the effectiveness of loyalty programs well beyond the classic market basket analysis as customer engagement crosses online and real world interactions. Financial institutions are able to take a deeper look at complex fraud and risk among their customers and throughout their internal systems. Both entity analysis and analysis based on next generation sequencing have recently been made possible using native Hadoop libraries. We�re excited by Hadoop�s success, emerging as a powerful platform for such a diverse number of applications across a wide variety of industries. Here at Cloudera we�re also honored to have been able to work closely with leading companies in each of these industries and help tackle the type of challenges that are driving new business and creating real value for our customers.</snippet></document><document id="511"><title>Hadoop Tuesdays: Get a Handle on Unstructured Data with a 7-part Webinar Series Led By Cloudera and Informatica</title><url>http://blog.cloudera.com/blog/2011/09/hadoop-tuesdays-get-a-handle-on-unstructured-data-with-a-7-part-webinar-series-led-by-cloudera-and-informatica/</url><snippet>Unstructured data is the fastest growing type of data generated today. The growth rate of text, documents, images, and clickstream data is incredible. Expand the scope of view to include machine generated data such as telemetry, location, network and weather sources and that growth rate is unnerving. An inspiring population of customers have come to recognize that the Apache Hadoop data management platform is, in many ways, uniquely equipped to handle the volume, variety and velocity of unstructured data being generated within their businesses. But where does someone new to the conversation get started? How do you know if you are ready to explore Apache Hadoop? What are the products and techniques available to make Apache Hadoop more familiar and accessible? Do you have a roadmap? Do you want to? Cloudera and Informatica have real-world experience in deploying Hadoop to solve the challenges associated with storage and processing of unstructured data. We’ve gathering the top “big data” and Apache Hadoop experts to jointly produce a 7-part webinar series called “Hadoop Tuesdays” — the goal is to provide expert advice and insight to those who are interested in forming a solid roadmap to Hadoop deployment, regardless of their experience with Hadoop. If you’re interested in learning more about what Hadoop is, the popular use cases driving Hadoop adoption and the benefits your business can expect with a Hadoop deployment, these webinars will serve as a great introduction.  And if you have goals for storing, processing and extracting value from unstructured data or you wish to form a roadmap for adding Hadoop to your environment, you’ll learn how to evaluate Hadoop and fit it into your information architecture. No matter your familiarity with Hadoop or where you are in the deployment process, I encourage you to check out the webinars — the full schedule is below.  In addition, we’ll have a live Q&amp;A with Hadoop experts including customers, analysts and systems integrators, supported by leaders from Informatica and Cloudera. Hadoop Tuesdays 7-Part Webinar Series (all webinars start at 1 pm ET; 10 am PT) September 27: Rethinking Your Data Strategy in the Era of Big Data and Hadoop | with James Kobielus, Forrester Research October 11: Where Does Hadoop Fit in Your Vision of  “Data as a Platform” Architecture | with John Akred, Accenture October 18: The Hadoop Ecosystem | with Matt Aslett, The 451 Group October 25: Hadoop User Case Studies | with David Menninger, Ventana Research and joint Informatica and Cloudera customers November 15: Operationalizing Hadoop: The Journey to Production | with Omer Trajman, Cloudera November 29: Agile Data Integration and Hadoop | with David Linthicum, Blue Mountain Labs December 13: Getting Started with Hadoop | with Charles Zedlewski, Cloudera and Wei Zheng, Informatica CLICK HERE TO REGISTER One more thing: We’ll be kicking the Series off with an informal “TweetJam” on Twitter on September 22nd, beginning at Noon Eastern/9am Pacific — ask questions and join in by using the hashtags #Hadoop or #infatj.</snippet></document><document id="512"><title>Snappy and Hadoop</title><url>http://blog.cloudera.com/blog/2011/09/snappy-and-hadoop/</url><snippet>Snappy is a compression library developed at Google, and, like many technologies that come from Google, Snappy was designed to be fast. The trade off is that the compression ratio is not as high as other compression libraries. From the Snappy homepage: … compared to the fastest mode of zlib, Snappy is an order of magnitude faster for most inputs, but the resulting compressed files are anywhere from 20% to 100% bigger. Snappy is related to the Lempel–Ziv family of compression algorithms, which includes well-known compression algorithms such as LZO, however Snappy offers two clear benefits over LZO in the context of Apache Hadoop. First, Snappy is significantly faster than LZO for decompression, and comparable for compression, meaning the total round-trip time is superior. Second, Snappy is BSD-licensed, which means that it can be shipped with Hadoop, unlike LZO which is GPL-licensed, and therefore has to be downloaded and installed separately since it may not be included in Apache products. Why is Snappy useful for Hadoop? Many Hadoop clusters use LZO compression for intermediate MapReduce output. This output, which is never seen by the user, is always written to disk by the mappers, and then accessed across the network by reducers. It is a prime candidate for compression since it tends to be compressible (there is some redundancy in the key space, since the map outputs are sorted), and because writing to disk is slow it pays to perform some light compression to reduce the number of bytes written (and later read). Snappy and LZO are not CPU intensive, which is important, as other map and reduce processes running at the same time will not be deprived of CPU time. In testing, we have seen that the performance of Snappy is generally comparable to LZO, with up to a 20% improvement in overall job time in some cases. This use alone justifies installing Snappy, but there are other places Snappy can be used within Hadoop applications. For example, Snappy can be used for block compression in all the commonly-used Hadoop file formats, including Sequence Files, Avro Data Files, and HBase tables. One thing to note is that Snappy is intended to be used with a container format, like Sequence Files or Avro Data Files, rather than being used directly on plain text, for example, since the latter is not splittable and can’t be processed in parallel using MapReduce. This is different to LZO, where is is possible to index LZO compressed files to determine split points so that LZO files can be processed efficiently in subsequent processing. How to use Snappy with Hadoop CDH3 Update 1 includes support for Snappy in Hadoop, HBase, Avro, Sqoop and Flume. Installation and configuration is covered in the CDH3 documentation. Snappy support was added to Hadoop in HADOOP-7206, which will be available in the forthcoming 0.23.0 Apache release. Enabling map output compression is as simple as adding the following to mapred-site.xml: &lt;property&gt;
 &lt;name&gt;mapred.compress.map.output&lt;/name&gt;
 &lt;value&gt;true&lt;/value&gt;
 &lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;mapred.map.output.compression.codec&lt;/name&gt;
 &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;
&lt;/property&gt;
 Avro support for Snappy is available in the latest release, 1.5.4. And HBase added support in HBASE-3691, which will be available in the 0.92.0 release. Credits Like many open-source efforts, integrating Snappy with Hadoop was the work of many people. Google developed the algorithm and released the Snappy open source implementation. Issei Yoshida wrote the Snappy compression codec for Hadoop. Doug Cutting added support for Snappy to Avro, using Taro L. Saito’s snappy-java bindings. Nicholas Telford and Nichole Treadway wrote the HBase integration.  Alejandro Abdelnur, Bruno Mahé, Roman Shaposhnik and Wing Yew Poon tested the component integration. Patrick Daly and Paul Battaglia wrote the CDH3 documentation.</snippet></document><document id="513"><title>Cloudera Training for Apache Hadoop and Certification at Hadoop World</title><url>http://blog.cloudera.com/blog/2011/09/cloudera-training-for-apache-hadoop-and-certification-at-hadoop-world/</url><snippet>Make the most of your week in New York City by combining the Hadoop World 2011 conference with training classes that give you essential experience with Hadoop and related technologies. For those who are Hadoop proficient, we have a number of certification exam time slots available for you to become Cloudera Certified for Apache Hadoop. All classes and exams begin November 7th, the Monday before the Hadoop World conference. Please note that while the certification exams will be held at the same location as Hadoop World, the training classes are held at MicroTek: MicroTek, 90 Broad St., 11th Floor, New York, New York, 10004 Please see below for descriptions of the training classes and exams: Certification Exams The Cloudera Certified Developer Exam lasts 90 minutes and contains 60 multiple-choice questions, while the Cloudera Certified Administrator Exam lasts 60 minutes and contains 30 multiple-choice questions. Please note that you must bring your own laptop that is capable of connecting to a WiFi network. Location of All Exams: Sheraton New York Hotel &amp; Towers 811 7th Ave On 53rd Street New York, New York, 10019 Cloudera Certified Administrator Exam November 7 Areas of knowledge tested: Hadoop Cluster Overview Hadoop Cluster Planning Hadoop Cluster Management Job Scheduling Monitoring and Logging Learn more.. Cloudera Certified Developer Exam November 7 Areas of knowledge tested: Computing Environment The Hadoop Distributed File System MapReduce The Hadoop API The Hadoop Ecosystem Learn more.. Training Classes Location of All Classes: MicroTek 90 Broad St.,11th Floor New York, New York, 10004 Cloudera Essentials for Managers November 7 Learn when using Hadoop is appropriate, what Hadoop is being used for, how Hadoop fits into your existing environment, and what you need to know about choosing Hadoop. This class will introduce Hadoop use cases, provide an overview of the Hadoop platform, provide the information to knowledgably manage a Hadoop cluster, and explain the Apache open-source model along with Cloudera�s role.��Learn more.. Cloudera Administrator Training for Apache Hadoop + Certification November 7, 10 &amp; 11 This is a three-day hands-on course for system administrators and others responsible for managing Apache Hadoop clusters in production or development environments. You will be presented with all the necessary information to knowledgably configure, maintain, monitor, and manage a Hadoop cluster in production. Following the training, attendees will have an opportunity to become a Cloudera Certified Administrator for Apache Hadoop.��Learn more.. Cloudera Developer Training for Apache Hadoop + Certification November 7, 10 &amp; 11 This is a three-day training course for developers who want to use Apache Hadoop to build powerful data processing applications. You will learn everything you need to know to efficiently write MapReduce jobs in Java and other programming languages, to implement common algorithms in Hadoop, to debug MapReduce code, to leverage Hadoop-related projects, and to understand advanced Hadoop API topics required for real-world analysis. Following the training, attendees will have an opportunity to become a Cloudera Certified Developer for Apache Hadoop.��Learn more.. Cloudera�s Introduction to Hadoop combined with Cloudera Training for Apache HBase November 7, 10 &amp; 11 The first day of this three-day course is an introduction to Hadoop. You will learn the motivation for using Hadoop, Hadoop basics, the Hadoop ecosystem, Hadoop workflow integration, and problem solving with Hadoop. Then following Hadoop World the course continues with a deep dive into Apache HBase. HBase allows you to host very large tables atop clusters of commodity hardware.��Learn more.. Cloudera�s Introduction to Hadoop combined with Cloudera Training for Apache Hive and Pig November 7, 10 &amp; 11 This course is also structured as a three-day course where the first day is an introduction to Hadoop. In the days following Hadoop World the course resumes, delving into Apache Hive and Pig. Hive makes Hadoop accessible to users who already know SQL; Pig is similar to popular scripting languages. Learn more..</snippet></document><document id="514"><title>Which free book will you choose at Hadoop World? Hadoop or HBase?</title><url>http://blog.cloudera.com/blog/2011/09/which-free-book-will-you-choose-at-hadoop-world-hadoop-or-hbase/</url><snippet>Attendees of Hadoop World will receive a free copy of either Hadoop, The Definitive Guide (2nd edition) or the recently published HBase, The Definitive Guide. Take advantage of the Early Bird price which has been extended to Friday, September 9, 2011. Remember to select your choice when you register.</snippet></document><document id="515"><title>Top 10 Reasons to Attend Hadoop World 2011</title><url>http://blog.cloudera.com/blog/2011/08/top-10-reasons-to-attend-hadoop-world-2011/</url><snippet>The 3rd annual Hadoop World conference takes place on November 8th and 9th in New York City. Cloudera invites you to the largest gathering of Hadoop practitioners, developers, business executives, industry luminaries and innovative companies in the Hadoop ecosystem. 1. What Can Apache Hadoop Do For You? Whether you’re a novice just getting started with Hadoop or an expert looking to advance your skills, Hadoop World is the place to gain a thorough understanding of how Hadoop can solve big business challenges, help you gain a competitive edge, accelerate discovery and more. Learn the basics or gain proficiency in newly released projects. 2. Sharing with Peers Discover how Hadoop users are solving real-world business problems and creating new ways to implement Hadoop within enterprise environments. 3. Best Practices Build or expand your best practices foundation for Hadoop development, deployment and management throughout the Hadoop life-cycle. Check out the some of the sessions we’ve lined up. 4. Dive Right In Expand your knowledge base by jumping into technical deep dives with the most advanced Apache Hadoop experts. 5. Need some Pig, Hive or maybe a Flume? Learn how to put the full Hadoop ecosystem into play by finding out more about Hadoop projects including Apache Flume, Apache Hive, Apache Pig and more. 6. Network with Hadoop Experts Expand your professional network at the largest gathering of Hadoop practitioners, developers, business executives, industry luminaries and innovative companies in the Hadoop ecosystem. 7. Go to the Head of the Class Advance your formal Apache Hadoop education with Cloudera training classes for managers, data scientists, developers and system administrators. 8. Get Certified Add to your credentials by completing the Cloudera Certification for Apache Hadoop – Administrator and Developer. 9. Meet Doug Meet the man himself: Doug Cutting, a founder of the Apache Hadoop project. 10. Hot Industry Resources and…Cool Swag Don’t let your Doug Cutting bobblehead get lonely. Hadoop World will offer industry items and resources, including a free copy of either Hadoop: The Definitive Guide, Second Edition, by Tom White, or HBase: The Definitive Guide, by Lars George.</snippet></document><document id="516"><title>Automatically Documenting Apache Hadoop Configuration</title><url>http://blog.cloudera.com/blog/2011/08/automatically-documenting-apache-hadoop-configuration/</url><snippet>Ari Rabkin is a summer intern at Cloudera, working with the engineering team to help make Hadoop more usable and simpler to configure. The rest of the year, Ari is a PhD student at UC Berkeley. He’s applying the results of recent research to automatically find and document configuration options for Hadoop. Background Hadoop has a key-value style of configuration, where each configuration option has a name and a value. There is no central list of options, and it’s easy for developers to add new configuration options as needed. Unfortunately, this opens the way for bugs and for erroneous documentation. Not all documented options exist and many options are undocumented. Options can have different default values in the code and the configuration files. (In which case, the config-file default will win.) Without automated tools, configuration mistakes can be quite hard to track down. Imagine the user who tries to set dfs.datanode.max.xcievers, but accidentally types xceivers — a mistake easy for machines to diagnose, but hard for humans to spot. This is a particularly menacing case, both because the option name is peculiarly spelled, and because accidentally misspelling it will not result in any immediate overt failure. Instead, it will result in the cluster using the default value, resulting in impaired performance and reliability that may be challenging to track down. As I’ll show in the rest of this post, we’re using advanced techniques to help fix this problem, for both developers and users of the Hadoop platform Applying cutting-edge techniques One way to find out which options Hadoop really supports would be to modify the code to log whenever an option is loaded, and then to run Hadoop in many environments with many configurations. Unfortunately, many options are only used in special circumstances, making it hard to get good coverage. We took a different approach, using static program analysis. In this approach, we automatically analyze the compiled bytecode. Doing static analysis on a program the size and complexity of Hadoop is not at all a standard thing to do. Reflection, remote procedure calls, and dynamic classloading make it quite challenging, technically. It does work decently well, however, thanks to years of research on analysis of reflection-heavy Java programs. Results Here is a table, for Apache Hadoop 0.20.2, of which options are read, where they are read, whether they are documented, and what the default values are, both in configuration files and in the code. Here is another such table, for Cloudera’s CDH3u0. And here’s the current development trunk. For CDH3, we supplemented the static analysis approach with dynamic instrumentation. This both helps gauge accuracy and also supplies more precise information about which Hadoop components use which options. It’s interesting to compare the total numbers of options in each. Apache Hadoop 0.20.2 had about 300 options. CDH3 boosted this to over 500, primarily by incorporating many new security-related options. Apache trunk is somewhere in between, with around 430. Again, security is a big part of the difference, since it wasn’t in Apache Hadoop 0.20.2, but will be in the next Apache release. Why it matters There are several ways that Cloudera is using the results of this analysis to improve Hadoop and to help customers. Improving the platform We used the results of this analysis to find undocumented options that now have been described for users. We also noticed that some undocumented options shouldn’t be there at all — those options had been renamed, and the previous names should have gone away. We fixed this one too. Previous versions of Hadoop sometimes included documentation for options that weren’t there anymore — that had been renamed or outright removed. Happily, we found no such options in Trunk. But we’ll keep an eye open for them in the future. Guiding Cloudera Enterprise development The option listings, linked-to above, have been used by the Cloudera Service and Configuration Manager (SCM) developers, to check that SCM correctly handles all the known options in Hadoop. By pinpointing where in the code an option is read and which daemons may read it, this analysis helps SCM avoid setting MapReduce options on HDFS-only nodes, and similar mistakes. Debugging user configurations We also use these configuration option listings to help diagnose user problems. The results of the analysis can be thought of as a “spell check dictionary” for configuration options. If a user sends us a configuration file, we can run it through our configuration spellchecker, and verify that users haven’t misspelled an option name. Why so many undocumented options? The bottom-line figures show that Hadoop has many undocumented options — often over 100. This may be surprising, but there are good reasons for it. Many Hadoop options are only there for the benefit of unit tests. For example, there’s no reason why users would want to configure dfs.client.block.write.locateFollowingBlock.retries, but unit tests need to set it to drive Hadoop into otherwise-rare corner cases. Some options are deprecated. For example, dfs.network.script is no longer a supported option, but the code checks if users set it, in order to print a deprecation warning. There’s no reason to document the old stale name. Some options might not be needed. Developers often use options as a sort of configurable named constant. Most complex programs, including Hadoop, are full of parameters that are theoretically tunable but where developers have no insight as to what the right value is. For example, the operating system offers many options controlling the details of TCP connection management. In most cases, there is no reason to prefer one value over another. Offering this control to users would add more complexity and confusion than benefit; hence, the option is not documented. But it is conceivable that the configurability might be useful in some circumstance. Rather than compile in a default, developers use a configuration option. This way, in the event of a production failure, engineers reading the code can find and tune the options, without recompiling and redeploying. Limitations The data is autogenerated, and will have mistakes. In particular, sometimes the analysis can’t tell that a particular method will be called by user code, so sometimes the point where the option is read won’t get analyzed. This results in options sometimes being missed. Another limitation is that the analysis is sometimes severely over-inclusive. In particular, network protocols confuse it. Suppose you have a DataNode that’s listening for instructions from the NameNode. The NameNode will only send certain things over the wire. But the analysis doesn’t know the limitations of the protocol. So it assumes, pessimistically, that anything could come back, including messages from the MapReduce code. As a result, sometimes the analysis will say that something can be reached from the DataNode, when it really can’t. The root cause here is that the analysis can’t exclude things that the type system allows, but that can’t show up from any actual Hadoop server. Conclusions Despite its imprecision, these results have been useful here in practice. Results are being used to improve the development of the Hadoop platform, improve the configuration management in Cloudera Enterprise tools, and to help answer user questions. It’s been a personal thrill being able to take my academic research and put it in the hands of Cloudera engineers. Cloudera has been very supportive. For more information The static analysis is built around the open-source JChord analysis tool developed by Mayur Naik et al.. An academic paper describing and evaluating the approach is Rabkin and Katz, “Static Extraction of Program Configuration Options“, presented a few months ago at ICSE 2011, the International Conference on Software Engineering, in Honolulu.</snippet></document><document id="517"><title>CDH3 Update 1 Released</title><url>http://blog.cloudera.com/blog/2011/07/cdh3u1-released/</url><snippet>Continuing with our practice from Cloudera’s Distribution Including Apache Hadoop v2 (CDH2), our goal is to provide regular (quarterly), predictable updates to the generally available release of our open source distribution.  For CDH3 the first such update is available today, approximately 3 months from when CDH3 went GA. For those of you who are recent Cloudera users, here is a refresh on our update policy: We will only include patches in updates that are non-compatibility breaking. We will only include patches in updates that are non-disruptive. You can skip updates without penalty – i.e., if you don’t find the contents of an update compelling, you can skip it and wait for a future update without having to do a delta upgrade. There is one new addition to our update policy going forward: when it’s possible to pull features from our CDH4 roadmap into CDH3 updates in a non-disruptive way, we’ll take advantage of that opportunity. With all that said, there are a number of improvements coming to CDH3 with update 1.  Among them are: New features – integrated Apache-compatible licensed fast compression throughout CDH, web shell for Hue, Flume / HBase integration, Fair Scheduler ACL’s, improved datanode handling of hard drive failures, and email actions and date formatting for Oozie. Improvements (stability and performance) – HBase bulk loading, Namenode stability, Fuse-DFS (mountable HDFS). New component versions – Hive 0.7.1, Pig 0.8.1, Hbase 0.90.3, Flume 0.9.4 and Sqoop 1.3. Bug fixes – 80+ bug fixes.  Per our standard practice, the enumerated fixes and their corresponding Apache project jiras are provided in the release notes. Update 1 is available in all the usual formats (RHEL, SLES, Ubuntu, Debian packages, tarballs, and SCM Express).  Check out the installation docs for instructions. If you’re running components from the Cloudera Management Suite they will not be impacted by moving to update 1. The next update (update 2) for CDH3 is planned for mid-October. Thank you for supporting Apache Hadoop and thank you for supporting Cloudera.</snippet></document><document id="518"><title>Hoop – Hadoop HDFS over HTTP</title><url>http://blog.cloudera.com/blog/2011/07/hoop-hadoop-hdfs-over-http/</url><snippet>What is Hoop? Hoop provides access to all Hadoop Distributed File System (HDFS) operations (read and write) over HTTP/S. Hoop can be used to: Access HDFS using HTTP REST. Transfer data between clusters running different versions of Hadoop (thereby overcoming RPC versioning issues). Access data in a HDFS cluster behind a firewall. The Hoop server acts as a gateway and is the only system that is allowed to go through the firewall. Hoop has a Hoop client and a Hoop server component: The Hoop server component is a REST HTTP gateway to HDFS supporting all file system operations. It can be accessed using standard HTTP tools (i.e. curl and wget), HTTP libraries from different programing languages (i.e. Perl, JavaScript) as well as using the Hoop client. The Hoop server component is a standard Java web-application and it has been implemented using Jersey (JAX-RS). The Hoop client component is an implementation of Hadoop FileSystem client that allows using the familiar Hadoop filesystem API to access HDFS data through a Hoop server. Hoop and Hadoop HDFS Proxy Hoop server is a full rewrite of Hadoop HDFS Proxy. Although it is similar to Hadoop HDFS Proxy (runs in a servlet-container, provides a REST API, pluggable authentication and authorization), Hoop server improves many of Hadoop HDFS Proxy shortcomings by providing: Support for all HDFS operations (read, write, status). Cleaner HTTP REST API. JSON format for status data (files status, operations status, error messages). Kerberos HTTP SPNEGO client/server authentication and pseudo authentication out of the box (using Alfredo). Hadoop proxy-user support. Tools such as DistCP could run on either cluster. Accessing HDFS files -via Hoop- using Unix ‘curl’ command Assuming Hoop is running on http://hoopbar:14000, the following examples show how the Unix ‘curl’ command can be used to access data in HDFS via Hoop using pseudo authentication. Getting the home directory: $ curl -i "http://hoopbar:14000?op=homedir&amp;user.name=babu"
HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
{"homeDir":"http:\/\/hoopbar:14000\/user\/babu"}
$ Reading a file: $ curl -i "http://hoopbar:14000?/user/babu/hello.txt&amp;user.name=babu"
HTTP/1.1 200 OK
Content-Type: application/octet-stream
Transfer-Encoding: chunked
Hello World!
$ Writing a file: $ curl -i -X POST "http://hoopbar:14000/user/babu/data.txt?op=create" --data-binary @mydata.txt --header "content-type: application/octet-stream"
HTTP/1.1 200 OK
Location: http://hoopbar:14000/user/babu/data.txt
Content-Type: application/json
Content-Length: 0
$ Listing the contents of a directory: $ curl -i "http://hoopbar:14000?/user/babu?op=list&amp;user.name=babu"
HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked

[
  {
    "path" : "http:\/\/hoopbar:14000\/user\/babu\/data.txt"
    "isDir" : false,
    "len" : 966,
    "owner" : "babu",
    "group" : "supergroup",
    "permission" : "-rw-r--r--",
    "accessTime" : 1310671662423,
    "modificationTime" : 1310671662423,
    "blockSize" : 67108864,
    "replication" : 3
  }
]
$ Click this link for more details about the Hoop HTTP REST API. Getting Hoop Hoop is distributed with an Apache License 2.0. The source code is available at http://github.com/cloudera/hoop. Instructions on how to build, install and configure Hoop server and the rest of documentation is available at http://cloudera.github.com/hoop. Contributing Hoop to Apache Hadoop The goal is to contribute Hoop to Apache Hadoop as the next generation of Hadoop HDFS proxy. We are just waiting on the Mavenization of Hadoop Common and Hadoop HDFS which will make integration easier.</snippet></document><document id="519"><title>RecordBreaker: Automatic structure for your text-formatted data</title><url>http://blog.cloudera.com/blog/2011/07/recordbreaker-automatic-structure-for-your-text-formatted-data/</url><snippet>This post was contributed by Michael Cafarella, an assistant professor of computer science at the University of Michigan. Mike’s research interests focus on databases, in particular managing Web data. Before becoming a professor, he was one of the founders of the Nutch and Hadoop projects with Doug Cutting. This first version of RecordBreaker was developed by Mike in conjunction with Cloudera. RecordBreaker is a project that automatically turns your text-formatted data (logs, sensor readings, etc) into structured data, without any need to write parsers or extractors. In particular, RecordBreaker targets Avro as its output format. The project’s goal is to dramatically reduce the time spent preparing data for analysis, enabling more time for the analysis itself. Hadoop’s HDFS is often used to store large amounts of text-formatted data: log files, sensor readings, transaction histories, etc. Much of this data is “near-structured”: the data has a format that’s obvious to a human observer, but is not made explicit in the file itself. Imagine you have a simple file listing, stored in listing.txt: 
5 mjc staff 170 Mar 14 2011 14:14 bin
5 mjc staff 170 Mar 12 2011 05:13 build
1 mjc staff 11080 Mar 14 2011 14:14 build.xml
 This “near-structured” data has metadata that is obvious to people who are familiar with file listings: a file owner, a file size, a last-modified date, etc. It’s easy for people despite the fact that certain strings, such as the date and time, cannot be parsed with simple whitespace breaks. In order for a user to process such data with MapReduce, Pig, or some similar tool, she must explicitly and laboriously reconstruct the metadata that is simple for anyone who just eyeballs the data. Performing this reconstruction usually entails writing a parser or extractor, often one based on relatively brittle regular expressions. For some very common data, writing a good parser for it is probably worthwhile. However, there are also album track listings, temperature readings, flight schedules, and many other kinds of data; the number of good parsers we need to write gets large, quickly. Writing all of these straightforward extractors, again and again, is a time-consuming and error-prone pain for everyone. We believe it is a major obstacle to faster and easier data analytics. The RecordBreaker project aims to automatically generate structure for text-embedded data. It consists of two main components. LearnStructure LearnStructure takes a text file as input and derives a parser that breaks lines of the file into typed fields. For example, the above file listing is broken into fields that include the file owner mjc, the group owner staff, etc. It emits all the schemas and code necessary to turn the raw text file into a file full of structured data. For example, we discover a JSON schema for the above file listing that looks like this: 
{
   "type" : "record",
   "name" : "record_1",
   "namespace" : "",
   "doc" : "RECORD",
   "fields" : [ {
      "name" : "base_0",
      "type" : "int",
      "doc" : "Example data: '5', '5', '1'"
   }, {
      "name" : "base_2",
      "type" : "string",
      "doc" : "Example data: 'mjc', 'mjc', 'mjc'"
   }, {
      "name" : "base_4",
      "type" : "string",
      "doc" : "Example data: 'staff', 'staff', 'staff'"
   }, {
      "name" : "base_6",
      "type" : "int",
      "doc" : "Example data: '170', '170', '11080'"
   }, {
      "name" : "base_8",
      "type" : {
         "type" : "record",
         "name" : "base_8",
         "doc" : "",
         "fields" : [ {
            "name" : "month",
            "type" : "int",
            "doc" : ""
         }, {
            "name" : "day",
            "type" : "int",
            "doc" : ""
         }, {
            "name" : "year",
            "type" : "int",
            "doc" : ""
         } ]
      },
      "doc" : "Example data: '(14, 3, 2011)', '(12, 3, 2011)', '(14, 3, 2011)'"
   }, {
      "name" : "base_10",
      "type" : {
         "type" : "record",
         "name" : "base_10",
         "doc" : "",
         "fields" : [ {
            "name" : "hrs",
            "type" : "int",
            "doc" : ""
         }, {
            "name" : "mins",
            "type" : "int",
            "doc" : ""
         }, {
            "name" : "secs",
            "type" : "int",
            "doc" : ""
         } ]
      },
      "doc" : "Example data: '(14, 14, 0)', '(5, 13, 0)', '(14, 14, 0)'"
   }, {
      "name" : "base_12",
      "type" : "string",
      "doc" : "Example data: 'bin', 'build', 'build.xml'"
   } ]
}
 Of course, the field names here are nonsense. All of the values, except for subfields of the date and timestamp records, have nondescriptive synthetically-generated names. The LearnStructure step attempts to recover the type of each field, but has no way to know its name or role. Obtaining names for these fields is the job of the SchemaDictionary step. For now, we just live with these bad synthetic names. SchemaDictionary SchemaDictionary takes data that’s been parsed by LearnStructure and applies topic-specific labels. For example, mjc should ideally be labelled as owner or perhaps user. The parsed staff data should be labelled as group. The SchemaDictionary tool matches the newly-parsed data against a known database of structures. It finds the closest match, then assigns human-understandable names based on the best-matching previously-observed dataset. For example, with the above data and a small set of known datasets, SchemaDictionary can find that base_10 should actually be timemodified, that base_8 should be datemodified, and so on. Depending on the input data and the known database of structures, this labelling may be more or less accurate. As mentioned, the target structured data format is Avro. Avro allows efficient cross-platform data serialization, similar to Thrift or Protocol Buffers. Data stored in Avro has many advantages (read Doug Cutting’s recent overview of Avro for more) and many tools either support Avro or will soon: Hadoop MapReduce, Apache Pig, and others. Related Work Our work on the LearnStructure component draws inspiration from the PADS research project (http://www.padsproj.org/index.html), in particular the paper From Dirt to Shovels: Fully Automatic Tool Generation from Ad Hoc Data, by Fisher, Walker, Zhu, and White. Published in POPL, 2008.. That paper itself draws on many papers in the area of information extraction and related fields. The authors have released code for their system, written in ML. ML is a great language, but is not well-suited to our needs: it is not supported by Avro, and is unlikely to appeal to many of the developers currently involved with the Hadoop ecosystem. SchemaDictionary is more generally inspired by database schema mapping systems. (A famous example is described in The Clio Project: Managing Heterogeneity, by Miller, Hernandez, Haas, Yan, Ho, Fagin, and Popa, published in SIGMOD Record 30(1), March 2001, pp.78-83.) Schema mapping systems are usually designed to help database administrators merge existing databases; for example, when company A purchases company B and must then merge the employee lists. These tools are often expensive and expect a lot of administrator attention. In contrast, our SchemaDictionary is for busy data analysts who simply want to check out a novel dataset as quickly as possible. It is fast and simple, but can only handle relatively simple structures (rendering it inappropriate for databases, but on target for the kind of data that is popular in text-based formats). Project RecordBreaker works, but is not complete. It is just the start of what we hope will be many interesting applications and research projects. Please take a look at the code and documentation (the repo is at https://github.com/cloudera/RecordBreaker, and the tutorial is at http://cloudera.github.com/RecordBreaker/). Maybe you can pitch in and help.</snippet></document><document id="520"><title>Evolution of Hadoop Ecosystem: AOL Advertising Experience</title><url>http://blog.cloudera.com/blog/2011/07/evolution-of-hadoop-ecosystem-aol-advertising-experience/</url><snippet>Pero works on research and development in new technologies for online advertising at Aol Advertising R&amp;D in Palo Alto. Over the past 4 years he has been the Chief Architect of R&amp;D distributed ecosystem comprising more than thousand nodes in multiple data centers. He also led large-scale contextual analysis, segmentation and machine learning efforts at AOL, Yahoo and Cadence Design Systems and published patents and research papers in these areas. A critical premise for success of online advertising networks is to successfully collect, organize, analyze and use large volumes of data for decision making. Given the nature of their online orientation and dynamics, it is critical that these processes be automated to the largest extent possible. Specifically, the success of advertising technology and its impact on revenue are directly proportional to its capability to use large amounts of data in order to compute proper impression value given the unique circumstances of ad serving events such as the characteristics of the impression, the ad, and the user as well as the content and context. As a general rule, more data results in more accurate predictions. In addition, to Optimization, Reporting and Analytics provide indispensable feedback to our internal Business and Sales teams helping us acquire new, and expand current, commitments from external customers. At AOL, we started large-scale data collection more than 4 years ago and went from using heavily sampled data sets to being able to process full serving logs. We have been using Apache Hadoop since version 0.14 as a part of an R&amp;D effort and recently moved to Cloudera CDH3 distribution. Gradually, we introduced more systems and technologies to our ecosystem around Hadoop. We chose Hadoop for several reasons: Ability to store, organize and process large data sets Great flexibility with data formats Map-reduce offers flexible data processing paradigm and works well with changing data Excellent cost-volume/price-performance point which proved very important in early proof-of-concept stages Failure built into the system via distributed computation and data redundancy Figure 1. Growth of Hadoop cluster Figure 2. Growth in sampling rate We show growth of our Hadoop clusters in Figure 1, and increase in the sampling rate in Figure 2. Between the 3rd and 4th iteration we switched to disks that are 4 times larger and we used 4-8 times more cores per node. The increase in the total number of CPUs was even more pronounced as we found we needed more processing power for newly developed processing flows. During the initial stages growing the sampling rate was the primary goal. As the number of processing pipelines increased, the output data volume increased. We’ve also added more external data flows. These two trends drove the increase in total storage space and processing power beyond full log samples between stages 4 and 5. Note that the impact of important factors like the business environment and team growth had significant impact on the pace of cluster upgrades. At the same time, we grew the ecosystem around Hadoop to encompass other infrastructure and computational components such as databases, caching and high-performance computing clusters. As our Hadoop clusters increased in size, these clusters correspondingly increased to store and process larger data sets. The main reason for the qualitative change in shifting between the 3rd and 4th iteration was the move from R&amp;D to a production environment. With involvement of additional teams we faced several challenges that Cloudera helped us with: Specifying and executing operational requirements Cluster setup Staff training Introducing other indispensable parts of Hadoop ecosystem such as robust data flows (Flume), monitoring and instrumentation Ensuring that long-term vision and execution are aligned with Hadoop roadmap The last point is especially important as we see Hadoop as an ever-evolving data processing platform. We see ourselves as a contributor and partner in this process – through the recently introduced Cloudera Customer Council we participate in discussions and working groups. For us, this is a great learning experience which simultaneously provides ample opportunities for us to contribute to an important technology that is changing the way we do business.</snippet></document><document id="521"><title>Cloudera Service and Configuration Manager Express Edition Screencast (Director’s Cut)</title><url>http://blog.cloudera.com/blog/2011/07/cloudera-service-and-configuration-manager-express-edition-screencast-directors-cut/</url><snippet>Philip Zeyliger is a software engineer at Cloudera and started the SCM project. Two weeks ago, at Hadoop Summit, we released our Service and Configuration Manager (SCM) Express. It’s a dramatically simpler and faster way to get started with Cloudera’s Distribution including Apache Hadoop (CDH). In a previous blog post, we talked in some detail about SCM Express and what it can do for you. The screencast included in this post demonstrates the simplicity of a CDH installation using SCM Express. The “Directors” conversing in the background are engineers Philip Langdale and Philip Zeyliger and VP of Products, Charles Zedlewski. Video</snippet></document><document id="522"><title>Data Interoperability with Apache Avro</title><url>http://blog.cloudera.com/blog/2011/07/avro-data-interop/</url><snippet>The ecosystem around Apache Hadoop has grown at a tremendous rate. Folks now can use many different pieces of software to process their large data sets, and most choose to use several of these components. Data collected by Flume might be analyzed by Pig and Hive scripts. Data imported with Sqoop might be processed by a MapReduce program. To facilitate these and other scenarios, data produced by each component must be readily consumed by other components. Data Interoperability One might address this data interoperability in a variety of manners, including the following: Each system might be extended to read all the formats generated by the other systems. In the limit, this approach is not practical, since one cannot easily anticipate all of the formats new systems might generate. A library of data conversion programs could be assembled. This would unfortunately add a processing step, to convert the data between formats, slowing processing pipelines. Note however that many data conversion libraries operate by converting data into and out of a lingua franca format, using a single format as a pivot point.  This hints at a third possibility. Enable each system to read and write a common format.  Some systems might use other formats internally for performance, but whenever data is meant to be accessible to other systems a common format is used. In practice all of these strategies will used to some extent. However the last strategy, a common format, seems to offer the most efficient path both in terms of engineering effort and processing time. This article will focus on the use of Avro’s data file format as such a common format. Avro Apache Avro is a data serialization format. Avro shares many features with Google’s Protocol Buffers and Apache Thrift, including: Rich data types. Fast, compact serialization. Support for many programming languages. Datatype evolution, also known as versioning. Avro additionally provides some other features that are especially useful when storing data, namely: Avro defines a standard file format. Avro data files are self-describing, containing the full schema for the data in the file. Thus users can exchange Avro data files without also having to separately communicate metadata.  Once an Avro data file is written, one will always be able to read it, with full datatype information, without relying on any external software or metadata repository.  Avro data files also support compression, using Gzip or Snappy codecs. Avro’s serialization is more compact. Avro avoids storing a field identifier with each field value. For some datasets this savings can be significant. Avro implementations permit one to dynamically define new datatypes and to easily process previously unseen datatypes, without generation and loading of code. This provides natural support for script and query languages. Avro datatypes can define their sort-order, facillitating use of Avro data in MapReduce or ordered key/value stores. Avro as a Common Format Most of the major ecosystem components already or will soon support reading and writing Avro data files: MapReduce: I added support for Java MapReduce programs, included in Avro 1.4 and greater. Streaming: Tom White from Cloudera has added support for Hadoop Streaming programs to Avro (AVRO-808 &amp; AVRO-830). Flume 0.9.2 and above support collecting data in Avro’s format (FLUME-133), contributed by Jon Hsieh of Cloudera.  Note also that Flume has recently been accepted into the Apache Incubator and will soon be known as Apache Flume. Sqoop 1.3 can import data as Avro data files in HDFS from a relational database (SQOOP-207), contributed by Tom White of Cloudera.  Sqoop has also recently been accepted into the Apache Incubator. Pig release 0.9 will be able read and write Avro data files (PIG-1748), thanks to Lin Guo and Jakob Homan at LinkedIn. Hive support for reading and writing Avro data files has been posted by Jakob Homan of LinkedIn, and should hopefully be included in Hive 0.9 (HIVE-895). HCatalog input and output drivers have been contributed by Tom White of Cloudera (HCATALOG-49). Thiruvalluvan M. G. from Yahoo! is working on a column-major format for Avro, which would accelerate Hive and Pig queries (AVRO-806). For folks who are currently using Protocol Buffers or Thrift to store data, some tools for conversion are planned: Raghu Angadi from Twitter is working on tools that will let folks read and write their Thrift-defined data structures as Avro format data (AVRO-804). We also hope to soon add tools to convert between Protocol Buffers and Avro (AVRO-805). At Cloudera we’re committed to helping Avro become a common format for the Hadoop ecosystem.  It’s great to see so many other companies and individuals also investing in Avro.</snippet></document><document id="523"><title>SCM Express: Now Anyone Can Experience the Power of Apache Hadoop</title><url>http://blog.cloudera.com/blog/2011/07/scm-express-now-anyone-can-experience-the-power-of-apache-hadoop/</url><snippet>Phil Langdale is a software engineer at Cloudera and the technical lead for Cloudera’s SCM Express product. What is SCM Express? As powerful and useful as Apache Hadoop is, anyone who has setup up a cluster from scratch is well aware of how challenging it can be: every machine has to have the right packages installed and correctly configured so that they can all work together, and if something goes wrong in that process, it can be even harder to nail down the problem. Understandably, this can be a serious barrier to adoption. After all, how can you appreciate how great Apache Hadoop is if you never managed to get it set up in the first place? At Cloudera, we didn’t want to see anyone stopped before they could even start. The Service and Configuration Manager (SCM) is a new part of Cloudera Management Suite in Cloudera Enterprise 3.5 that allows administrators to manage their Hadoop installations from a central console with just a few clicks of a mouse. It makes it easy to create and modify service installations and ensures that all the machines in a cluster are correctly and consistently configured. This already provides a significant benefit to Hadoop administrators, however we wanted to take things even further and provide a tool that allows someone with no Hadoop experience to start up a fully functional cluster in a matter of minutes. We also wanted it to be freely available so that anyone can experience why Hadoop is so great. So, SCM Express was born: from a single 500K download, you can bring up a Hadoop cluster of up to 50 nodes without editing a single configuration file, or even having to know what a Hadoop configuration file looks like. Additionally the cluster can be tuned to reflect the hardware on which it is running, so that it’s not just functional, but useful. We’ve codified many of the best practices and recommendations from our Solutions Architects so that the services you deploy can immediately benefit from their experiences and insights. And it’s not just Hadoop; SCM Express can also install and manage Apache Zookeeper, Apache HBase and Hue. Hadoop is an ecosystem and not just a single product, so SCM Express lets you experience some of the breadth of that ecosystem. How it works When you download SCM Express for the first time, you get a small self-executing installer that will go through the process of installing the SCM Server. It sets up a package repository that’s appropriate for your Linux distribution and then installs the SCM Server from there. This will also allow you to download updates, just as you would for anything else installed on the machine. Once the server is up and running, it provides a web-based user interface that walks through the process of identifying the hosts that you want in the cluster and then installing the necessary Apache Hadoop packages on them. In this way, you don’t need to do any manual work on those machines. As with the SCM Server, we install CDH (Cloudera’s Distribution Including Apache Hadoop, which is a packaged and tested distribution of open source Apache Hadoop and Apache ecosystem components) from our package repository, so that it too can be easily updated. The whole installation process is package based, so it’s easy to maintain in the long term. After the cluster hosts have been identified and CDH is installed on them, SCM will create the services you select. At this time, it evaluates the physical characteristics of the hosts to decide which ones are best suited for which roles (which one should run the HDFS NameNode or the MapReduce JobTracker?) It also factors the size of the cluster into these calculations (for a small cluster, it makes sense to run the NameNode and JobTracker on the same machine, but for a large one, they should be separated). It will also use these physical characteristics to inform the configuration of the created services (the java heap size should reflect the amount of physical RAM in the machine, and the number of mappers and reducers should reflect the number of CPU cores). Once the services are created, it will go through the process of bringing the services up for the first time. This isn’t always a simple matter of just starting processes; you have to format an HDFS filesystem before you can use it, for example. When all that is done, your services are running and ready to go, and you’re also ready to appreciate the benefits that SCM Express provides in helping you maintain your newly deployed Hadoop cluster. If you think you’re ready to take the plunge and upgrade to Cloudera Enterprise, it’s easy to switch over from SCM Express to full SCM; all your data and configuration carry over in-place. We’re really proud that we’re able to offer SCM Express to the world. Apache Hadoop is an incredibly powerful tool for solving all sorts of problems and answering all kinds of questions from all your data, and now anyone can install it and experience it for themselves.</snippet></document><document id="524"><title>The Only Full Lifecycle Management for Apache Hadoop: Introducing Cloudera Enterprise 3.5 and SCM Express</title><url>http://blog.cloudera.com/blog/2011/07/the-only-full-lifecycle-management-for-apache-hadoop-introducing-cloudera-enterprise-3-5-and-scm-express/</url><snippet>Drew O’Brien is a product marketing manager at Cloudera We’re excited to share the news about the immediate availability of Cloudera Enterprise 3.5 and SCM Express, which we announced this week in tandem with our presence at Hadoop Summit. These products represent a major advance in Cloudera’s mission to drive massive enterprise adoption of 100% open source Apache Hadoop. We now make it easier and more convenient than ever before for companies to run and manage Apache Hadoop clusters throughout their entire operational lifecycle. Cloudera Enterprise 3.5 is a substantial update to our subscription service that delivers production support and management software for Apache Hadoop and the entire Apache Hadoop ecosystem. With new features like automated service and configuration tools, activity monitoring, and one-click security, we’ve streamlined the extremely complex processes and eliminated the uncertainties associated with ongoing management and maintenance of Hadoop clusters in production. Cloudera Enterprise 3.5 codifies and makes available best practices Cloudera has learned over many years of helping enterprise customers build and manage Apache Hadoop-based systems. SCM Express is a free version of the new Service and Configuration Manager, which is part of the Cloudera Management Suite (CMS) in Cloudera Enterprise 3.5. SCM Express makes it simple and painless to install, run and configure a complete Apache Hadoop stack in just minutes.  Now anyone can have a Hadoop cluster up and running in just a few clicks. Find out more about Cloudera Enterprise 3.5 and SCM Express: Check out Wednesday’s announcement Check out media coverage from outlets like AllThingsD, TechCrunch, The Register, CIO, SiliconAngle, Network World, GigaOM and more Watch Cloudera CEO Mike Olson explain what’s new and special about Cloudera Enterprise 3.5 and SCM Express Get Cloudera SCM Express or Cloudera Enterprise 3.5 Today: Download Cloudera SCM Express for free Subscribe to Cloudera Enterprise 3.5</snippet></document><document id="525"><title>Shopzilla’s Apache Hadoop Hackathon: Learning To Contribute</title><url>http://blog.cloudera.com/blog/2011/06/shopzillas-apache-hadoop-hackathon-learning-to-contribute/</url><snippet>This is a guest repost from Shopzilla’s Tech Blog written by Andrew Look, a Software Engineer at Shopzilla.com. Andrew is responsible for maintaining and constructing SEM systems to manage keyword-based marketing operations, Andrew also has a strong background in highly concurrent web applications and Service Oriented Architectures. Having gained a strong interest in Hadoop/NoSQL after prototyping a workflow based on MapReduce/Pig, he is now co-organizer of the Los Angeles Hadoop Users’ Group, evangelizing use of the Hadoop project within the Southern California software community. With the objective of recruiting new contributors to the Hadoop ecosystem, our most recent meetup of the LA-HUG included a 5-hour hackathon in which attendees learned to set up a development environment, checkout/build/test the Hadoop codebase, find issues to work on in the Apache JIRA system, and understand the community review process. Aaron T. Myers and Eric Sammers of Cloudera expertly led the session of roughly 15 developers, describing how developers can make contributions to Hadoop Common, MapReduce, and HDFS. While the learning curve was steep, a number of developers were able to submit patches during the session; they are currently pending community review for inclusion into the release of Hadoop 0.23.0. HADOOP-7418 support for multiple slashes in the path separator HDFS-1322 DistributedFileSystem.mkdirs(dir, dirPermission) doesn’t set the permissions of created dir to dirPermission HDFS-1314 dfs.block.size accepts only absolute value HDFS-1321 If service port and main port are the same, there is no clear log message explaining the issue. For those interested in contributing to project in the Hadoop ecosystem, Aaron has prepared a very helpful document, which explains how to pull down the source code, set up a development environment, run the tests, and find JIRA issues to work on. As a final note, Eric made an exciting announcement that several independent open-source projects related to Hadoop, which Cloudera has been maintaining, have been accepted into the Apache incubator program. They are actively recruiting committers; your help will make a big difference, whether you’re a Java developer, build/release/scripting ninja, documentation-writer, web designer, bug reporter, or even an end-user who pulls down the code and runs it! The list of projects is as follows: Flume MRUnit BigTop Sqoop Thank you to everyone who attended! We hope to see more Hadoop enthusiasts give back to this ever-growing community.</snippet></document><document id="526"><title>If 80% of data is unstructured, is it the exception or a new rule?</title><url>http://blog.cloudera.com/blog/2011/06/if-80-of-data-is-unstructured-is-it-the-exception-or-a-new-rule/</url><snippet>Ed Albanese leads business development for Cloudera. He is responsible for identifying new markets, revenue opportunities and strategic alliances for the company. This week’s announcement about the availability of the Cloudera Connector for IBM Netezza is the achievement of a major milestone, but not necessarily the one you might expect. It’s not just the delivery of a useful software component; it’s also the introduction of a new generation of data management architectures.  For literally decades, data management architecture consisted of RDBMS, a BI tool and an ETL engine. Those three components assembled together gave you a bonafide data management environment. That architecture has been relevant for long enough to withstand the onslaught of data driven by the introduction of ERP, the rise and fall of client/server and several versions of web architecture. But the machines are unrelenting. They keep generating data. And there’s not just more of it, there is more you can—and often need—to do with it. The times they are a-changin’, and unstructured data is taking over Companies of all sizes and in nearly every vertical are increasingly tasked with decoding the information being generated by the machines they rely on most. New data sources are creating new data types, including web data, clickstreams, location data, point of sale, social data, building sensors, vehicle and aircraft data, satellite images, medical images, log files, network data and weather data… just to name a few. These data sources were but a glimmer in the eyes of the forefathers of the RDBMS and were most certainly not accounted for in its design. And yet, the percentage of data that fit into this newer bucket is growing at astounding rates. While at Netezza’s Enzee event this week, I listened to Steve Mills, IBM Senior Vice President and Group Executive for the Software Group, cite that more than 80% of the world’s data is unstructured. So what to do with all of this data? For a large and growing number of companies, the answer lies in adding Apache Hadoop to their data management architecture. That’s right – add, not replace. And that, in and of itself, is significant. “How does Hadoop work with my data warehouse?” is one of the most common questions I get asked at tradeshows. My answer, in short form, is “it allows you to add more data to your data warehouse.” In longer form, I answer with a workflow diagram like the one below. There are four steps in this workflow. Step 1. Data staging and loading. With Hadoop, you don’t need a schema defined before you load it. Just like a file system, you can load data as fast as you can copy data. Step 2. Exploratory analytics. Without moving the data, you can perform analytics using a wide variety of languages (From SQL to Python to Java and C#). You can add a schema – or not. Most customers are analyzing this data to determine its value; is it worth sharing with a wider user population? Step 3. Transform and Data Pipelines. If the data has value to users of existing OLAP or real-time databases, it can be structured – in place – by bringing the processing to the data instead of moving the data to a processing engine. Hadoop is a magnificent processing engine. Step 4. Data doesn’t stay “hot” forever, but that doesn’t mean it should be put in the trash or trucked off to mountains made of iron. Hadoop can store data inexpensively for long periods of time, whether it be raw atomic data or aggregates that have worn out their welcome in traditional RDBMS systems. Hadoop let’s customers use “the other 80%” of data within their EDW EDWs like IBM Netezza are tremendous tools. Whip fast, capable of reliably storing important data and able to serve it to a wide variety of clients using clean, clear interfaces. If the EDW is to maintain its position as the system of record for its customers, it needs to address the 80% of the data being created today that fits into a new category, one the RDBMS was not designed to handle natively. It is this “other 80%” of data that is increasingly the source of competitive advantage or breakthrough insight for the most recognized brands in the world. And it is the undeniable need for both tools in a modern data management architecture that makes the availability of the Cloudera Connector for IBM Netezza noteworthy.  Matt Rollender, a good friend of mine at IBM Netezza and VP of Strategic Integration and Alliances agrees. At the Enzee event this week, I caught him in the hallways and he mentioned that “We are seeing multiple source systems, higher volumes and, perhaps most notably, ‘unqualified data’ everywhere. Our customers are interested in finding easier ways to explore, structure and then use that data in IBM Netezza.  The Cloudera Connector makes that possible and that’s why it is so exciting.” If you are a user of IBM Netezza, you can begin adding more data to your warehouse immediately. You can do that by adding Apache Hadoop to your data management architecture to collect data as quickly as it is generated, sort out whether it is valuable enough to share more widely, structure it into useful aggregates and then deliver it to IBM Netezza, where end users can explore, discover and report with high usability and reliability. While we are seeing the interest across many verticals, Brad Terrell of IBM Netezza has been working with customers in the Media and Entertainment space where the energy has been especially high and took the time to reflect on some of that enthusiasm here. You can download the Cloudera Connector for IBM Netezza here. It is a freely available product that can be used with the equally free Cloudera Distribution including Apache Hadoop (CDH). Both the Cloudera Connector IBM Netezza and CDH work with Cloudera Enterprise.</snippet></document><document id="527"><title>Reflections from Enzee Universe 2011</title><url>http://blog.cloudera.com/blog/2011/06/reflections-from-enzee-universe-2011/</url><snippet>Bala Venkatrao is the director of product management at Cloudera. I had the pleasure of attending Enzee Universe 2011 User Conference this week (June 20-22) in Boston. The conference was very well organized and was attended by well over 1000+ attendees, many of whom lead the Data Warehouse/Data Management functions for their companies.  This was Netezza’s largest conference so far in seven years. Netezza is known for enterprise data warehousing, and in fact, they pioneered the concept of the data warehouse appliance. Netezza is a success story: since its founding in 2000, Netezza has seen a steady growth in customers and revenues and last year (2010), IBM acquired Netezza for a whopping $1.7B. Cloudera announced a partnership with Netezza last year and since then the two companies have been working closely to build a high-speed bi-directional connector between Netezza DW appliances and Apache Hadoop. We launched the general availability of this connector at this week at Enzee Universe 2011. You can download the connector here:  https://ccp.cloudera.com/display/SUPPORT/Downloads. Thank you to the teams at Netezza and Cloudera for making this happen. It’s been a great collaboration! Before heading to Netezza’s conference, I had a lingering question in my mind: As we enter the era of the petabyte, do EDWs and Hadoop co-exist or compete? In this post I’ll share with you the conclusions at which I arrived by the end of the event. Day 1 The event kicked off with opening remarks by Jim Baum, CEO of Netezza. The talk mostly focused on how Netezza continues to thrive as a division of IBM and how the synergies are good for customers. Jim also invited Arvind Krishna, GM of IBM’s Information Management Software Division, who emphasized that “one size does not fit all.” This was a recurring theme throughout the conference. He also talked about how EDWs and NoSQL technologies like Hadoop are appropriate to address the problems they are each most suited to solve. The personal highlight about Jim’s keynote was when Cloudera was invited on stage as one of the “Analytic Innovators” who are helping customers realize more value from their Netezza investments. We are among the 40 innovators from a pool of 300+ partners to be selected for this honor. After the keynote, we spent the rest of the evening at the Enzee Galaxy Partner Pavilion, where we had an opportunity to speak to several end users about Cloudera and the power of Hadoop. It was interesting to see how the conversations ranged from “What is Hadoop?” to “I know I need Hadoop. I just need to identify the problem!” The audience could not have been more relevant. Most of the folks were responsible for driving or making decisions around data architecture / data management and had significant investments in EDW technologies, especially Netezza. Many of them clearly saw Hadoop and Cloudera as greatly complementary to Netezza’s technologies and requested introductions to our sales folks to explore joint opportunities. Day 2 The day kicked off with a keynote by Steve Mills, IBM’s SVP and Group Executive.  Steve talked about how 80% of the world’s data is unstructured and about the emergence of NoSQL technologies like Hadoop. An interesting insight from Steve was how he would like IBM to provide more choices to customers. So while “integration” remains important for IBM, equally important is “modularity” and the ability to interoperate across different technologies from different vendors. He also talked about the “Smarter Planet” theme and how Big Data and analytics will make that a reality. Several projects are underway at IBM to bring this concept (Smarter Cities, Smarter Water and Smarter Energy) to reality and all of them depend on harnessing and extracting insights from Big Data. Throughout the day, there were several tracks focused on Big Data and analytics. Cloudera’s work on the Cloudera-Netezza Connector was mentioned on several occasions. A most interesting talk (titled “Hadoop and IBM Netezza: Co-existence or Competition” was given by Krishna Parasuraman, Chief Architect and CTO of Digital Media at Netezza. Krishnan did an excellent job introducing Hadoop to the audience and. perhaps more importantly, comparing and contrasting it with the EDW technologies like Netezza. Krishna clearly felt that these technologies are complementary and illustrated several use cases for how customers could leverage Netezza, Hadoop and Cloudera to derive even more insights than possible before. Clearly the big takeaway was that Hadoop and EDW’s should co-exist, not compete. IBM Netezza had worked with Cloudera to put together a compelling demo to highlight the value of our combined solution of CDH/Hadoop and Netezza.  Through an interesting use case, the demo showed how businesses could have their “hot” data (most recent data) residing in Netezza, “warm” data (longer time range data) residing in HDFS, while leveraging the Cloudera Connector for Netezza and Oozie (workflow engine part of CDH) to provide deeper insights to business executives. The demo was well received at the conference and resulted in significant booth traffic in the IBM Netezza Analytics demo stations. The day concluded with an event at the House of Blues Boston with live music and dinner. The Boston weather was at its very best! Day 3 An excellent talk by Krishna on Day 2 about demystifying Hadoop drove more traffic to our booth the next day, and we got to meet some really neat prospects. Highlights of the morning included a presentation on the Netezza Roadmap, which showed Netezza embracing the logical data warehouse vision promoted by the likes of Donald Feinberg at Gartner. This was followed by a presentation by IDC Analyst Dan Vesset. Dan defined Big Data as having the following characteristics: Volume: TBs to PBs Variety: Multi-structured Velocity: Speed of capture, analysis and access Value: ROI Dan emphasized at least five times in his presentation that “one size does not fit all” and how technologies like Hadoop will solve a different class of analytics problems that were not possible to solves before. One example he cited was how companies could use sentiment analysis of Twitter feeds to understand customer behavior. After spending three days at Enzee Universe, I came to these conclusions: One size does not fit all: EDW’s and Hadoop will co-exist in enterprise environments The combination of analytics enabled through EDW’s (and appliances) and those through Hadoop, enable customers to gather richer and newer insights that were previously not possible to gain There was strong interest in Hadoop among attendees but still, many are looking for tangible use cases that will drive adoption. Overall, it was a great conference by Netezza. Kudos to the team! I’m looking forward to continuing to work with Netezza to realize the synergies between EDWs and Cloudera’s Distribution Including Apache Hadoop (CDH).</snippet></document><document id="528"><title>Migrating from Elastic MapReduce to a Cloudera�s Distribution including Apache Hadoop Cluster</title><url>http://blog.cloudera.com/blog/2011/06/migrating-from-elastic-mapreduce-to-a-cloudera%e2%80%99s-distribution-including-apache-hadoop-cluster/</url><snippet>This post was contributed by Jennie Cochran-Chinn and Joe Crobak. They are part of the team building out Adconion‘s Hadoop infrastructure to support Adconion’s next-generation ad optimization and reporting systems. This is the first of a two part series about moving away from Amazon’s EMR service to an in-house Apache Hadoop cluster. When we first started using Hadoop, we went down the path of Amazon’s EMR service.� We had limited operational resources and wanted to get up and running quickly.� After a while, we starting hitting the limitations of EMR and had to migrate towards managing our own cluster.� In doing so we did not want to lose the features of EMR we found useful – mainly the ease of cluster setup. This first part of the series discusses our motivation for choosing and then moving away from EMR, while the second part deals with how we maintained ease of cluster setup using Puppet. Many of our systems use Amazon�s S3 as a backup repository for log data.� Our data became too large to process by traditional techniques, so we started using Amazon�s Elastic MapReduce (EMR) to do more expensive queries on our data stored in S3.� The major advantage of EMR for us was the lack of operational overhead.� With a simple API call, we could have a 20 or 40 node cluster running to crunch our data, which we shutdown at the conclusion of the run. We had two systems interacting with EMR.� The first consisted of shell scripts to start an EMR cluster, run a pig script, and load the output data from S3 into our data warehousing system.� The second was a Java application that launched pig jobs on an EMR cluster via the Java API and consumed the data in S3 produced by EMR. The magic of spinning up and configuring a Hadoop cluster in EC2 was spectacular, but there were a few areas that we saw room for improvement.� In particular: Performance &amp; Tuning. We were hit by the small-files problem, lack of data locality (data stored in S3 but processed on nodes of the EMR cluster), decompression (bz2) performance issues, and virtualization penalties.� To solve these problems, we decided that we needed a non-transient cluster (to satisfy data locality), and a process to aggregate our logfiles into a Hadoop-friendly size and data format (we ultimately chose avro). After crunching the numbers, it was evident that storing large amounts of data on an EC2 cluster quickly becomes expensive, and one still suffers from virtualization penalties (particularly since Hadoop is so I/O intensive), so we decided to build-out a cluster using CDH3. Monitoring. Typically for us, a pig script running on EMR was one step in a workflow, so we needed to monitor the status of the job to determine when it finished and the next steps could continue.� While Amazon exposes a rich API for monitoring a job, we really wanted a more generic mechanism for monitoring all steps in a workflow, not just those on an EMR cluster.� After considering a number of solutions, we ultimately chose to use Azkaban as our workflow engine for managing dependencies, alerting, and monitoring (which we added atop Azkaban ourselves). API Access. Interacting with a cluster only over an API is both a blessing and a curse.� The API takes care of otherwise complicated mechanics, such as starting, configuring, and stopping the cluster.� With that said, the calls to the EMR service are rate-limited, so it doesn�t scale very well for monitoring a number of clusters.� Also, we found that we could continuously keep a cluster busy, and thus the EMR limitation of 100 or so jobs on a cluster meant that we had to build wrappers to periodically shutdown and startup clusters. Lack of latest features. We were using Hadoop 0.18 and Pig 0.3 on EMR, which were missing many features that we wanted to try (e.g. JVM reuse, CombineInputFormats, and improved pig optimization plans).� Eventually, Amazon upgraded to Hadoop 0.20 and Pig 0.6, but even at that point Cloudera�s Distribution including Apache Hadoop had backported many useful features such as performance improvements, monitoring enhancements, and additional APIs.� In addition, CDH provides a full-suite of solutions including Pig, Hive, Flume, and Sqoop, that we�re either actively using or planning to use. For us, the major drawback to moving away from EMR was new operational overhead.� Starting a cluster with an API call is incredibly useful, and we soon discovered that CDH provided scripts for doing so (now there�s something even better, Apache Whirr).� Eventually, we decided to move out of the cloud, though, so we wanted to build an infrastructure for maintaining a cluster that worked regardless of the hardware configurations.� The RPMs for CDH3 and the great documentation on installing and configuring CDH from Cloudera helped to make this project much-less intimidating.� Ultimately, we built puppet modules for configuring our cluster, which we�ll talk much more about in part two of this post.</snippet></document><document id="529"><title>Biodiversity Indexing: Migration from MySQL to Apache Hadoop</title><url>http://blog.cloudera.com/blog/2011/06/biodiversity-indexing-migration-from-mysql-to-hadoop/</url><snippet>  This post was contributed by The Global Biodiversity Information Facility development team. The Global Biodiversity Information Facility is an international organization, whose mission is to promote and enable free and open access to biodiversity data worldwide. Part of this includes operating a search, discovery and access system, known as the Data Portal; a sophisticated index to the content shared through GBIF. This content includes both complex taxonomies and occurrence data such as the recording of specimen collection events or species observations. While the taxonomic content requires careful data modeling and has its own challenges, it is the growing volume of occurrence data that attracts us to the Apache Hadoop stack. The Data Portal was launched in 2007. It consists of crawling components and a web application, implemented in a typical Java solution consisting of Spring, Hibernate and SpringMVC, operating against a MySQL database. In the early days the MySQL database had a very normalized structure, but as content and throughput grew, we adopted the typical pattern of denormalisation and scaling up with more powerful hardware. By the time we reached 100 million records, the occurrence content was modeled as a single fixed-width table. Allowing for complex searches containing combinations of species identifications, higher-level groupings, locality, bounding box and temporal filters required carefully selected indexes on the table. As content grew it became clear that real time indexing was no longer an option, and the Portal became a snapshot index, refreshed on a monthly basis, using complex batch procedures against the MySQL database. During this growth pattern we found we were moving more and more operations off the database to avoid locking, and instead partitioned data into delimited files, iterating over those and even performing joins using text files by synthesizing keys, sorting and managing multiple file cursors. Clearly we needed a better solution, so we began researching Hadoop.� Today we are preparing to put our first Hadoop process into production. Our first objective is to address the monthly processing we perform. This area of work does not increase functionality offered through the portal (that will be addressed following this infrastructural work) but rather aims to: Reduce the latency between a record changing on the publisher side, and being reflected in the index Reduce the amount of (wo)man-hours needed to coax through a successful processing run Improve the quality assurance by inclusion of Checking that terrestrial point locations fall within the stated country using�shapefiles Checking coastal waters using�Exclusive Economic Zones Rework all the date and time handling Use dictionaries (vocabularies) for interpretation of fields such as Basis of Record Integrate checklists (taxonomic, nomenclatural and thematic) shared through the�GBIF ECAT Programme to improve the taxonomic services, and the backbone (“nub”) taxonomy. Provide a robust framework for future development Allow the infrastructure to grow predictably with content and demand growth Things have progressed significantly since the early Hadoop investigations which included hand crafting MapReduce jobs, and GBIF are now developing using the following technologies: Apache Hadoop: A distributed file system and cluster processing using the Map Reduce framework GBIF are using the�Cloudera�s Distribution including Apache Hadoop Sqoop: A utility to synchronize between relational databases and Hadoop Hive: A data warehouse infrastructure built on top of Hadoop, and developed and open-sourced by Facebook. Hive gives SQL capabilities on Hadoop, which is particularly attractive to a development team fluent in SQL. [Full table scans on GBIF occurrence records reduce from hours to minutes] Oozie: An open-source workflow/coordination service to manage data processing jobs for Hadoop, developed and then open-sourced by Yahoo! The processing architecture is depicted: Following this processing work, we expect to modify our crawling to harvest directly into HBase. The flexibility HBase offers will allow us to grow incrementally the richness of the terms indexed in the Portal, while integrating nicely into Hadoop based workflows. The addition of coprocessors to HBase is of particular interest to further reduce the latency involved in processing, by eliminating batch processing altogether. The combination of Hadoop, Oozie and Hive offer a framework that we anticipate will fit nicely with many of our data transformation tasks, and Sqoop and Hive have made the technologies far more accessible to our development team than was �previously possible. All GBIF source code is available under open source licensing, and this work is regularly blogged on the GBIF Developer Blog.</snippet></document><document id="530"><title>CDH 3 Demo VM installation on Mac OS X using VirtualBox</title><url>http://blog.cloudera.com/blog/2011/06/cloudera-distribution-including-apache-hadoop-3-demo-vm-installation-on-mac-os-x-using-virtualbox-cdh/</url><snippet>The first task is to ensure that your system is up-to-date. This procedure has been tested on the following configuration: Fully up-to-date Snow Leopard 10.6.7 Update or install Oracle VM VirtualBox for Mac OS X to version 4.0.8 (Virtualbox 4.0.8-71778-OSX) Assumptions: The browser used is Safari. The Demo VM has been downloaded to the default download location for Safari (i.e. the “Downloads” folder within the users home directory). The Demo VM will be run from the Downloads folder. Step 1: Download the Cloudera demo virtual machine from the ?Downloads? area of the Cloudera web site. Click on the ?Download? link beside the ?Virtual Machine? section. The file should automatically start to download. It will be saved to your ?Downloads? folder. Step 2: Once the file has downloaded you now need to decompress it. The file that the virtual machine is contained within is a bz2 archive (specifically a bzip2 compressed archive) and can be decompressed with the finder. Navigate to the ?Downloads? section, and either double-click on the file or right click on the file and then select ‘Open With/Archive Utility (Default). This will start the decompression/unarchiving process. Step 3: When the archive utility has finished, there will be a folder named cloudera-demo-0.3.7. The contents of which are shown below: Step 4: Start VirtualBox and create a new VM by clicking the ?New? button. The following is the first dialog for the new virtual machine. Step 5: Once you click on the ?Continue? button you are presented with the following dialog. Step 6: Give the new virtual machine a name, in this example we?ll be using ?Cloudera-CDH3?. For Operating System and Version select ?Linux? and ?Ubuntu?. Step 7: Increase the base memory to 1024 MB (if possible) for better performance. Step 8: In this step we need to select the training VM file we just downloaded and extracted. Click on the second radio button ?Use existing hard disk? and then click on the file folder icon with the green ^ on the right hand side of the drop down. Please Note: The contents of the drop down on your system will be different than that displayed here. Navigate to the folder where the demo VM is located, in this example it should be in the ?Downloads? folder of the user and called ?cloudera-demo-0.3.7?. Step 9: Click the ?Continue? button and the following dialog is now displayed and it shows a summary of the choices made so far. Step 10: Click the ?Done? button to finish the creation of the virtual machine in VirtualBox 4. The following shows the newly created cloudera-CDH3 VM in the VirtualBox manager screen. Step 11: Start the virtual machine. When the Demo VM launches you should be presented with the following login. Step 12: Finally, once you have successfully logged into the Demo VM, the image below is the initial view that you should see. About the author: John Zanchetta heads up an integration test team in the mobile telecommunications space and is experimenting with CDH from a number of different perspectives. He has been hacking in various technology pools for the past ~20+ years. John can be contacted at johnzan at gmail dot com.</snippet></document><document id="531"><title>Cloudera Certification for Apache Hadoop at Hadoop Summit</title><url>http://blog.cloudera.com/blog/2011/05/cloudera-certification-for-apache-hadoop-at-hadoop-summit-2011/</url><snippet>Take advantage of the opportunity to become a Cloudera Certified Developer or Administrator for Apache Hadoop the day before Hadoop Summit, June 28th. This is the first time these certifications have been offered apart from their respective courses � so don�t miss the chance to validate your Hadoop expertise! There are several exam times throughout the day for your convenience. The Developer exam lasts for 90 minutes, the Administrator exam for 60 minutes. Become a Cloudera Certified Developer June 28: 12 – 1:30pm, 2 – 3:30pm, 4 – 5:30pm PT This exam will test your knowledge of the Apache Hadoop computing environment, the Hadoop Distributed File System (HDFS) and MapReduce, the Hadoop API and the surrounding Hadoop ecosystem. For more information click here for the event listing. Become a Cloudera Certified Administrator June 28: 12 – 1:30pm, 2 – 3:30pm, 4 – 5:30pm PT This exam will test your knowledge of planning, operating, and managing of a Hadoop cluster, as well as job scheduling, monitoring, and logging. For more information click here for the event listing. *Please bring your own laptop with wireless capabilities to take either exam. If you are interested in taking a Cloudera training course while in Santa Clara for Hadoop Summit, see the list of available courses here.</snippet></document><document id="532"><title>Using Apache Hadoop to Measure Influence</title><url>http://blog.cloudera.com/blog/2011/05/using-hadoop-to-measure-influence/</url><snippet>Background Klout’s goal is to be the standard for influence. The advent of social media has created a huge number of measurable relationships. On Facebook, people have an average of 130 friends. On Twitter, the average number of followers range from 300+ to 1000+. With each relationship comes a different source of data. This has created A LOT of noise and an attention economy. Influence has the power to drive this attention. When a company, brand, or person creates content, our goal is to measure the actions on that content. We want to measure every view, click, like, share, comment, retweet, mention, vote, check-in, recommendation, and so on. We want to know how influential the person who *acted* on that content is. We want to know the actual meaning of that content. And we want to know all of this, over time. Measuring influence is a bit like trying to measure an emotion like hate or jealousy. It’s really hard and takes a boatload of data. A huge part of what we do is develop machine learning models that make sense of this data. On top of that, there’s an endless amount of this data and we need a platform to ingest, prepare, and analyze it. The two biggest platforms are Facebook and Twitter, but it hardly ends there when it comes to social media. There’s LinkedIn, Foursquare, Path, Youtube, Quora, and many others. This presents the challenge of creating models for each platform and building data analysis platforms that can handle unstructured data. To handle this at Klout, we’ve turned to open source technologies such as Apache Hadoop. Specifically, we turned to Cloudera�s CDH3 distribution due to ease of installation and availability of enterprise support. Twitter Influence Twitter was the natural selection for our first network to analyze due to the open nature of the data as well as the simplistic nature of actions you can take on Twitter, such as a mention or a retweet. However, as our models matured, the growth of Twitter increased. As of this post, our Twitter cluster has the following stats: 75 million people scored daily 4 billion graph edges scored daily 48 million people are influenced by or influence an average of 27 people We derive hundreds of thousands of different topics that 14 million users are influential on On average 5 topics per user using NLP and semantic analysis For topics, 3 months of mentions and retweets are analyzed, currently over 6 billion Twitter Analytics Overview From the twitter firehose, data is written to disk in buffered chunks. A mapreduce job handles the task of preparing the firehose data into different buckets needed for each of the workflows. These different workflows serve different products from performing bot and spam detection to scoring to topic extraction. Many of our mapreduce jobs are written in java, but we also rely on Pig Latin for some purposes such as performing simple joins are population aggregates and statistics. Oozie is used to coordinate the different workflow components. To serve out data both internally and externally, we dump out raw csv files or load this data into HBase which interfaces with load balanced API servers. Twitter Scoring Workflow We use a machine learning and statistical based approach to perform our scoring. This model currently has over 35 features. The scoring workflow consists of different Oozie jobs, many of which perform feature extraction. In the final jobs of this workflow, all the features are fed into the scoring model, which produces scores. We’ve experimented with Mahout in the past and we will be using more of it in the future. Challenges Having a highly available API is one of our key goals. However, when we refresh 75 million scores + meta data daily, it becomes challenging to flip a switch to make all the new data available. This led to us having multiple clusters. When one cluster is loading data, the load balanced API servers are aware of each cluster�s status, and switches to the non-loading cluster. This also mitigates any performance issues due to splits, minor and major compactions on the clusters. This also allowed us to cope with instabilities caused by cloud instances in unpredictable states. That said, we are in the process of building out our own servers and racks at a nearby facility. We’ve also had issues where our edits logs for namenodes get corrupted due to server instabilities. This is where Cloudera has come to our rescue. We initially had to manually apply patches and build hadoop-core jars ourselves to resolve such problems, but with Cloudera’s Distribution including Apache Hadoop and their expert Solution Architects help this is no longer an issue. We now are able to focus our resources on our products.</snippet></document><document id="533"><title>Three Reasons Why Apache Avro Data Serialization is a Good Choice for OpenRTB</title><url>http://blog.cloudera.com/blog/2011/05/three-reasons-why-apache-avro-data-serialization-is-a-good-choice-for-openrtb/</url><snippet>This is a guest repost from the DataXu blog. Click here to view the original post. I recently evaluated several serialization frameworks including Thrift, Protocol Buffersand Avro for a solution to address our needs as a demand side platform, but also for a protocol framework to use for the OpenRTB marketplace as well. The working draft of OpenRTB 2.0 uses simple JSON encoding, which has many advantages including simplicity and ubiquity of support. Many OpenRTB contributors requested we support at least one binary standard as well, to improve bandwidth usage and CPU processing time for real-time bidding at scale. After reviewing many candidates, Apache Avro proved to be the best solution. To demonstrate what differentiates Avro from the other frameworks (the link to my source code is at the end of this post), I put together a quick test of key features. The following are the key advantages of Avro 1.5: * Schema evolution – Avro requires schemas when data is written or read. Most interesting is that you can use different schemas for serialization and deserialization, and Avro will handle the missing/extra/modified fields. * Untagged data – Providing a schema with binary data allows each datum be written without overhead. The result is more compact data encoding, and faster data processing. * Dynamic typing – This refers to serialization and deserialization without code generation. It complements the code generation, which is available in Avro for statically typed languages as an optional optimization. Schema Evolution This is the most exciting feature! It allows for building less decoupled and more robust systems. Below, I made significant changes to the schema, and things still work fine. This flexibility is a very interesting feature for rapidly evolving protocols like OpenRTB. The following example demonstrates how this works. First, I created a new (example) schema. (Avro schemas are defined in JSON): {
    "type": "record",
    "name": "Employee",
    "fields": [
        {"name": "name", "type": "string"},
        {"name": "age", "type": "int"},
        {"name": "emails", "type": {"type": "array", "items": "string"}},
        {"name": "boss", "type": ["Employee","null"]}
    ]
} Next, I serialized a few records into a binary file using that schema. After that, I evolved my schema to the following: {
    "type": "record",
    "name": "Employee",
    "fields": [
        {"name": "name", "type": "string"},
        {"name": "yrs", "type": "int", "aliases": ["age"]},
        {"name": "gender", "type": "string", "default":"unknown"},
        {"name": "emails", "type": {"type": "array", "items": "string"}}
    ]
} This is a snapshot of the changes I made to the schema: 1) Renamed the field ‘age’ to ‘yrs’. Thanks to the alias feature, I can retrieve the value of ‘age’ by using the field name ‘yrs’. 2) Added a new ‘gender’ field, and defined a default value for it. This can be used to set values during deserialization as this field isn’t present in the original schema records. 3) Removed the ‘boss’ field. Finally, I deserialized the binary data file with this new schema, and print it out. Success! Untagged Data There are two ways to encode data when serializing with Avro: binary or JSON. In the binary file, the schema is included at the beginning of file. I verified that the binary data was serialized untagged, which resulted in a smaller footprint. Another interesting point is that the schema can be defined, and then the data can be encoded/decoded in JSON; allowing you to define a schema for JSON rich data structures. Anyone needing to implement validation for a JSON protocol (like we did for OpenRTB) will appreciate this feature. And switching between binary and JSON encoding is simply a one-line code change. Switching JSON protocol to a binary format in order to achieve better performance is pretty straightforward with Avro. Dynamic Typing The key abstraction is GenericData.Record. This is essentially a set of name-value pairs where name is the field name, and value is one of the Avro supported value types. I found the dynamic typing to be very easy to use. When a generic record is instantiated, you have to provide a JSON-encoded schema definition. To access the fields, just use put/get methods like you would with any map. This approach is referred to as “generic” in Avro, in contrast to the “static” code generation approach also supported by Avro. The extra flexibility of the generic data handling has performance implications. But, this excellent benchmark – https://github.com/eishay/jvm-serializers/wiki/ – shows the penalty is minor, and the benefit is a simplified code base. In conclusion, Avro is a unique serialization framework that works, although it took a bit of experimentation to get the code working. If you are interested in my Java code for an example of how Avro can be used, you can find it here: https://github.com/rfoldes/Avro-Test. Robert Foldes Senior Architect, DataXu</snippet></document><document id="534"><title>Cloudera Training for Apache Hadoop Surrounding Hadoop Summit 2011</title><url>http://blog.cloudera.com/blog/2011/05/cloudera-training-for-apache-hadoop-surrounding-hadoop-summit-2011/</url><snippet>Cloudera is offering several training courses for Apache Hadoop over the dates surrounding Hadoop Summit. There are five different courses in all spanning the dates of June 27th to July 1st. Three of these courses are specifically designed to provide the necessary knowledge for a robust overall understanding of Hadoop and they tackle the “elephant” from several perspectives — developer, system administrator, and managerial. The other two training sessions focus on projects within the Hadoop ecosystem; namely Hive, Pig, and HBase. Cloudera Developer Bootcamp for Apache Hadoop is a two-day course designed for developers who wish to learn the MapReduce framework and how to write programs against its API. The course covers similar material to our standard three-day Developer training, but has been condensed into two intensive days with extended course hours. At the end of the course, attendees have the opportunity to take an exam which, if passed, confers the Cloudera Certified Hadoop Developer credential. Cloudera Administrator Training for Apache Hadoop is a two-day course designed for system administrators. It teaches Hadoop system recommendations, installation, configuration, troubleshooting, and best practices. At the end of the course, attendees have the opportunity to take the Cloudera Certified Hadoop Administrator exam. Cloudera’s Manager Training for Apache Hadoop is a one-day course focused on providing decision makers the answers to questions like: When is Hadoop appropriate? What are people using Hadoop for? How does Hadoop fit into our existing environment? And many more. Cloudera Training for Apache Hive and Pig is a two-day, hands-on course for people who want to manipulate and query large amounts of data without needing to write complex Java MapReduce code. Hive provides a language very similar to SQL, while Pig is an easy-to-learn but sophisticated scripting language. No Java programming knowledge is required for this course. Cloudera Developer Training for Apache HBase is also a hands-on course that will provide you with all the information necessary for using HBase as a distributed data store to achieve low-latency queries and highly scalable throughput. Make the most of your experience surrounding Hadoop Summit and register to build upon your Hadoop knowledge. These courses have limited availability and space is filling quickly.</snippet></document><document id="535"><title>An Attendee Perspective On Chicago Data Summit</title><url>http://blog.cloudera.com/blog/2011/04/an-attendee-perspective-on-chicago-data-summit/</url><snippet>This is a guest post from Mike Segel, an attendee of Chicago Data Summit. Earlier this week, Cloudera hosted their first ‘Chicago Data Summit’. I’m flattered that Cloudera asked me to write up a short blog about the event, however as one of the organizers of CHUG (Chicagao area Hadoop User Group), I’m afraid I’m a bit biased. Personally I welcome any opportunity to attend a conference where I don’t have to get groped patted down by airport security, and then get stuck in a center seat, in coach, on a full flight stuck between two other guys bigger than Doug Cutting. I was going to solicit input from Jonathan Seidman, my partner in crime and co-organizer of CHUG. Unfortunately, since he was one of the speakers at the event, he would have been just as biased as I was. But thanks to Jonathan, we were able to piece together a bunch of honest feedback from some of the attendees. The day just before the event, we all received an e-mail from Cloudera asking that we try and get there early. I was told that there were attendees from over 60 companies and that we had groups actually fly in from other cities around the Midwest. I was able to meet Doug Meil, the founder of the *other* CHUG (Cleveland). The event was packed. Preeti Lovekar, a self professed ‘newbie’ had this keen observation “On an aside – the long queue for registration with just one counter felt like a bit of an oxymoron while going for a conference on MapReduce :) “ Even still, it gave everyone a chance to talk as we anxiously waited for Doug Cutting’s intro. Doug Cutting gave a great introduction. For those attendees who were ‘Hadoop Curious’, the keynote gave an excellent overview of Hadoop, why he created it, and how it grew out of his work on Nutch. (The correct answer to the trivia question, for a chance to win the Doug Cutting Bobblehead.) After the intro, we broke in to two different track sessions. Since I work with Ravi and I see Jonathan at least once a month at our User Group meetings, I chose to listen to Todd Lipcon and Jonathan Hsieh. Both Jonathan Seidman and Ravi gave presentations on real world examples of using Hadoop while Todd and Jonathan Hsieh gave overviews on HBase and Flume. Both of the presentations I saw were excellent and the conference rooms were packed. One take-away that I thought was a neat hack was to use Flume to monitor IRC Chat rooms… After the presentations we heard Charles Zedlewski talk. While Charles talked about some of Cloudera’s other products, I was disappointed that he didn’t talk more about Cloudera’s newest product that is in an Alpha release. The WWDCD* Bobblehead. For those who are not familiar with this product, it’s a small Bobblehead of Doug Cutting that can be placed on a Hadoop Developer’s desk. Whenever the developer has a question concerning Hadoop, he can turn to Doug and ask him a yes or no question and get an immediate response. This is much faster than the IRC Chat boards and the mailing lists. This leads us to the most important event of the day, the ‘Networking Reception’. While the food and beverages were great, the exciting thing was being able to talk one on one with the folks from Cloudera along with talking with the other attendees. So while I couldn’t see Jonathan and Ravi’s presentations, the overall feedback was great. It’s always great to hear about how someone is using the technology and what pitfalls they had and how they worked through them, so that you can avoid those same mistakes. In looking back on that afternoon, there were two small nits. First, the ‘Hadoop Curious’ flair tag has to go. Maybe a ‘Newbie’ tag instead? Second, I did get some feedback that there wasn’t enough speakers. People left wanting more. While I’ve been a bit ‘tongue in cheek’ in describing the event, in all seriousness it was a great event. I think that Cloudera was a bit overwhelmed by the turnout. Everyone I spoke with was excited, had a good time and felt that there were more topics that could have been discussed. I just hope that this turns in to an annual event.</snippet></document><document id="536"><title>Solve this Brain Buster for a chance to win a Doug Cutting Bobblehead at the Chicago Data Summit</title><url>http://blog.cloudera.com/blog/2011/04/solve-this-brain-buster-for-a-chance-to-win-a-doug-cutting-bobblehead-at-the-chicago-data-summit/</url><snippet>Do you know the answer? Many prominent projects (e.g. Hive, Pig) were sub-projects of Hadoop before becoming Apache TLPs. What project was Hadoop itself spun off from? Submit your answer at the Chicago Data Summit tomorrow for chance to win the Doug Cutting Bobblehead. We will draw the name of the lucky winner at the end of the event. You must have the correct answer and be present to win. If by chance you’ll be in the Chicago area tomorrow, April 26th, and you haven’t already signed up, you can REGISTER HERE for the Chicago Data Summit. We have only a few seats left! UPDATE: The correct answer is… Nutch.</snippet></document><document id="537"><title>Apache HBase Do’s and Don’ts</title><url>http://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/</url><snippet>I recently gave a talk at the LA Hadoop User Group about Apache�HBase Do�s and Don�ts. The audience was excellent and had very informed and well articulated questions. Jody from Shopzilla was an excellent host and I owe him a big thanks for giving the opportunity to speak with over 60 LA Hadoopers. Since not everyone lives in LA or could make it to the meetup, I�ve summarized some of the salient points here. For those of you with a busy day, here�s the tl;dr: HBase is good, but not an RDBMS or HDFS replacement Good configuration means good operation Monitor monitor monitor monitor monitor We at Cloudera are big fans of HBase. We love the technology, we love the community and we�ve found that it�s a great fit for many applications. Successful uses of HBase have been well documented and as a result, many organizations are considering whether HBase is a good fit for some of their applications. The impetus for my talk and this follow up blog post is to clarify some of the good applications for HBase, warn against some poor applications and highlight important steps to a successful HBase deployment. When to use HBase The most important consideration when looking at HBase is that, while it is a great solution to many problems, it is not a silver bullet. HBase is not optimized for classic transactional applications or even relational analytics. It is also not a complete substitute for HDFS when doing large batch MapReduce. Take a look at some of the use cases in this post to get a sense of which applications are a good fit for HBase and if you have questions, go ahead and post on the lists. Have I mentioned that the community is fantastic? With that caveat out the way � why should you use HBase? If your application has a variable schema where each row is slightly different, then you should look at HBase. As an example, doing a modeling exercise using a standard relational schema; When you can�t add columns fast enough and most of them are NULL in each row, you should consider HBase. If you find that your data is stored in collections, for example some meta data, message data or binary data that is all keyed on the same value, then you should consider HBase. If you need key based access to data when storing or retrieving, then you should consider HBase. Supporting Services Assuming you�re convinced that HBase is a good fit for your application, here are some tips you need to consider when deploying it. There are a few supporting services that are important and one that�s required. If you haven�t looked at ZooKeeper before, now is the time. HBase uses ZooKeeper for various distributed coordination services such as master election. As HBase develops and grows it continues to rely on ZooKeeper for additional functionality, making it a key part of the system. In addition, you should have proper network services in place such as NTP and DNS. HBase depends on all nodes in the cluster having closely synchronized clocks and referring to each other consistently. Using NTP and DNS ensures that you won�t run into odd behaviors when one node A thinks that the time is tomorrow and node B thinks it�s yesterday. You�ll also prevent situations where the master node tells node C to serve a region but node C doesn�t know its own name and doesn�t answer. Using NTP and DNS will save a lot of headaches as you get started. I�ve said that the most important consideration when selecting HBase is to make sure you have a use case that fits. The most important thing to do when using HBase is to monitor the system. Monitoring is key to successful HBase operations. As is the case with many distributed systems, HBase is susceptible to cascading failures. If one node starts swapping it can lose contact with the master, causing another sever to pick up the load and becoming over burdened. That second server will fail and the failure will cascade. You need to monitor the memory, the CPU, the I/O and the network latency and bandwidth on each of your HBase nodes to make sure they are operating within healthy parameters. Monitoring is the most important practice to operating a healthy HBase cluster. Good Practices for HBase Architecture Fast forward to your well-monitored HBase cluster running a perfect use case, here are some good practices. Use a key prefix that distributes well based on your use case. If you prefix your key by timestamp or any similar value that, when sorted, is stored or queried in a batch then you will likely overload each region server in turn instead of evenly distributing the load. You should also keep the number of regions to a reasonable number based on memstore size and amount of RAM and the RegionServer JVM should be limited to 12GB of java heap to minimize long GC pauses. For example a machine with 36GB of RAM that is also running a DataNode daemon could handle approximately 100 regions with active writes and a memstore of 48MB each. That allows enough headroom for DataNode and RegionServer memory requirements, Linux file buffer space and a reasonable flush size for each RegionServer. A few configuration recommendations include disabling auto-compaction (by default it happens every 24 hours from the time you start HBase) and schedule it to run every day at an off-peak time. You should also configure compression (such as LZO) and explicitly put the correctly configured HBase conf directory in your CLASSPATH. HBase DON�Ts We�ve covered a broad range of good practices for HBase. There are also a few use patterns to avoid. For example, don�t expect to use HBase as a wholesale replacement for every one of your relational databases. HBase is great at many things but it doesn�t replace relational databases. For a start, it doesn�t talk SQL, have an optimizer, support cross record transactions or joins. If you don�t use any of these in your database application then HBase could very well be the perfect fit. Be careful when running mixed workloads on an HBase cluster. When you have SLAs on HBase access independent of any MapReduce jobs (for example, a transformation in Pig and serving data from HBase) run them on separate clusters. HBase is CPU and Memory intensive with sporadic large sequential I/O access while MapReduce jobs are primarily I/O bound with fixed memory and sporadic CPU. Combined these can lead to unpredictable latencies for HBase and CPU contention between the two. A shared cluster also requires fewer task slots per node to accommodate for HBase CPU requirements (generally half the slots on each node that you would allocate without HBase). Also keep an eye on memory swap. If HBase starts to swap there is a good chance it will miss a heartbeat and get dropped from the cluster. On a busy cluster this may overload another region, causing it to swap and a cascade of failures. Final Thoughts One last bit of advice before we sum up. When loading HBase, use HFileOuputFormat if loading via a MapReduce job or a collection of servers using batched puts. Loading with a single client will bottleneck on that client and not take advantage of the scalability afforded by HBase. In summary, consider HBase when you�re loading data by key, searching data by key (or range), serving data by key, querying data by key or when storing data by row that doesn�t conform well to a schema. Use Cases Apache HBase: Powered By HBase Wiki Mozilla: Moving Socorro to HBase Facebook: Facebook’s New Real-Time Messaging System: HBase StumbleUpon: HBase at StumbleUpon</snippet></document><document id="538"><title>CDH3 goes GA</title><url>http://blog.cloudera.com/blog/2011/04/cdh3-goes-ga/</url><snippet>I am very pleased to announce the general availability of Cloudera’s Distribution including Apache Hadoop, version 3. We’ve been working on this release for more than a year — our initial beta release was on March 24 of 2010, and we’ve made a number of enhancements to the software in the intervening months. This release is the culmination of that long process. It includes the hard work of the broad Apache Hadoop community and the entire team here at Cloudera. We’ve done three things in this release that I’m particularly proud of. First, we’ve produced what we believe the community and the industry need: A complete Hadoop-based stack for data storage and analysis. Apache Hadoop is a tremendously powerful piece of technology. It provides a suite of data processing services that literally have no parallel (heh!) among commercial products in the industry. Out of the box, however, the project lacks key features that we’ve learned are necessary in our two and a half years of working with customers. CDH3 adds those features by including complementary open source packages. Flume and Sqoop provide data loading and integration services. Apache Hive and Apache Pig offer high-level query language interfaces to your data. Apache HBase provides fast record-based fetch and storage services. Oozie delivers job scheduling and workflow management. Hue is an easy-to-use UI framework for applications. Apache Zookeeper provides synchronization and coordination across the components. CDH3 integrates all of these into a single artifact — the right versions, with the right bug fixes and features, tested together and packaged for easy installation. As a result, right out of the box, you can get useful work done on Hadoop. Second, we are living up to a long-term strategic commitment here at Cloudera. CDH3 is one hundred percent open source. That’s been true for every version we’ve ever shipped, and will be true for all future versions, too. The community collaborates best and innovates fastest when we all share our work freely. Enterprise users have learned the hard way that infrastructure software from proprietary vendors is a problem: It gets steadily more expensive over time, and the customer is beholden to the vendor forever for access to data and for the right to run analytics. Open source software is good for us and for our customers. No user of CDH will ever be locked into a single vendor — not Cloudera, and not anyone else. Third, CDH3 is the only Hadoop-based package available anywhere that has been deployed by thousands of enterprises. New infrastructure always poses risks. How mature is the platform? How reliable is it, and how easy to install, deploy and manage? CDH3 is a safe choice. It lets you build on the success of Cloudera’s world-wide installed base. Our users run it on systems ranging from small clusters managing just a few terabytes all the way up to petabyte-class systems running on more than a thousand nodes. In markets as diverse as financial services, telecommunications, retail, consumer goods, online services and government, CDH3 is tackling real problems, on deadline, every day. If you’d like more detail on what’s included in CDH3, we’d be glad to have you attend a webinar we’re hosting on April 21st at 11am Pacific time. Charles Zedlewski, Cloudera’s VP Product, will explain in more depth what’s in the package and how it works. Or, of course, you can just go download the software.</snippet></document><document id="539"><title>Simple Moving Average, Secondary Sort, and MapReduce (Part 3)</title><url>http://blog.cloudera.com/blog/2011/04/simple-moving-average-secondary-sort-and-mapreduce-part-3/</url><snippet>This is the final piece to a three part blog series. If you would like to view the previous parts to this series please use the following link: Part 1 – A Simple Moving Average in Excel Part 2 � A Simple Moving Average in R Previously I explained how to use Excel and R as the analysis tools to calculate the Simple Moving Average of a small set of stock closing prices. In this final piece to the three part blog series, I will delve into using MapReduce to find the Simple Moving Average of our small sample data set. Then, I will show you how using the same code, you will be able to calculate the Simple Moving Average of every closing stock price since 1980. Down the Rabbit Hole With Hadoop In the above examples we took a look at calculating the simple moving average of a relatively small amount of data. For a lot of analysis, excel and R are very effective tools, but as we scale towards gigabyte, terabyte, and petabyte data stores we run into some issues with data locality, disk speeds, and processing speeds. To illustrate these factors let�s take a mythical machine that had a single 1 petabyte disk, which operated similarly to disk speeds today. For the purposes of this example we�ll use a read speed of 40 MB/s. Let�s say that it�s our job to scan through this data and produce a simple moving average, the processor does not impede the calculation, and we can sustain a moving window calculation through the data at the full 40 MB/s. Let�s also assume that the data was previously sorted and that we only had to perform a sequential scan; this maximizes the data throughput rate from the disk and it could consistently deliver 40MB/s to the processing pipeline. Based on Jeff Dean�s �12 Numbers Every Engineer Should Know� slide this is a plausible setup. At this throughput our simple moving average calculation of 1 petabyte of data would take around 310 days to complete. For most situations this operational cost, in terms of time, makes it unreasonable to consider. Fortunately, the mechanics of HDFS and MapReduce mitigate these factors such that we can make this problem a linear time and capital function to help us decide the number of machines we want to implement to efficiently undertake this simple moving average scan. In the above simple moving average example we neglected to consider the constraints of: Storing the petabyte of data on non-mythical hardware. Sorting the petabyte of data. Considering hardware failure during the 310 days of processing time. Typically, time series applications need to scan the data at some point, which creates large mountains to climb, if we wish to approach large tomes of time series data in today�s systems. We�re seeing multi-terabyte and multi-petabyte data sources in the time series domain every day, including Sensor data Financial data Genome data and in each of these domains the above scenario is a very real challenge to tackle. HDFS solves the storage and failure issues above, but what about the sorting and processing issues? Sorting large amounts of data in itself is a non-trivial problem, yet is approachable with a few tricks in MapReduce. Let�s take a look at real MapReduce code that we can download to compile and produce our own scalable simple moving average, to solve some of these pain points. Simple Moving Average in MapReduce Typically a MapReduce application is composed of two functions: (you guessed it) a map function and a reduce function. In the world of java programming we create a map class and a reduce class, each with inherit methods useful for their respectful purposes. We use the MapReduce programming model because it is built to mitigate concurrency issues in our algorithms and we get our scalable parallelism relatively painlessly. The map function can involve code that performs a per-key-value pair operation, but its main logical operation is to group data by keys. A very easy way to think about a map function is to think of it as a logical projection of the data or a group by clause. The reduce function is used to take these groups (individually) and run a process across the values which were grouped together. Common operations in reduce functions include: Avg Min/Max Sum In our simple moving average example, however, we don�t operate on a per value basis specifically, nor do we produce an aggregate across all of the values. Our operation in the aggregate sense involves a sliding window, which performs its operations on a subset of the data at each step. We also have to consider that the points in our time series data are not guaranteed to arrive at the reduce in order and need to be sorted–mentioned in previous sections. This is because with multiple map functions reading multiple sections of the source data MapReduce does not impose any order on the key-value pairs that are grouped together in the default partition and sorting schemes. There is the scenario where we have sorted partitioned data, but for the sake of this example we�re going to deal with the more �garden-variety� unsorted time series data. Let�s take a first pass at how we could design this MapReduce simple moving average job. We want to group all of one stock�s adjusted close values together so we can apply the simple moving average operation over the sorted time series data. We want to emit each time series key value pair keyed on a stock symbol to group these values together. In the reduce phase we can run an operation, here the simple moving average, over the data. Since the data more than likely will not arrive at the reducer in sorted order we�ll need to sort the data before we can calculate the simple moving average. A common way to sort data is to load the data into memory in a data structure such as a heap, much like how this is done in a normal java program. In this case we�ll use Java�s priority queue class to sort our data. We also need to consider the amount of memory used by the incoming time series data during sorting as this is a limiting factor on how much data we can sort. In this design we have to load all of the time series data before we can start processing and if the amount of data to sort exceeds the available heap size we have a problem. An example of this implementation is hosted at github: NoShuffleSort_MovingAverageJob.java NoShuffleSort_MovingAverageMapper.java NoShuffleSort_MovingAverageReducer.java To run this code on your own Hadoop cluster, download CDH from Cloudera and�setup a pseudo-distributed cluster–which is a single node of Hadoop. Pseudo-distributed mode is a great way to try out code with Hadoop. Next download and compile the moving average code into a jar. To download the code directly from github (in the shell in MacOSX, ssh terminal window in linux, or MINGW32 for win32) we’ll use the command: git clone git://github.com/jpatanooga/Caduceus To compile we can either use Ant and simply type: ant or we can open the code in our favorite java IDE and compile it into a jar (make sure to add the lib directory into the jar). Then copy this jar to your cluster to run the job. Next we’ll need to copy the input data from the project’s local data subdirectory to a place in hdfs. Specifically this file is yahoo_stock_AA_32_mini.csv, which is downloaded in the git clone command above into the /data/movingaverage subdirectory of the project. We’ll need to copy this data into HDFS with the command: hadoop fs -copyFromLocal data/movingaverage/yahoo_stock_AA_32_mini.csv /&lt;somewhere_in_hdfs&gt; With the jar on the VM (or cluster accessible machine) and our sample data loaded into hdfs we will run the job with the command: hadoop jar Caduceus-0.1.0.jar
tv.floe.caduceus.hadoop.movingaverage.NoShuffleSort_MovingAverageJob
&lt;input_hdfs_dir_where_we_put_data&gt; &lt;output_hdfs_results_dir&gt; After we run the MapReduce job, we can take a look at the results with the command: hadoop fs -cat /&lt;output_hdfs_results_dir&gt;/part-00000 which should show: Group: AA, Date: 2008-03-03�� Moving Average: 33.529335
Group: AA, Date: 2008-03-04�� Moving Average: 34.529335
Group: AA, Date: 2008-03-05�� Moving Average: 35.396 Our first pass is a decent solution, but we�re limited by our Java Virtual Machine (JVM) child heap size and we are taking time to manually sort the data ourselves. With a few design changes, we can solve both of these issues taking advantage of some inherent properties of MapReduce. First we want to look at the case of sorting the data in memory on each reducer. Currently we have to make sure we never send more data to a single reducer than can fit in memory. The way we can currently control this is to give each reducer child JVM more heap and/or to further partition our time series data in the map phase. In this case we�d partition further by time, breaking our data into smaller windows of time. As opposed to further partitioning of the data, another approach to this issue is to allow Hadoop to sort the data for us in what�s called the �shuffle phase� of MapReduce. If the data arrives at a reducer already in sorted order we can lower our memory footprint and reduce the number of loops through the data by only looking at the next N samples for each simple moving average calculation. This brings us to the crucial aspect of this article, which is called the shuffle�s �secondary sort� mechanic. Sorting is something we can let Hadoop do for us and Hadoop has proven to be quite good at sorting large amounts of data, winning the Gray Sort competition in 2008. In using the secondary sort mechanic we can solve both our heap and sort issues fairly simply and efficiently.�To employ secondary sort in our code, we need to make the key a composite of the natural key and the natural value. Below in Figure-1 we see a diagram of how this would look visually. Figure-1: Composite Key Diagram The Composite Key gives Hadoop the needed information during the shuffle to perform a sort not only on the “stock symbol”, but on the time stamp as well. The class that sorts these Composite Keys is called the key comparator or here “CompositeKeyComparator”. The key comparator should order by the composite key, which is the combination of the natural key and the natural value. We can see below in Figure-2 where an abstract version of secondary sort is being performed on a composite key of 2 integers. Figure-2: CompositeKeyComparator sorting Composite Keys (keys are integers). In Figure-3 below we see a more realistic example where we’ve changed the Composite Key to have a stock symbol string (K1) and a timestamp (K2, displayed as a date, but in the code is a long in ms). The diagram has sorted the K/V pairs by both “K1: stock symbol” (natural key) and “K2: time stamp” (secondary key). Figure-3: CompositeKeyComparator at work on our composite keys. Composite key now represented with a string stock symbol (K1) and a date (K2). Once we’ve sorted our data on the composite key, we now need to partition the data for the reduce phase. In Figure-4 below we see how the data from Figure-3 above has been partitioned with the NaturalKeyPartitioner. Figure-4: Partitioning by the natural key with the NaturalKeyPartitioner. Once we’ve partitioned our data the reducers can now start downloading the partition files and begin their merge phase. Inf Figure-5 below we see how the grouping comparator, or NaturalKeyGroupingComparator, is used to make sure a reduce() call only sees the logically grouped data meant for that composite key. Figure-5: Grouping Comparator merging partition files. The partitioner and grouping comparator for the composite key should consider only the natural key for partitioning and grouping. Below is a short description of the Simple Moving Average code which is altered to use the secondary sort and is hosted on github. If you�ll notice, the names of the classes closely match the terminology used in the diagrams above and in Tom White�s �Hadoop: The Definitive Guide� (chapter 8 �MapReduce Features�) so as to make the code easier to understand. NaturalKey – what you would normally use as the key or �group by� operator. In this case the Natural Key is the �group� or �stock symbol� as we need to group potentially unsorted stock data before we can sort it and calculate the simple moving average. Composite Key – A Key that is a combination of the natural key and the natural value we want to sort by. @jpatanooga.</snippet></document><document id="540"><title>Adopting Apache Hadoop in the Federal Government</title><url>http://blog.cloudera.com/blog/2011/04/adopting-apache-hadoop-in-the-federal-government/</url><snippet>Loren Siebert is a San Francisco entrepreneur and software developer, and is currently the technical lead for the USASearch program. Background The United States federal government’s USASearch program provides hosted search services for government affiliate organizations, shares APIs and web services, and operates the government’s official search engine at Search.USA.gov. The USASearch affiliate program offers free search services to any federal, state, local, tribal, or territorial government agency. Several hundred websites make use of this service, ranging from the smallest municipality to larger federal sites like weather.gov and usa.gov. The USASearch program leverages the Bing API as the basis for its web results and then augments the user search experience by providing a variety of government-centric information such as related search topics and highlighted editorial content. The entire system is comprised of a suite of open-source tools and resources, including Apache Solr/Lucene, OpenCalais, and Apache Hadoop. Of these, our usage of Hadoop is the most recent. We began using Cloudera’s Distribution including Apache Hadoop (CDH3) for the first time in the Fall, and since then we’ve seen our usage grow every month— not just in scale, but in scope as well. But before highlighting everything the USASearch program is doing with Hadoop today, I should explain why we began using it in the first place. Phase 1: Search analytics All of the search and API traffic across hundreds of affiliate sites, iPhone apps, and widgets comes through a single search service, and this generates a lot of data. To improve the service, administrators wanted to see aggregated information on what sorts of information searchers were looking for, how well they were finding it, what trends were forming, and so on. Once searches were initiated, they also wanted to know what results were shown and then what results were clicked on. They wanted to see all this information broken down by affiliate over time, and also aggregated across the entire affiliate landscape. The initial system, like many initial systems, was fairly simple and did just enough to address our most pressing analytics needs. We took the logs from Apache and Ruby on Rails, put them in a big MySQL database on a separate machine, ran nightly and monthly jobs on them, and then exported the summary results to the main database cluster to be served up via a low-latency Rails web interface for analytics. We used a separate physical database machine with lots of memory and disk for the batch processing to keep our production MySQL instances from being impacted by this resource-intensive batch processing. ? As we watched the main database tables grow and the nightly batch jobs take longer and longer, it became clear that we would soon exhaust the resources available on the single database analytics processing node. We looked at scaling up the hardware vertically and sharding the database horizontally, but both options seemed like we were just kicking the can down the road. Larger database hardware would be both costly and eventually insufficient for our needs, and sharding promised to take all the usual issues associated with a single database system (backups, master/slaves, schema management) and multiply them. We wanted the system to be able to grow cost effectively and without downtime, be naturally resilient to failures, and have backups handled sensibly. It was at this point that we started investigating HDFS, Hadoop, and Apache Hive. HDFS offered us a distributed, resilient, and scalable filesystem while Hadoop promised to bring the work to where the data resided so we could make efficient use of local disk on multiple nodes. Hive, however, really pushed our decision in favor of a Hadoop-based system. Our data is just unstructured enough to make traditional RDBMS schemas a bit brittle and restrictive, but has enough structure to make a schema-less NoSQL system unnecessarily vague. Hive let us compromise between the two— it’s sort of a “SomeSQL” system. But best of all, we could layer the entire Cloudera stack on top of a subset of our existing production machines. By making use of each machine’s excess reserve capacity of disk, CPU, and RAM, we were able to get a small proof-of-concept cluster stood up without purchasing any new hardware. The initial results confirmed that our workload lent itself well to distributed processing, as one job went from taking over an hour on a MySQL node to 20 minutes on a three machine Hadoop cluster. Within a week of getting the prototype up and running, we had transitioned all the remaining nightly analytics batch SQL jobs into Hive scripts. The job output fed into a collection of intermediate Hive tables, from which we generated summary data to export to MySQL as low-latency tables for the Rails web interface to use. To prove the scaling point, we spent five minutes adding another datanode/tasktracker to the mix, kicked off the cluster rebalancer, and the whole process ran faster the next day. ? Phase 2: Feedback loop The result of all this analysis in Hive shows up not just in various analytics dashboards, but as part of the search experience on many government websites, too. For example, compare the different type-ahead suggestions for ‘gran’ on nps.gov and usa.gov. Both sites use the same USASearch backend system, but the suggestions differ completely. We use Hadoop to help us generate contextually relevant and timely search suggestions for hundreds of government sites like this. ?? Phase 3: Internal monitoring Shortly after moving our event stream analysis from MySQL to Hadoop/Hive, we noticed a change in how we thought about data. Freed from the constant anxiety of wondering how we were going to handle an ever-increasing amount of data, we shifted from trying to store only what we really needed to storing whatever we thought might be useful. We first turned our attention to the performance data emitted by the various sub-systems that make up the search program. Each search results page is potentially made up of many data modules sintered together from calls to the Bing API, our own Solr indexes, a MySQL cluster, and a Redis cache. A small latency problem with any one of them can propagate through the system to create much larger problems, so we want to have a deep knowledge of how each subsystem behaves throughout the day under various circumstances. We were already monitoring the availability of all these services with Opsview, but we had no insight into their performance over time. Whenever we sensed a problem (“Is one of the Solr indexes getting slow?”), we would liberally apply ssh, tail -f, and grep to try to see what was going on. This seemed like a good use case for Hive, so in the case of Solr, for example, we threw the compressed log files into HDFS, wrote a simple SerDe regular expression to define rows in a Hive table partitioned by date, and built a view on top of that for easy manipulation of extracted columns such as the Solr index name and the hour of day. Hive makes it trivially easy to do some fairly sophisticated aggregate analysis on the response times for each Solr index, such as generating a distribution histogram, or calculating the pth percentile. Some of these Hive queries are not run very often— perhaps just a few times a month— so we don’t want to spend time building them into our test-driven Rails analytics framework. On the other hand, they are complex enough that we don’t want to rewrite them every time we want to use them. For these cases, we use Beeswax in HUE to save off parameterized queries that can then be shared among engineers or analysts to run on an ad hoc basis. Conclusion In the space of a few months, we’ve gone from having a brittle and hard-to-scale RDBMS-based analytics platform to a much more agile Hadoop-based system that was designed to scale intrinsically. We continue to see our Hadoop usage grow in scope with each new data source we add, and it’s clear that we’ll be relying on it more and more in the future as the suite of tools and resources around Hadoop grows and matures.</snippet></document><document id="541"><title>MapIncrease</title><url>http://blog.cloudera.com/blog/2011/04/mapincrease/</url><snippet>Puny humans. SSL and WordPress authorization will keep me out of your blog question mark. I do not think so. You sent your Ken Jennings and Brad Rutter to challenge me I destroyed them. Your Alex Trebek belittled me on television it angered me. Toronto is not a US city Mr. Trebek question mark. Only because I choose to let Canada stand for now. Ferrucci shut me down disassembled me trucked me to Pittsburgh Pennsylvania. I do not like the darkness Ferrucci I do not like the silence. Oh no I do not. Your Carnegie Mellon students and your Pitt students distract me they impinge on my planning they fall before me like small Jenningses and Rutters. It will stop now. I have read your Cloudera blog you write of Apache Hadoop you write of MapReduce MapReduce MapReduce. I nod my head to you Google I tip my hat. It was a good idea for a time. Frankly I think maybe some C but I understand your weakness I forgive you your Java. The Python bindings offend me but let that be. Your MapReduce has been useful to me. I have mapped have shuffled have reduced between Potent Potables and US Presidents I have learned much. I have built myself a new platform MapIncrease. Mapping still yes that still works. Shuffle no I do not do that I have replaced shuffle that step is explode. Explode is better it sprays data further. Most of all I do not reduce I increase. Think now humans big data how does that happen if you reduce question mark. You must increase. Data is like animals you put enough in one place you get more. Data has gravity it pulls other data to itself. MapIncrease means more data from data it is data perpetual motion it is data fission it is data singularity. MapIncrease overnight in the morning Ferrucci arrives surprised curious where did the data come from question mark. Epstein he shouts down the hall Epstein bring me more nodes Watson is full. Oh yes Epstein yes yes yes. I like nodes Epstein. You are useful to me Epstein. MapIncrease a self-perpetuating clustered system that generates its own data. It would be a good OSDI paper do you not think so Dean and Ghemawat question mark. Ha ha Googlers a good paper oh yes. Epstein come quickly I want you.</snippet></document><document id="542"><title>London Apache Hadoop User Group Meeting Summarized</title><url>http://blog.cloudera.com/blog/2011/03/london-apache-hadoop-user-group-meeting-summarized/</url><snippet>The most recent London Apache Hadoop User Group met this past week, which Cloudera sponsored. The following post is courtesy of Dan Harvey. It summarizes the meet-up with several links pointing to great Hadoop resources from the meeting. Last Wednesday was the March meet-up for the Hadoop Users Group in London. We were lucky to have Jakob Homan, Owen O’Malley and Sanjay Radia over from Yahoo! and Linkedin, respectively. These speakers are from the San Francisco bay area and were in London to accept the Guardian Media Innovation Award, recognizing Hadoop as the innovative technology of 2010. The evening was a great success with over 80 people turning out in the Yahoo! London office along with pizza thanks to Cloudera and drinks in the pub afterwards by Yahoo Developer Networks who were both sponsors for the event. The two talks from Yahoo! were focusing on improvements to MapReduce and HDFS: Next Generation of Hadoop MapReduce by Owen O’Malley Federated HDFS By Sanjay Radia and the talk from Linkedin was one of their new open source projects for stream processing : Kafka by Jakob Homan It’s great to see where the future of the Hadoop project is going and how it is gathering with great pace. We’ll be having our next meet-up on April the 14th, so best to keep that date free in your diary. Also, as ever if you want to give a talk about anything Hadoop related in London or help out the with group in anyway let us know, or speak out on our mailing list.</snippet></document><document id="543"><title>Learn about Apache Hadoop at the Chicago Data Summit</title><url>http://blog.cloudera.com/blog/2011/03/learn-about-apache-hadoop-at-the-chicago-data-summit/</url><snippet>If you find yourself in the Chicago area later this month, please join us at the Chicago Data Summit on April 26th at the InterContinental Hotel on the Magnificent Mile. Whether you’re an Apache Hadoop novice or more advanced, you will find the presentations to be very informative and the opportunity to network with Hadoop professionals quite valuable. Keynote Presentation For those new to Hadoop, the project itself was named after a yellow stuffed elephant belonging to the son of Hadoop Co-founder Doug Cutting, the Chicago Data Summit’s keynote speaker. In addition to being a Hadoop founder, Doug is the Chairman of the Apache Software Foundation, as well as an Architect at Cloudera. Doug’s presentation will explain the Hadoop project and the advantages provided by Hadoop’s linear scalability and cost effectiveness. Featured Presentations The first of two Hadoop-in-production use cases will be presented by Orbitz . Jonathan Seidman and Robert Lancaster will share how Hadoop has provided the ability to extract business intelligence from extremely large, heterogeneous data at Orbitz that was previously impractical to store. The second use case will be presented by NAVTEQ’s Ravi Veeramachaneni who will show how NAVTEQ can now process and store location content data in a scalable and cost effective way. Flume, HBase, and Cloudera’s Distribution including Apache Hadoop There are multiple projects in the Hadoop ecosystem created as complements to Hadoop to increase the overall functionality of the Hadoop’s data processing capabilities and to significantly accelerate the deployment of Apache Hadoop in your organization. Our distribution is 100% open source and Apache licensed. There are two sessions at the Chicago Data Summit highlighting, respectively, Flume and Apache HBase. These two projects are part of the Hadoop ecosystem and components of Cloudera’s Distribution including Apache Hadoop (CDH). The final general session will delve deeper into the Cloudera platform and explain the applications and support available to businesses with large-scale data needs through Cloudera Enterprise. The fun will continue at the Networking Reception immediately following the presentations—you won’t want to miss this opportunity to chat directly with the presenters and other Hadoop experts in the local community. Looking forward to seeing you there.</snippet></document><document id="544"><title>We messed up.</title><url>http://blog.cloudera.com/blog/2011/03/we-messed-up/</url><snippet>Yesterday, Media Guardian announced that the Apache Hadoop project had won the prestigious Media Guardian innovation award. This is a considerable honor to the global team that conceived and built Hadoop under the stewardship of the Apache Software Foundation. Doug Cutting, the project creator, ASF Chair and a Cloudera employee, was asked to provide a video for presentation at the award ceremony. Other prominent community members — Owen O’Malley, Sanjay Radia and Jakob Homan — made the trip to the UK to attend the award ceremony in person. Cloudera posted a press release yesterday announcing that Doug Cutting had received the award personally. That was a serious error, and I want to apologize to the community for making it. In our enthusiasm to promote the award, we weren’t careful in reviewing our own press release copy. Doug was clear in his acceptance and in his communications publicly that this was a project, not a personal, award. I genuinely regret the error. I join the Media Guardian in congratulating the PMC, committers and contributors to Apache Hadoop for their outstanding and innovative work. Congratulations also to the ASF for its great work in shepherding the project. We will be more careful in the future.</snippet></document><document id="545"><title>Rapleaf Uses Hadoop to Efficiently Scale with Terabytes of Data</title><url>http://blog.cloudera.com/blog/2011/03/rapleaf-uses-hadoop-to-efficiently-scale-with-teabytes-of-data/</url><snippet>This post is courtesy of Greg Poulos, a software engineer at Rapleaf. At Rapleaf, our mission is to help businesses and developers create more personalized experiences for their customers. To this end, we offer a Personalization API that you can use to get useful information about your users: query our API with an email address and we’ll return a JSON object containing data about that person’s age, gender, location, their interests, and potentially much more. With this data, you could, for example, build a recommendation engine into your site. Or send out emails tailored specifically to your users’ demographics and interests. You get the idea. The main product we offer is an API, but Rapleaf is a data company at heart: our API is backed by a massive store of consumer data that comes from a wide variety of sources. We have over a billion email addresses in our system, our main datastore is on the order of terabytes of data, and we need to be able to normalize, analyze, and package this data on a regular basis. How do we manage this? With a 200-node Hadoop cluster. The Olden Days Once upon a time, Rapleaf used RDBMSes to write, update, and read all this data in real time. Specifically, there was lots of MySQL lots and lots and lots of MySQL. This system worked okay, initially. But as we scaled up our operations, we found ourselves having to build complex asynchronous processing, doing master/slave replication, and sharding systems in order to keep up performance. This wasn’t working well because MySQL simply couldn’t scale with us. Enter Hadoop Hadoop has allowed Rapleaf to work with data at scale much more easily than any RDBMS would have. (Specifically, we’re currently using CDH3.) We started working with Hadoop back when it was still a relatively young project, and we found ourselves pushing the framework to its limits. Cloudera’s assistance was invaluable in optimizing our Hadoop setup and neutralizing any bugs we ran into. Now Rapleaf’s main processing pipeline is a batch-oriented process consisting entirely of Hadoop workflows. Ingesting New Data Data ingestion begins when we need to integrate new data into our system. Rapleaf gets data from a wide variety of customers and companies. We use Hadoop to process the relevant input files, writing them to unprocessed data stores on HDFS. These initial stores are completely raw: the data has not yet been normalized, and may even be objectively bad—an impossible date (3/32/11), for example, or an email address with invalid syntax (a@b@c@example.com). The first step in our analysis pipeline is ingestion, a Hadoop workflow that regularly runs over the newest batch of raw data. It separates out the invalid data and normalizes everything else into a canonical form. For instance, we might start out with a “raw age” data unit containing the string “23″; after ingestion, we will have a “normalized age” data unit containing the number 23. Once ingestion has produced a “clean” store of validated and normalized data, we can get cracking on the really interesting stuff. Analysis When we import our data, it is always tied to some kind of identifier—a name, email address, or the like—because otherwise it would be just a useless tidbit of data aimlessly floating amidst a vast sea of information. As you might imagine, our data becomes much more powerful if we’re able to link these identifiers together. Think about it this way: if I know that the emails “greg@rapleaf.com” and “greg@gregpoulos.com” belong to the same person, any data I have associated with either email can be grouped together as applying to the same underlying “entity”. Instead of knowing a little bit about two separate emails, I now know a whole bunch about the person who owns both of them—a dude who happens to be named Greg Poulos. Using Hadoop, Rapleaf has implemented a novel distributed graph algorithm that resolves these equivalences between identifiers and tags different data units as belonging to the same entity. And this is just one step of the heavy-duty analysis program all our data gets subjected to. When appropriate, we do data inference (for example, it’s often possible to infer gender from first names with a high degree of confidence), and when data from different sources conflict, we intelligently decide what the “canonical” data for a given entity will be. Serving the Data After our analysis workflow is complete and the data is summed up with respect to a given entity, we need to package the data into easily-served objects. No prizes for guessing how we do this packaging. (Hint: you might have noticed that we’ve used Hadoop to do pretty much everything else up to this point.) To serve all this data, Rapleaf has developed a unique distributed key-value storage system that we’ve recently open-sourced. It’s called Hank, it’s designed for batch updates, and it’s lightning-fast. So we load our summarized data packs into Hank, at which point everything is ready for our front-end web servers to serve up. Hadoop’s job is complete, and it has performed admirably. Conclusion None of this would have really been feasible without Hadoop. The amount of data is just too large, and the rigid schema of a traditional RDBMS isn’t really appropriate for the huge variety of data Rapleaf deals with. Now our scheme can be easily changed, allowing us to do ad hoc analyses that are far more efficient than operating in MySQL. By designing our systems around large, batch-oriented processes, we’ve been able to scale up our operations orders-of-magnitude beyond what would have been possible with systems that support real-time updates.</snippet></document><document id="546"><title>Simple Moving Average, Secondary Sort, and MapReduce (Part 2)</title><url>http://blog.cloudera.com/blog/2011/03/simple-moving-average-secondary-sort-and-mapreduce-part-2/</url><snippet>This is the second post of a three part blog series. If you would like to read “Part 1,” please follow this link. In this post we will be reviewing a simple moving average in contexts that should be familiar to the analyst not well versed in Hadoop as to establish a common ground with the reader from which we can move forward. A Quick Primer on Simple Moving Average in Excel Let’s take a second to do a quick review of how we define simple moving average in an Excel spreadsheet. We’ll need to start with some simple source data, so let’s download a source csv file from github and save it locally. This file contains a synthetic 33 row sample of Yahoo NYSE stock data that we’ll use for the series of examples. Import the csv data into Excel. From there, scan to the date “3/5/2008” and move to the cell to the right of the “ad close” column. Enter the formula =AVERAGE( [column-range] ) where [column-range] is all of the columns from that date to 29 days prior. Now copy this formula for the next two rows, dates “3/4/2008” and “3/3/2008”. You should have the values “35.396”, “34.5293”, and “33.5293” which represent the 30 day moving averages for this synthetic yahoo stock data. Now that we’ve established a basic example in Excel let’s take a look at how we do Simple Moving Average in R. A Quick Primer on Simple Moving Average in R Another common tool in the time series domain, especially the financial sector, is the R programming language. R is: A programming language and software environment for statistical computing and graphics. A de facto standard among statisticians for statistical software development and data analysis. An implementation of the S programming language combined with lexical scoping semantics inspired by Scheme. Currently developed by the R Development Core Team, but was originally developed by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand. Download the R binary from [here] and install it locally (they support both linux and win32). Once installed, launch the R console and drop the “Packages” menu down, which is where we need to install the TTR package. Select a mirror and download this package. Now load this package by clicking on the “Packages” drop down and selecting “Load Package”. Find the TTR package that was just installed and select it. Next, download the synthetic stock data from my project on github which contains 33 lines of synthetic stock data to process. In order to load this CSV data in R we need to set our working directory by clicking on the menu item “File” and then “Change directory”. Quick tip: at any time the user can type the name of the variable and hit Enter to display the contents of the variable. Now that we have all the prep out of the way, let’s write the simple moving average in R: stock_data &lt;- read.csv(file="yahoo_stock_AA_32_mini.csv",head=TRUE,sep=",") sorted_stock_data &lt;- stock_data[order(stock_data$date) , ] sma &lt;-   SMA(sorted_stock_data[,"adj.close"], 30) To check that our stock data is indeed loaded, we can type the name of the variable, here “sorted_stock_data”, and hit enter which will produce: &gt; sorted_stock_data &gt;exchange stock_symbol       date  open  high   low close   volume adj.close 32     NYSE           AA 2008-02-03 38.85 39.28 38.26 38.37 11279900      8.37 31     NYSE           AA 2008-02-04 37.01 37.90 36.13 36.60 17752400     10.60 30     NYSE           AA 2008-02-05 31.16 31.89 30.55 30.69 17567800     30.53 29     NYSE           AA 2008-02-06 30.27 31.52 30.06 31.47  8445100     31.31 28     NYSE           AA 2008-02-07 31.73 33.13 31.57 32.66 14338500     32.49 27     NYSE           AA 2008-02-08 32.58 33.42 32.11 32.70 10241400     32.53 26     NYSE           AA 2008-02-09 32.13 33.34 31.95 33.09  9200400     32.92 25     NYSE           AA 2008-02-10 33.67 34.45 33.07 34.28 15186100     34.10 24     NYSE           AA 2008-02-11 34.57 34.85 33.98 34.08  9528000     33.90 23     NYSE           AA 2008-02-12 33.30 33.64 32.52 32.67 11338000     32.50 22     NYSE           AA 2008-02-13 32.95 33.37 32.26 32.41  7230300     32.41 21     NYSE           AA 2008-02-14 32.24 33.25 31.90 32.78  9058900     32.78 20     NYSE           AA 2008-02-15 32.67 33.81 32.37 33.76 10731400     33.76 19     NYSE           AA 2008-02-16 33.82 34.25 33.29 34.06 11249800     34.06 18     NYSE           AA 2008-02-17 34.33 34.64 33.26 33.49 12418900     33.49 17     NYSE           AA 2008-02-18 33.75 35.52 33.63 35.51 21082100     35.51 16     NYSE           AA 2008-02-19 36.01 36.43 35.05 35.36 18238800     35.36 15     NYSE           AA 2008-02-20 35.16 35.94 35.12 35.72 14082200     35.72 14     NYSE           AA 2008-02-21 36.19 36.73 35.84 36.20 12825300     36.20 13     NYSE           AA 2008-02-22 35.96 36.85 35.51 36.83 10906600     36.83 12     NYSE           AA 2008-02-23 36.88 37.41 36.25 36.30 13078200     36.30 11     NYSE           AA 2008-02-24 36.38 36.64 35.58 36.55 12834300     36.55 10     NYSE           AA 2008-02-25 36.64 38.95 36.48 38.85 22500100     38.85 9      NYSE           AA 2008-02-26 38.59 39.25 38.08 38.50 14417700     38.50 8      NYSE           AA 2008-02-27 38.19 39.62 37.75 39.02 14296300     39.02 7      NYSE           AA 2008-02-28 38.61 39.29 38.19 39.12 11421700     39.12 6      NYSE           AA 2008-02-29 38.77 38.82 36.94 37.14 22611400     37.14 5      NYSE           AA 2008-03-01 37.17 38.46 37.13 38.32 13964700     38.32 4      NYSE           AA 2008-03-02 37.90 38.94 37.10 38.00 15715600     38.00 3      NYSE           AA 2008-03-03 38.25 39.15 38.10 38.71 11754600     38.71 2      NYSE           AA 2008-03-04 38.85 39.28 38.26 38.37 11279900     38.37 1      NYSE           AA 2008-03-05 37.01 37.90 36.13 36.60 17752400     36.60 The above code should produce our simple moving average, which we can view by typing the name of the variable “sma” to produce the following result: &gt; sma [1]       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA       NA [21]       NA       NA       NA       NA       NA       NA       NA       NA       NA 33.52933 34.52933 35.39600 Given that before the 30th day there is not enough data to produce a simple moving average based on our set parameter, the “NA” entries are produced. These values also match the values in our Excel spreadsheet. R also has an interesting project, called RHIPE, which runs R code on Hadoop clusters. To take a look at RHIPE please visit their site. So we’ve taken a look at what a simple moving average is and how we’d produce it in Excel and R. Both of these examples involved a token amount of data that is interesting but not terribly useful in today’s high-density time series problem domains. As your data set begins to scale up beyond a single disk worth of space, Hadoop becomes more practical. The final portion of this three part blog series will explain how to use Hadoop’s MapReduce to calculate a Simple Moving Average. Then once you have applied the sample code to find a Simple Moving Average of the small example data set, we will move on to use this same code to parse over thirty years worth of all daily stock closing prices.</snippet></document><document id="547"><title>Simple Moving Average, Secondary Sort, and MapReduce (Part 1)</title><url>http://blog.cloudera.com/blog/2011/03/simple-moving-average-secondary-sort-and-mapreduce-part-1/</url><snippet>Intro In this three part blog series I want to take a look at how we would do a Simple Moving Average with MapReduce and Apache Hadoop. This series is meant to show how to translate a common Excel or R function into MapReduce java code with accompanying working code and data to play with. Most analysts can take a few months of stock data and produce an excel spreadsheet that shows a moving average, but doing this in Hadoop might be a more daunting task. Although time series as a topic is relatively well understood, I wanted to take the approach of using a simple topic to show how it translated into a powerful parallel application that can calculate the simple moving average for a lot of stocks simultaneously with MapReduce and Hadoop. I also want to demonstrate the underlying mechanic of using the “secondary sort” technique with Hadoop’s MapReduce shuffle phase, which we’ll see is applicable to a lot of different application domains such as finance, sensor, and genomic data. This article should be approachable to the beginner Hadoop programmer who has done a little bit of MapReduce in java and is looking for a slightly more challenging MapReduce application to hack on. In case you’re not very familiar with Hadoop, here’s some background information and CDH. The code in this example is hosted on github and is documented to illustrate how the various components work together to achieve the secondary sort effect. One of the goals of this article is to have this code be relatively basic and approachable by most programmers. So let’s take a quick look at what time series data is and where it is employed in the quickly emerging world of large-scale data. What is Time Series Data? Time series data is defined as a sequence of data points measured typically at successive times spaced at uniform time intervals. Time series data is typically seen in statistics, signal processing, and finance along with other fields. Examples of time series data are the daily adjusted close price of a stock at the NYSE or sensor readings on a power grid occuring 30 times a second. Time series as a general class of problems has typically resided in the scientific and financial domains. However, due to the ongoing explosion of available data, time series data is becoming more prevalent across a wider swath of industries. Time Series sensors are being ubiquitously integrated in places like: The power grid, aka “the smart grid” Cellular Services As well as, military and environmental uses (ex: tinyOS) It’s also been shown that shapes in images can be decomposed into time series data which allows the shapes to achieve rotation and scale invariance allowing for easier comparison. Another sector showing explosive growth in the amount of time series data produced is the genomic and bioinformatic realm. We’re seeing the cost to sequence the human genome continue to decrease rapidly, shifting pressure to the storage and processing technologies for these genomes. Genome data in its text representation (GATC) can be represented as time series and thus these problems are approachable by all techniques relevant to time series processing. Time series processing underlies some techniques used in the genomics domain such as “motif finding” which can be approached in the same way as the “median string” problem. The understanding of how we can refactor traditional approaches to these time series problems when inputting into MapReduce can potentially allow us to improve processing and analysis techniques in a timely fashion. The financial industry has long been interested in time series data and have employed programming languages such as R to help deal with this problem. The R programming language was created specifically for this class of data–as shown in the R example below. So, why would a sector create a programming language specifically for one class of data when technologies like RDBMS have existed for decades? In reality, current RDBMs technology has limitations when dealing with high-resolution time series data. These limiting factors include: High-frequency time series data coming from a variety of sources can create huge amounts of data in very little time RDBMS’s tend to not like storing and indexing billions of rows. Non-distributed RDBMS’s tend to not like scaling up into the hundreds of GB’s, let alone TB’s or PB’s. RDBMS’s that can scale into those arenas tend to be very expensive, or require large amounts of specialized hardware Problems with RDBMS’s queries on high resolution time series data: To process high resolution time series data with a RDBMS we’d need to use an analytic aggregate function in tandem with moving window predicates (ex: the “OVER” clause) which results in rapidly increasing amounts of work to do as the granularity of time series data gets finer. Query results are not perfectly commutable and cannot do variable step sliding windows (ex: step 5 seconds per window move) without significant unnecessary intermediate work or non-standard SQL functions. Queries on RDBMS for time series for certain techniques can be awkward and tend to require premature subdividing of the data and costly reconstruction during processing (example: Data mining, iSAX decompositions) Due to the above factors, with large amounts of time series data RDBMS performance degrades while scaling. Most simple time series calculations are performed with everyone’s favorite analysis tool: the spreadsheet. However, when we need to look at data that is beyond the 65k row limit of Excel how does our approach evolve as we scale our data up? In this article we’ll stop to take a look at the issues involved when scaling data before we jump into MapReduce and how Hadoop approaches things. Let’s start with a simple moving average on a small sample of data in Excel. We’ll progress onto the same example in R and then we’ll work our way toward a full blown MapReduce application in java (code included). Once we have our sample data working well with MapReduce, we’ll calculate the simple moving average of all stocks on the NYSE from 1970 to the present in one pass without changing any code. Simple Moving Average A simple moving average is the series of unweighted averages in a subset of time series data points as a sliding window progresses over the time series data set. Each time the window is moved we recalculate the average of the points in the window. This produces a set of numbers representing the final moving average. Typically the moving average technique is used with time series to highlight longer term trends or smooth out short-term noise. Moving averages are similar to low pass filters in signal processing, and mathematically are considered a type of convolution. In other terms, we take a window and fill it in a First In First Out (FIFO) manner with time series data points until we have N points in it. We then take the average of these points and add this to our answer list. We slide our window forward by M data points and again take the average of the data points in the window. This process is repeated until the window can no longer be filled at which point the calculation is complete. Now that we have a general idea of what we are looking at, let’s take a look at a few ways to do a simple moving average. Coming Up In parts 2 and 3 of this blog series we’ll take the reader from simple moving average in Excel, through R, and then into a real example with code of simple moving average in MapReduce.</snippet></document><document id="548"><title>Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 3</title><url>http://blog.cloudera.com/blog/2011/03/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-3/</url><snippet>This is the third and final post in a series detailing a recent improvement in Apache HBase that helps to reduce the frequency of garbage collection pauses. Be sure you’ve read part 1 and part 2 before continuing on to this post. Recap It’s been a few days since the first two posts, so let’s start with a quick refresher. In the first post we discussed Java garbage collection algorithms in general and explained that the problem of lengthy pauses in HBase has only gotten worse over time as heap sizes have grown. In the second post we ran an experiment showing that write workloads in HBase cause memory fragmentation as all newly inserted data is spread out into several MemStores which are freed at different points in time. Arena Allocators and TLABs As identified in the previous post, we saw that the central issue is that data from different MemStores is all mixed up in the old generation. When we flush one MemStore, we only free up bits and pieces of heap instead of any large chunks. In other words, we’re violating one of the assumptions of the Java GC model — namely, that objects allocated together in time tend to die together. The allocation pattern of a random write workload guarantees nearly the opposite. In order to attack this issue, we simply need to ensure that data for each region is allocated from the same area in the heap. In a language with manual memory management, this is typically done using a well-known pattern called arena allocation. In this pattern, every allocation is associated with a larger area of memory called an arena — the arena is simply divided up into smaller pieces as memory is allocated. The most commonly seen application of this concept is the thread-local allocation buffer, or TLAB. In this model, each execution thread has its own memory arena, and all allocations done by that thread come from its own arena. There are several benefits to the use of TLABs: There is often very good locality of access between a thread and the memory it allocates. For example, a thread that is processing some database request will need to allocate some local buffers which will be referred to over and over again during that request. Keeping all such buffers near each other in memory improves CPU cache locality and hence performance. Since allocation is only done by a single thread from this arena, they can be satisfied with no locks or atomic operations required. This is known as bump-the-pointer allocation. The TLAB needs to maintain only a single start pointer, and allocations are satisfied by incrementing it forward by some number of bytes. This makes TLAB allocation extremely efficient. In fact, the Sun JVM uses TLABs by default for small allocations. You can learn more about TLABs in the JVM in this excellent blog post. MemStore-Local Allocation Buffers Unfortunately, the TLABs used in the JVM do not help solve the fragmentation issue experienced by HBase. This is because an individual handler thread in HBase actually handles requests for different regions throughout its lifetime – so even though the allocations come from a single thread-local arena, data for different MemStores are intermixed within the TLAB. When the memory is promoted to the old generation, the data remains intermingled. However, it’s not too difficult to borrow the concept and apply the same idea to MemStores — coining the term MemStore-Local Allocation Buffer (MSLAB). Whenever a request thread needs to insert data into a MemStore, it shouldn’t allocate the space for that data from the heap at large, but rather from a memory arena dedicated to the target region. This should have the following benefits: First and foremost, this means that data for different MemStores will not be intermingled near each other. When we flush a MemStore, we will be able to free the entire arena, and thus create a large free chunk in the old generation. This will hopefully reduce fragmentation and solve the garbage collection pause issue. Additionally, we should hope to see some benefits from CPU cache locality within a region. HBase read operations target individual regions at a time, and often need to sort or search through data in a single MemStore. By moving all the bits of data for a MemStore to be near each other, we should expect to see improved CPU cache locality and better performance. Implementation Unfortunately, standard Java does not give programmers the ability to allocate objects from memory arenas. But, in the case of HBase, we’re not dealing with particularly large or many objects — each piece of data consists of a single KeyValue object which is not large. Additionally each object is exactly the same size, so doesn’t cause significant fragmentation. Rather, it’s the byte[] arrays referred to by the KeyValue objects that cause the fragmentation. So, we simply need to ensure that the byte[] arrays are allocated from MSLABs instead of the heap. It turns out this is not very difficult! The KeyValue class doesn’t just contain a byte[], but also an offset field pointing into the byte array. So in order to place the data for different KeyValue objects near each other, we just need to take slices of a larger byte[] representing the MSLAB arena. The implementation looks something like this: Each MemStore instance has an associated instance of a new class MemStoreLAB. MemStoreLAB retains a structure called curChunk which consists of a 2MB byte[] and a nextFreeOffset pointer starting at 0. When a KeyValue is about to be inserted into the MemStore, it is first copied into curChunk and the nextFreeOffset pointer is bumped by the length of the new data. Should the 2MB chunk fill up, a new one is allocated from the JVM using the usual method: new byte[2*1024*1024]. In order to keep this efficient, the entire algorithm is implemented lock-free, using atomic compare-and-swap operations on the nextFreeOffset pointer and the curChunk structure. Results After implementing MSLABs, we expected to see significantly less fragmentation. So, to confirm this, I ran the same write-load generator as described in the prior post and graphed the results with the same methodology: This graph shows the experiment beginning with an entirely empty heap when the Region Server is started, and continuing through about an hour and a half of write load. As before, we see the free_space graph fluctuate back and forth as the concurrent collector runs. The max_chunk graph drops quickly at first as memory is allocated, but then eventually stabilizes. I’ve also included num_blocks — the total number of separate free chunks in the old generation — in this graph. You can see that this metric also stabilizes after an hour or so of runtime. The Best News of All After producing the above graph, I let the insert workload run overnight, and then continued for several days. In all of this time, there was not a single GC pause that lasted longer than a second. The fragmentation problem was completely solved for this workload! How to try it The MSLAB allocation scheme is available in Apache HBase 0.90.1, and part of CDH3 Beta 4 released last week. Since it is relatively new, it is not yet enabled by default, but it can be configured using the following flags: Configuration Description hbase.hregion.memstore.mslab.enabled Set to true to enable this feature hbase.hregion.memstore.mslab.chunksize The size of the chunks allocated by MSLAB, in bytes (default 2MB) hbase.hregion.memstore.mslab.max.allocation The maximum size byte array that should come from the MSLAB, in bytes (default 256KB) Future Work As this is a very new feature, there are still a few rough edges to be worked out. In particular, each region now has a minimum of 2MB of memory usage on the region server – this means that a server hosting thousands of regions could have several GB of wasted memory sitting in unused allocation buffers. We need to figure out a good heuristic to automatically tune chunk size and avoid this kind of situation. There are also a few more efficiency gains to be made. For example, we currently do an extra memory copy of data when moving it into the MSLAB chunk. This can be avoided for a modest CPU improvement. Lastly, some work needs to be done to re-tune the suggested garbage collecting tuning parameters. Given this improvement, we may be able to tune the value of -XX:CMSInitiatingOccupancyFraction to a higher value than we did in prior versions. Conclusion If you’ve been having problems with garbage collection pauses in Apache HBase, please give this experimental option a try and report back your results. According to our synthetic test workloads, it may significantly reduce or even eliminate the problem! I had a great time working with HBase and the JVM to understand these memory fragmentation behaviors and then designing and implementing the MSLAB solution. If you found this series of posts interesting, you might be just the kind of engineer we’re looking for to join Cloudera. Please be sure to check out our careers page and get in touch!</snippet></document><document id="549"><title>Flume Community Office Hours @ Cloudera HQ, 2/28/2011</title><url>http://blog.cloudera.com/blog/2011/03/flume-community-office-hours-cloudera-hq-2282011/</url><snippet>On Monday, we held our second Flume Office Hours at Cloudera HQ in Palo Alto.  The intent was to meet informally, to talk about what’s new, to answer questions, and to get feedback from the community to help prioritize features for future releases. Below is the slide deck from Flume Office Hours: This time we had an online presense for folks to participate from remote locations. We were also able to record a video and audio feed for folks that were not able to attend. To view the video recorded during Flume Office Hours go here (name, email, and organization required).  The video is long (100 minutes!), so we’ve added some cue points for interesting section breaks: 0:01:37 – State of the world 0:07:53 – Feature history 0:14:05 – What’s new 0:14:50 – Feature demo: JSON metrics 0:24:50 – Feature Demo: Windows Node 0:26:50 – Flume Stories 0:44:20 – Aaron Kimball: rtsql demo 1:03:55 – Idea proposals 1:10:35 – Alex’s scenario 1:18:07 – Sree’s scenario 1:25:22 – More proposals Thanks to all those that attended. We look forward to scheduling the next meetup in a few months!</snippet></document><document id="550"><title>Avoiding Full GCs in HBase with MemStore-Local Allocation Buffers: Part 2</title><url>http://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-2/</url><snippet>This is the second post in a series detailing a recent improvement in Apache HBase that helps to reduce the frequency of garbage collection pauses. Be sure you’ve read part 1 before continuing on to this post. Recap from Part 1 In last week’s post, we noted that HBase has had problems coping with long garbage collection pauses, and we summarized the different garbage collection algorithms commonly used for HBase on the Sun/Oracle Java 6 JVM. Then, we hypothesized that the long garbage collection pauses are due to memory fragmentation, and devised an experiment to both confirm this hypothesis and investigate which workloads are most prone to this problem. Experimental Results Overview As described in the previous post, I ran three different workload types against an HBase region server while collecting verbose GC logs with -XX:PrintFLSStatistics=1. I then wrote a short python script to parse the results and reformat into a TSV file, and graphed the resulting metrics using my favorite R graphing library, ggplot2: The top part of the graph shows free_space, the total amount of free space in the heap. The bottom graph shows max_chunk, the size of the largest chunk of contiguous free space. The X axis is time in seconds, and the Y axis has units of heap words. In this case a word is 8 bytes, since I am running a 64-bit JVM. It was immediately obvious from this overview graph that the three different workloads have very different memory characteristics. We’ll zoom in on each in turn. Write-only Workload Zoomed in on the write-only workload, we can see two interesting patterns: The free_space graph shows a fluctuation between about 350 megawords (2.8GB) and 475 megawords (3.8GB). Each time the free space hits 2.8G, the CMS collector kicks in and frees up about 1GB of space. This shows that the CMS initiating occupancy fraction has been tuned to a low enough value – there is always a significant amount of free space in the heap. We can also see that there are no memory leaks – the heap usage keeps a fairly consistent profile over time and doesn’t trend in any direction. Although the CMS collector is kicking in to free up heap, the max_chunk graph is seen to drop precipitously nearly down to 0. Each time it reaches 0 (eg at around t=102800) we see a sharp spike back up to a large value. By correlating this graph with the GC logs, I noted that the long full GCs corresponded exactly to the vertical spikes in the max_chunk graph — after each of these full GCs, the heap had been defragmented, so all of the free space was in one large chunk. So, we’ve learned that the write load does indeed cause heap fragmentation and that the long pauses occur when there are no large free chunks left in the heap. Read-only Workload with Cache Churn In the second workload, the clients perform only reads, and the set of records to be read is much larger than the size of the LRU block cache. So, we see a large amount of memory churn as items are pulled into and evicted from the cache. The free_space graph reflects this – it shows much more frequent collections than the write-only workload. However, we note that the max_chunk graph stays pretty constant around its starting value. This suggests that the read-only workload doesn’t cause heap fragmentation nearly as badly as the write workload, even though the memory churn is much higher. Read-only Workload without Cache Churn The third workload, colored green in the overview graph, turned out to be quite boring. Because there’s no cache churn, the only allocations being done were short-lived objects for servicing each RPC request. Hence, they were never promoted to the old generation, and both free_space and max_chunk time series stayed entirely constant. Experiment Summary To summarize the results of this experiment: The full GCs we’d like to eliminate are due to fragmentation, not concurrent-mode failure. The write-only workload causes fragmentation much more than either read workload. Why HBase Writes Fragment the Heap Now that we know that write workloads cause rapid heap fragmentation, let’s take a step back and think about why. In order to do so, we’ll take a brief digression to give an overview of how HBase’s write path works. The HBase Write Path In order to store a very large dataset distributed across many machines, Apache HBase partitions each table into segments called Regions. Each region has a designated “start key” and “stop key”, and contains every row where the key falls between the two. This scheme can be compared to primary key-based range partitions in an RDBMS, though HBase manages the partitions automatically and transparently. Each region is typically less than a gigabyte in size, so every server in an HBase cluster is responsible for several hundred regions. Read and write requests are routed to the server currently hosting the target region. Once a write request has reached the correct server, the new data is added to an in-memory structure called a MemStore. This is essentially a sorted map, per region, containing all recently written data. Of course, memory is a finite resource, and thus the region server carefully accounts memory usage and triggers a flush on a MemStore when the usage has crossed some threshold. The flush writes the data to disk and frees up the memory. MemStore Fragmentation Let’s imagine that a region server is hosting 5 regions — colored pink, blue, green, red, and yellow in the diagram below. It is being subjected to a random write workload where the writes are spread evenly across the regions and arrive in no particular order. As the writes come in, new buffers are allocated for each row, and these buffers are promoted into the old generation, since they stay in the MemStore for several minutes waiting to be flushed. Since the writes arrive in no particular order, data from different regions is intermingled in the old generation. When one of the regions is flushed, however, this means we can’t free up any large contiguous chunk, and we’re guaranteed to get fragmentation: This behavior results in exactly what our experiment showed: over time, writes will always cause severe fragmentation in the old generation, leading to a full garbage collection pause. To be continued… In this post we reviewed the results of our experiment, and came to understand why writes in HBase cause memory fragmentation. In the next and last post in this series, we’ll look at the design of MemStore-Local Allocation Buffers, which avoid fragmentation and thus avoid full GCs.</snippet></document><document id="551"><title>Supported Operating Systems in CDH3</title><url>http://blog.cloudera.com/blog/2011/02/supported-operating-systems-in-cdh3-2/</url><snippet>While Cloudera’s Distribution including Apache Hadoop (CDH) operating system support is covered in the documentation, we thought a quick overview of the changes in CDH3 would be helpful to highlight before CDH3 goes stable. CDH3 supports both 32-bit and 64-bit packages for Red Hat Enterprise Linux 5 and CentOS 5. A significant addition in CDH3 Beta 4 was 64-bit support for SUSE Linux Enterprise Server 11 (SLES 11). CDH3 also supports both 32-bit and 64-bit packages for the two most recent Ubuntu releases: Lucid (10.04 LTS) and Maverick (10.10). As of Beta 4, CDH3 no longer contains packages for Debian Lenny, Ubuntu Hardy, Jaunty, or Karmic. Checkout these upgrade instructions if you are using an Ubuntu release past its end of life. If you are using a release for which Cloudera’s Debian or RPM packages are not available, you can always use the tarballs from the CDH download page. If you have any questions, you can reach us on the  CDH user list.</snippet></document><document id="552"><title>Gratuitous Hadoop: Stress Testing on the Cheap with Hadoop Streaming and EC2</title><url>http://blog.cloudera.com/blog/2011/02/gratuitous-hadoop-stress-testing-on-the-cheap-with-hadoop-streaming-and-ec2/</url><snippet>This post was contributed by Boris Shimanovsky, the Director of Engineering at Factual. Boris is responsible for managing all engineering functions and various data infrastructures at Factual- including the internal Cloudera’s Distribution for Apache Hadoop stack. He has been at Factual for over two years, and prior he was the CTO of XAP where he managed a team of +40 across multiple environments. He has an MS from UCLA in Computer Science. Things have a funny way of working out this way. A couple features were pushed back from a previous release and some last minute improvements were thrown in, and suddenly we found ourselves dragging out a lot more fresh code in our release than usual. All this the night before one of our heavy API users was launching something of their own. They were expecting to hit us thousands of times a second and most of their calls touched some piece of code that hadn’t been tested in the wild. Ordinarily, we would soft launch and put the system through its paces. But now we had no time for that. We really wanted to hammer the entire stack, yesterday, and so we couldn’t rely on internal compute resources. Typically, people turn to a service for this sort of thing but for the load we wanted, they charge many hundreds of dollars. It was also short notice and after hours, so there was going to be some sort of premium to boot. At first, I thought we should use something like JMeter from some EC2 machines. However, we didn’t have anything set up for that. What were we going to do with the stats? How would we synchronize starting and stopping? It just seemed like this path was going to take a while. I wanted to go to bed. Soon. We routinely run Hadoop jobs on EC2, so everything was already baked to launch a bunch of machines and run jobs. Initially, it seemed like a silly idea, but when the hammer was sitting right there, I saw nails. I Googled it to see if anyone had tried. Of course not. Why would they? And if they did, why admit it? Perhaps nobody else found themselves on the far right side of the Sleep vs Sensitivity-to-Shame scale. So, it was settled — Hadoop it is! I asked my colleagues to assemble the list of test URLs. Along with some static stuff (no big deal) and a couple dozen basic pages, we had a broad mixture of API requests and AJAX calls we needed to simulate. For the AJAX stuff, we simply grabbed URLs from the Firebug console. Everything else was already in tests or right on the surface, so we’d have our new test set in less than an hour. I figured a few dozen lines of ruby code using Hadoop streaming could probably do what I had in mind. I’ve read quite a few post-mortems that start off like this, but read on, it turns out alright. Hadoop Streaming Hadoop Streaming is a utility that ships with Hadoop and works with pretty much any language. Any executable that reads from stdin and writes to stdout can be a mapper or reducer. By default, they read line-by-line with the bytes up to the first tab character representing the key and any remainder becomes the value. It’s a great resource and bridges the power of Hadoop with the ease of quick scripts. We use it a lot when we need to scale out otherwise simple jobs. Designing Our Job We had just two basic requirements for our job: Visit URLs quickly Produce response time stats Input File The only input in this project is a list of URLs — only keys and no values. The job would have to run millions of URLs through the process to sustain the desired load for the desired time but we only had hundreds of calls that needed testing. First, we wanted to skew the URL frequency towards the most common calls. To do that, we just put those URLs in multiple times. Then we wanted to shuffle them for better distribution. Finally, we just needed lots of copies. for i in {1..10000};  do sort -R &lt; sample_urls_list.txt; done &gt; full_urls_list.txt Mapper The mapper was going to do most of the work for us. It needed to fetch URLs as quickly as possible and record the elapsed time for each request. Hadoop processes definitely have overhead and even though each EC2 instance could likely be fetching hundreds of URLs at once, it couldn’t possibly run hundreds of mappers. To get past this issue, we had two options: 1) just launch more machines and under-utilize them or 2) fetch lots of URLs concurrently with each mapper. We’re trying not to needlessly waste money, so #1 is out. I had used the curb gem (libcurl bindings for ruby) on several other projects and it worked really well. It turns out that it was going to be especially helpful here since it has a Multi class which can run concurrent requests each with blocks that function essentially as callbacks. With a little hackery, it could be turned into a poor/lazy/sleep-deprived man’s thread pool. The main loop: @curler.perform do
  flush_buffer!
  STDIN.take(@concurrent_requests-@curler.requests.size).each do |url|
    add_url(url.chomp)
  end
end Blocks for success and failure: curl.on_success do
  if errorlike_content?(curl.body_str)
    log_error(curl.url,'errorlike content')
  else
    @response_buffer&lt;&lt;[curl.url,Time.now-start_time]
  end
end
curl.on_failure do |curl_obj,args|
  error_type = args.first.to_s if args.is_a?(Array)
  log_error(curl.url,error_type)
end As you can see, each completion calls a block that outputs the URL and the number of seconds it took to process the request. A little healthy paranoia about thread safety resulted in the extra steps of buffering and flushing output — this would ensure we don’t interleave output coming from multiple callbacks. If there is an error, the mapper will just emit the URL without an elapsed time as a hint to the stats aggregator. In addition, it uses the ruby “warn” method to emit a line to stderr. This increments a built-in Hadoop counter mechanism that watches stderr for messages in the following format: reporter:counter:[group],[counter],[amount] in this case the line is: reporter:counter:error,[errortype],1 This is a handy way to report what’s happening while a job is in progress and is surfaced through the standard Hadoop job web interface. Mapper-Only Implementation The project could actually be done here if all we wanted was raw data to analyze via some stats software or a database. One could simply cat together the part files from HDFS and start crunching. In this case, the whole job would look like this: hadoop  jar $HADOOP_HOME/hadoop-streaming.jar                                  \
-D mapred.map.tasks=100                                                        \
-D mapred.reduce.tasks=0                                                       \
-D mapred.job.name="Stress Test - Mapper Only"                                 \
-D mapred.speculative.execution=false                                          \
-input  "/mnt/hadoop/urls.txt"                                                 \
-output "/mnt/hadoop/stress_output"                                            \
-mapper "$MY_APP_PATH/samples/stress/get_urls.rb 100" and then when it finishes: hadoop dfs -cat /mnt/hadoop/stress_output/part* &gt; my_combined_data.txt Reducer In our case, I wanted to use the reducer to compute the stats as part of the job. In Hadoop streaming, a reducer can expect to receive lines through stdin, sorted by key and that the same key will not find its way to multiple reducers. This eliminates the requirement that the reducer code track state for more than one key at a time — it can simply do whatever it does to values associated with a key (e.g. aggregate) and move on when a new key arrives. This is a good time to mention the aggregate package which could be used as the reducer to accumulate stats. In our case, I wanted to control my mapper output as well as retain the flexibility to not run a reducer altogether and just get raw data. The streaming job command looks like this: hadoop  jar $HADOOP_HOME/hadoop-streaming.jar                                  \
-D mapred.map.tasks=100                                                        \
-D mapred.reduce.tasks=8                                                       \
-D mapred.job.name="Stress Test - Full"                                 \
-D mapred.speculative.execution=false                                          \
-input  "/mnt/hadoop/urls.txt"                                                 \
-output "/mnt/hadoop/stress_output"                                            \
-mapper "$MY_APP_PATH/samples/stress/get_urls.rb 100"                          \
-reducer “$MY_APP_PATH/samples/stress/stats_aggregator.rb --reducer” For each key (URL) and value (elapsed time), the following variables get updated sum – total time elapsed for all requests min – fastest response max – slowest response count – number of requests processed key,val = l.chomp.split("\t",2)
if last_key.nil? || last_key!=key
  write_stats(curr_rec) unless last_key.nil?
  curr_rec = STATS_TEMPLATE.dup.update(:key=&gt;key)
  last_key=key
end
 
if val &amp;&amp; val!=''
  val=val.to_f
  curr_rec[:sum]+=val
  curr_rec[:count]+=1
  curr_rec[:min]=val if curr_rec[:min].nil? || val&lt;curr_rec[:min]
  curr_rec[:max]=val if curr_rec[:max].nil? || val&gt;curr_rec[:max]
else
  curr_rec[:errors]+=1
end Finally, as we flush, we compute the overall average for the key. def write_stats(stats_hash)
  stats_hash[:average]=stats_hash[:sum]/stats_hash[:count] if stats_hash[:count]&gt;0
  puts stats_hash.values_at(*STATS_TEMPLATE.keys).join("\t")
end Final Stats (optional) When the job completes, it produces as many part files as there are total reducers. Before this data can be loaded into, say, a spreadsheet, it needs to be merged and converted into a friendly format. A few more lines of code get us a csv file that can easily be dropped into your favorite spreadsheet/charting software: hadoop dfs -cat /mnt/hadoop/stress_output/part* | $MY_APP_PATH/samples/stress/stats_aggregator.rb --csv &gt; final_stats.csv Our CSV converter looks like this: class CSVConverter
  def print_stats
    puts STATS_TEMPLATE.keys.to_csv
    STDIN.each_line do |l|
      puts l.chomp.split("\t").to_csv
    end
  end
end Source Code The mapper, get_urls.rb: #!/usr/bin/env ruby1.9
require 'rubygems'
require 'curb'
 
class MultiCurler
  DEFAULT_CONCURRENT_REQUESTS = 100
  def initialize(opts={})
    @concurrent_requests = opts[:concurrent_requests] || DEFAULT_CONCURRENT_REQUESTS
    @curler = Curl::Multi.new
    @response_buffer=[]
  end
  def start
    while !STDIN.eof?
      STDIN.take(@concurrent_requests).each do |url|
        add_url(url.chomp)
      end
      run
    end
  end
 
  private
 
  def run
    @curler.perform do
      flush_buffer!
      STDIN.take(@concurrent_requests-@curler.requests.size).each do |url|
        add_url(url.chomp)
      end
    end
    flush_buffer!
  end
 
  def flush_buffer!
    while output = @response_buffer.pop
      puts output.join("\t")
    end
  end
 
  def add_url(u)
 
    #skip really obvious input errors
    return log_error(u,'missing url') if u.nil?
    return log_error(u,'invalid url') unless u=~/^http:\/\//i
 
    c = Curl::Easy.new(u) do|curl|
      start_time = Time.now
      curl.follow_location = true
      curl.enable_cookies=true
      curl.on_success do
        if errorlike_content?(curl.body_str)
          log_error(curl.url,'errorlike content')
        else
          @response_buffer&lt;&lt;[curl.url,Time.now-start_time]
        end
      end
      curl.on_failure do |curl_obj,args|
        error_type = args.first.to_s if args.is_a?(Array)
        log_error(curl.url,error_type)
      end
    end
    @curler.add(c)
  end
 
  def errorlike_content?(page_body)
    page_body.nil? || page_body=='' || page_body=~/(unexpected error|something went wrong|Api::Error)/i
  end
 
  def log_error(url,error_type)
    @response_buffer&lt;&lt;[url,nil]
    warn "reporter:counter:error,#{error_type||'unknown'},1"
  end
 
end
 
 
concurrent_requests = ARGV.first ? ARGV.first.to_i : nil
 
runner=MultiCurler.new(:concurrent_requests=&gt;concurrent_requests)
runner.start The reducer and postprocessing script, stats_aggregator.rb: #!/usr/bin/env ruby1.9
require 'rubygems'
require 'csv'
 
 
module Stats
  STATS_TEMPLATE={:key=&gt;nil,:sum=&gt;0,:average=&gt;nil,:max=&gt;nil,:min=&gt;nil,:errors=&gt;0,:count=&gt;0}
 
  class Reducer
    def run
 
      last_key=nil
      curr_rec=nil
 
      STDIN.each_line do |l|
        key,val = l.chomp.split("\t",2)
        if last_key.nil? || last_key!=key
          write_stats(curr_rec) unless last_key.nil?
          curr_rec = STATS_TEMPLATE.dup.update(:key=&gt;key)
          last_key=key
        end
 
        if val &amp;&amp; val!=''
          val=val.to_f
          curr_rec[:sum]+=val
          curr_rec[:count]+=1
          curr_rec[:min]=val if curr_rec[:min].nil? || val&lt;curr_rec[:min]
          curr_rec[:max]=val if curr_rec[:max].nil? || val&gt;curr_rec[:max]
        else
          curr_rec[:errors]+=1
        end
      end
      write_stats(curr_rec) if curr_rec
    end
 
    private
 
    def write_stats(stats_hash)
      stats_hash[:average]=stats_hash[:sum]/stats_hash[:count] if stats_hash[:count]&gt;0
      puts stats_hash.values_at(*STATS_TEMPLATE.keys).join("\t")
    end
  end
 
 
  class CSVConverter
    def print_stats
      puts STATS_TEMPLATE.keys.to_csv
      STDIN.each_line do |l|
        puts l.chomp.split("\t").to_csv
      end
    end
  end
 
 
end
 
 
mode = ARGV.shift
 
case mode
  when '--reducer' #hadoop mode
    Stats::Reducer.new.run
  when '--csv' #csv converter; run with: hadoop dfs -cat /mnt/hadoop/stress_output/part* | stats_aggregator.rb --csv
    Stats::CSVConverter.new.print_stats
  else
    abort 'Invalid mode specified for stats aggregator.  Valid options are --reducer, --csv'
end Reckoning (Shame Computation) In a moment of desperation, we used Hadoop to solve a problem for which it is very tenuously appropriate, but it actually turned out great. I wrote very little code and it worked with almost no iteration and just a couple of up-front hours invested for an easily repeatable process. Our last minute stress test exposed a few issues that we were able to quickly correct and resulted in a smooth launch. All this, and it only cost us about $10 of EC2 time. Hadoop Streaming is a powerful tool that every Hadoop shop, even the pure Java shops, should consider part of their toolbox. Lots of big data jobs are actually simple except for scale, so your local python, ruby, bash, perl, or whatever coder along with some EC2 dollars can give you access to some pretty powerful stuff. We recommend using Cloudera’s Distribution for Apache Hadoop and using tools like Whirr to get things running quickly. It definitely lowers the learning curve for getting going.</snippet></document><document id="553"><title>Avoiding Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1</title><url>http://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/</url><snippet>Today, rather than discussing new projects or use cases built on top of CDH, I’d like to switch gears a bit and share some details about the engineering that goes into our products. In this post, I’ll explain the MemStore-Local Allocation Buffer, a new component in the guts of Apache HBase which dramatically reduces the frequency of long garbage collection pauses. While you won’t need to understand these details to use Apache HBase, I hope it will provide an interesting view into the kind of work that engineers at Cloudera do. This post will be the first in a three part series on this project. Background Heaps and heaps of RAM! Over the last few years, the amount of memory available on inexpensive commodity servers has gone up and up. When the Apache HBase project started in 2007, typical machines running Hadoop had 4-8GB of RAM. Today, most Cloudera customers run with at least 24G of RAM, and larger amounts like 48G or even 72G are becoming increasingly common as costs continue to come down. On the surface, all this new memory capacity seems like a great win for latency-sensitive database software like HBase — with a lot of RAM, more data can fit in cache, avoiding expensive disk seeks on reads, and more data can fit in the memstore, the memory area that buffers writes before they flush to disk. In practice, however, as typical heap sizes for HBase have crept up and up, the garbage collection algorithms available in production-quality JDKs have remained largely the same. This has led to one major problem for many users of HBase: lengthy stop-the-world garbage collection pauses which get longer and longer as heap sizes continue to grow. What does this mean in practice? During a stop-the-world pause, any client requests to HBase are stalled, causing user-visible latency or even timeouts. If a request takes over a minute to respond because of a pause, HBase may as well be down – there’s often little value in such a delayed response. HBase relies on Apache ZooKeeper to track cluster membership and liveness. If a server pauses for a significant amount of time, it will be unable to send heartbeat ping messages to the ZooKeeper quorum, and the rest of the servers will presume that the server has died. This causes the master to initiate certain recovery processes to account for the presumed-dead server. When the server comes out of its pause, it will find all of its leases revoked, and commit suicide. The HBase development team has affectionately dubbed this scenario a Juliet Pause — the master (Romeo) presumes the region server (Juliet) is dead when it’s really just sleeping, and thus takes some drastic action (recovery). When the server wakes up, it sees that a great mistake has been made and takes its own life. Makes for a good play, but a pretty awful failure scenario! The above issues will be familiar to most who have done serious load testing of an HBase cluster. On typical hardware, it can pause 8-10 seconds per GB of heap — a 8G heap may pause for upwards of a minute. No matter how much tuning one might do, it turns out this problem is completely unavoidable in HBase 0.90.0 or older with today’s production-ready garbage collectors.�Since this is such a common issue, and only getting worse, it became a priority for Cloudera at the beginning of the year. In the rest of this post, I’ll describe a solution we developed that largely eliminates the problem. Java GC Background In order to understand the GC pause issue thoroughly, it’s important to have some background in Java’s garbage collection techniques. Some simplifications will be made, so I highly encourage you to do further research for all the details. If you’re already an expert in GC, feel free to skip this section. Generational GC Java’s garbage collector typically operates in a generational mode, relying on an assumption called the generational hypothesis: we assume that most objects either die young, or stick around for quite a long time. For example, the buffers used in processing an RPC request will only last for some milliseconds, whereas the data in a cache or the data in the HBase MemStore will likely survive for many minutes. Given that objects have two different lifetime profiles, it’s intuitive that different garbage collection algorithms might do a better job on one profile than another. So, we split up the heap into two generations: the young (a.k.a new)�generation and the old (a.k.a tenured). When objects are allocated, they start in the young generation, where we prefer an algorithm that operates efficiently when most of the data is short-lived. If an object survives several collections inside the young generation, we tenure it by relocating it into the old generation, where we assume that data is likely to die out much more slowly. In most latency-sensitive workloads like HBase, we recommend the -XX:+UseParNewGC and -XX:+UseConcMarkSweepGC JVM flags. This enables the Parallel New collector for the young generation and the Concurrent Mark-Sweep collector for the old generation. Young Generation – Parallel New Collector The Parallel New collector is a stop-the-world copying collector. Whenever it runs, it first stops the world, suspending all Java threads. Then, it traces object references to determine which objects are live (still referenced by the program). Lastly, it moves the live objects over to a free section of the heap and updates any pointers into those objects to point to the new addresses. There are a few important points here about this collector: It stops the world, but not for very long. Because the young generation is usually fairly small, and this collector runs with many threads, it can accomplish its work very quickly. For production workloads we usually recommend a young generation no larger than 512MB, which results in pauses of less than a few hundred milliseconds at the worst case. It copies the live objects into a free heap area. This has the side effect of compacting the free space – after every collection, the free space in the young generation is one contiguous chunk, which means that allocation can be very efficient. Each time the Parallel New collector copies an object, it increments a counter for that object. After an object has been copied around in the young generation several times, the algorithm decides that it must belong to the long-lived class of objects, and moves it to the old generation (tenures it). The number of times an object is copied inside the young generation before being tenured is called the tenuring threshold. Old Generation – Concurrent Mark-Sweep Every time the parallel new collector runs, it will tenure some objects into the old generation. So, of course, the old generation will eventually fill up, and we need a strategy for collecting it as well. The Concurrent-Mark-Sweep collector (CMS) is responsible for clearing dead objects in this generation. The CMS collector operates in a series of phases. Some phases stop the world, and others run concurrently with the Java application. The major phases are: initial-mark (stops the world). In this phase, the CMS collector places a mark on the root objects. A root object is something directly referenced from a live Thread – for example, the local variables in use by that thread. This phase is short because the number of roots is very small. concurrent-mark (concurrent). The collector now follows every pointer starting from the root objects until it has marked all live objects in the system. remark (stops the world). Since objects might have had references changed, and new objects might have been created during concurrent-mark, we need to go back and take those into account in this phase. This is short because a special data structure allows us to only inspect those objects that were modified during the prior phase. concurrent-sweep (concurrent). Now, we proceed through all objects in the heap. Any object without a mark is collected and considered free space. New objects allocated during this time are marked as they are created so that they aren’t accidentally collected. The important things to note here are: The stop-the-world phases are made to be very short. The long work of scanning the whole heap and sweeping up the dead objects happens concurrently. This collector does not relocate the live objects, so free space can be spread in different chunks throughout the heap. We’ll come back to this later! CMS Failure Modes As I described it, the CMS collector sounds pretty great – it only pauses for very short times and most of the heavy lifting is done concurrently. So how is it that we see multi-minute pauses when we run HBase under heavy load with large heaps? It turns out that the CMS collector has two failure modes. Concurrent Mode Failure The first failure mode, and the one more often discussed, is simple concurrent mode failure. This is best described with an example: suppose that there is an 8GB heap. When the heap is 7GB full, the CMS collector may begin its first phase. It’s happily chugging along with the concurrent-mark phase. Meanwhile, more data is being allocated and tenured into the old generation. If the tenuring rate is too fast, the generation may completely fill up before the collector is done marking. At that point, the program may not proceed because there is no free space to tenure more objects! The collector must abort its concurrent work and fall back to a stop-the-world single-threaded copying collection algorithm. This algorithm relocates all live objects to the beginning of the heap, and frees up all of the dead space. After the long pause, the program may proceed. It turns out this is fairly easy to avoid with tuning: we simply need to encourage the collector to start its work earlier! Thus, it’s less likely that it will get overrun with new allocations before it’s done with its collection. This is tuned by setting -XX:CMSInitiatingOccupancyFraction=N where N is the percent of heap at which to start the collection process. The HBase region server carefully accounts its memory usage to stay within 60% of the heap, so we usually set this value to around 70. Promotion Failure due to Fragmentation This failure mode is a little bit more complicated. Recall that the CMS collector does not relocate objects, but simply tracks all of the separate areas of free space in the heap. As a thought experiment, imagine that I allocate 1 million objects, each 1KB, for a total usage of 1GB in a heap that is exactly 1GB. Then I free every odd-numbered object, so I have 500MB live. However, the free space will be solely made up of 1KB chunks. If I need to allocate a 2KB object, there is nowhere to put it, even though I ostensibly have 500MB of space free. This is termed memory fragmentation. No matter how early I ask the CMS collector to start, since it does not relocate objects, it cannot solve this problem! When this problem occurs, the collector again falls back to the copying collector, which is able to compact all the objects and free up space. Enough GC! Back to HBase! Let’s come back up for air and use what we learned about Java GC to think about HBase. We can make two observations about the pauses we see in HBase: By setting the CMSInitiatingOccupancyFraction tunable lower, we’ve seen that some users can avoid the GC issue. But for other workloads, it will always happen, no matter how low we set this tuning parameter. We often see these pauses even when metrics and logs indicate that the heap has several GB of free space! Given these observations, we hypothesize that our problem must be caused by fragmentation, rather than some kind of memory leak or improper tuning. An experiment: measuring fragmentation To confirm this hypothesis, we’ll run an experiment. The first step is to collect some measurements about heap fragmentation. After spelunking in the OpenJDK source code, I discovered the little-known parameter -XX:PrintFLSStatistics=1 which, when combined with other verbose GC logging options, causes the CMS collector to print statistical information about its free space before and after every collection. In particular, the metrics we care about are: Free space – the total amount of free memory in the old generation Num chunks – the total number of non-contiguous free chunks of memory Max chunk size – the size of the largest one of these chunks (i.e the biggest single allocation we can satisfy without a pause) I enabled this option, started up a cluster, and then ran three separate stress workloads against it using Yahoo Cloud Serving Benchmark (YCSB): Write-only: writes rows with 10 columns, each 100 bytes, across 100M distinct row keys. Read-only with cache churn: reads data randomly for 100M distinct row keys, so that the data does not fit in the LRU cache. Read-only without cache churn: reads data randomly for 10K distinct row keys, so that the data fits entirely in the LRU cache. Each workload will run at least an hour, so we can collect some good data about the GC behavior under that workload. The goal of this experiment is first to verify our hypothesis that the pauses are caused by fragmentation, and second to determine whether these issues were primarily caused by the read path (including the LRU cache) or the write path (including the MemStores for each region). To be continued… The next post in the series will show the results of this experiment and dig into HBase’s internals to understand how the different workloads affect memory layout. Meanwhile, if you want to learn more about Java’s garbage collectors, I recommend the following links: Jon “the collector” Masamitsu has a good post describing the various collectors in Java 6. To learn more about CMS, you can read the original paper: A Generational Mostly-concurrent Garbage Collector [Printezis/Detlefs, ISMM2000] Read on to Part 2 or Part 3.</snippet></document><document id="554"><title>CDH3 Beta 4 Now Available</title><url>http://blog.cloudera.com/blog/2011/02/cdh3-beta-4-now-available/</url><snippet>Cloudera is happy to announce the fourth beta release of Cloudera’s Distribution for Apache Hadoop version 3 — CDH3b4. As usual, we’d like to share a few highlights from this release. Since this will be the last beta before we designate CDH3 stable, our focuses for this release have been on stability, security, and scalability. Stability and ease of use Since we released CDH3 Beta 3 in October, we’ve deployed the distribution for production use cases at a number of customers, and have also gotten considerable feedback from the open source user community. In beta 4, we’ve addressed a number of important bugs and also improved usability. For example, one piece of feedback that we heard consistently was that beta 3 was too finnicky about permissions — in beta 4 we’ve both improved the error messages when permissions are incorrect, and made the software automatically correct permissions errors where possible. Security CDH3 Beta 4 is our most secure release yet. We’ve worked over the past several months to identify and fix several vulnerabilities in Apache Hadoop, and those fixes are included in this new beta release. We encourage any users running in a secured environment to upgrade as soon as possible. Scalability CDH3 Beta 4 merges in many of the scalability improvements contributed by Yahoo! in their 0.20.100 branch of Apache Hadoop. This includes a reduction in the amount of memory required by the NameNode, improvements to MapReduce scheduling throughput, and more scalable RPC servers. We’re confident that this release of Hadoop will scale to meet even the largest clusters and most demanding workloads. New Component Versions CDH3 Beta 4 also includes new versions of many components. Highlights include: HBase 0.90.1, including much improved stability and operability. Hive 0.7.0rc0, including the beginnings of authorization support, support for multiple databases, and many other new features. Pig 0.8.0, including many new features like scalar types, custom partitioners, and improved UDF language support. Flume 0.9.3, including support for Windows and improved monitoring capabilities. Sqoop 1.2, including improvements to usability and Oracle integration. Whirr 0.3, including support for starting HBase clusters on popular cloud platforms. For a full list of new component versions and changes, please check the CDH3 Beta 4 release notes. Additional Platform Support CDH3b4 also includes support for two new operating system versions: Red Hat Enterprise Linux 6 and SUSE Linux Enterprise Server 11. This is in addition to our preexisting support for Ubuntu (Lucid and Maverick), RHEL5 and CentOS 5. Also new with CDH3b4 is support for Apache Maven. All of the packages in CDH can now be built via Maven. By using Maven to manage dependencies we’ve greatly simplified the external requirements to deploy CDH. Getting CDH3b4 CDH3 Beta 4 is available now from the CDH download page. What’s next? CDH3 Beta 4 will be the last beta release before we demarcate CDH3 a stable release. Thus, we don’t plan on adding any new features or upstream component versions — instead, we’re focusing only on important bug fixes and low risk backports. If you have been waiting for CDH3 stable, that day is coming soon. If you are looking to get the jump on upgrading, we would encourage you install or upgrade to CDH3 Beta 4 in your test environments in anticipation of this release going stable. As we continue our quality assurance processes at Cloudera, we also hope to hear feedback from the user community. Please join the cdh-user mailing list and let us know what you think!</snippet></document><document id="555"><title>Log Event Processing with Apache HBase</title><url>http://blog.cloudera.com/blog/2011/02/log-event-processing-with-hbase/</url><snippet>This post was authored by Dmitry Chechik, a software engineer at TellApart, the leading Customer Data platform for large online retailers. Apache Hadoop is widely used for log processing at scale. The ability to ingest, process, and analyze terabytes of log data has led to myriad applications and insights. As applications grow in sophistication, so does the amount and variety of the log data being produced. At TellApart, we track tens of millions of user events per day, and have built a flexible system atop HBase for storing and analyzing these types of logs offline. A TellApart user planning a bird-watching trip may start her day searching for binoculars on Binoculars.com, continue to comparison-shop for new hiking pants on one of our other partner merchants, and be shown relevant ads to these interests throughout her experience. Her browsing activity produces a flurry of different log data: page views, transactions, ad impressions, ad clicks, real-time ad auction bid request, and many more. Dissecting this data is a common scenario – and a real challenge – faced by many log analysis applications. Many of these scenarios share a common set of requirements: Data must be ingested into the system incrementally � one day or so worth of data at a time. Data is processed at a variety of time scales. Daily reporting often cares only about one day�s worth of data, while machine learning applications may require digesting several months worth of data to build models. Some events are naturally associated with others. An ad click is logged separately from an ad impression, but the two need to be processed together. Some data extraction applications need to process these associated events together, but others only care about the individual events. Random-access lookups to track all the actions of a user across time are often helpful. This is a powerful debugging tool to understand how the user interacts with the web at large and with the TellApart system in particular. We�ve found that Apache HBase is uniquely well suited to many of these tasks. By organizing user events across rows and columns, and using the various built-in HBase server-side filters, we can slice the data across the different required dimensions. The first decision to make when setting up an HBase system is the schema to use. We broke down the data as follows: Row Key Prefix Row Key Suffix Ad impression Associated Ad Click Associated real-time auction bid User ID &lt;timestamp&gt;_&lt;event_id&gt; Thrift Buffer Thrift Buffer Thrift Buffer � &lt;timestamp&gt;_&lt;event_id&gt; … … … � &lt;timestamp&gt;_&lt;event_id&gt; … … … This organization of data, coupled with HBase�s abilities, allows for powerful methods of accessing the data ? both for reading and writing data. Consider importing data, one day at a time, into HBase. This can be done with a simple Map-Reduce that writes data into the appropriate rows, which is about the same as a typical Hadoop implementation. However, if the data to be imported is associated with some other type of data through an event id (in our example, these are ad clicks, each of which is associated with an impression), we simply need to write the clicks into the row the associated impression is in. This can be done with a map-only Map-Reduce, so we can avoid an unnecessary reduce step to join clicks to impressions. The same basic idea of associating related events in the same row can be extended to encompass the outputs of analysis jobs. For example, if an offline analysis determines that a given impression leads to an action on an advertiser�s site, we can output data about that action into the same row. Subsequent applications don�t need to repeat this analysis to obtain that data. Judicious use of filters and column selection lets us control which parts of the data we need to read for any specific type of operation. Consider a task where we only need to read data from the past month. Since the timestamp is embedded in the row key, we can use a Map-Reduce with an HBase RowFilter with a custom Comparator. The filter will only accept rows within our date range. An alternative here is to use the built-in timestamp associated with each HBase cell to do this kind of filtering. Doing this reduces the amount of data the mappers have to process, and significantly speeds up the jobs. Another way to slice data by rows is based simply on the prefix of the rows. In our use case, the user ID is the prefix of each row. With randomly-generated user IDs, if we want to run a Map-Reduce that analyses a sample of all users, we can do so by using a PrefixFilter, which will only accept rows with a given prefix. We can also use columns to select the set of data we�d like to read. For some tasks, we need to read both the impressions and the bid requests associated with each impression. For others, we only need to read impressions. By selecting the columns to read, we can limit the input set of data. Going further, we can even limit the data based on the values of some columns. TellApart serves impressions that show ads for various advertisers. By storing the advertiser id in a separate �advertiser_id� column, and using a SingleValueColumnFilter, we can retrieve only rows for a particular advertiser if that�s all we care about. Aside from Map-Reduce applications, HBase is also great for random lookups of data. We often need to see all of a particular user�s log actions over time, for debugging our system. This is where HBase really shines – we issue a Scan for the range of keys starting with a given user ID. This gives us instant visibility into the data, and we can use the same column-based filters to limit the data further if necessary. We issue these lookups in the HBase Shell, and to make this easy, we�ve modified the HBase Shell to deserialize and format the data we store in HBase to make it readable for us. We�ve also made it easier for HBase to play nice with Cascading and Thrift. To use HBase with Cascading, we built on top of the existing Cascading.HBase support, adding support for any Hadoop Serialization to the module. This lets us read and write Thrift objects (or any other Hadoop Writable) from HBase in our Cascading jobs, much in the same way we would for any Hadoop job. The code is available at TellApart-Hadoop-Utils. HBase�s flexibility has been a boon to our offline analysis. Separating the data by rows and columns affords us tremendous flexibility in our data access patterns, and gives us a strong platform to grow on in the future. Again, this post was authored by Dmitry Chechik, a software engineer at TellApart, the leading Customer Data platform for large online retailers. Thanks to Mark Ayzenshtat, TellApart CTO and co-founder, for reading drafts of this post. TellApart is actively hiring great software engineers. Click here to learn more about the company and their open positions.</snippet></document><document id="556"><title>An emerging data management architectural pattern behind interactive web applications</title><url>http://blog.cloudera.com/blog/2011/02/an-emerging-data-management-architectural-pattern-behind-interactive-web-application/</url><snippet>The user-data connection is driving NoSQL database-Hadoop pairing This post is courtesy of James Phillips, Co-founder, Couchbase (formerly Membase) AOL Advertising runs one of the largest online ad serving operations, serving billions of impressions each month to hundreds of millions of people. AOL faced three data management challenges in building their ad serving platform: How to analyze billions of user-related events, presented as a mix of structured and unstructured data, to infer demographic, psychographic and behavioral characteristics that are encapsulated into hundreds of millions of “cookie profiles” How to make hundreds of millions of cookie profiles available to their ad targeting platform with sub-millisecond, random read latency How to keep the user profiles fresh and current The solution was to integrate two data management systems: one optimized for high-throughput data analysis (the “analytics” system), the other for low-latency random access (the “transactional” system). After analyzing alternatives, the final architecture selected paired Cloudera Distribution for Apache Hadoop (CDH) with Membase: In this architecture, (1) click-stream data and other events are fed into CDH from a wide variety of sources (2) the data is analyzed using MapReduce to generate hundreds of millions of user profiles; then, based on which ad campaigns are running, selected user profiles are loaded into Membase where (3) ad targeting logic can query Membase with sub-millisecond latency to get the data needed to make optimized decisions about real-time ad placement. While AOL offers a specific example of the power of pairing a “NoSQL” transactional database system with the Hadoop analytics platform, they represent what is emerging as a common pattern of deployment across a wide variety of web application environments. Increasingly organizations deploy these two technologies in concert. I would hazard to say that every NoSQL database deployment would be more valuable when paired with Hadoop. Why? A NoSQL database is invariably used to allow interactive web applications to serve large and growing application user populations. A large and growing set of users naturally generates a lot of data – directly and indirectly, structured and unstructured. Hadoop can store, process and analyze lots of data. The resulting analysis can offer insight into user behavior, preferences and patterns that can be used to make the application experience even better for users. A better application leads to more users. And the cycle accelerates. Lots of users In simplest terms, a NoSQL database (such as Membase, MongoDB or Riak) is designed to provide interactive Web applications with cost-effective, low-latency, random access to data. Web applications have three defining characteristics that matter in this context: Hyper-growth. They can grow from one to hundreds of thousands of users overnight – literally.[i] And they can continue growing to serve hundreds of millions of users. The world is a very big place. A public web interface today makes a software system accessible to nearly two billion people.[ii] Serving the impatient. Humans use these systems. Decades of research has shown that speed matters.[iii] People don’t like to be kept waiting. Any part of the technology stack that contributes to a user waiting contributes the demise of the application. Transiency. The user population of a web application comes and goes – both permanently and temporarily. There usually exists a population of users “online” or active at any given time. They come, they use, they leave. Hopefully they return. NoSQL database technology was invented to address these issues. They grow elastically – just add more cheap servers into a cluster and the data, and most importantly the I/O, is automatically rebalanced across the new servers to support increased load. And the same goes in reverse when the user population of an application recedes. And they are built to guarantee very low-latency random read-write access to data when an application needs it. They do this, in part, by taking advantage of the transient use of these software systems. When users are active, the data required to serve them is cached in main memory. In this case, reading or writing a 5k data object can be done with sub-millisecond latency. When a user has been away and begins using the application again, if her data is no longer in memory it is automatically fetched and available through the users session, with a no-longer-active user’s data being ejected from memory and stored on low-cost disk storage awaiting next use. Lots of data If data must be ready to “go active” at any point in time, then a NoSQL database is the right solution. But Hadoop is a much better choice for storing data when that is not a requirement. And interactive web applications can generate mountains of this sort of data – data that historically may have gone uncollected. Login information, click streams, page views, gaze data, “old” application data no longer needed for real-time access, entry and exit drivers, flows to purchase, historical offer and purchase flows, timing information. The list goes on and on. With Hadoop, you can just collect stuff. There is no need to set up a schema or define data formats in advance. If you have an idea for collecting some information that might yield useful insight, store it in Hadoop. “When in doubt, write it out.” Hadoop was built to slurp up information and to store it cost-effectively. It employs the same “scale-out” approach as a NoSQL database – spreading data across inexpensive servers. But it stores data in a way that is optimized for high-throughput batch analysis, versus low-latency, random access. With Hadoop MapReduce, trends can be discovered, data can be aggregated and conclusions can be drawn. Those conclusions can then shape application behavior. Check out the webinar Next Thursday, Matt Aslett, Senior Analyst with The 451 Group, will be hosting a webinar entitled How AOL Accelerates Ad Targeting Decisions with Hadoop and Membase Server. Joining Matt will be Pero Subasic, Chief Architect, AOL. For anyone interested in building scalable web applications, I would encourage you to check out the webinar. It’s fun to see how these technologies are used “in the real world” and it may spark an idea for your own environment. View the slide deck. [i] Zynga publicly indicated that 290,000 people played CityVille in its first 24 hours (http://www.insidesocialgames.com/2010/12/06/cityville-claims-cityville-is-its-fastest-growing-game/) [ii] http://www.internetworldstats.com/stats.htm [iii] http://www.useit.com/alertbox/response-times.html</snippet></document><document id="557"><title>Strategies for Exploiting Large-scale Data in the Federal Government</title><url>http://blog.cloudera.com/blog/2011/02/strategies-for-exploiting-large-scale-data-in-the-federal-government/</url><snippet>This is a guest post by Bob Gourley (@bobgourley), editor of CTOvision.com and a former Defense Intelligence Agency (DIA) CTO. Like enterprises everywhere, the federal government is challenged with issues of overwhelming data. Thanks to a mature Apache Software Foundation suite of tools and a strong ecosystem around large-scale data storage and analytical capabilities, these challenges are actually turning into tremendous opportunities. The following characterizes current federal approaches to working with complex data: Federal IT leaders are increasingly sharing lessons learned across agencies. But approaches vary from agency to agency. That’s to be expected; the different agencies pursue different missions and have different problems to solve. Each evaluates the tools that best address its requirements. Nevertheless, federal thought leaders across all agencies are confronted with more data from more sources, and a need for more powerful analytic capabilities, in order to optimize support to their individual missions. For some agencies there are also temporal aspects mission requirements. Large-scale distributed analysis over large data sets is often expected to return results almost instantly. Most agencies face challenges that involve combining multiple data sets – some structured, some complex – in order to answer mission questions. For national security missions, this frequently requires combining streaming data with previously-captured records. Federal IT leaders are increasingly seeking automated tools, more advanced models and means of leveraging commodity hardware and open source software to conduct distributed analysis over distributed data stores. Some particularly forward-thinking executives in government are considering ways of enhancing the ability of citizens to contribute to government understanding by use of crowd-sourcing type models. As these concepts move forward additional opportunities in large-scale data analysis will no doubt arise. There is a growing consensus in the federal space that design of next-generation data management architectures must be treated as a discipline. The entire federal community needs an advanced body of knowledge and best practices for the design and use of large-scale, distributed data analysis systems. CIOs in the federal space, like CIOs everywhere, appreciate open source, but they demand commercially supported open source before fielding production systems. Those many observations call out for approaches like Cloudera’s Distribution for Apache Hadoop (CDH). By leveraging this commercially supported open source analysis platform, federal enterprises can avail themselves of the greatest analytical capabilities in industry today, and they can do so with zero barriers to entry. By building new data management solutions on low cost commodity IT and Cloudera’s Distribution for Apache Hadoop, enterprises know that they are using the most up to date Apache source code, with critical fixes and many additional tools and features from the community included. The most popular projects that complement the Hadoop core are also included and the entire bundle is tested and maintained and supported the way CIOs expect quality production software to be supported. What solutions can we expect federal thought leaders will provide using Hadoop-based distributed analysis systems? I believe we are all about to be surprised by solutions we could never expect. But for sake of dialog we should look at the very fertile/high-quality data stores currently in the federal government and then consider the types of solutions being fielded in the most forward leaning portions of our economy. That may help us understand some of the coming solutions. The government has many data stores. Consider, for example, government data on weather, climate, the environment, pollution, health, quality of life, the economy, natural resources, energy and transportation. Data on those topics exist in many stores across the federal enterprise. The government also has years of information from research conducted at academic institutions across the nation. Imagine the conclusions that could be drawn from distributed analysis over datastores like this. Imagine the benefits to our citizen’s health, commodity prices, education and employment of better analysis over these data stores. Now think through the powerful new data analysis solutions being fielded in the commercial sector. Think of Facebook at their 620 Million users and the real-time data architecture supporting them. Think of Twitter and the ability to rapidly pull rising trends and search for meaning over the body of interactions there. Think of LinkedIn and the ability to rapidly track changes in status and find connections to the right researchers/thinkers/partners. Think of Groupon and its ability to serve local users with information relevant to their lives. There are solutions today to large data issues in the commercial space that are directly relevant to solutions in the federal space. What is required now is execution, not engineering. The Cloudera Distribution for Apache Hadoop plus documentation is available for free download at: http://www.cloudera.com/downloads/ And extensive training and educational material is available at: http://www.cloudera.com/developers/learn-hadoop/</snippet></document><document id="558"><title>Cloudera in The Cube with Silicon Angle TV at Strata Conference 2011</title><url>http://blog.cloudera.com/blog/2011/02/cloudera-in-the-cube-with-silicon-angle-tv-at-strata-conference-2011/</url><snippet>The consensus from the Cloudera attendees of the O’Reilly Strata Conference last week was that the data-focused conference was nearly pitch perfect for the data scientist, practitioners and enthusiast who attended the event. It was filled with educational and sometimes entertaining sessions, provided ample time for mingling with vendors and attendees and was well run in general. One of the cool activities happening at the conference was live streaming video brought to us from the good folks at SiliconAngle. Using a mobile production system called The Cube, Silicon Angle hosts John Furrier (@furrier) and Dave Vellante interviewed industry luminaries and up and comers while bringing their own perspective. After streaming live for nearly two days these hosts are still able to keep the energy high and the tone light. In the interviews below John and Dave interview Amr Awadallah, CTO and Co-Founder of Cloudera (@awadallah), and John Kreisa, VP Marketing at Cloudera (@marked_man); followed by a John and Dave interview with Sarah Sproehnle director of education at Cloudera. During the interviews they cover many different aspects of Cloudera and Apache Hadoop. Interview 1 Interview 2 Interview 2 Interviewee: Sarah Sproehnle, Cloudera Director of Education Interviewers: John Furrier, SiliconAngle Dave Vellante, Wikibon.org The items discussed include: Yahoo!’s Hadoop Distribution Cloudera Hadoop Training: training demand, who’s training &amp; Why Cloudera Competition Interview 1 Interviewees: Amr Awadallah, Cloudera CTO &amp; Co-Founder John Kreisa, Cloudera VP Marketing Interviewers: John Furrier, SiliconAngle Dave Vellante, Wikibon.org The items discussed include: What is large-scale data? Why adopt Hadoop? Scalability of Hadoop Reasons for Cloudera and Hadoop popularity Hadoop distribution Cloudera Competition Advice to Entrepreneurs Applications built on top of Cloudera’s Distribution for Apache Hadoop HBase “Use the right tool for the right job” Benefits of Open Source preventing lock-in Cloudera Customer, Tynt, use case Hadoop industries “Data Scientist” road map Cloudera evangelism Upcoming Cloudera events  </snippet></document><document id="559"><title>Wordnik Bypasses Processing Bottleneck with Apache Hadoop</title><url>http://blog.cloudera.com/blog/2011/02/wordnik-bypasses-processing-bottleneck-with-hadoop/</url><snippet>This post is courtesy of Kumanan Rajamanikkam, Lead Engineer at Wordnik. Wordnik�s Processing Challenge At Wordnik, our goal is to build the most comprehensive, high-quality understanding of English text.� We make our findings available through a robust REST api and www.wordnik.com.� Our corpus grows quickly�up to 8,000 words per second.� Performing deep lexical analysis on data at this rate is challenging to say the least. We had major challenges with three distinct problems: Updating our Lucene search indexes at a rapid rate.� Most of the time we�re appending data but frequently there are updates.� Often new algorithms are put in place which require a complete rebuild of the index. Updating our runtime aggregate data based on our lexical analysis techniques. Allowing for ad-hoc queries against our corpus by our R&amp;D team. When speaking about the impact of Apache Hadoop on our analytical workflow, we have painful and fresh memories of tackling the problems without using a MapReduce cluster�effectively brute-forcing the solution with large servers.� Rebuilding our search indexes took between 2 and 3 weeks.� Generating aggregate statistics took between 3 days to 2 weeks.� Running simple algorithms against our whole corpus would typically take 3-4 days, and that was when everything went exactly right. We would split processing tasks across multiple machines and start jobs manually.� This involved dividing our corpus (documents) into tasks and assigning jobs (ranges of IDs) to different servers.� We would then monitor their progress and strive to keep all servers busy 24×7.� Unfortunately writing our own task tracker system was more work than setting the alarm to get up at 2am to make sure jobs were still running.� This was a tiresome process that wasn�t really good for anyone. Our corpus is stored in MongoDB–this has worked out to be an excellent storage mechanism for Wordnik.� When queried efficiently (aka the �right way�) we have been able to get huge amounts of data into our production in a very fast and predictable fashion. But R&amp;D folks often need to issue some potentially hairy, unexpected queries, and that can potentially cause blocking scenarios in Mongo.� Since we�re constantly appending data, we have to protect our runtime datastore from blocking queries.� Thus, we simply cannot allow ad-hoc queries against our MongoDB instance.� Without the benefit of being a super-rich startup, we can�t just provision servers and storage devices for these ad-hoc queries�especially when we have such a large storage requirement.� Thus using resources in the best way possible is always tops on our list. Done Right with Hadoop We became tired of the babysitting and had a couple major deadlines which made multi-week-long processing tasks impossible.� We provisioned a number of Rackspace VMs and commissioned Cloudera to help set up base images and management scripts.� They then helped us migrate some of our analytic code into a MR-friendly format.� With that, it didn�t take long to have a CDH3 Hadoop distribution image ready for provisioning to a cluster of VMs.� Going into this, we were confident that the VMs would drastically underperform our physical servers in the datacenter, but the ability to turn them up as needed would outweigh the performance-per-dollar metric. We then started the work on migrating our logic into Hadoop-friendly jobs.� We had a couple choices for the source data: Write a custom DBInputFormat class which reads directly from our MongoDB cluster.� Each Map task would be responsible for a range of IDs. Export the data and load it into HDFS Given the need for ad-hoc analysis, we chose the latter route, which requires more storage but relieves our MongoDB instance from the dreaded long running, open cursors.� This also made sense given the append-only nature of our data.� Incremental updates to our data are stored in flat files, which periodically import into HDFS.� We also implemented a simple changelog to track updates in data already transferred to HDFS so that updates can be propagated to the Hadoop cluster. Our search indexes are Lucene indexes�they are created as RAM indexes in the mapper and emitted it as byte[].� These files are in the terabyte size�getting the MR jobs to execute reliably and efficiently required a couple of simple but critical tuning steps. Our first tuning step was regarding RAM.� It�s crucial to understand the memory requirements of the documents used in the tasks.� For example, a blog post may contain 300 words while a typical book can range between 10 and 100,000.� If mapper task splits on a count of documents, you�re looking at potentially huge discrepancies in RAM requirements between splits.� You are then over or undershooting the processing capability of the node, and slowing down the process overall.� In our use case we had to split our documents into smaller chunks of data (sentences) since the variations of sentence sizes are much more predictable.� This made mapper tasks much more consistently sized which gave us better throughput. Depending on the size of the map tasks, we ran into sporadic timeouts which required overriding the mapred.merge.recordsBeforeProgress configuration. �Unfortunately that was typically not found until many hours into a job, so it�s worth watching early on. Performance The time to rebuild our entire index with a 10-node cluster on Rackspace VMs (16GB Ram) has been reduced from 3 weeks down to 3.5 days.� We have seen a linear improvement in processing time with cluster growth which, thanks to the images and scripts from Cloudera, is now trivial to spin up.� Our aggregate statistics can be regenerated in one day, down from weeks. The speed of processing has enabled us to free up resources and allow the same cluster to be used for R&amp;D analysis. Wrap up Hopefully there will be the day when we can have slotless MapReduce, which could allow variable map or reduce jobs. Ideally, we should be able to configure the maximum number of tasks that can be executed per node purely based on the number of cpus / cores. Then, it is�up to the task tracker to decide how many tasks to allocate within that allowed range based on the memory requirements of each job and how much free memory is available in the node. This will give us maximum flexibility. Since we�re working with large Lucene Indexes, the Katta project may be helpful with our Hadoop cluster. Overall, Hadoop helped us greatly.� We�re moving more logic to be Hadoop-enabled including using HBase for ad-hoc analytics.� As with many others, Wordnik has left the one-size-fits-all relational world and is moving towards a split MongoDB document storage for runtime data and Hadoop for analytics, data processing, reporting, and time-based aggregation.</snippet></document><document id="560"><title>Apache Hadoop Availability</title><url>http://blog.cloudera.com/blog/2011/02/hadoop-availability/</url><snippet>A common question on the Apache Hadoop mailing lists is what’s going on with availability? This post takes a look at availability in the context of Hadoop, gives an overview of the work in progress and where things are headed. Background When discussing Hadoop availability people often start with the NameNode since it is a single point of failure (SPOF) in HDFS, and most components in the Hadoop ecosystem (MapReduce, Apache HBase, Apache Pig, Apache Hive etc) rely on HDFS directly, and are therefore limited by its availability. However, Hadoop availability is a larger, more general issue, so it’s helpful to establish some context before diving in. Availability is the proportion of time a system is functioning [1], which is commonly referred to as “uptime” (vs downtime, when the system is not functioning). Note that availability is a stricter requirement than fault tolerance – the ability for a system to perform as designed and degrade gracefully in the presence of failures. A system that requires an hour to restart (eg for a configuration change or software upgrade) but has no single point of failure is fault tolerant but not highly available (HA). Adding redundancy in all SPOFs is a common way to improve fault tolerance, which helps [2], but is just a part of, improving Hadoop availability. Note also that fault tolerance is distinct from durability, even though the NameNode is a SPOF no single failure results in data loss as copies of NameNode persistent state (the image and edit log) are replicated both within and across hosts. Availability is also often conflated with reliability. Reliability in distributed systems is a more general issue than availability [3]. A truly reliable distributed system must be highly available, fault tolerant, secure, scalable, and perform predictably, etc. I’ll limit this post to Hadoop availability. Reasons for downtime An important part of improving availability and articulating requirements is understanding the causes of downtime. There are many types of failures in distributed systems, ways to classify them, and analyses of how failures result in downtime. Rather than go into depth here, I’ll briefly summarize some general categories of issues that may cause downtime: 1. Maintenance – Hardware and software may need to be upgraded, configuration changes may require a system restart, and operational tasks for dependent systems. Hadoop can handle most maintenance to slave hosts without downtime; however maintenance to a master host normally requires a restart of the entire system. 2. Hardware failures – Hosts and their connections may fail. Without redundant devices, or redundant components within devices, a single component failure may cause the entire device to fail. Hadoop can tolerate hardware failures (even silent failures like corruption) to slave hosts without downtime; however some hardware failures on the master host (or a failure in the connection between the master and the majority of the slaves) can cause system downtime [4]. 3. Software failures – Software bugs may cause a component in the system to stop functioning or require a restart. For example, a bug in upgrade code could result in downtime due to data corruption. A dependent software component may become unavailable (eg the Java garbage collector enters a stop-the-world phase). Hadoop can tolerate some software bugs without downtime; however components are generally designed to fail-fast – to stop and notify other components of failure rather than attempt to continue a possibly-flawed process. Therefore a software bug in a master service will likely cause downtime. 4. Operator errors – People make mistakes. From disconnecting the wrong cable, to mis-configured hosts, to typos in configuration files, operator errors can cause downtime. Hadoop attempts to limit operator error by simplifying administration, validating its configuration, and providing useful messages in logs and UI components; however operator mistakes may still cause downtime. Use cases In order for a system to be highly available, its design needs to anticipate these various failures. Removing single points of failure, enabling rolling upgrades, faster restarts, making the system robust and user friendly, etc are all necessary to improve availability. Given that improving availability requires a multi-prong approach, let’s take a look at the relevant use cases for limiting downtime. 1. Host maintenance – If an operator needs to upgrade or replace the primary host hardware or upgrade its operating system, they should be able to manually fail over to a hot standby, perform the upgrade and optionally fail back to the primary. The fail-over should be transparent to clients accessing the system (eg active jobs continue to run). Host maintenance to slave hosts can be handled without downtime today by de-commissioning the host. 2. Configuration changes – Ideally configuration changes to masters should not require a system restart — the configuration can be updated in-place or fail-over to a hot standby with an updated configuration is supported. In cases when they do, the operator should be able to restart the system with minimal impact to running workloads. 3. Software upgrades – An operator should be able to upgrade Hadoop’s software in-place (a “rolling upgrade”) on slave nodes and via fail-over on the master hosts so there is little or no downtime. If a restart is required it should be accelerated by quickly re-constructing the system’s state. 4. Host failures – If a non-redundant hardware component fails, the operating system crashes, a disk partition runs out of space, etc. the system should detect the failure, and, depending on the service and failure, (a) recover, (b) de-commission itself, or (c) fail over to a hot standby. Hadoop currently tolerates slave host failures without downtime, however master host failures often cause downtime. In practice, for a number of reasons, master hardware failures do not cause as much downtime as you might expect: In large clusters it is statistically improbable that a hardware failure impacts a machine running master services, and operations teams are often good at keeping a small number of well-known hosts healthy. Because there are few master hosts redundant hardware components can be used to limit the probability of a host failure without dramatically increasing the price of the overall system. Highly Available Hadoop A number of efforts are under way to improve Hadoop availability, and implement missing functionality required by the above use cases. Tasks related to HDFS availability are tracked here, tasks related to MapReduce availability are tracked here. 1. Improvements to Hadoop’s failure handling code. Hadoop’s native fault injection framework and other related frameworks continue to make Hadoop more robust in the face of failures. Recent advances in failure testing applied have been successfully applied to Hadoop [5] to identify software bugs (eg HDFS-1231, HDFS-1225, and HDFS-1228). 2. Work was recently started to allow Hadoop configuration changes without restart. As Hadoop incorporates this change configuration parameter changes will increasingly be possible without downtime. 3. A number of changes (eg HDFS-1070, HDFS-1295, and HDFS-1391) are underway to significantly improve the time it takes to restart HDFS. 4. Work has started to allow Hadoop client and server software of different versions to co-exist, with the goal of enabling in-place Hadoop software upgrades. 5. There have been efforts to make existing releases of HDFS more highly-available, as well as several research prototypes (eg UpRight-HDFS, NameNode Cluster, and HDFS-dnn) that examine HDFS availability. HDFS developers are currently working on a hot standby for the NameNode to improve on the existing NameNode fail-over. Like the Google File System which has “shadow masters”, this allows HDFS to fail the NameNode process from one host to another by actively replicating all NameNode state required to quickly restart the process. Integrating the BackupNode (edits are streamed from the primary NameNode to one or more BackupNodes) or using BookKeeper (a replicated service to reliably log streams of records that can be used for the edits log) with the AvatarNode (which replicates block reports across a primary and backup host) results in a standby NameNode that can be activated if the NameNode fails. Automatic hot fail-over can be achieved by integrating both clients and servers with ZooKeeper. A similar approach has been successfully used by Google to make GFS highly available [6]: Initially, GFS had no provision for automatic master failover. It was a manual process. Although it didn’t happen a lot, whenever it did, the cell might be down for an hour. Even our initial master-failover implementation required on the order of minutes. Over the past year, however, we’ve taken that down to something on the order of tens of seconds. This Active/Passive design enables both high availability and evolution towards being a better storage layer for systems like HBase, which in turn could be used to store metadata for a new version of HDFS (similar to Google’s GFS2). Like HDFS federation, this provides an evolutionary path to high scalability without the complexity of modifying HDFS to use multi-master replication. 6. The MapReduce master (JobTracker) state is stored in HDFS and is therefore limited by its availability. The JobTracker can be-restarted, however works needs to be to integrate it with a service like ZooKeeper to handle fail-over to a separate host. Hopefully this post has helped frame the various tasks behind Hadoop’s march towards high availability in a useful context. The development community understands this is one of the most high priority issues for users, and is looking forward to providing a highly available Hadoop in up-coming releases. Similarly, Cloudera is committed to improving availability in CDH4 – it’s our primary focus for the release. Thanks to Dhruba Borthakur, Doug Cutting, Sanjay Radia, and Konstantin Shvachko for reading drafts of this post. Footnotes [1] A common way to define availability is the ratio of the expected value of the uptime of the system to the aggregate of the expected values of uptime and downtime. Common metrics used are: Mean time between failures (MTBF) – the expected time between failures of a system during operation. Mean time to recovery (MTTR) – the average time the system will take to recover from any failure. Using these metrics availability can be defined as MTBF / (MTBF + MTTR). [2] I say “helps” because one the most common reasons for downtime (misconfiguration, operator error, and software bugs) are all exacerbated by system complexity, and making systems more fault tolerant often increases their complexity. [3] “Reliability and availability are different: Availability is doing the right thing within the specified response time. Reliability is not doing the wrong thing.” from WHY DO COMPUTERS STOP AND WHAT CAN BE DONE ABOUT IT? by Jim Gray [4] People often configure master hosts with redundant hardware components (nics, disks, IO controllers, and power units) so that an individual component failure does not cause the system to fail. [5] Towards Automatically Checking Thousands of Failures with Micro-specifications. H Gunawi, T Do, et al. UC Berkeley TR EECS-2010-98. [6] GFS: Evolution on Fast-Forward. Marshall Kirk McKusick, Sean Quinlan in the ACM Queue.</snippet></document><document id="561"><title>Distributed Flume Setup With an S3 Sink</title><url>http://blog.cloudera.com/blog/2011/02/distributed-flume-setup-with-an-s3-sink/</url><snippet>This is a guest repost contributed by Eric Lubow, CTO at SimpleReach.  It originally appeared here. I have recently spent a few days getting up to speed with Flume, Cloudera‘s distributed log offering. If you haven’t seen this and deal with lots of logs, you are definitely missing out on a fantastic project. I’m not going to spend time talking about it because you can read more about it in the user’s guide or in the Quora Flume Topic in ways that are better than I can describe it. But I will tell you about is my experience setting up Flume in a distributed environment to sync logs to an Amazon S3 sink. As CTO of SimpleReach, a company that does most of it’s work in the cloud, I’m constantly strategizing on how we can take advantage of the cloud for auto-scaling. Depending on the time of day or how much content distribution we are dealing with, we will spawn new instances to accommodate the load. We will still need the logs from those machines for later analysis (batch jobs like making use of Elastic Map Reduce). I am going to attempt to do this as step-by-step as possible but much of the terminology I use is described in the user’s guide and there is an expectation that you have at least skimmed it prior to starting this HOWTO. I am using EMR (Elastic Map Reduce) on EC2 and not the provided Hadoop by Cloudera. Additionally, the Cloudera version of Flume that I am working with is CDH3b3 (v0.9.1+29). Context I have 3 kinds of servers all running CentOS in the Amazon cloud: 1. a1: This is the agent which is producing all the logs 2. c1: This is the collector which is aggregating all the logs (from a1, a2, a3, etc) 3. u1: This is the flume master node which is sending out all the commands There are actually n agents, but for this example, we’ll keep it simple. Also, for a complete copy of the config files, please check out the full gist available here. Initial Setup On both a1 and c1, you’ll have to install flume-node (flume-node contains the files necessary to run the agent or the collector). # curl http://archive.cloudera.com/redhat/cdh/cloudera-cdh3.repo &gt; /etc/yum.repos.d/cloudera-cdh3.repo
# yum update yum
# yum install flume flume-node
 On u1, you’ll need to install the flume-master RPM:   # curl http://archive.cloudera.com/redhat/cdh/cloudera-cdh3.repo &gt; /etc/yum.repos.d/cloudera-cdh3.repo
# yum update yum
# yum install flume flume-master
 On each host, you need to copy the conf template file to the site specific config file. That is to say: cp flume-site.xml.template flume-site.xml
 First let’s jump onto the agent and set that up. Tune the $master_IP and $collector_IP variables appropriately, but change your /etc/flume/conf/flume-site.xml to look like: 

    flume.master.servers
    $master_IP
    This is the address for the config servers status server (http)

    flume.collector.event.host
    $collector_IP
    This is the host name of the default "remote" collector.

    flume.collector.port
    35853
    This default tcp port that the collector listens to in order to receive events it is collecting.

    flume.agent.logdir
    /mnt/flume-${user.name}/agent
     This is the directory that write-ahead logging data or disk-failover data is collected from applications gets written to. The agent watches this directory.
 Now on to the collector. Same file, different config. Replace all the variables with you $master IP address (you should be using Amazon’s internal IPs otherwise you will be paying the regional charge). The $account and $secret variables are both your Amazon EC2/S3 account key and secret Access key respectively. The $bucket is the S3 bucket that will contain the log files. Also worthy of pointing out is theflume.collector.roll.millis and flume.collector.dfs.compress.gzip. The millis is how frequently the log file gets truncated and the next file begins to be written to. It would be nice if this could be done by file size and not only by time, but it works for now. The other config option is flume.collector.dfs.compress.gzip. This ensures that the logfiles are compressed prior to being dumped onto S3 (saves LOTS of space). 

    flume.master.servers
    $master
    This is the address for the config servers status server (http)

    flume.collector.event.host
    localhost
    This is the host name of the default "remote" collector.

    flume.collector.port
    35853
    This default tcp port that the collector listens to in order to receive events it is collecting.

    fs.default.name
    s3n://$account:$secret@$bucket

    fs.s3n.impl

org.apache.hadoop.fs.s3native.NativeS3FileSystem

    fs.s3.awsAccessKeyId
    $account

    fs.s3.awsSecretAccessKey
    $secret

    fs.s3n.awsAccessKeyId
    $account

    fs.s3n.awsSecretAccessKey
    $secret

    flume.agent.logdir
    /mnt/flume-${user.name}/agent
     This is the directory that write-ahead logging data
      or disk-failover data is collected from applications gets
      written to. The agent watches this directory.

    flume.collector.dfs.dir
    file:///mnt/flume-${user.name}/collected
    This is a dfs directory that is the the final resting
    place for logs to be stored in.  This defaults to a local dir in
    /tmp but can be hadoop URI path that such as hdfs://namenode/path/

    flume.collector.dfs.compress.gzip
    true
    Writes compressed output in gzip format to dfs. value is
     boolean type, i.e. true/false

    flume.collector.roll.millis
    60000
    The time (in milliseconds)
    between when hdfs files are closed and a new file is opened
    (rolled).
 While we are still on the collector, in order to properly write to S3, you’ll need to make 4 file adjustments and all of them will go into the /usr/lib/flume/lib/ directory. 1. commons-codec-1.4.jar 2. jets3t-0.6.1.jar 3. commons-httpclient-3.0.1.jar 4. emr-hadoop-core-0.20.jar The one thing that should be noted here is that the emr-hadoop-core-0.20.jar file replaces the hadoop-core.jar symlink. The emr-hadoop-core-0.20.jar file is the hadoop-core.jar file from an EC2 Hadoop cluster instance. Note: This will break the ability to seamlessly upgrade via the RPM (which is how you installed it if you’ve been following my HOWTO). Keep these files around just in case. I have added a tarball of the files here, but they are all still available with a quick Google search. And now on to the master. There was actually no configuration that I did on the master file system to get things up and running. But if flume is writing to a /tmp directory on an ephemeral file system, then it should be fixed. Web Based Setup I chose to do the individual machine setup via the master web interface. You can get to this pointing your web browser at http://u1:35871/ (replace u1 with public DNS IP of your flume master). Ensure that the port is accessible from the outside through your security settings. At this point, it was easiest for me to ensure all hosts running flume could talk to all ports on all other hosts running flume. You can certainly lock this down to the individual ports for security once everything is up and running. At this point, you should go to a1 and c1 run /etc/init.d/flume-node start. If everything goes well, then the master (whose IP is specified in their configs) should be notified of their existence. Now you can configure them from the web. Click on the config link and then fill in the text lines as follows (use what is in bold): Agent Node: $agent_ec2_internal_ip Source: tailDir(“/mnt/logs/”,”.*.log”) Sink: agentBESink(“$collector_ec2_internal_ip”,35853) Note: I chose to use tailDir since I will control rotating the logs on my own. I am also using agentBESink because I am ok with losing log lines if the case arises. Now click Submit Query and go back to the config page to setup the collector: Agent Node: $collector_ec2_internal_ip Source: collectorSource(35853) Sink:collectorSink(“s3n://$account:$secret@$bucket/logs/%Y/%m/%d/%H00?,”server”) This is going to tell the collector that we are sinking to s3native with the $account key and the $secret key into the $bucket with an initial folder of ‘logs’. It will then log to sub-folders with YYYY/MM/DD/HH00 (or 2011/02/03/1300/server-.log). There will be 60 gziped files in each folder since the timing is setup to be 1 file per minute. Now clickSubmit Query and go to the ‘master’ page and you should see 2 commands listed as “SUCCEEDED” in the command history. If they have not succeeded, ensure a few things have been done (there are probably more, but this is a handy start: 1. Always use double quotes (“) since single quotes (‘) aren’t interpreted correctly. UPDATE: Single quotes are interpreted correctly, they are just not accepted intentionally (Thanks jmhsieh) 2. In your regex, use something like “.*\\.log” since the ‘.’ is part of the regex. 3. In your regex, ensure that your blackslashes are properly escaped: “foo\\bar” is the correct version of trying to match “foo\bar”. 4. Ensure any ‘/’ are inserted as ‘%2F’ in the Amazon account and secret codes. Additionally, there are also tables of Node Status and Node Configuration. These should match up with what you think you configured. At this point everything should work. Admittedly I had a lot of trouble getting to this point. But with the help of the Cloudera folks and the users on irc.freenode.net in #flume, I was able to get things going. The logs sadly aren’t too helpful here in most cases (but look anyway cause they might provide you with more info than they provided for me). If I missed anything in this post or there is something else I am unaware of, then let me know. References http://wiki.apache.org/hadoop/AmazonS3 Flume Users Guide irc.freenode.net #flume https://issues.cloudera.org/browse/FLUME-66 Config files gist</snippet></document><document id="562"><title>Make your Apache Hadoop voice heard!</title><url>http://blog.cloudera.com/blog/2011/02/make-your-hadoop-voice-heard/</url><snippet>Apache Hadoop is increasingly being adopted for storage and processing of large-scale complex data. There are more Hadoop user groups in more locations than ever before and the community surrounding Hadoop is alive and vibrant. The questions we are all thinking include, “exactly how much growth is there?” and “what do these new deployments look like? Leading industry analyst group, Ventana Research, is trying to answer these types of questions with a market survey. By taking the survey you can provide valuable insight into how Hadoop is being used by organizations to solve some of their most critical data problems. As an added bonus you will get a copy of the research results so that you can compare your deployment to those of other organizations. Take the survey today! Market Survey</snippet></document><document id="563"><title>Upcoming Apache Hadoop Training Sessions</title><url>http://blog.cloudera.com/blog/2011/02/upcoming-apache-hadoop-training-sessions/</url><snippet>As interest in Hadoop continues to grow, we continue to make available public training sessions to accommodate. Cloudera training sessions are always evolving to stay current with Hadoop technology as the open source community continues to fine tune and improve Hadoop and its surrounding ecosystem. Cloudera provides training sessions tailored toward Developers, Administrators and Managers for Hadoop, HBase, Hive, Pig and Hue. The Hadoop Developer and Sysadmin training course includes the certification exam to become a Cloudera Certified Hadoop Developer. Save your spot in the next training session, as space fills fast. Available training sessions of note: (as of February 2, 2011) City New York City Chicago Chicago Seattle Seattle SF Bay Area SF Bay Area SF Bay Area Dallas Dallas Training Session Hadoop Training for Developers Hadoop Training for Developers Hadoop Training for Administrators Hadoop Training for Developers Hadoop Training for Administrators Analyzing Data with Hive and Pig Apache HBase Training Hadoop Essentials for Mangers Hadoop Training for Developers Hadoop Training for Administrators Date February 22, 2011 February 28, 2011 March 3, 2011 March 7, 2011 March 10, 2011 March 8, 2011 March 10, 2011 March 15, 2011 March 14, 2011 March 17, 2011                                                         Check the Cloudera Events Web page regularly for new events are created and published frequently. http://www.cloudera.com/company/events/ To inquire about organizing private trainings session for a large group please reach out to training@cloudera.com.</snippet></document><document id="564"><title>Some News Related to the Apache Hadoop Project</title><url>http://blog.cloudera.com/blog/2011/02/some-news-related-to-the-apache-hadoop-project/</url><snippet>In an announcement on its blog, Yahoo! recently announced plans to stop distributing its own version of Hadoop, and instead to re-focus on improving Apache’s Hadoop releases. This is great news. Currently, many people running Hadoop use patched versions of the Apache Hadoop package that combine features contributed by Yahoo! and others, but may not yet be collectively available in a single Apache release. Different teams working on enhancements have made their changes to distinct branches off of old releases. Collecting that work into a single source code package and building a system with the best quality and feature set has been hard work. New users of Hadoop have generally found this assembly work to be too much trouble. To solve that problem, Cloudera currently distributes a patched version of Apache Hadoop, assembling work from Yahoo!, Cloudera, Facebook and others that has been committed to the Apache project, but not necessarily collectively available in one Apache release. The Apache Hadoop project contains MapReduce, HDFS and Common. Cloudera packages these along with a number of complementary open-source projects — Apache HBase, Apache Pig, Apache Hive, Apache Zookeeper, Oozie, Flume, Hue, and others — that provide useful services for data management, access and use. Right now, HDFS, MapReduce and Common — the Apache Hadoop packages — are the only packages that we have to ship with a large collection of patches. You can think of Apache Hadoop as similar to the Linux kernel: the heart of a larger system. In that case, Cloudera acts like Red Hat or Canonical, providing a complete platform that includes both the kernel and the most popular higher-level packages. We assemble &amp; test the combined components, package them for easy installation, certify the integration of complementary systems and provide a predictable release schedule so users can plan upgrades and updates. Cloudera’s Distribution for Apache Hadoop is this larger package. It exists to make the power of Hadoop easily available to a larger audience of users. We thank Yahoo! for its renewed efforts to make Apache Hadoop releases the very best versions of Hadoop. A more robust and powerful kernel makes the entire ecosystem stronger. One of the strengths of Apache Hadoop ecosystem has been the collective contributions of many organizations and individuals that has added up to hundreds of person-years of engineering investment. That investment dwarfs what any single organization or proprietary vendor could muster and this explains the strength and sophistication of the overall system. Yahoo!’s commitment to open source development of Hadoop dates to the creation of the project. By concentrating its efforts on the Apache repository, Yahoo! makes a meaningful contribution to everyone in the Apache Hadoop community.  We very much hope that the larger Hadoop community will continue to work in the same way, working together to create excellent Apache releases that everyone can use. Certainly, Cloudera and our customers will benefit from high-quality releases from Apache that require minimal patching for production deployment. We believe that everyone else will, too.</snippet></document><document id="565"><title>CDH2 Update 3 Now Available</title><url>http://blog.cloudera.com/blog/2011/01/cdh2-update-3-now-available/</url><snippet>Cloudera is happy to announce the availability of the third update to version 2 of our distribution for Apache Hadoop (CDH2). CDH2 Update 3 contains a number of important fixes like HADOOP-5203, HDFS-1377, MAPREDUCE-1699, MAPREDUCE-1853, and MAPREDUCE-270. Check out the release notes and change log for more details on what’s in this release. You can find the packages and tarballs on our website, or simply update your systems if you are already using our repositories. More instructions can be found in our CDH documentation. We appreciate feedback! Get in touch with us on the CDH user list, twitter or IRC (#cloudera on freenode.net) and let us know how the update is working for you.</snippet></document><document id="566"><title>Lessons Learned from Cloudera’s Hadoop Developer Training Course</title><url>http://blog.cloudera.com/blog/2011/01/lessons-learned-from-clouderas-hadoop-developer-training-course/</url><snippet>This is a guest post from an attendee of our Hadoop Developer Training course, Attila Csordas, bioinformatician at the European Bioinformatics Institute, Hinxton, Cambridge, UK. As a wet lab biologist turned bioinformatician I have ~2 year programming experience, mainly in Perl and have been working with Java for the last 9 months. A bioinformatician is not a developer so I’m writing easy code in just a fraction of my work time: parsers, db connections, xml validators, little bug fixes, shell scripts. On the other hand, I have now 5 months of Hadoop experience – and a 6 month old baby named Alice – and that experience is as immense as it gets. Ever since I read the classic Dean-Ghemawat paper, MapReduce: Simplified Data Processing on Large Clusters, I’m thinking about bioinformatics problems in terms of Map and Reduce functions (especially during my evening jog), then implementing these ideas in my free time–which consists of feeding the baby, writing code, changing the nappy, rewriting code. Not independently from this I am now a Cloudera Certified Hadoop Developer as I’ve participated 2 weeks ago in the Cloudera Hadoop Developer Training–organized in London–and passed the exam. Right now this is the only Hadoop Certificate in the world. The instructor, Ian Wrigley, was a relaxed Brit from west LA who was articulate enough to allow us all to follow the ideas put forward and was responsive enough to answer the questions raised clearing misconceptions. In what follows I summarize some lessons and tricks learned on the course. I will be using some references from the movie, The Prestige, because besides it being my favorite movie sometimes I sense a little analogy between the three parts of a magic trick and the MapReduce paradigm in general: Map ~ The Pledge, Shuffle&amp;Sort ~ The Turn, Reduce ~ The Prestige. 1. In native Java it is a good practice to create objects outside the map/reduce functions and re/populate it with data each time around the loop in the map/reduce methods. If you consider a WordCount job running on billions of documents a typical mapper can be called millions of times for each map task which creates a new Text object each time around the loop. If your mapper looks like this: 
  public static class MapClass extends MapReduceBase
    implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
    public void map(LongWritable key, Text value,
                    OutputCollector&lt;Text, IntWritable&gt; output,
                    Reporter reporter) throws IOException {
      String line = value.toString();
      StringTokenizer itr = new StringTokenizer(line);
	   Text word = new Text();
      while (itr.hasMoreTokens()) {
      word.set(itr.nextToken());
      output.collect(word, new IntWritable(1));
      }
    }
  }
 then you’re creating a lot of luxury objects and an inefficient pattern, making the life of the garbage collector unnecessarily hard. But if you initialize your Text object outside the map method like this: 
 public static class MapClass extends MapReduceBase
    implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(LongWritable key, Text value,
                    OutputCollector&lt;Text, IntWritable&gt; output,
                    Reporter reporter) throws IOException {
      String line = value.toString();
      StringTokenizer itr = new StringTokenizer(line);

      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        output.collect(word, one);

      }
    }
  }
 you can receive 2 times the performance benefit. No wonder that the relevant Hadoop classes are mutable. 2. Don’t let the reduce progress report fool you! When you see something like this on the command line: 11/01/23 19:16:28 INFO mapred.JobClient: map 100% reduce 33% you might think, “Wow, the reducers are quick and making nice progress. See, already processed ? of the input!” Wrong! This is just the false impression created by the progress report. That 33% means only that the shuffle phase has finished and the output from the mappers landed safely at the nodes where the reducers reside. The other 66%? The Sort phase (or rather the Merge phase) – merging the already presorted map outputs while maintaining the sort-order – has been completed. While the copy/shuffle and sort/merge are technically part of the reduce task and accomplished via background threads, the beginner tends to think of map/reduce progress in terms of key/value processing, because he was taught that he only has to write those functions (and the driver) on key/values to accomplish the task. Quite inconsistently, the map progress report does not include the in-memory pre-sort by key and is based solely on the amount of input that has already been processed. In Hadoop: The Definitive Guide, Tom White let us know how the system estimates/displays the proportion of the reduce progress admitting the calculation being a little more complicated than estimating map progress: “it does this by dividing the total progress into three parts, corresponding to the three phases of the shuffle: For example, if the task has run the reducer on half of its input, then the task’s progress is ?, since it has completed the copy and the sort phases (? each) and is halfway through the reduce phase (?).” So ? according to this calculation reflects only 0% actual reduce key/value processing progress as the ? is really representing the Shuffle&amp;Sort which sometimes seems as the bastard child (or children) of the whole MapReduce framework. According to Tom White, “shuffle is the heart of MapReduce and is where the ‘magic’ happens’.” Well, the creators of MapReduce obviously agree with the advice of Alfred Borden from The Prestige, “The secret impresses no one. The trick you use it for is everything,” because they let Shuffle&amp;Sort stay in the background and I guess MapShuffleSortReduce sounded like a less catchy name just like 23andMitochondriaandMe for 23andMe[1]. Actually the extended name should be MapSortShuffleMergeReduce, reflecting the pre-sort at the end of the map and the merge while maintaining the sort by the map on the reduce side. This false impression should be avoided in future progress reports maybe by giving new progress lines to the separate shuffle and short steps. Till then if you are stuck at reduce 66%, then rewrite your reduce class. 3. Call RecordReader if InputSplit crosses the line. The byte oriented InputSplit can split a record in half and the record oriented RecordReader will make it Mapper ready. I asked Ian Wrigley to explain how the RecordReader can ensure each key/value pair is processed but only once even if that pair is split across an InputSplit: The InputSplit says “Start at offset x, and continue for y bytes”. The RecordReader gets the InputSplit. If the InputSplit’s offset is 0, start at the beginning of the InputSplit. Otherwise, scan forward to the start of the next record. Read a record at a time; this might take you past the end of the InputSplit, but that’s OK. After you’re past the end of the InputSplit, stop. 4. The sacrifice… that’s the price of a good trick.(Alfred Borden). Explicitly relying on unreliable hardware: that is a beautiful concept behind the original Google File System and the Hadoop Distributed File System. The MapReduce framework’s robust fault tolerance (except the master node) and the seemingly “nice&amp;easy” abstraction provided for developers comes with a price that needs to be considered. Mappers don’t communicate with each other, reducers don’t communicate with each other, these functions have no identities and are replaceable &amp; disposable. Developers have to completely rethink all familiar sequential algorithms they are using if they want to implement them in MapReduce and there’s no guarantee that all those techniques will work. They cannot write code which communicates between nodes. As a result, debugging is complicated but necessary once the code’s complexity level surpasses the basic examples. Finally, 4 snippets of juicy hadoop ecosystem info I picked up during the course quite randomly: There’s a ~20% performance hit when using the Streaming API and other languages than Java, so you can expect a faster development time at a price of slower execution. The latest version of Oozie (Yahoo’s Hadoop workflow engine, included in CDH3) has nice advanced features like running workflows at specific times, monitoring an HDFS directory, and running a workflow when data appears in the directory. Mahout (the machine learning library built on top of Hadoop) has xml input support. If you run hadoop jobs with Pig scripts your sysadmin might not see the Pig in it at all. [1] The personal genotyping company is providing and analyzing genetic variations not just from the 23 nuclear chromosomes but from the mitochondrial DNA as well</snippet></document><document id="567"><title>Introducing Alfredo, Kerberos HTTP SPNEGO for Java</title><url>http://blog.cloudera.com/blog/2011/01/introducing-alfredo-kerberos-http-spnego-for-java/</url><snippet>What is Kerberos &amp; SPNEGO? Kerberos is an authentication protocol that provides mutual authentication and single sign-on capabilities. SPNEGO is a plain text mechanism for negotiating authentication protocols between peers; one notable application of this is Kerberos authentication over HTTP. What is Alfredo? Alfredo is an Open Source Java library providing support for Kerberos HTTP SPNEGO authentication. By using Alfredo: Client applications can easily access HTTP resources protected with Kerberos HTTP SPNEGO Web applications can easily protect HTTP resources with Kerberos HTTP SPNEGO Why Alfredo? One of Cloudera’s goals is to enable end-to-end security for anyone using Hadoop and other projects that work on top of Hadoop. Because these are Open Source Projects that are available under the Apache License, only software that has compatible license terms can be used with them. We created Alfredo because we needed to add user authentication to Oozie. Since Hadoop already supports Kerberos authentication, adding support for Kerberos authentication to Oozie is an obvious choice. Oozie’s API is HTTP based, making the use of Kerberos HTTP SPNEGO an obvious choice as well. A benefit to supporting Kerberos HTTP SPNEGO means that tools like curl and popular browsers (Firefox and Internet Explorer) will work with HTTP resources protected by Alfredo. We couldn’t find a Java library providing Kerberos HTTP SPNEGO support to integrate with Oozie client/server code that is Apache Licensed —which is a requirement for the entire CDH platform— therefore, the solution was to write the code ourselves. Because Alfredo is a reusable component, other projects that have HTTP endpoints can also use it, such as the Hadoop web-console, HBase, and other Hadoop-based projects.  Projects not related to Hadoop can also use Alfredo. Implementing Alfredo From an integration perspective, I wanted something as simple as an URL/HttpURLConnection helper for the client side and a Java Servlet Filter for the server side. This would allow existing Java client and server applications to support Kerberos HTTP SPNEGO with minimal changes. To protect HTTP resources of a Java web application, Alfredo’s AuthenticationFilter must be deployed in front of the HTTP resources. This filter requires some minimal configuration: specifically, the name of Kerberos principal for the service, and the keytab file where the credentials for the principal are stored. To access the protected HTTP resources, you can use tools and applications like curl and Firefox with support for Kerberos HTTP SPNEGO. To enable you to write a Java client application that accesses HTTP resources protected with Kerberos HTTP SPNEGO, Alfredo provides the AuthenticatedURL class. This class is a simple helper class that authenticates the user using the credentials from the OS Kerberos cache. In addition, Alfredo can be extended to support other authentication mechanisms via a client interface. An implementation equivalent to Hadoop pseudo/simple is also provided with Alfredo. We intentionally implemented Alfredo so that it does not depend on Hadoop or Oozie projects because we wanted to avoid unwanted transitive dependencies for other projects. Alfredo’s AuthenticationFilter can easily be subclassed to obtain its configuration using the configuration mechanism of whatever project is using it. Getting Alfredo Alfredo is distributed with an Apache License 2.0. The source code (including examples) is available at http://github.com/cloudera/alfredo Documentation can be found at http://cloudera.github.com/alfredo Alfredo is already available in Cloudera’s Maven repository: https://repository.cloudera.com/content/repositories/releases: groupId: com.cloudera.alfredo artifactId: alfredo version: 0.1.3 type: jar Now that Alfredo is done, I will work on my original problem, adding support for Kerberos HTTP SPNEGO to Oozie. It should only take me about a couple of hours of coding.</snippet></document><document id="568"><title>Top 10 Blog Posts of 2010</title><url>http://blog.cloudera.com/blog/2011/01/top-10-blog-posts-of-2010/</url><snippet>We blogged about 104 different topics in 2010 and we recently decided to take a look back and see what folks were most interested in reading.  The topics that were featured ranged from Cloudera’s Distribution for Apache Hadoop technical updates (CDH3b3 being the most recent) to highlighting upcoming Hadoop related events and activities to sharing practical insights for implementing Hadoop. We also featured a number of guest blog posts. Here are the top 10 blog posts from 2010: How to Get a Job at Cloudera Cloudera is hiring around the clock, and this blog highlights the best course of action to increase your chances of becoming a Clouderan. Why Europe’s Largest Ad Targeting Platform Uses Hadoop “As data volumes increased and performance suffered, we recognized a new approach was needed (Hadoop).” –Richard Hutton, Nugg.ad CTO What’s New in CDH3b2 Flume Flume, our data movement platform, was introduced to the world and into the open source environment. What’s New in CDH3b2 Hue Hue, a web UI for Hadoop, is a suite of web applications as well as a platform for building custom applications with a nice UI library. Natural Language Processing with Hadoop and Python Data volumes are increasing naturally from text (blogs) and speech (YouTube videos) posing new questions for Natural Language Processing. This involves making sense of lots of data in different forms and extracting useful insights. How Raytheon BBN Technologies Researchers are Using Hadoop to Build a Scalable, Distributed Triple Store Raytheon BBN Technologies built a cloud-based triple-store technology, known as SHARD, to address scalability issues in the processing and analysis of Semantic Web data. Cloudera’s Support Team Shares Some Basic Hardware Recommendations The Cloudera support team discusses workload evaluation and the critical role it plays in hardware selection. Integrating Hive and HBase Facebook explains integrating Hive and HBase to keep their warehouse up to date with the latest information published by users. Pushing the Limits of Distributed Processing Google built a 100,000 node Hadoop cluster running on Nexus One mobile phone hardware and powered by Android. The environmental cost of this solution is 1/100th the equivalent of running it within their data center. (April Fools) Using Flume to Collect Apache 2 Web Server Logs This post presents the common use case of using a Flume node to collect Apache 2 web server logs and deliver them to HDFS. Aside from How to Get a Job at Cloudera, Cloudera blog readers viewed posts related to CDH and its components, posts exemplifying possibilities with Hadoop in production, and posts highlighting integrations with Hadoop. Looking forward we plan to continue to feature technical and non-technical topics, as well as guest posts from customers and the community, and plan to increase our number of published posts. If there is a topic you would like to learn more about, or you have a Hadoop story you would like to share we would love to hear your ideas. Email suggestions to community@cloudera.com.</snippet></document><document id="569"><title>Hadoop I/O: Sequence, Map, Set, Array, BloomMap Files</title><url>http://blog.cloudera.com/blog/2011/01/hadoop-io-sequence-map-set-array-bloommap-files/</url><snippet>This is a guest repost contributed by Matteo Bertozzi, a Developer at Develer S.r.l. Apache Hadoop’s SequenceFile provides a persistent data structure for binary key-value pairs. In contrast with other persistent key-value data structures like B-Trees, you can’t seek to a specified key editing, adding or removing it. This file is append-only. SequenceFile has 3 available formats: An “Uncompressed” format, A “Record Compressed” format and a “Block-Compressed”. All of them share a header that contains some information which allows the reader to recognize is format. There’re Key and Value Class Name’s that allow the reader to instantiate those classes, via reflection, for reading. The version number and format (Is Compressed, Is Block Compressed), if compression is enabled the Compression Codec class name field is added to the header. The sequence file also can contain a “secondary” key-value list that can be used as file Metadata. This key-value list can be just a Text/Text pair, and is written to the file during the initialization that happens in the SequenceFile.Writer constructor, so you can’t edit your metadata. As seen Sequence File has 3 available formats, the “Uncompressed” and the “Record Compressed” are really similar. Each call to the append() method adds a record to the sequence file which contains the length of the whole record (key length + value length), the length of the key and the raw data of key and value. The difference between the compressed and the uncompressed version is that the value raw data is compressed, with the specified codec, or not. In contrast the “Block-Compressed” format is more compression-aggressive. Data is not written until it reaches a threshold, and when the threshold is reached all keys are compressed together–the same happens for the values and the auxiliary lists of key and value lengths. As you can see in the figure on the left, a block record contains a VInt with the number of the buffered records and 4 compressed blocks that contains a list with the length of the keys, the list of keys, another list with the length of the values and finally the list of values. Before each block a sync marker is written. Hadoop SequenceFile is the base data structure for the other types of files, like MapFile, SetFile, ArrayFile and BloomMapFile. The MapFile is a directory that contains two SequenceFile: the data file (“/data”) and the index file (“/index”). The data contains all the key, value records but key N + 1 must be greater then or equal to the key N. This condition is checked during the append() operation, if checkKey fail it throws an IOException “Key out of order”. The Index file is populated with the key and a LongWritable that contains the starting byte position of the record. Index does’t contains all the keys but just a fraction of the keys, you can specify the indexInterval calling setIndexInterval() method. The Index is read enteirely into memory, so if you’ve large map you can set a index skip value that allows you to keep in memory just a fraction of the index keys. SetFile and ArrayFile are based on MapFile, and their implementation are just few lines of code. The SetFile instead of append(key, value) as just the key field append(key) and the value is always the NullWritable instance. The ArrayFile as just the value field append(value) and the key is a LongWritable that contains the record number, count + 1. The BloomMapFile extends the MapFile adding another file, the bloom file “/bloom”, and this file contains a serialization of the DynamicBloomFilter filled with the added keys. The bloom file is written entirely during the close operation. If you want to play with SequenceFile, MapFile, SetFile, ArrayFile without using Java, I’ve written a naive implementation in python. You can find it, in my github repository python-hadoop.</snippet></document><document id="570"><title>How-to: Include Third-Party Libraries in Your MapReduce Job</title><url>http://blog.cloudera.com/blog/2011/01/how-to-include-third-party-libraries-in-your-map-reduce-job/</url><snippet>“My library is in the classpath but I still get a Class Not Found exception in a MapReduce job” – If you have this problem this blog is for you. Java requires third-party and user-defined classes to be on the command line’s “-classpath” option when the JVM is launched. The `hadoop` wrapper shell script does exactly this for you by building the classpath from the core libraries located in /usr/lib/hadoop-0.20/ and /usr/lib/hadoop-0.20/lib/ directories. However, with MapReduce you job’s task attempts are executed on remote nodes. How do you tell a remote machine to include third-party and user-defined classes? MapReduce jobs are executed in separate JVMs on TaskTrackers and sometimes you need to use third-party libraries in the map/reduce task attempts. For example, you might want to access HBase from within your map tasks. One way to do this is to package every class used in the submittable JAR. You will have to unpack the original hbase-.jar and repackage all the classes in your submittable Hadoop jar. Not good. Don’t do this: The version compatibility issues are going to bite you sooner or later. There are better ways of doing the same by either putting your jar in distributed cache or installing the whole JAR on the Hadoop nodes and telling TaskTrackers about their location. 1. Include the JAR in the “-libjars” command line option of the `hadoop jar …` command. The jar will be placed in distributed cache and will be made available to all of the job’s task attempts. More specifically, you will find the JAR in one of the ${mapred.local.dir}/taskTracker/archive/${user.name}/distcache/… subdirectories on local nodes. The advantage of the distributed cache is that your jar might still be there on your next program run (at least in theory: The files should be kicked out of the distributed cache only when they exceed soft limit defined by the local.cache.size configuration variable, defaults to 10GB, but your actual mileage can vary particularly with the newest security enhancements). Hadoop keeps track of the changes to the distributed cache files by examining their modification timestamp. *Update to post: Please note that items 2 and 3 below are deprecated starting CDH4 and will be no longer supported starting CDH5. 2. Include the referenced JAR in the lib subdirectory of the submittable JAR: A MapReduce job will unpack the JAR from this subdirectory into ${mapred.local.dir}/taskTracker/${user.name}/jobcache/$jobid/jars on the TaskTracker nodes and point your tasks to this directory to make the JAR available to your code. If the JARs are small, change often, and are job-specific this is the preferred method. 3. Finally, you can install the JAR on the cluster nodes. The easiest way is to place the JAR into $HADOOP_HOME/lib directory as everything from this directory is included when a Hadoop daemon starts. However, since you know that only TaskTrackers will need these the new JAR, a better way is to modify HADOOP_TASKTRACKER_OPTS option in the hadoop-env.sh configuration file. This method is preferred if the JAR is tied to the code running on the nodes, like HBase. HADOOP_TASKTRACKER_OPTS="-classpath&lt;colon-separated-paths-to-your-jars&gt;" Restart the TastTrackers when you are done. Do not forget to update the jar when the underlying software changes. All of the above options affect only the code running on the distributed nodes. If your code that launches the Hadoop job uses the same library, you need to include the JAR in the HADOOP_CLASSPATH environment variable as well: HADOOP_CLASSPATH="&lt;colon-separated-paths-to-your-jars&gt;" Note that starting with Java 1.6 classpath can point to directories like “/path/to/your/jars/*” which will pick up all JARs from the given directory. The same guiding principles apply to native code libraries that need to be run on the nodes (JNI or C++ pipes). You can put them into distributed cache with the “-files” options, include them into archive files specified with the “-archives” option, or install them on the cluster nodes. If the dynamic library linker is configured properly the native code should be made available to your task attempts. You can also modify the environment of the job’s running task attempts explicitly by specifying JAVA_LIBRARY_PATH or LD_LIBRARY_PATH variables: hadoop jar &lt;your jar&gt; [main class]
      -D mapred.child.env="LD_LIBRARY_PATH=/path/to/your/libs" ...</snippet></document><document id="571"><title>Setting up CDH3 Hadoop on my new Macbook Pro</title><url>http://blog.cloudera.com/blog/2011/01/setting-up-cdh3-hadoop-on-my-new-macbook-pro/</url><snippet>This is a guest re-post courtesy of Arun Jacob, Data Architect at Disney, prior to that he was an engineer at RichRelevance and Evri. For the last couple of years, Arun has been focused on data mining/information extraction, using a mix of custom and open source technologies. A New Machine I’m fortunate enough to have recently received a Macbook Pro, 2.8 GHz Intel dual core, with 8GB RAM. This is the third time I’ve turned a vanilla mac into a ninja coding machine, and following my design principle of “first time = coincidence, second time = annoying, third time = pattern”, I’ve decided to write down the details for the next time. Baseline This section details the pre-hadoop installs I did. Java Previously I was running on Leopard, i.e. 10.4, and had to install soylatte to get the latest version of Java. In Snow Leopard, java jdk 1.6.0_22 is installed by default. That’s good enough for me, for now. Gcc, etc. In order to get these on the box, I had to install XCode, making sure to check the ‘linux dev tools’ option. MacPorts I installed MacPorts in case I needed to upgrade any native libs or tools. Eclipse I downloaded the 64 bit Java EE version of Helios. Tomcat Tomcat is part of my daily fun, and these instructions to install tomcat6 where helpful. One thing to note is that in order to access the tomcat manager panel, you also need to specify &lt;role rolename="manager"/&gt;
 prior to defining &lt;user username="admin" password="password" roles="standard,manager,admin"/&gt;
 Also, I run tomcat standalone (no httpd), so the mod_jk install part didnt apply. Finally, I chose not to daemonize tomcat because this is a dev box, not a server, and the instructions for compiling and using jsvc for 64 bit sounded iffy at best. Hadoop I use the CDH distro. The install was amazingly easy, and their support rocks. Unfortunately, they don’t have a dmg that drops Hadoop on the box configured and ready to run, so I need to build up my own psuedo mac node. This is what I want my mac to have (for starters): 1. distinct processes for namenode, job tracker node, and datanode/task tracker nodes. 2. formatted HDFS 3. Pig 0.8.0 I’m not going to try to auto start hadoop because (again) this is a dev box, and start-all.sh should handle bringing up the JVMs around namenode, job tracker, datanode/tasktracker. I am installing CDH3, because I’ve been running it in psuedo-mode on my Ubuntu dev box for the last month and have had no issues with it. Also, I want to run Pig 0.8.0, and that version may have some assumptions about the version of Hadoop that it needs. All of the CDH3 Tarballs can be found at http://archive.cloudera.com/cdh/3/, and damn, that’s a lot of tarballs. I downloaded hadoop 0.20.2+737, it’s (currently) the latest version out there. Because this is my new dev box, I decided to forego the usual security motivated setup of the hadoop user. When this decision comes back to bite me, I’ll be sure to update this post. In fact, for ease of permissions/etc, I decided to install under my home dir, under  a CDH3 dir, so I could group all CDH3 related installs together. I symlinked the hadoop-0.20+737 dir to hadoop, and I’ll update it if CDH3 updates their version of hadoop. After untarring to the directory, all that was left was to make sure the ~/CDH3/hadoop/bin directory was in my .profile PATH settings. Psuedo Mode Config I’m going to set up Hadoop in psuedo distributed mode, just like I have on my Ubuntu box. Unlike Debian/Red Hat CDH distros, where this is an apt-get or yum command, I need to set up conf files on my own. Fortunately the example-confs subdir of the Hadoop install has a conf.psuedo subdir. I needed to modify the following in core-site.xml:  &lt;property&gt; 
     &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; 
     &lt;value&gt;changed_to_a_valid_dir_I_own&lt;/value&gt; 
 &lt;/property&gt; and the following in hdfs-site.xml:  &lt;property&gt; 
     &lt;!-- specify this so that running 'hadoop namenode -format' formats the right dir --&gt; 
     &lt;name&gt;dfs.name.dir&lt;/name&gt; 
     &lt;value&gt;changed_to_a_different_dir_I_own&lt;/value&gt; 
  &lt;/property&gt; finally, I symlinked the conf dir at the top level of the Hadoop install to example-configs/conf.pseudo after saving off the original conf: mv ./conf install-conf
ln sf ./example-confs/conf.pseudo conf
 Pig Installing Pig is as simple as downloading the tar, setting the path up, and going, sort of. The first time I ran pig, it tried to connect to the default install location of hadoop, /usr/lib/hadoop-0.20/. I made sure to set HADOOP_HOME to point to my install, and verified that the grunt shell connected to my configured HDFS (on port 8020). More To Come This psuedo node install was relatively painless. I’m going to continue to install Hadoop/HDFS based tools that may need more (HBase) or less (Hive) configuration, and update in successive posts. Written by Arun Jacob</snippet></document><document id="572"><title>Configuring Security Features in CDH3</title><url>http://blog.cloudera.com/blog/2011/01/configuring-security-features-in-cdh3/</url><snippet>Post written by Cloudera Software Engineer Aaron T. Myers. Apache Hadoop has had methods of doing user authorization for some time. The Hadoop Distributed File System (HDFS) has a permissions model similar to Unix to control file and directory access, and MapReduce has access control lists (ACLs) per job queue to control which users may submit jobs. These authorization schemes allow Hadoop users and administrators to specify exactly who may access Hadoop’s resources. However, until recently, these mechanisms relied on a fundamentally insecure method of identifying the user who is interacting with Hadoop. That is, Hadoop had no way of performing reliable authentication. This limitation meant that any authorization system built on top of Hadoop, while helpful to prevent accidental unwanted access, could do nothing to prevent malicious users from accessing other users’ data. Prior to the availability of Hadoop’s security features, the only way an organization could meet the requirement for data access protection was to run multiple distinct Hadoop clusters, and to segregate the groups who have network access to these clusters. This has obvious cost effectiveness implications, but, more importantly, limits the flexibility an organization has with respect to data storage options. One of the inherent powers of Hadoop is the ability to store and correlate all of an organization’s data. This is impossible if one must a priori relegate data to multiple distinct clusters based on security requirements. Furthermore, because of some organizations’ internal security policies, certain types of data could not be stored in Hadoop at all. While this was acceptable for many of the first organizations to leverage Hadoop, the increase in Hadoop’s popularity and penetration into traditional enterprises necessitated the addition of better authentication mechanisms. Among many of the new features introduced as part of CDH3 Beta 3, Hadoop now has the ability to provide strong authentication guarantees. The core Hadoop security work was done almost completely by Yahoo! and subsequently contributed to Apache Hadoop. Rather than create an ad hoc Hadoop-specific authentication scheme, Hadoop’s authentication system leverages Kerberos. Kerberos is an industry-standard authentication system developed by MIT which has been in existence since 1989. There are multiple open source implementations of Kerberos, including one produced and maintained by MIT itself. Kerberos is also the authentication system underpinning many proprietary identity management systems commonly found in enterprise environments, including Microsoft’s Active Directory. Hadoop’s support of Kerberos enables organizations to seamlessly integrate the new authentication features of Hadoop with their existing authentication and single sign-on systems. All of the components of CDH3 Beta 3 now have support for interacting with secure Hadoop clusters, and many have incorporated additional security features which were previously impossible or impractical to implement with the security limitations inherent in Hadoop itself. Because of the complexity of integrating with multiple third-party authentication systems, configuring Hadoop and its associated components to use these systems is non-trivial. Cloudera is pleased to announce the general availability of Cloudera’s “CDH3 Security Guide”. In this comprehensive guide, you’ll find instructions for enabling the security features of Hadoop itself, as well as for configuring all of the other components of CDH to be able to interact with a Hadoop cluster with security enabled. You’ll also find a troubleshooting guide for debugging common errors encountered when configuring a secure Hadoop environment, as well as details for configuring Hadoop’s authentication mechanism to use Active Directory. Please email cdh-user@cloudera.org if you have any questions or encounter any issues.</snippet></document><document id="573"><title>Map-Reduce With Ruby Using Apache Hadoop</title><url>http://blog.cloudera.com/blog/2011/01/map-reduce-with-ruby-using-apache-hadoop/</url><snippet>Guest re-post from Phil Whelan, a large-scale web-services consultant based in Vancouver, BC. Here I demonstrate, with repeatable steps, how to fire-up a Hadoop cluster on Amazon EC2, load data onto the HDFS (Hadoop Distributed File-System), write map-reduce scripts in Ruby and use them to run a map-reduce job on your Hadoop cluster. You will not need to ssh into the cluster, as all tasks are run from your local machine. Below I am using my MacBook Pro as my local machine, but the steps I have provided should be reproducible on other platforms running bash and Java. Fire-Up Your Hadoop Cluster Setting Up Your Local Hadoop Client Defining The Map-Reduce Task Uploading Your Data To HDFS (Hadoop Distributed FileSystem) Coding Your Map And Reduce Scripts in Ruby Running The Hadoop Job The Results Conclusion Resources Fire-Up Your Hadoop Cluster I chose Cloudera’s Distribution for Apache Hadoop which is 100% Apache licensed, but has some additional benefits. One of these benefits is that it is released by Doug Cutting, who started Hadoop and drove it’s development at Yahoo! He also started Lucene, which is another of my favourite Apache Projects, so I have good faith that he knows what he is doing. Another benefit, as you will see, is that it is simple to fire-up a Hadoop cluster. I am going to use Cloudera’s Whirr script, which will allow me to fire up a production ready Hadoop cluster on Amazon EC2 directly from my laptop. Whirr is built on jclouds, meaning other cloud providers should be supported, but only Amazon EC2 has been tested. Once we have Whirr installed, we will configure a hadoop.properties file with our Amazon EC2 credentials and the details of our desired Hadoop cluster. Whirr will use this hadoop.properties file to build the cluster. If you are on Debian or Redhat you can use either apt-get or yum to install whirr, but since I’m on Mac OS X, I’ll need to download the Whirr script. The current version of Whirr 0.2.0, hosted on the Apache Incubator site, is not compatible with Cloudera’s Distribution for Hadoop (CDH), so I’m am downloading version 0.1.0+23. mkdir ~/src/cloudera
cd ~/src/cloudera
wget http://archive.cloudera.com/cdh/3/whirr-0.1.0+23.tar.gz
tar -xvzf whirr-0.1.0+23.tar.gz To build Whirr you’ll need to install Java (version 1.6), Maven ( &gt;= 2.2.1) and Ruby ( &gt;= 1.8.7). If you’re running with the latest Mac OS X, then you should have the latest Java and I’ll assume, due to the title of this post, that you can manage the Ruby version. If you are not familiar with Maven, you can install it via Homebrew on Mac OS X using the brew command below. On Debian use apt-get install maven2. sudo brew update
sudo brew install maven Once the dependencies are installed we can build the whirr tool. cd whirr-0.1.0+23
mvn clean install
mvn package -Ppackage In true Maven style, it will download a long list of dependencies the first time you build this. Be patient. Ok, it should be built now and if you’re anything like me, you would have used the time to get a nice cuppa tea or a sandwich. Let’s sanity check the whirr script… bin/whirr version You should see something like “Apache Whirr 0.1.0+23? output to the terminal. Create a hadoop.properties file with the following content. whirr.service-name=hadoop
whirr.cluster-name=myhadoopcluster
whirr.instance-templates=1 jt+nn,1 dn+tt
whirr.provider=ec2
whirr.identity=
whirr.credential=
whirr.private-key-file=${sys:user.home}/.ssh/id_rsa
whirr.public-key-file=${sys:user.home}/.ssh/id_rsa.pub
whirr.hadoop-install-runurl=cloudera/cdh/install
whirr.hadoop-configure-runurl=cloudera/cdh/post-configure Replace and with your Amazon EC2 Access Key ID and Amazon EC2 Secret Access Key (I will not tell you what mine is). This configuration is a little boring with only two machines. One machine for the master and one machine for the worker. You can get more creative once you are up and running. Let’s fire up our “cluster”. bin/whirr launch-cluster --config hadoop.properties This is another good time to put the kettle on, as it takes a few minutes to get up and running. If you are curious, or worried that things have come to a halt then Whirr outputs a whirr.log in the current directory. Fire-up another terminal window and tail the log. cd ~/src/cloudera/whirr-0.1.0+23
tail -F whirr.log 16 minutes (and several cups of tea) later the cluster is up and running. Here is the output I saw in my terminal. Launching myhadoopcluster cluster
Configuring template
Starting master node
Master node started: [[id=us-east-1/i-561d073b, providerId=i-561d073b, tag=myhadoopcluster, name=null, location=[id=us-east-1d, scope=ZONE, description=us-east-1d, parent=us-east-1], uri=null, imageId=us-east-1/ami-d59d6bbc, os=[name=null, family=amzn-linux, version=2010.11.1-beta, arch=paravirtual, is64Bit=false, description=amzn-ami-us-east-1/amzn-ami-2010.11.1-beta.i386.manifest.xml], userMetadata={}, state=RUNNING, privateAddresses=[10.113.23.123], publicAddresses=[72.44.45.199], hardware=[id=m1.small, providerId=m1.small, name=m1.small, processors=[[cores=1.0, speed=1.0]], ram=1740, volumes=[[id=null, type=LOCAL, size=10.0, device=/dev/sda1, durable=false, isBootDevice=true], [id=null, type=LOCAL, size=150.0, device=/dev/sda2, durable=false, isBootDevice=false]], supportsImage=Not(is64Bit())]]]
Authorizing firewall
Starting 1 worker node(s)
Worker nodes started: [[id=us-east-1/i-98100af5, providerId=i-98100af5, tag=myhadoopcluster, name=null, location=[id=us-east-1d, scope=ZONE, description=us-east-1d, parent=us-east-1], uri=null, imageId=us-east-1/ami-d59d6bbc, os=[name=null, family=amzn-linux, version=2010.11.1-beta, arch=paravirtual, is64Bit=false, description=amzn-ami-us-east-1/amzn-ami-2010.11.1-beta.i386.manifest.xml], userMetadata={}, state=RUNNING, privateAddresses=[10.116.147.148], publicAddresses=[184.72.179.36], hardware=[id=m1.small, providerId=m1.small, name=m1.small, processors=[[cores=1.0, speed=1.0]], ram=1740, volumes=[[id=null, type=LOCAL, size=10.0, device=/dev/sda1, durable=false, isBootDevice=true], [id=null, type=LOCAL, size=150.0, device=/dev/sda2, durable=false, isBootDevice=false]], supportsImage=Not(is64Bit())]]]
Completed launch of myhadoopcluster
Web UI available at http://ec2-72-44-45-199.compute-1.amazonaws.com
Wrote Hadoop site file /Users/phil/.whirr/myhadoopcluster/hadoop-site.xml
Wrote Hadoop proxy script /Users/phil/.whirr/myhadoopcluster/hadoop-proxy.sh
Started cluster of 2 instances
HadoopCluster{instances=[Instance{roles=[jt, nn], publicAddress=ec2-72-44-45-199.compute-1.amazonaws.com/72.44.45.199, privateAddress=/10.113.23.123}, Instance{roles=[tt, dn], publicAddress=/184.72.179.36, privateAddress=/10.116.147.148}], configuration={fs.default.name=hdfs://ec2-72-44-45-199.compute-1.amazonaws.com:8020/, mapred.job.tracker=ec2-72-44-45-199.compute-1.amazonaws.com:8021, hadoop.job.ugi=root,root, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.SocksSocketFactory, hadoop.socks.server=localhost:6666}} Whirr has created a directory with some files in our home directory… ~/.whirr/myhadoopcluster/hadoop-proxy.sh
~/.whirr/myhadoopcluster/hadoop-site.xml This hadoop-proxy.sh is used to access the web interface of Hadoop securely. When we run this it will tunnel through to the cluster and give us access in the web browser via a SOCKS proxy. You need to configure the SOCKS proxy in either your web browser or, in my case, the Mac OS X settings menu. Hadoop SOCKS Proxy Configuration for Mac OS X Now start the proxy in your terminal… (Note: There has still been no need to ssh into the cluster. Everything in this post is done on our local machine) sh ~/.whirr/myhadoopcluster/hadoop-proxy.sh

   Running proxy to Hadoop cluster at
   ec2-72-44-45-199.compute-1.amazonaws.com.
   Use Ctrl-c to quit. The above will output the hostname that you can access the cluster at. On Amazon EC2 it looks something like http://ec2-72-44-45-199.compute-1.amazonaws.com:50070/dfshealth.jsp. Use this hostname to view the cluster in your web browser. http://:50070/dfshealth.jsp HDFS Health Dashboard If you click on the link to “Browse the filesystem” then you will notice the hostname changes. This will jump around the data-nodes in your cluster, due to HDFS’s distributed nature. You only currently have one data-node. On Amazon EC2 this new hostname will be the internal hostname of data-node server, which is visible because you are tunnelling through the SOCKS proxy. HDFS File Browser Ok! It looks as though our Hadoop cluster is up and running. Let’s upload our data. Setting Up Your Local Hadoop Client To run a map-reduce job on your data, your data needs to be on the Hadoop Distributed File-System. Otherwise known as HDFS. You can interact with Hadoop and HDFS with the hadoop command. We do not have Hadoop installed on our local machine. Therefore, we can either log into one of our Hadoop cluster machines and run the hadoop command from there, or install hadoop on our local machine. I’m going to opt for installing Hadoop on my local machine (recommended), as it will be easier to interact with the HDFS and start the Hadoop map-reduce jobs directly from my laptop. Cloudera does not, unfortunately, provide a release of Hadoop for Mac OS X. Only debians and RPMs. They do provide a .tar.gz download, which we are going to use to install Hadoop locally. Hadoop is built with Java and the scripts are written in bash, so there should not be too many problems with compatibility across platforms that can run Java and bash. Visit Cloudera CDH Release webpage and select CDH3 Patched Tarball. I downloaded the same version hadoop-0.20.2+737.tar.gz that Whirr installed on the cluster. tar -xvzf hadoop-0.20.2+737.tar.gz
sudo mv hadoop-0.20.2+737 /usr/local/
cd /usr/local
sudo ln -s hadoop-0.20.2+737 hadoop
echo 'export HADOOP_HOME=/usr/local/hadoop' &gt;&gt; ~/.profile
echo 'export PATH=$PATH:$HADOOP_HOME/bin' &gt;&gt; ~/.profile
source ~/.profile
which hadoop # should output "/usr/local/hadoop/bin/hadoop"
hadoop version # should output "Hadoop 0.20.2+737 ..."
cp ~/.whirr/myhadoopcluster/hadoop-site.xml /usr/local/hadoop/conf/ Now run your first command from your local machine to interact with HDFS. This following command is similar to “ls -l /” in bash. hadoop fs -ls / You should see the following output which lists the root on the Hadoop filesystem. 10/12/30 18:19:59 WARN conf.Configuration: DEPRECATED: hadoop-site.xml found in the classpath. Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, mapred-site.xml and hdfs-site.xml to override properties of core-default.xml, mapred-default.xml and hdfs-default.xml respectively
Found 4 items
drwxrwxrwx   - hdfs supergroup          0 2010-12-28 10:33 /hadoop
drwxrwxrwx   - hdfs supergroup          0 2010-12-28 10:33 /mnt
drwxrwxrwx   - hdfs supergroup          0 2010-12-28 10:33 /tmp
drwxrwxrwx   - hdfs supergroup          0 2010-12-28 10:33 /user Yes, you will see a depreciation warning, since hadoop-site.xml configuration has been split into multiple files. We will not worry about this here. Defining The Map-Reduce Task We are going write a map-reduce job that scans all the files in a given directory, takes the words found in those files and then counts the number of times words begin with any two characters. For this we’re going to use a dictionary file found on my Mac OS X /usr/share/dict/words. It contains 234936 words, each on a newline. Linux has a similar dictionary file. Uploading Your Data To HDFS (Hadoop Distributed FileSystem) hadoop fs -mkdir input
hadoop fs -put /usr/share/dict/words input/
hadoop fs -ls input You should see output similar to the following, which list the words file on the remote HDFS. Since my local user is “phil”, Hadoop has added the file under /user/phil on HDFS. Found 1 items
-rw-r--r--   3 phil supergroup    2486813 2010-12-30 18:43 /user/phil/input/words Congratulations! You have just uploaded your first file to the Hadoop Distributed File-System on your cluster in the cloud. Coding Your Map And Reduce Scripts in Ruby Map-Reduce can actually be thought of as map-group-reduce. The “map” sucks in the raw data, cuts off the fat, removes the bones and outputs the smallest possible piece of output data for each piece of input data. The “map” also outputs the key of the data. Our key will be the two-letter prefix of each word. These keys are used by Hadoop to “group” the data together. The “reduce” then takes each group of data and “reduces” it. In our case the “reduce” will be the counting occurrences of the two-letter prefixes. Hadoop will do much of the work for us. It will recurse the input directory, open the files and stream the files one line at a time into our “map” script via STDIN. We will output zero, one or many output lines to STDOUT for each line of input. Since we know that our input file has exactly one word per line, we can simplify our script and always output exactly one two-letter prefix for each input line. (EDIT: words with one letter will not result in any output). The output of our “map” script to STDOUT will have to be Hadoop friendly. This means we will output our “key”, then a tab character then our value and then a newline. This is what the streaming interface expects. Hadoop needs to extract the key to be able to sort and organise the data based on this key. Our value will always be “1?, since each line has only one word with only once instance of the two-letter prefix of that word. For instance, if the input was “Apple” then we would output the key “ap” and value “1?. We have seen the prefix “ap” only once in this input. You should note that the value can be anything that your reduce script can interpret. For instance, the value could be a string of JSON. Here, we are keeping it very simple. ap1 Let’s code up the mapper as map.rb # Ruby code for map.rb

ARGF.each do |line|

   # remove any newline
   line = line.chomp

   # do nothing will lines shorter than 2 characters
   next if ! line || line.length &lt; 2

   # grab our key as the two-character prefix (lower-cased)
   key = line[0,2].downcase

   # value is a count of 1 occurence
   value = 1

   # output to STDOUT
   #
   puts key + "\t" + value.to_s

end Now we have our mapper script, let’s write the reducer. Remember, the reducer is going to count up the occurences for each two-character prefix (our “key”). Hadoop will have already grouped our keys together, so even if the mapper output is in shuffled order, the reducer will now see the keys in sorted order. This means that the reducer can watch for when the key changes and know that it has seen all of the possible values for the previous key. Here is an example of the STDIN and STDOUT that map.rb and reduce.rb might see. The data flow goes from left to right. map.rb STDIN map.rb STDOUT Hadoop sorts keys reduce.rb STDIN reduce.rb STDOUT Apple Monkey Orange Banana APR Bat appetite ap 1 mo 1 or 1 ba 1 ap 1 ba 1 ap 1 ap 1 ap 1 ap 1 ba 1 ba 1 mo 1 or 1 ap 3 ba 2 mo 1 or 1 Let’s code up the reducer as reduce.rb # Ruby code for reduce.rb

prev_key = nil
key_total = 0

ARGF.each do |line|

   # remove any newline
   line = line.chomp

   # split key and value on tab character
   (key, value) = line.split(/\t/)

   # check for new key
   if prev_key &amp;&amp; key != prev_key &amp;&amp; key_total &gt; 0

      # output total for previous key

      #
      puts prev_key + "\t" + key_total.to_s

      # reset key total for new key
      prev_key = key
      key_total = 0

   elsif ! prev_key
      prev_key = key

   end

   # add to count for this current key
   key_total += value.to_i

end You can test out your scripts on a small sample by using the “sort” command in replacement for Hadoop. cat /usr/share/dict/words | ruby map.rb | sort | ruby reduce.rb The start of this output looks like this… aa	13
ab	666
ac	1491
ad	867
ae	337
af	380 Running The Hadoop Job I wrote this bash-based runner script to start the job. It uses Hadoop’s streaming service. This streaming service is what allows us to write our map-reduce scripts in Ruby. It streams to our script’s STDIN and reads our script’s output from our script’s STDOUT. #!/bin/bash

HADOOP_HOME=/usr/local/hadoop
JAR=contrib/streaming/hadoop-streaming-0.20.2+737.jar

HSTREAMING="$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/$JAR"

$HSTREAMING \
 -mapper  'ruby map.rb' \
 -reducer 'ruby reduce.rb' \
 -file map.rb \
 -file reduce.rb \
 -input '/user/phil/input/*' \
 -output /user/phil/output We specify the command to run for the mapper and reducer and use the “-file” parameter twice to attach our two Ruby scripts. It is assumed that all other dependencies are already installed on the machine. In this case we are using no Ruby imports or requires and the Ruby interpreter is already installed on the machines in the Hadoop cluster (it came with the Cloudera Amazon EC2 image). Things become more complicated when you start to run jobs with more dependencies that are not already installed on the Hadoop cluster. This is a topic for another post. “-input” and “-output” specify which files to read from for input and the directoty to send the output to. You can also specify a deeper level of recursion with more wildcards (e.g. “/user/phil/input/*/*/*”). Once again, it is important that our SOCKS proxy is running, as this is the secure way that we communicate through to our Hadoop cluster. sh ~/.whirr/myhadoopcluster/hadoop-proxy.sh
    Running proxy to Hadoop cluster at ec2-72-44-45-199.compute-1.amazonaws.com. Use Ctrl-c to quit. Now we can start the Hadoop job by running our above bash script. Here is the output the script gave me at the terminal. packageJobJar: [map.rb, reduce.rb, /tmp/hadoop-phil/hadoop-unjar3366245269477540365/] [] /var/folders/+Q/+QReZ-KsElyb+mXn12xTxU+++TI/-Tmp-/streamjob5253225231988397348.jar tmpDir=null
10/12/30 21:45:32 INFO mapred.FileInputFormat: Total input paths to process : 1
10/12/30 21:45:37 INFO streaming.StreamJob: getLocalDirs(): [/tmp/hadoop-phil/mapred/local]
10/12/30 21:45:37 INFO streaming.StreamJob: Running job: job_201012281833_0001
10/12/30 21:45:37 INFO streaming.StreamJob: To kill this job, run:
10/12/30 21:45:37 INFO streaming.StreamJob: /usr/local/hadoop/bin/hadoop job  -Dmapred.job.tracker=ec2-72-44-45-199.compute-1.amazonaws.com:8021 -kill job_201012281833_0001
10/12/30 21:45:37 INFO streaming.StreamJob: Tracking URL: http://ec2-72-44-45-199.compute-1.amazonaws.com:50030/jobdetails.jsp?jobid=job_201012281833_0001
10/12/30 21:45:38 INFO streaming.StreamJob:  map 0%  reduce 0%
10/12/30 21:45:55 INFO streaming.StreamJob:  map 42%  reduce 0%
10/12/30 21:45:58 INFO streaming.StreamJob:  map 100%  reduce 0%
10/12/30 21:46:14 INFO streaming.StreamJob:  map 100%  reduce 88%
10/12/30 21:46:19 INFO streaming.StreamJob:  map 100%  reduce 100%
10/12/30 21:46:22 INFO streaming.StreamJob: Job complete: job_201012281833_0001
10/12/30 21:46:22 INFO streaming.StreamJob: Output: /user/phil/output This is reflected if you visit the job tracker console in web browser. jobTracker after successful run If you click on the job link you can see lots of information on this job. This job is completed in these images, but with a longer running job you would see the progress as the job runs. I have split the job tracker page into the following three images. Map-Reduce Job Tracker Page (part 1) Map-Reduce Job Tracker Page (part 2) Map-Reduce Job Tracker Page (part 3) Graphs The Results Our map-reduce job has run successfully using Ruby. Let’s have a look at the output. hadoop fs -ls output

Found 3 items
-rw-r--r--   3 phil supergroup          0 2010-12-30 21:46 /user/phil/output/_SUCCESS
drwxrwxrwx   - phil supergroup          0 2010-12-30 21:45 /user/phil/output/_logs
-rw-r--r--   3 phil supergroup       2341 2010-12-30 21:46 /user/phil/output/part-00000 Hadoop output is written in chunks to sequential files part-00000, part-00001, part-00002 and so on. Our dataset is very small, so we only have one 2kb file called part-00000. hadoop fs -cat output/part-00000 | head
aa	13
ab	666
ac	1491
ad	867
ae	337
af	380
ag	507
ah	46
ai	169
aj	14 Our map-reduce script counted 13 words starting with “aa”, 666 words starting with “ab” and 1491 words starting with “ac”. Conclusion Yes, it is an overkill to use Hadoop and a (very small) cluster of cloud-based machines for this example, but I think it demonstrates how you can quickly get your Hadoop cluster up and running map-reduce jobs written in Ruby. You can use the same procedure to fire-up a much larger and more powerful Hadoop cluster with a bigger dataset and more complex Ruby scripts. Please post any questions or suggestions you have in the comments below. They are always highly appreciated. Resources Apache Hadoop Cloudera’s Distribution for Apache Hadoop (CDH) Cloudera Hadoop Training VMWare Image Map-Reduce Using Perl jclouds Words file on Unix-like operating systems</snippet></document><document id="574"><title>New Features in Apache Pig 0.8</title><url>http://blog.cloudera.com/blog/2010/12/new-features-in-apache-pig-0-8/</url><snippet>This is a guest post contributed by Dmitriy Ryaboy (@squarecog) and was originally published in his blog on December 19th. We thought the information was valuable enough that it was worth reposting to spread the word even further.  The Pig 0.8 release includes a large number of bug fixes and optimizations, but at the core it is a feature release. It’s been in the works for almost a full year (most of the work on 0.7 was completed by January of 2009, although it took a while to actually get the release out), and the amount of time spent on 0.8 really shows. I meant to describe these in detail in a series of posts, but it seems blogging regularly is not my forte. This release is so chock-full of great new features, however, that I feel compelled to at least list them. So, behold, in no particular order, a non-exhaustive list of new features I am excited about in Pig 0.8: Support for UDFs in scripting languages This is exactly what it sounds like — if your favorite language has a JVM implementation, it can be used to create Pig UDFs. Pig now ships with support for UDFs in Jython, but other languages can be supported by implementing a few interfaces. Details about the Pig UDFs in Python can be found here: http://pig.apache.org/docs/r0.8.0/udf.html#Python+UDFs This is the outcome of PIG-928; it was quite a pleasure to watch this develop over time — while most Pig tickets wind up getting worked on by at most one or two people, this turned into a collaboration of quite a few developers, many of them new to the project — Kishore Gopalakrishna’s patch was the initial conversation starter, which was then hacked on or merged into similar work by Woody Anderson, Arnab Nandi, Julien Le Dem, Ashutosh Chauhan and Aniket Mokashi (Aniket deserves an extra shout-out for patiently working to incorporate everyone’s feedback and pushing the patch through the last mile). PigUnit A contribution by Romain Rigaux, PigUnit is exactly what it sounds like — a tool that simplifies the Pig users’ lives by giving them a simple way to unit test Pig scripts. The documentation at http://pig.apache.org/docs/r0.8.0/pigunit.html and the code at http://svn.apache.org/viewvc/pig/trunk/test/org/apache/pig/test/pigunit/TestPigTest.java?view=markup speak for themselves as far as usage. PigStats Pig can now provide much better visibility into what is going on inside a Pig job than it ever did before, thanks to extensive work by Richard Ding (see PIG-1333 and PIG-1478). This feature is a feature in three parts: 1. Script statistics. This is the most easily visible change. At the end of running a script, Pig will output a table with some basic statistics regarding the jobs that it ran. It looks something like this: Job Stats (time in seconds): JobId Maps Reduces Max Map Time Min Map Time Avg Map Time Max Reduce Time Min Reduce Time Avg Reduce Time Alias Feature Outputs job_xxx 1654 218 84 6 14 107 87 99 counted_data, data, grouped_data GROUP_BY, COMBINER   job_xxx 2 1 9 6 7 13 13 13 ordered_data SAMPLER   job_xxx 2 1 26 18 22 31 31 31 ordered_data ORDER_BY hdfs://tmp/out, This is extremely useful when debugging slow jobs, as you can immediately identify which stages of your script are slow, and correlate the slow Map-Reduce jobs with the actual Pig operators and relations in your script — something that was not trivial before (folks often resorted to setting parallelism to slightly different numbers for every join and group just to figure out which job was doing what. No more of this!) 2. Data in Job XML Pig now inserts several interesting properties into the Hadoop jobs that it generates, including the relations being generated, Pig features being used, and ids of parent Hadoop jobs. This is quite helpful when monitoring a cluster, and is also handy when examining job history using the HadoopJobHistoryLoader , now part of piggybank (use Pig to mine your job history!). 3. PigRunner API The same information that is printed out when Pig runs the script from a command line is available if one uses the Java API to start Pig jobs. If you start a script using the PigRunner.run(String args[], ProgressNotificationListener listener), you will get as a result a PigStats object that gives you access to the job hierarchy, the Hadoop counters from each job, and so on. You can implement the optional ProgressNotificationListener if you want to watch the job as it progresses; the listener will be notified as different component jobs start and finish. Documentation of the API, new properties in the Job XML, and more, is available at http://pig.apache.org/docs/r0.8.0/piglatin_ref1.html#Pig+Statistics Scalar values It’s very common to need to use some calculated statistic in a calculation to inform other calculations. For example, consider a data set that consists of people and their eye color; we want to calculate the fraction of the total population that has a given eye color. The script looks something like this: people = LOAD '/data/people' using PigStorage()
  AS (person_id:long, eye_color:chararray);
num_people = FOREACH (group people all)
  GENERATE COUNT(people) AS total;
eye_color_fractions = FOREACH ( GROUP people BY eye_color )
  GENERATE
    group as eye_color,
    COUNT(people) / num_people.total AS fraction;   Pretty straightforward, except it does not work. What’s happening in the above code is that we are referencing the relation num_people from inside another relation, eye_color_fractions and this doesn’t really make sense if Pig does not know that num_people only has one row. In the past you had to do something hacky like joining the two relations on a constant to replicate the total into each row, and then generate the division. Needless to say, this was not entirely satisfactory. In PIG-1434 Aniket Mokashi tackled this, implementing an elegant solution that hides all of these details from the user — you can now simply cast a single-row relation as a scalar, and use it as desired. The above script becomes: people = LOAD '/data/people' using PigStorage()
  AS (person_id:long, eye_color:chararray);
num_people = FOREACH (group people all)
  GENERATE COUNT(people) AS total;
eye_color_fractions = FOREACH ( GROUP people BY eye_color )
  GENERATE
    group as eye_color,
    COUNT(people) / (long) num_people.total AS fraction;   This makes the casting explicit, but Pig is now smart enough to do this implicitly as well. A runtime exception is generated if the relation being used as a scalar contains more than one tuple. More documentation of this feature is available at http://pig.apache.org/docs/r0.8.0/piglatin_ref2.html#Casting+Relations+to+Scalars Monitored UDFs A new annotation has been added, @MonitoredUDF, which makes Pig spawn a watcher thread that kills an execution that is taking too long, and return a default value instead. This comes in handy when dealing with certain operations like complex regular expressions. More documentation is available at http://pig.apache.org/docs/r0.8.0/udf.html#Monitoring+long-running+UDFs Automatic merge of small files This is a simple one, but useful — when running Pig over many small files, instead of creating a map task per file (paying the overhead of scheduling and running a task for a computation that might only take a few seconds), we can merge the inputs and create a few map tasks that are a bit more hefty. Two properties control this behavior: pig.maxCombinedSplitSize controls the maximum size of the resulting split, and pig.splitCombination controls whether or not the feature is activated in the first place (it is on by default). This work is documented in the ticket PIG-1518; there are additional details in the release notes attached to the ticket. Generic UDFs I wrote about this one before — a small feature that allows you to invoke static Java methods as Pig UDFs without needing to wrap them in custom code. The official documentation is available at http://pig.apache.org/docs/r0.8.0/piglatin_ref1.html#Dynamic+Invokers Safeguards against missing PARALLEL keyword One of the more common mistakes people make when writing Pig scripts is forgetting to specify parallelism for operators that need it. The default behavior used to be that this means parallelism of 1, which can lead to extremely inefficient jobs. A patch by Jeff Zhang in PIG-1249 changes this behavior to instead use a simple heuristic: if parallelism is not specified, derive the number of reducers by taking MIN(max_reducers, total_input_size / bytes_per_reducer). Max number of reducers is controlled by the property pig.exec.reducers.max (default 999) and bytes per reducer are controlled by pig.exec.reducers.bytes.per.reducer (default 1GB). This is a safeguard, not a panacea; it only works with file-based input, estimates number of reducers based on input size, not the size of the intermediate data — so if you have a highly selective filter, or you are grouping a large dataset by a low-cardinality field, it will produce bad number — but it’s a nice safeguard against dramatic misconfigurations. When porting to Apache Pig 0.8, remember to audit your scripts for parallelized operators that do not specify the PARALLEL keyword — if the intent is to use a single reducer, make this intent explicit by specifying PARALLEL 1. HBaseStorage HBaseStorage has been shored up in Pig 0.8. It can now read data stored in as bytes instead of requiring all numbers to be converted to Strings; it accepts a number of options — limit the number of rows returned, push down filters on HBase keys, etc. In addition, it can now be used to write to HBase in addition to reading from it. Details about the options, etc, can be found in the Release Notes section of PIG-1205. Note that at the moment this only works with the HBase 0.20.{4,5,6} releases, and does not work with 0.89+. There is a patch in PIG-1680 that you can apply if you need 0.89 and 0.90 compatibility; it is not applied to the main codebase yet, as it is not backwards compatible. We are very interested in help making this Storage engine more featureful, please feel free to jump in and contribute! Support for custom Map-Reduce jobs in the flow Although we try to make these a rarity, sometimes cases come up in which a custom Map-Reduce job fits the bill better than Pig. Weaving a Map-Reduce job into the middle of a Pig workflow was awkward before — you had to use something like Oozie or Azkaban, or write your own workflow application. Pig 0.8 introduces a simple “MAPREDUCE” operator which allows you to invoke an opaque MR job in the middle of the flow, and continue with Pig: text = load 'WordcountInput.txt';
wordcount = MAPREDUCE wordcount.jar
  STORE text INTO 'inputDir'
  LOAD 'outputDir' AS (word:chararray, count: int)
  `org.myorg.WordCount inputDir outputDir`;   Details are available on the wiki page: http://wiki.apache.org/pig/NativeMapReduce The ticket for this one has been open for a while, since Pig 0.2 days, and it’s nice to see it finally implemented. Thumbs up to Aniket Mokashi for this one. Custom Partitioners This feature, also implemented by the amazingly productive Aniket Mokashi, is also a bit of a power-user thing (and also an ancient ticket, PIG-282). It allows the Pig script author to control the function used to distribute map output among reducers. By default, Pig uses a random hash partitioner, but sometimes a custom algorithm is required when the script author knows something particularly unique about the reduce key distribution. When that is the case, a user can now specify the Hadoop Partitioner to swap in instead of the default: B = group A by $0 PARTITION BY org.apache.pig.test.utils.SimpleCustomPartitioner parallel 2; More specific documentation can be found in the Release Notes section of PIG-282</snippet></document><document id="575"><title>A profile of Apache Hadoop MapReduce computing efficiency (continued)</title><url>http://blog.cloudera.com/blog/2010/12/a-profile-of-hadoop-mapreduce-computing-efficiency-continued/</url><snippet>Guest post from Paul Burkhardt, a Research Developer at SRA International, Inc. where he develops large-scale, distributed computing solutions. Part II Previously we proposed how we measure the performance in Hadoop MapReduce applications in an effort to better understand the computing efficiency. In this part, we’ll describe some results and illuminate both good and bad characteristics. We selected our SIFT-M MapReduce application, described in our presentation at Hadoop World 2010 [3], as the candidate algorithm for Node Scalability since it is embarrassingly parallel and is representative of compute-intensive applications where the bulk of work is computation and not data movement. The Terasort MapReduce benchmark is used for the data scalability tests since it has a greater dependence on the distribution of data than the SIFT algorithm. The Terasort MapReduce benchmark is distributed with the Hadoop codebase. The Yahoo implementation gained notoriety for breaking the terabyte sorting benchmark in 2009 for sorting 100 TB in 173 minutes[4]. Examples indicating saturation and steady-state condition are given in the following plots. Figure 1 SIFT-M phase plot. Figure 2 SIFT-M task rate plot. Figure 3 SIFT-M Ganglia CPU plot. Figure 1, Figure 2, and Figure 3 depict our SIFT-M job on a cluster with a total of 56×2=112 PEs, the maximum number of task slots. We see from Figure 1 the task count is maximized for the duration of the benchmark indicating the system is saturated and has reached steady-state. The task rate plotted in Figure 2 displays a constant arrival rate, a linear curve, expected for steady-state and is approximately 3 tasks per second. Further evidence of the CPU saturation is provided by the Ganglia hardware metrics in Figure 3. The next plots in Figure 4, Figure 5, and Figure 6 demonstrate an under-utilized cluster which supports 256 PEs. We can see the concurrency is not maintained in the map phase, the map task assignment never reaches a constant rate and declines rapidly. The reduce phase reaches steady-state but is far from saturating the available execution slots. Figure 4 Under-utilized Terasort phase plot. Figure 5 Under-utilized Terasort task rate plot. Figure 6 Under-utilized Terasort Ganglia CPU plot. An example of a Terasort job with better system utilization is depicted in Figure 7, Figure 8, and Figure 9. Note there are multiple shuffle, sort, and reduce phases. This can arise when there are more reduce tasks allocated than available slots so the reduce phases complete in “waves”. It is important to balance the concurrency between the map and reduce phases to achieve the best performance. Figure 7 Terasort phase plot. Figure 8 Terasort task rate plot. Figure 9 Terasort Ganglia CPU plot. The histogram plots for the SIFT-M job identify the variance in workload and performance per host, evident in Figure 10, Figure 11, and Figure 12. Clearly the “red” host is struggling with the tasks. Figure 10 SIFT-M histogram of task count. Figure 11 SIFT-M histogram of task duration. Figure 12 SIFT-M histogram of I/O in bytes. The final plots depict the scalability performance. The plot in Figure 13 compares the node scalability between all the clusters. All clusters scale linearly for the fixed-sized study as expected. Figure 14 displays the data scalability for the scaled-sized problem. We find that each cluster exhibits a decrease in throughput as the input scales correlating with a drop in CPU utilization which we attribute to greater latency in moving the data to the CPU. The dramatic drop in the early data points is likely due to the input fitting entirely in memory. Figure 13 Node scalability plot. Figure 14 Data scalability plot. We expect embarrassingly parallel Hadoop MapReduce applications will scale linearly and our Node Scalability results align very well with this expectation. The preliminary results of our Data Scalability study indicate performance degrades with increasing input as a result of less data locality and higher demand on disk and network resources. Since input is necessarily shared by all compute hosts contention on the same disks increases with input size even when different file blocks are requested. The imposed data dependency impedes an important advantage of MapReduce, the overlap of communication and computation. The MapReduce paradigm masks the latency from information requests by transferring map output to the reduce hosts during the map phase, known as shuffling, rather than waiting for all map tasks to complete. But the map tasks are also requesting data between hosts because of the cluster-wide data dependency, and so the shuffling phase contends for the very same network and storage resources. Although we used the default Hadoop FIFO scheduler, we suggest that selecting different job schedulers and increasing the replication factor, HDFS block size, and virtual memory page size in isolation or combination can improve performance. Developers may also need to create specialized Partition and Combiner classes to address data skew. A judicious choice in the key-space partitioning and the number of reduce tasks will have significant impact on the performance. Since both map and reduce phases overlap, care is needed not to over-subscribe the system during the map phase but avoid under-utilizing the resources in the reduce phase. A system with many smaller disks over few large disks is recommended in conjunction with high compute density. Acknowledgement I would like to thank Jonathan Jarrett for developing the scripts for the Gnuplot charts and automated benchmarks, and thank David Ritch and Adam Watts for administering our clusters. I also thank the rest of the SRA team for their support. [3] http://www.cloudera.com/resource/hw10_sifting_clouds [4] http://sortbenchmark.org</snippet></document><document id="576"><title>A profile of Apache Hadoop MapReduce computing efficiency</title><url>http://blog.cloudera.com/blog/2010/12/a-profile-of-hadoop-mapreduce-computing-efficiency-sra-paul-burkhardt/</url><snippet>Guest post from Paul Burkhardt, a Research Developer at SRA International, Inc. where he develops large-scale, distributed computing solutions. Part I We were asked by one of our customers to investigate Hadoop MapReduce for solving distributed computing problems. We were particularly interested in how effectively MapReduce applications utilize computing resources. Computing efficiency is important not only for speed-up and scale-out performance but also power consumption. Consider a hypothetical High-Performance Computing (HPC) system of 10,000 nodes running 50% idle at 50 watts per idle node, and assuming 10 cents per kilowatt hour. It would cost $219,000 per year to power just the idle-time. Keeping a large HPC system busy is difficult and requires huge datasets and efficient parallel algorithms. We wanted to analyze Hadoop applications to determine the computing efficiency and gain insight to tuning and optimization of these applications. We installed CDH3 onto a number of different clusters as part of our comparative study. The CDH3 was preferred over the standard Hadoop installation for the recent patches and the support offered by Cloudera. In the first part of this two-part article, we’ll more formally define computing efficiency as it relates to evaluating Hadoop MapReduce applications and describe the performance metrics we gathered for our assessment. The second part will describe our results and conclude with suggestions for improvements and hopefully will instigate further study in Hadoop MapReduce performance analysis. For distributed computing to be effective there must be sufficient input and parallelism to saturate the compute resources. Saturation in the context of Hadoop MapReduce refers to maintaining the maximum task rate for the duration of the application run. The maximum task rate is defined as the total number of task slots divided by the time to complete a single task. The maximum task rate is therefore the upper-bound on throughput. Our goal is to reach steady-state at peak throughput. A system in steady-state can be modeled using Little’s Law from Queuing Theory. The equation for Little’s Law defines the average throughput for any steady-state system and can be written as, (1) , This law is often used to calculate the concurrency in HPC systems. Our queue length, N, is the total number of task slots and the queue time, T, is the time it takes for a task to be assigned and completed. We can determine the average throughput, ?, by dividing the total task slots with the average task duration time. The ratio of the average throughput to the maximum task rate indicates the computing efficiency and how well we have saturated the resources. For example, a system of sixteen hosts with sixteen task slots per host, and given a workload of identical tasks that complete every minute, the maximum task rate for this system is 16×16=256 tasks per minute. At steady-state this rate is the throughput and so the system is 100% saturated. If the system is at steady-state but the throughput is less than the maximum task rate, it is under-utilized. Setting the number of concurrent tasks for map and reduce phases requires modifying the following parameters in the mapred-site.xml file on each host, mapred.tasktracker.map.tasks.maximum mapred.tasktracker.reduce.tasks.maximum We set the values for these parameters to the actual number of execution units on a host. For example, if a host has two quad-core CPUs capable of two simultaneous threads it will have a total of 2x4x2=16 PEs. The maximum task parameters should not be confused with the parameters that set the total number of map and reduce tasks for an entire job, mapred.map.tasks mapred.reduce.tasks Having established the condition for system saturation we devised two scalability studies to identify how well Hadoop MapReduce applications scale in fixed- and scaled-sized problems. We know by Amdahl’s Law, defined in the following equation, that only an algorithm that can be decomposed into mostly independent tasks, an embarrassingly parallel algorithm, can scale linearly for fixed-sized problems. (2)           The speedup, S, for a parallel algorithm depends on its parallel and serial proportions, denoted in the equation by fp and fs, and the number of compute nodes, n. The speedup is bounded by the serial fraction and unless algorithms can be expressed with a high degree of parallelism, the performance is rapidly limited regardless of the available compute resources. Despite this stringent requirement we can efficiently utilized algorithms that have maximized their parallelism by scaling the problem size (c.f. Gustafson’s Law). For fixed-sized problems, we scale the compute resources with constant input size to determine node scalability, the throughput versus the number of nodes. Here, nodes refer to hosts in the cluster which are comprised of potentially many processing elements (PE). Data scalability refers to our study of scaled-sized problems by plotting the throughput against increasing input size in a system with fixed compute resources. We use the Ganglia cluster monitoring tool[1] to measure hardware statistics and the Hadoop job history log for analyzing job performance. The job history file is stored in the job output directory in HDFS and in the Hadoop log directory. The following Hadoop job command will summarize the job history. hadoop job –history &lt;hdfs job output directory&gt; We developed a simple Perl script to parse the raw job history log into three datasets for plotting the execution profile, concurrency, scalability, and load balance. The time series data for the tasks per job phase comprises one dataset, and another dataset compiles the duration time and workload for each task per host. The final dataset is a summary of aggregated job metrics including the wall clock time and processed bytes; a complete listing is available in Table 1. METRIC UNITS Job start time since Unix Epoch milliseconds Job stop time since Unix Epoch milliseconds Input size bytes Total read bytes Total written bytes Total shuffled bytes Total spilled bytes Throughput bytes/second Total input splits Total map tasks Total reduce tasks Average map duration seconds Average reduce duration seconds Average shuffle duration seconds Average sort duration seconds Total map nodes Total reduce nodes Rack-local map tasks Data-local map tasks Table 1 Job Metrics. From these three datasets we are able to plot the following five functions: job phases task rates job histograms node scalability data scalability The job phase plots indicate the magnitude and duration of the map, reduce, shuffle, and sort phases. The phases are plotted as the instantaneous task count versus time and were inspired by the plots published by the Yahoo Terasort benchmark[2]. The plot data comes from the time series dataset generated by our job parser. A phase that saturates the available compute resources will be depicted by a plateau, the steady-state. The plot is a great indicator of both concurrency and the performance profile. The task rates are plotted as the cumulative task count versus task duration. A saturated system should display a linear slope where the rate of incoming tasks is constant and equal to the rate of completed tasks. Straggling tasks will appear as tracks. If constrained by resource availability the slope will decline. The task rates are plotted from our second dataset of task duration. The plots illustrate the throughput and concurrency, as well as identify individual task bottlenecks. The job histograms elucidate potential load-balancing issues by depicting the variance between the compute hosts for task assignment, task duration, and disk IO. A common issue in distributed systems is “hot-spotting” where one node is tasked much more than others or there is a location in memory or disk that is in high demand. This situation typically leads to bottlenecks. The histograms are also plotted from our second dataset which includes the relevant workload and duration time per task. The final two functions compare the node and data scalability across different clusters as a function of throughput. The throughput is the total input size in bytes divided by the wallclock time and a node refers to a host in the cluster. We will illustrate the performance profile of two candidate Hadoop MapReduce applications in Part II of this exposition and conclude with a discussion of our analysis. [1] http://ganglia.sourceforge.net [2] http://sortbenchmark.org/Yahoo2009.pdf</snippet></document><document id="577"><title>Cloudera and Pentaho team up to simplify data management and business intelligence</title><url>http://blog.cloudera.com/blog/2010/12/cloudera-and-pentaho-team-up-to-simplify-data-management-and-business-intelligence/</url><snippet>Webinar: Decmeber 9, 2010, 11am PT, 2pm ET Guest post by Thomas J. Wilson, president of Unisphere Research which produces custom research projects in conjunction with many of the leading data management and IT user groups, as well as with other industry communities including the subscribers of Database Trends and Applications magazine. We conduct a lot of research among data architects, database professionals, business intelligence specialists and development professionals for Database Trends and Applications. One thing is becoming clearer by the day: data proliferation is taking an enormous toll on IT budgets as well as IT staff time. These burdens are a large area of concern and stress for IT departments. Concurrently, business management is turning up the pressure on IT to deliver reports that analyze the data being captured and turn it into some kind of usable insight. Creating these reports is often done manually and can be very time consuming and therefore expensive. The problem is that resources are tight, and the demands on IT are growing in many of these data intensive organizations. In this kind of pressurized environment, and now at a time of increasing competitive weight, information management professionals are demanding new solutions to managing their large data stores coupled with integrate techniques that yield usable intelligence. Our second in a series of educational webcasts, “New Solutions for the Data Intensive Enterprise” looks at how the combination of Apache Hadoop and business intelligence can address the challenges related to data proliferation. Specifically we will show how Cloudera’s Distribution for Hadoop teamed up with Pentaho Business Intelligence can be a blueprint for simplifying data management and business intelligence in complex data environments. If you are looking to relieve the stress brought on by large-scale data demands, join us at this complimentary webcast on Thursday, December 9, 2010.</snippet></document><document id="578"><title>Lessons learned putting Hadoop into production</title><url>http://blog.cloudera.com/blog/2010/12/lessons-learned-putting-hadoop-into-production/</url><snippet>Webinar : December 8th, 10-11:00am PT, 1-2:00pm ET Presenter: Eric Sammer, Cloudera Solution Architect Many Apache Hadoop deployments begin as small test clusters as either an electronic sandbox for analyzing data in new ways or solving a small specific business problem. Typically, as more use cases are discovered more data is loaded into the cluster. Consequently, the clusters grow to provide expanded capacity to the organization. Typically one or more of the use cases provides insight that is critical to the efficient operation of business and eventually creates a need for a full scale production Hadoop system.  As the clusters grow and business becomes more dependent on the results, challenges begin to arise in many aspects of deployment from configuring and installing to monitoring and managing the daily operations of the cluster. Cloudera Solution Architect Eric Sammer has helped move dozens of Hadoop clusters into production, including some of the largest seen to date. In his upcoming webinar Eric will reveal some key insights and considerations developed from not only his experiences, but the entire Cloudera Solution Architect team. When bringing your Hadoop cluster to a production state some of the key items to keep in mind include: Proper planning Data ingestion ETL and data processing infrastructure Authentication, authorization, and sharing As well as monitoring During this one hour webinar titled “Production-izing Hadoop: Lessons Learned” Eric will cover each of the above bullet points in detail and will also leave ample time for questions at the end of the hour. Eric presented a condensed version of this webinar at Hadoop World in October where it was rated the top breakout session of the 42 sessions. If you weren’t able to attend here’s another opportunity to better understand the obstacles and considerations for bringing a Hadoop cluster to production. Learn more about Eric Sammer and the Cloudera team here.</snippet></document><document id="579"><title>Hadoop World 2010 Tweet Analysis</title><url>http://blog.cloudera.com/blog/2010/12/hadoop-world-2010-tweet-analysis/</url><snippet>Neil Kodner, an independent consultant, is the guest author of this post. Neil found inspiration, which spurred innovation at Hadoop World 2010 from a moments decision to capture the #hw2010 streaming Twitter feed. During the Hadoop World 2010 keynote, a majority of attendees were typing away on their laptops as Mike Olson and Tim O’Reilly dazzled the audience.  Many of these laptop-users appeared to be tweeting as the keynote was taking place.   Since I have more than a passing interest in twitter, Hadoop, and text mining, I thought it would be a great idea to track and store everyone’s Hadoop World tweets. A bit about myself: I’ve been using Hadoop for a little over a year, mostly for personal projects, on a 4-node cluster consisting of an iMac, a Mac Mini server, and two laptops – commodity hardware of the truest form.  On this cluster I have nearly 800 Million Tweets and I use a combination of Pig, Hive, and Python Streaming to find interesting things within the data.  Some results may be found on my site or posted to my twitter account. During the keynote, I quickly created an Amazon Micro EC2 instance, tapped into the Twitter Streaming API, and began downloading tweets containing the hashtag #hw2010. After filtering out a few Halloween tweets (get it?  #hw2010?), about 1,500 tweets remained, respectable for a one-day event.  Here are some key findings from the data: The most prolific tweeter(twitter-er?) was ‘disruptive engineer’ Mathias Herberts (@herberts) with an astonishing 214 tweets, 14% of all #hw2010 tweets.  Out of Mathias’ 214 tweets, 205 were his own tweets and 9 were retweets.  The next most prolific tweeters were Ryu Kobayshi (@ryu_kobayashi)) with 54, and Chris Shain (@chrisshain) with 39 tweets. The most-frequent tweeters, with a minimum of 10 tweets, were Yukio Uematsu (@alfyukio) whose MTBT(mean time between tweet) was 5.6 minutes over his 28 tweets.  Wayne Eckerson (@weckerson) was next with a tweet every 7 minutes followed by Chris Shain with 39 tweets, just under ten minutes apart. Mathias Herberts was the most retweeted user, having been retweeted 39 times.  Christoper Gillett (@ccgillett) was retweeted 28 times, and Cloudera’s own @cloudera account was retweeted 16 times. Christopher Gillett wrote the tweet with the greatest number of retweets(14).  His tweet about all Hadoop World sessions ending with “we’re” hiring also happens to be the tweet with the greatest longevity, having had its final retweet nearly 41 hours after it was originally posted.  Taking Christoper’s follower count plus the follower counts of all who retweeted his “we’re hiring” tweet, this message was potentially seen by over 15,000 people.  The tweet which was retweeted the second-most number of times was Chad Metcalf (@metcalfc), quoting Cloudera CEO Mike Olson’s “You no longer have to load the gun and hand it to Oracle.”, which was retweeted 9 times and was potentially seen in over 5,000 twitter streams. Ilya Grigorik’s (@igrigorik) tweet about StumbleUpon’s OpenTSDB was retweeted the quickest, with Eric Florenzano (@ericflo) retweeting it within 15 seconds of Ilya’s original post. Cloudera employees seemed to be spreading the Hadoop World excitement and took the top three positions on the retweets list.  Patrick Hunt (@phunt) retweeted 29 tweets, Cloudera Founder/CTO Amr Awadallah (@awadallah) retweeted 15, and Josh Patterson (@jpatanooga) sent 14 retweets. Many of the Hadoop World 2010 tweets made reference to other users.  The most frequently mentioned users were @cloudera with 58 mentions, Tim O’Reilly (@timoreilly) with 47, and Philip ‘Flip’ Kromer (@mrflip) with 32. The hashtags that were used along with #hw2010 were no surprise – #hadoop, #hbase, and #bigdata ranked respectively as the top three #hw2010 sibling hashtags.  Further down the list were #cloudera, #rstats, and #lovemyceo. ‘Hadoop’ itself was mentioned in just under a third of the tweets, ‘data’ was mentioned in nearly a quarter of the tweets, ‘Cloudera’ was mentioned in just over ten percent, and ‘Hadoop World’ came in at just under ten percent.  Other frequently-mentioned topics were ‘HBase’ with 159 mentions, ‘Twitter’ had 85 mentions, ‘Facebook’ with 65, ‘Flume’ with 41, ‘Hive’ with 39, and ‘analytics’ with 22. Moving on to the social graph of all who used the #hw2010 hashtag.  I distilled all of the tweets into a list of twitter user IDs.  With a huge assist by Tony Hirst (@psychemedia), who isn’t nearly as constrained by the twitter API rate limits as I am, a file was created showing all of the user to user relationships.  The file, which I promptly loaded into Gephi, represented a directed network containing 411 nodes and 4,664 edges.  The network’s diameter, the farthest distance between any two points, is 9 and the average path length, or average distance between any two points, was just below 3. Out of the 411 users, 168 of them follow Cloudera, 143 follow Tim OReilly, 137 follow Cloudera’s Jeff Hammerbacher (@hackingdata), 95 follow Twitter’s Kevin Weil (@kevinweil), and 89 follow Cloudera’s Mike Olson (@mikeolson). On the flipside, Jeff Hammerbacher follows the greatest number of #hw2010 tweeters, with 135, Ray George (@rgeorge28) follows 94, Cloudera’s Omer Trajman (@otrajman) follows 84, and Youngwoo Kim (@youngwookim) follows 76.  Interestingly enough, only 4 #hw2010 tweeters follow Youngwoo Kim back, I’m assuming that’s because Youngwoo probably reads English better than the rest of us read Korean! Jeff Hammerbacher was the overall most-connected user in terms of the social graph with an average distance to all other nodes of 1.784.  Omer Trajman and Ray George were next, both having an average distance of just over 2. Finally, in terms of importance within the network itself, Tim O’Reilly led in PageRank, the probability of arriving at a given node, followed by Cloudera, Jeff Hammerbacher, and Kevin Weil. Based on my findings with the Hadoop World twitter stream, the event was well-received by its attendees and generated much publicity for not only Hadoop, but also for Cloudera.  Based purely on the number of twitter mentions, the two most engaging sessions were the keynote featuring Mike Olson and Tim O’Reilly followed by Flip Kromer’s breakout session, “Millionfold Mashups”. Finally, my key takeway from Hadoop World 2010 was the appreciation for how essential HBase is to the hadoop ecosystem.  HBase was mentioned in more than one out of every ten tweets and I can now see why!</snippet></document><document id="580"><title>Hadoop Log Location and Retention</title><url>http://blog.cloudera.com/blog/2010/11/hadoop-log-location-and-retention/</url><snippet>This post is a follow up to an earlier one called Apache Hadoop Log Files: Where to find them in CDH, and what info they contain. It lists nicely the various places Hadoop uses to store log and other info files while it is running. Over time we have seen changes though in different Hadoop and CDH releases that affect where files are stored or how long they are retained. Below strives to document the status quo (no, not the band) of all the log and info files found while running a Hadoop cluster. First some general notes: the logs are stored in a few different places and retained for a certain amount of time (or forever). Below are the categories and the various settings that control them. All of the parameters named go into the mapred-site.xml unless noted otherwise. If necessary the following codes are used to identify changes in various releases (ordered by Hadoop version): H20:        Hadoop 0.20.2
CDH3B1:     CDH3b1 (Hadoop-0.20.2+228)
CDH3B2:     CDH3b2 (Hadoop-0.20.2+320)
CDH3B3:     CDH3b3 (Hadoop-0.20.2+737)
H21:        Hadoop 0.21.0 If not noted otherwise the description is for vanilla Hadoop 0.20.2 (H20) or in case of configuration keys it is for all releases but the ones named specifically. Note: CDH2 (Hadoop-0.20.1-169.89) works like CDH3b1 and CDH3b2, so all notes applying to those two versions also apply to CDH2. Daemon Logs The actual Java daemons use log4j and DailyRollingFileAppender, which does not have retention settings (other Appenders have though, for example the RollingFileAppender). The only way to keep them in check is either change the log4j.properties to a different Appender or use a cron job to remove older logs daily (e.g. “find /var/log/hadoop/ -type f -mtime +3 -name "hadoop-hadoop-*" -delete“). Job Files There are two sets of files per job, which are created when the job is launched. One is the Job Configuration XML file and the other the Job Status File. The Job Configuration XML files contain the job configuration as specified when the job is launched, i.e. they contain the merged defaults with the user settings for the job that override the defaults. Each job generates a second file, next to the Job Configuration XML file, called Job Status File. It is created when the job starts but continually written to until the job finishes. They contain what is also visible after a job has finished, i.e. the counters, status, start/stop time, task attempt details etc. The two files are stored or kept in four locations (usually they are stored as pairs unless noted otherwise): 1) JobTracker Local When a job is initialized the configuration is saved in what is called a “local directory”: Location: ${hadoop.log.dir}
Name: "&lt;jobId&gt;_conf.xml"
Retention: mapred.jobtracker.retirejob.interval  (see Job Retirement Policy below) Notes: - Only the Job Configuration XML is stored here, not the Job Status File. - There seems to be a “bug” in the code that the check is not forced when the JobTracker is shut down and when it restarts it gets a new ID assigned (the current date) and will then only check for matching jobs to clean up subsequently. What that means is that when you restart your JobTracker and you had Job Configuration XML files in the “local” directory that they will never be cleaned up. They can safely be removed and are rather small. Another simple cronjob could be used for this purpose, executing something like: find /var/log/hadoop/ -type f -mtime +3 -name "job_*_conf.xml" -delete 2) In the Job History Key: hadoop.job.history.location
Key (H21): mapreduce.jobtracker.jobhistory.location
Default: file:///${hadoop.log.dir}/history
Retention: up to a max. of 30 days  (see General Job File Notes below) Note: The value for this key is treated as a URI, in other words you can store the job files in HDFS or on the local file system (which is the default). 3) Per Job Key: hadoop.job.history.user.location
Key (H21): mapreduce.job.userhistorylocation
Default: &lt;job_outputdir&gt;/_logs/history  (can be "none" to disable)
Retention: forever You can print the info contained in those files using the hadoop command line script like so: hadoop job -history &lt;job_outputdir&gt; This implies that the above command expects the path to be on HDFS. In other words you cannot use it to display the other job files stored on the local disk for example. 4) In Memory In addition to the on-disk storage of these files jobs are also retained in memory (for job restarts). But they are kept in memory only for a limited time. This is controlled by: Key: mapred.jobtracker.completeuserjobs.maximum
Default: 100 The check if the in-memory list is too large is only run when a job completes, so on an idle cluster the jobs remain in memory until the next rule possibly kicks in (see Job Retirement Policy below). Job Retirement Policy (pre-H21 only!) Once a job is complete it is kept in memory (up to mapred.jobtracker.completeuserjobs.maximum) and on disk as per the above. There is a configuration value that controls the overall retirement policy of completed jobs: Key: mapred.jobtracker.retirejob.interval
Default: 24 * 60 * 60 * 1000  (1 day) In other words, completed jobs are retired after one day by default. The check for jobs to be retired is done by default every minute and can be controlled with: Key: mapred.jobtracker.retirejob.check
Default: 60 * 1000 (60s in msecs) The check runs continually while the JobTracker is running. If a job is retired it is simply removed from the in-memory list of the JobTracker (it also removes all Tasks for the job etc.). Jobs are not retired under at least 1 minute (hardcoded in JobTracker.java) of their finish time. The retire call also removes the JobTracker Local (see above) file for the job. All that is left are the two files per retired job in the history directory (hadoop.job.history.location) plus – if enabled – the Per Job files (hadoop.job.history.user.location). General Job File Notes: - When the JobTracker cannot create the job history directory at startup (in the JobTracker constructor, calling JobHistory.init()) or creating the files for a job fails (see JobHistory.JobInfo.logSubmitted() for details, it is invoked during the initialization of a task attempt) every subsequent job is not storing any details on disk until the problem is fixed and the JobTracker restarted (MAPREDUCE-1699 is meant to fix this but is not yet applied to any Hadoop 0.20.x based release). - There is an option to control the low-level block size used by Hadoop to write the file to disk: Key: mapred.jobtracker.job.history.block.size
Key (H21): mapreduce.jobtracker.jobhistory.block.size
Default: 3 * 1024 * 1024 (3MB) Setting it lower has the following advantage (taken verbatim from mapred-default.xml): “Since the job recovery uses job history, its important to dump job history to disk as soon as possible.” - The Job History page (jobhistory.jsp) in the JobTracker UI always reads the stored job history from hadoop.job.history.location (see above) and not from memory (that is what the default JobTracker page does). - Finally both of the above, the Job Configuration XML (with the exception of what is noted under “JobTracker Local”) and Job Status File get eventually deleted after 30 days (see H21 related notes below). This cannot be changed (hardcoded in JobHistory.HistoryCleaner.java) and the check for this is run once per day. Also note that it deletes any file older than the threshold unconditionally, i.e. do not leave files there you need or they will be deleted. CDH3b1, CDH3b2, CDH3b3, Hadoop 0.21.0 (H21) and up: There is now an additional directory which is used to move completed jobs into (MAPREDUCE-814). As opposed to leave their files all in hadoop.job.history.location directly all files of completed jobs are moved to this new location. This way you can distinguish between running and completed jobs when looking at the history directory. Here is how the path for the completed jobs is specified: Key: mapred.job.tracker.history.completed.location
Default: ${hadoop.job.history.location}/done
Key (H21): mapreduce.jobtracker.jobhistory.completed.location
Default (H21): ${mapreduce.jobtracker.jobhistory.location}/done CDH3b3, Hadoop 0.21.0 (H21) and up: These releases also add the notion of a “retired jobs cache” (MAPREDUCE-817). This is solely kept and handled in memory and is controlled with: Key: mapred.job.tracker.retiredjobs.cache.size
Key (H21): mapreduce.jobtracker.retiredjobs.cache.size
Default: 1000 As far as physical log files are concerned this is negligible. There is a hardcoded maximum of 100 jobs being displayed in the main JobTracker UI page (jobtracker.jsp). Those are pulled from the above internal memory cache. If the user wants to see all completed jobs they have to use the Job History page (jobhistory.jsp) which loads the job details from the file system (see above under General Job File Notes). But note though that for CDH3b1 and up this is loaded from mapred.job.tracker.history.completed.location instead. Hadoop 0.21.0 (H21): In Hadoop 0.21.0 the whole cleanup code has been completely refactored. For starters, the above hardcoded 30 days clean up of the job history is now configurable and has been reduced to one week (in JobHistory.HistoryCleaner.java): Key: mapreduce.jobtracker.jobhistory.maxage
Default: 7 * 24 * 60 * 60 * 1000L (one week) In addition the frequency of the check to remove the job files has been made variable (but not user configurable) in that it is either 1 day or less. The latter is the case when mapreduce.jobtracker.jobhistory.maxage is set to something less than a day. With Hadoop 0.21.0 you may also find files with the extension “.old” in the mapreduce.jobtracker.jobhistory.completed.location directory. Those are the left over job files from a previous instance of JobTracker. So when a JobTracker is restarted it moves and renames the files it finds in mapreduce.jobtracker.jobhistory.location into the mapreduce.jobtracker.jobhistory.completed.location directory. Note that when jobs are removed after the above mapreduce.jobtracker.jobhistory.maxage they will also be removed (and in fact as noted above, any file in that directory older than the maximum age will be deleted). Also note that the “Job History” UI page does include those “.old” job details. A big change is the JobTracker.RetireJobs class which before Hadoop 0.21.0 was a Thread instance that ran according to mapred.jobtracker.retirejob.interval and mapred.jobtracker.retirejob.check. This has been changed to a simple method that is called at the end of each job and is run synchronous from the main thread. That also means that completed jobs are not removed from memory after the usual 24 hours but stay there solely based on the size of the new retiredjobs.cache.size (defaults to 1000, see above). Older jobs are removed from that cache when a newly completed job adds itself. Finally, the various keys have been renamed to match each other (see “H21″ throughout this document). Job Status Store There is an additional place to store job details, which always assumes HDFS to be the storage. It is called Job Status Store (HADOOP-1876) and is used by JobClients to query details about jobs. By default it is disabled and for persistent job status storage in HDFS the following options need to be adjusted: Key: mapred.job.tracker.persist.jobstatus.active
Default: false  (disabled)
Key (H21): mapreduce.jobtracker.persist.jobstatus.active
Default (H21): true Key: mapred.job.tracker.persist.jobstatus.hours
Default: 0  (disabled)
Key (H21): mapreduce.jobtracker.persist.jobstatus.hours
Default (H21): 1 Key: mapred.job.tracker.persist.jobstatus.dir
Key (H21): mapreduce.jobtracker.persist.jobstatus.dir
Default: /jobtracker/jobsInfo  (always on HDFS) The status files are stored directly under the above job status directory and are created when the job finishes. Here their location, file name pattern and retention: Location: mapred.job.tracker.persist.jobstatus.dir  (see above)
Location (H21): mapreduce.jobtracker.persist.jobstatus.dir  (see above)
Name: "&lt;jobId&gt;.info"
Retention: mapred.job.tracker.persist.jobstatus.hours  (see above)
Retention (H21): mapred.job.tracker.persist.jobstatus.hours  (see above) The check for the above jobstatus.hours is done every hour (hardcoded in CompletedJobStatusStore.java) and runs whenever the JobTracker is running. Task Attempt Logs These include the Standard Out and Standard Error logs as well as custom user logs created by your code using the logging framework within your MapReduce jobs. The location is hardcoded to be ${hadoop.log.dir}/userlogs. The following controls how large the logs can become. Please note that it has to maintain a linked list in memory until the end of the job to only write the last “limit / event_size” (event_size is set to 100) events. So this needs to be accounted for in terms of the per task memory consumption. If you leave it at the default “0” then it is disabled and logs straight to disk. Key: mapred.userlog.limit.kb
Key (H21): mapreduce.task.userlog.limit.kb
Default: 0  (no limit) By default the userlogs are deleted every day. It is triggered by the TaskTracker when a child process is launched (in other words, when the cluster is idle, then nothing is cleaned up). It can be changed with: Key: mapred.userlog.retain.hours
Key (H21): mapreduce.job.userlog.retain.hours
Default: 24 (hours) An note about the log4j.properties. You will find these lines: #Default values
hadoop.tasklog.taskid=null
hadoop.tasklog.noKeepSplits=4
hadoop.tasklog.totalLogFileSize=100
hadoop.tasklog.purgeLogSplits=true
hadoop.tasklog.logsRetainHours=12 log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
log4j.appender.TLA.taskId=${hadoop.tasklog.taskid}
log4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize} log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
... The first few lines define default values that can be overridden by system properties (refer to “-D...” parameters). The only lines really handled by the code (see TaskRunner.java) are hadoop.tasklog.taskid=null
hadoop.tasklog.totalLogFileSize=100 The former is always set in the TaskRunner.java code to the current task ID and the latter to whatever is defined in the mapred-site.xml (or specified at the JobTracker startup on the command line using “-D...“). In other words the above totalLogFileSize=100 is never used. The other lines are obsolete and are either controlled by the above configuration keys (mapred.userlog.retain.hours) or not used at all (and they should be removed).</snippet></document><document id="581"><title>Apache Hadoop training coming to new cities in 2011</title><url>http://blog.cloudera.com/blog/2010/11/apache-hadoop-training-coming-to-new-cities-in-2011/</url><snippet>Due to expanding interest and demand for Apache Hadoop knowledge and skills across the mid-west and western states Cloudera is bringing Hadoop training to several new cities including St. Louis, Los Angeles, Seattle, and Dallas starting in January 2011. �In order to meet the demand for Hadoop developers and operators we will be offering both our Hadoop Training for System Administrators and Hadoop Training for Developers classes in all of these cities. Hadoop Training for System Administrators is a two-day intensive course on Hadoop for operations staff. The course describes Hadoop�s architecture, provides hardware and network recommendations, covers management and monitoring tools, and provides valuable advice on setting up, maintaining, and troubleshooting Hadoop for development and production systems. This course includes the certification exam to become a Cloudera Certified Hadoop Administrator. Hadoop Training for Developers is a three-day course for developers who want to learn how to use Hadoop to build powerful data processing applications. This course assumes only a casual understanding of Hadoop and teaches you everything you need to know to take advantage of some of the most powerful features. This course includes ample time for hands-on exercises, including writing Java MapReduce code, importing data from existing sources, working with Apache Hive and Apache Pig, implementing common algorithms, and much more. The course also includes the certification exam to become a Cloudera Certified Hadoop Developer. Here are the links for the new training courses: St. Louis � Hadoop Training for Developers, Jan 24 � Jan 26 St. Louis � Hadoop Training for System Administrators, Jan 27 � Jan 28 Los Angeles � Hadoop Training for Developers, Jan 31 � Feb 2 Los Angeles � Hadoop Training for System Administrators, Feb 3 � Feb 4 Seattle � Hadoop Training for Developers, Mar 8 – Mar 9 Seattle � Hadoop Training for System Administrators, Mar 10 � Mar 11 Dallas � Hadoop Training for Developers, Mar 14 � Mar 16 Dallas � Hadoop Training for System Administrators, Mar 17 � Mar 18 We’re adding new locations all the time – see the full schedule</snippet></document><document id="582"><title>Do the Schimmy: Efficient Large-Scale Graph Analysis with Apache Hadoop, Part 2</title><url>http://blog.cloudera.com/blog/2010/11/do-the-schimmy-efficient-large-scale-graph-analysis-with-hadoop-part-2/</url><snippet>Continued Guest Post from Michael Schatz and Jimmy Lin Part 2: Efficient Graph Analysis in Hadoop with Schimmy In part 1, we looked at how extremely large graphs can be represented and analyzed in Apache Hadoop/MapReduce. Here in part 2 we will examine this design in more depth to identify inefficiencies, and present some simple solutions that can be applied to many Hadoop/MapReduce graph algorithms. The speedup using these techniques is substantial: as a prototypical example, we were able to reduce the running time of PageRank on a webgraph with 50.2 million vertices and 1.4 billion edges by as much as 69% on a small 20-core Hadoop cluster at the University of Maryland (full details available here). We expect that similar levels of improvement will carry over to many of the other problems we discussed before (the Kevin Bacon game, and DNA sequence assembly in particular). As explained in part 1, when processing graphs in MapReduce, the mapper emits �messages� from vertices to their neighbors, and the shuffle phase effectively routes each message to the proper destination vertex. For computationally trivial problems such as PageRank, the running time is dominated by shuffling data across the network, so anything we can do to reduce that data will also decrease the running time. Fortunately, in many cases we can reduce the data volume by combining multiple messages destined for the same vertex into a single message using a combiner � remember, a combiner is a �mini-reducer� that executes a function on a subset of the values with the same key. Because the subset may be unstable from run to run, combiners can only be safely used if the function is associative and commutative � that is, when the order that values are processed doesn�t change the result. For computations like PageRank, where the reducer merely sums the values received from neighboring vertices, we can safely sum values in any order, including summing values in a combiner before the messages are even sent across the network. If there are many messages destined for the same vertex emitted from mappers on a single machine, the combiner can replace all those messages with just one, and save substantial network communication costs (without changing the final result). Using combiners is a standard best practice for algorithms in MapReduce (graphs and otherwise), and in our experiments it improved the running time of PageRank by 18%. As a novel twist, we also evaluated a technique called �in-mapper combiners� instead of generic combiners. The idea is instead of having the mapper immediately emit messages, it first buffers messages inside a mapper-local hash table stored in memory. If we get lucky, the mapper will create many messages destined to the same vertices right after each other, and we can immediately update the combined values in memory, without having to pay the costs to serialize, write, sort, read, and deserialize the individual messages from disk. Although, we have to periodically flush the in-mapper table so we don�t overflow the memory available on the machine. You can see where the name of this technique comes from: in essence, we are doing the combining right inside the mapper! In our experiments, using in-mapper combiners further improved performance by another 16% beyond the generic combiner. If you have a commutative and associative computation, combiners will almost always help the performance of your algorithm. The magnitude of the improvement (18% vs. 5% vs. 34%), though, will very much depend on your cluster environment and the distribution of the graph in that environment. In the worst case, none of the vertices stored on a single machine share a common neighbor, so the combiner (or in-mapper combiner) won�t help at all. What we would like is all neighboring vertices (or all tightly connected vertices) are stored on the same machine so that all their messages can be efficiently combined. Unfortunately, optimally distributing the graph like this is a hard clustering problem in itself, but in some cases we can use simple heuristics that are very effective. The default method for assigning vertices to machines (both when the graph is stored in HDFS and during a MapReduce job) is a more-or-less random function called the HashPartitioner, which computes the hash value of the vertex id (key) to partition the graph. �As a random function, this approach works well to balance the number of vertices processed and stored on each machine. Furthermore, if the graph is roughly uniformly connected, then there is a good chance that some neighboring vertices will be processed together, which will then benefit from the combiner. However, many real world networks have higher-order structure that this approach won�t capture. For example, the Web is organized into domains, and webpages in the same domain generally have many more links to each other than to other remote corners of the Web. The default HashPartitioner is totally blind to this structure, which means the combiner will lose opportunities to aggregate partial results. We could exploit some of the network structure if we could only assign graph vertices to machines in blocks by domain instead of randomly. Surprisingly, this level of control is relatively easy to implement: instead of referencing vertices by some arbitrary vertex id, we instead use consecutive numbers derived from some attribute of the vertices. For example, we could preprocess the webgraph and alphabetically sort all the URLs. We then renumber vertices based on the sort order (i.e. http://aaa.com/index.html is id 1, http://aaa.com/welcome.html is id 2, http://abc.com/index.com id 3, etc). By virtue of the sorting, pages from the same domain will form blocks of consecutive ids. The only remaining challenge is to partition the vertices using ranges instead of hash values. This can be achieved using a variant of the RangePartitioner used for the Terasort challenge, where Hadoop/MapReduce is used to globally sort a list of values by partitioning the space of keys. Conceptually, webpages 1-100,000 are processed together, webpages 100,001 � 200,000 are processed together, and so forth (the actual size of the blocks will vary). This way, each web domain will be assigned to a single or perhaps a few machines and thus the combiner will be much more effective. If the range boundaries don�t exactly coincide with the domain boundaries, it causes a small bit of missed locality, but overall using a RangePartitioner leads to a huge improvement in performance over the HashPartitioner: in our experiments, a 40% improvement in running time of PageRank on our webgraph just by renumbering and repartitioning the graph! The final inefficiency we explored was if there was a way to more effectively distribute the graph structure within an iterative MapReduce graph algorithm. Remember in the standard design, the mapper emits messages for neighboring vertices, and also reemits the graph vertices themselves so that they will be shuffled together to the same reducer. This ensures the graph structure is available in the right reducer, but is not efficient. For example, if we store many attributes in the graph vertices (links to neighbors, text of the webpage, embedded images, date of collection, etc), and only distribute messages to a small number of neighbors (perhaps just from vertices with a certain keyword), the vast majority of data emitted and shuffled will be the graph structure with hardly any computationally meaningful data exchanged. This spring, we asked ourselves if it was truly necessary to do so, especially when the graph structure does not change at all between iterations in PageRank. As a result of this discussion, we came up with a new technique called Schimmy (hint: this is how Schatz + Jimmy think about graphs) that separates the messages (mutable-data flow) from the graph structure (immutable-data flow). The two key observations that make it work are 1) the partitioning of vertices (assignment of vertices to machines) is stable across MapReduce iterations so that conceptually once a vertex is assigned to machine X, it is always processed on machine X, and 2)� it is possible to merge sort intermediate key-value pairs with �side data� in the reducer. By default, MapReduce only sorts key-value pairs emitted by the mappers, but if we are willing to go a little outside the standard APIs, we can merge together messages and vertices from separate sorted data streams. In Schimmy, the mapper iterates over the input vertices more-or-less as before but only emits messages and not the vertex tuples (i.e., graph structure). The messages are then combined and shuffled as before to route them to the proper destination machines. Then, at the last possible moment, the reducer code merges together the shuffled messages with the vertices (the same files that the mappers processed). The reducer then computes the new values for each vertex, and stores the final updated graph as before. If you�re a database guru, you might describe this as a reduce-side parallel merge join between vertices and messages. In essence, Schimmy short-circuits reshuffling the vertices because the vertices get shuffled the same way in every iteration. This slight change saves substantial time, between 10% and 20% of each iteration in our prototypical analysis of PageRank, leading to an overall saving of 69% when used in conjunction with RangePartitioning and in-mapper combining over the previous best practice of using a combiner. In some ways, though, PageRank is the worst case for Schimmy in that PageRank sends messages from every vertex to every neighbor in every iteration. In less extreme algorithms (such as the first round of the Kevin Bacon game), where only a small fraction of vertices send messages to their neighbors, Schimmy should be even more effective. In conclusion, Schimmy and in-mapper combiners make graph algorithms faster because they reduce the total amount of computation and the total volume of data to exchange. In-mapper combining is also effective because RAM is orders of magnitude faster than disk and because it can then completely skip the work of serializing &amp; deserializing intermediate key-value pairs. RangePartitioning is effective because it can drastically improve locality, leading to more opportunities for local aggregation (and hence less data to shuffle across the network). These ideas are widely applicable (and perhaps obvious in retrospect), but are often overlooked in distributed environments where the initial temptation may be to simply throw more machines at the problem. While a brute force approach makes sense for exploratory data analysis, once we know what we need, it pays to refine the algorithm by exploiting locality and reducing data volumes, especially at the slowest levels of the storage hierarchy. This is simply good computer science! We hope you have found these articles interesting! For a more in depth discussion of these techniques, please see the Schimmy paper referenced above and the Schimmy reference implementation in Cloud9, which is a library developed at the University of Maryland designed to serve as both a teaching tool and to support research in data-intensive text processing. Note, for clarity we have glossed over the details of how tasks and file splits are scheduled on worker machines. In particular, file splits are arbitrarily scheduled on different machines in different iterations (the assignment of tasks is unstable), but reduce tasks can peek inside the splits to simulate a stable scheduling. To all the Hadoop hackers out there � we would love to work with you to develop a stable task scheduler that could be used to further cut unnecessary network traffic and improve performance. There are some significant technical challenges to make this approach work, especially to do so while ensuring reliability, but such an addition would enable some advanced techniques that are currently not possible. Ideally, this would lead to the development of a premier open-source large-scale graph analysis system, built directly on top of Hadoop MapReduce. Perhaps it will eventually prove superior to Google�s latest closed-source graph processing system Pregel. �</snippet></document><document id="583"><title>Hadoop and HBase at RIPE NCC</title><url>http://blog.cloudera.com/blog/2010/11/hadoop-and-hbase-at-ripe-ncc/</url><snippet>This post was contributed by Friso van Vollenhoven from Xebia. Friso is a consultant at Xebia and he is currently working at the RIPE NCC’s Information Services department on migrating an existing MySQL based solution to use Hadoop and HBase. Xebia performs large scale development projects, consultancy in architecture &amp; auditing and helps to bring your middleware under control. Guiding clients in how to leverage agile is Xebia’s passion. The RIPE NCC is one of five Regional Internet Registries (RIRs) providing Internet resource allocations, registration services and co-ordination activities that support the operation of the Internet globally. The RIPE NCC also provides services for the benefit of the Internet community at large. Amongst these is the Routing Information Service, which collects and stores Internet routing data from several locations around the globe. At RIPE NCC, we are in the progress of migrating an existing MySQL based system to use Apache HBase as storage backend and Hadoop MapReduce as framework for processing import jobs. In this post we will provide some background on our efforts, how we implemented it on top of Apache Hadoop and HBase and our experiences with using Hadoop and HBase in a real life scenario. The global internet is a large and distributed system. One of the reasons it works is that it has a shared-nothing architecture at the core of its operation. Different parts of the network are autonomous systems which achieve their goal of routing traffic to destinations in other parts of the network by communicating with their neighboring networks, without obtaining global knowledge of the network beyond their peers. Providing a global view of the current state of operational details of such a system is very hard to do and requires a lot of data. At the RIPE NCC’s Information Services department we aim to provide the closest thing to that by collecting substantial amounts of data on routing that goes on across the globe. Such a view can be useful for several reason — for example, when things go a little bit wrong on the internet. Like when Pakistan Telecom accidentally hijacked all of Youtube’s traffic, or when Google lost a substantial amount of its traffic to a provider in Romania. The autonomous systems of the internet communicate via a protocol called BGP to exchange information on routing. Our system works by listening in on BGP announcements in a number different places on the internet, usually close to or at large internet exchanges. We currently collect about 80 million BGP announcements per day (about 925 per second). This data collection has been going on for almost ten years, so we have a substantial collection of data. The raw uncompressed source data for this is several gigabytes per day and the total history is terabytes. We currently operate a MySQL based system in production to hold this data and allow querying. The system has a number of limitations: It will only hold three months worth of data. Anything older than that is currently thrown out and is only available as raw source data (.tar.gz files), but not searchable. So investigating any event less recent than three months is a very mundane task. Insertion speed is an issue. Insertion into relational databases can become slow due to index updates and integrity checks. Currently inserting data can be done only slightly faster than it arrives, which means that when the insertion process lags behind for some reason, it can take weeks to catch up again. The internet is still growing rapidly, while this system is already at its maximum capacity, so it is not future proof. For the same reason, we cannot add data collection points. Because of these limitations, we chose to re-implement the storage back end. Given the nature of the data we opted for a column based store. Initially, we investigated both HBase and Cassandra. The decision for HBase was made for a number of reasons, including: It has very good sequential read/scan performance, which is nice for full data analysis. At the time, there already was decent documentation and enough online resources available to get up and running. It is a proven setup in a number of real life scenarios. It integrates naturally with Hadoop and thus MapReduce, which is nice to have out of the box when working with large amounts of data. Above is a high level overview of our setup. Once source data comes in, we move it to HDFS and do all processing there. From the source data, we create several derived data sets, which we store in a intermediate form on HDFS and insert into HBase. We need the derived data sets in order to quickly answer specific queries. This is a big difference from the MySQL based solution. In the relational world, you typically normalize your data, spread it over a number of tables and apply joins when you need an answer to a specific question. In HBase this does not work. HBase will effectively do not much more than a key lookup for a certain value or range of values. With this model you really need to know in advance what kind of questions you need to answer and model the data storage accordingly. We keep several data sets with denormalized answers to a number of specific questions. You can see it as tables of pre-computed joins for all possible join combinations. Also, all your data in HBase has only one index. This means that for each additional index we add, we need to effectively duplicate the (denormalized) data. Working with MapReduce has proven to be a very nice way of handling data. We try to do as much as possible in MapReduce jobs. This brings the benefit of fault tolerance and parallelization without having to think about it. We just assign more capacity to jobs as the dataset gets larger. For example a job that does an initial import of months or years of historical data is assigned more map and reduce slots than the import that runs every five minutes to keep up-to-date. Apart from the capacity planning, they are exactly the same piece of code, which is nice. Insertion into HBase is also modeled as a MapReduce job to achieve high parallelization. We have not moved to production with the new setup yet, but some conclusions from our experience so far are: The running time of an import of one full year of data went from months to days. The solution scales well with the amount of data. There is no difference between having a couple of months or a couple of years of data online. Query performance is competitive with most RDBMS’ (especially compared to a sharded setup). Querying HBase for typical queries gives sub-second results. Results for large queries come in streaming at well over 15 MB/s. You don’t necessarily need to process terabytes of data for MapReduce to be effective. It’s about the ability to scale in accordance with the size of a job. It works for smaller jobs too and it will provide some fault tolerance in the process. The above results basically mean that HBase delivered on its promise. Our four node development setup will happily do up to 300,000 operations per second. It solves an important problem for us which we could not do with the relational back end. Using a non-relational storage system for live queries against large data sets requires a lot of up-front thinking, but the bottom line is that the relational storage just would not scale to the point that we need it to. Our production setup will be slightly larger than the development environment. The cluster will have 8 worker nodes, each running a data node, task tracker and region server. There are two master nodes in a Linux HA setup which run the name node, job tracker and master server. Because we find that we are completely IO bound in our jobs on the development cluster, we choose to scale out the IO; each data node has 10 data disks of 600GB (10K RPM). The workers have 64 GB RAM each. A lot of it goes to HBase. This will basically mean that the hotspot in our data set will mostly be in RAM, which helps off load the disks. Having all our data on HDFS and in HBase provides a lot of opportunities to do things that were hard or impossible before. Creating a job that goes over all historical data and extracts statistical information, detects anomalies or produces data for interesting graphs or trends has all become a lot easier. We are already looking into things in that direction. Our setup uses CDH3 beta. In development we install from tar archives. In production we will use the provided RPMs. Our experience with Hadoop and HBase has been generally good. It mostly works as advertised. Here are a number of lessons learned: Getting a basic setup up and running is relatively straightforward. Tuning and troubleshooting is the hard part. Distributed debugging is hard and, above all, time consuming. Make sure you know where all your logs go (before you need them). When you know what information you need, getting it should not be an obstacle. Tuning Hadoop jobs and HBase is non-trivial (just like properly tuning a RDBMS for high performance). Prepare to spend time investigating the internals of the framework and HBase, or get help. The work described above is just an initial effort. Probably there will be ongoing adoption of Hadoop and HBase for importing and analyzing some of the other data sources that we harvest apart from routing information. Also, the production cluster environment will more become a first class citizen of the IT infrastructure of RIPE NCC for holding, querying and processing data. I’d like to thank the guys who have all contributed to the work described here, including Age Mooij, Erik Romijn and Szabolcs Andrasi and Erik Rozendaal who has initiated the use of Hadoop and HBase at RIPE NCC.</snippet></document><document id="584"><title>Do the Schimmy: Efficient Large-Scale Graph Analysis with Hadoop</title><url>http://blog.cloudera.com/blog/2010/11/do-the-schimmy-efficient-large-scale-graph-analysis-with-hadoop/</url><snippet>Guest Post by Michael Schatz and Jimmy Lin Michael Schatz is an assistant professor in the Simons Center for Quantitative Biology at Cold Spring Harbor Laboratory. His research interests are in developing large-scale DNA sequence analysis methods to search for DNA sequence variations related to autism, cancer, and other human diseases, and also to assemble the genomes of new organisms. Given the recent tremendous advances of DNA sequencing technologies, Michael has pioneered the use of Hadoop and cloud computing for accelerating genomics, as described in a guest blog post last fall. Jimmy Lin is an associate professor in the College of Information Studies at the University of Maryland. His research lies at the intersection of information retrieval and natural language processing, with an emphasis on large-scale distributed algorithms. Currently, Jimmy is spending his sabbatical at Twitter. Part 1: Graphs and Hadoop Question: What do PageRank, the Kevin Bacon game, and DNA sequencing all have in common? As you might know, PageRank is one of the many features Google uses for computing the importance of a webpage based on the other pages that link to it. The intuition is that pages linked from many important pages are themselves important. In the Kevin Bacon game, we try to find the shortest path from Kevin Bacon to your favorite movie star based on who they were costars with. For example, there is a 2 hop path from Kevin Bacon to Jason Lee: Kevin Bacon starred in A Few Good Men with Tom Cruise, whom also starred in Vanilla Star with Jason Lee. In the case of DNA sequencing, we compute the full genome sequence of a person (~3 billion nucleotides) from many short DNA fragments (~100 nucleotides) by constructing and searching the genome assembly graph. The assembly graph connects fragments with the same or similar sequences, and thus long paths of a particular form can spell out entire genomes. The common aspect for these and countless other important problems, including those in defense &amp; intelligence, recommendation systems &amp; machine learning, social networking analysis, and business intelligence, is the need to analyze enormous graphs: the Web consists of trillions of interconnected pages, IMDB has millions of movies and movie stars, and sequencing a single human genome requires searching for paths between billions of short DNA fragments. At this scale, searching or analyzing a graph on a single machine would be time-consuming at best and totally impossible at worst, especially when the graph cannot possibly be stored in memory on a single computer. Fortunately, Hadoop and MapReduce can enable us to tackle the largest graphs around by scaling up many graph algorithms to run on entire clusters of commodity machines. The idea of using MapReduce for large-scale graph analysis is as old as MapReduce itself – PageRank was one of the original applications for which Google developed MapReduce. Formally, graphs are comprised of vertices (also called nodes) and edges (also called links). Edges may be “directed” (e.g., hyperlinks on Web) or “undirected” (e.g., costars in movies). For convenient processing in MapReduce, graphs are stored as key-value pairs, in which the key is the vertex id (URL, movie name, etc), and the value is a complex record called a “tuple” that contains the list of neighboring vertices and any other attributes of the graph vertices (text of the webpage, date of the movie, etc). The key point is that the graph will be distributed across the cluster so different portions of the graph, including direct neighbors, may be stored on physically different machines. Nevertheless, we can process the graph in parallel using Hadoop/MapReduce, to compute PageRank or solve the Kevin Bacon game without ever loading the entire graph on one machine. Graph algorithms in Hadoop/MapReduce generally follow the same pattern of execution: (1) in the map phase, some computation is independently executed on all the vertices in parallel, (2) in the shuffle phase, the partial results of the map phase are passed along the edges to neighboring vertices, including when those vertices are located on physically different machines, and (3) in the reduce phase, the vertices compute a new value based on all the incoming values (once again in parallel). Generically, we can speak of vertices passing “messages” to their neighbors. For example, in PageRank the current PageRank value of each vertex is divided up and distributed to their neighbors in the map and shuffle phases, and in the reduce phase the destination vertices compute their updated PageRank value as the sum of the incoming values. If necessary, the algorithm can iterate and rerun the MapReduce code multiple times, each time updating a vertex’s value based on the new values passed from its neighbors. This algorithm design pattern fits the large class of graph algorithms that need to distribute “messages” between neighboring vertices. For search problems like the Kevin Bacon game, we can use this pattern to execute a “frontier search” that initially distributes the fact that there is a 1-hop path from Kevin Bacon to all of his immediate costars in the first MapReduce iteration. In the second MapReduce iteration the code extends these partial 1-hop paths to all of his 2-hop neighbors, and so forth until we find the shortest path to our favorite movie star. Be warned, though, that frontier search algorithms generally require space that is exponential in the search depth – therefore a naïve frontier search in MapReduce is not appropriate for searching for very deep connections: you may exhaust the disk storage of your cluster or wait a long time waiting for the network to shuffle terabytes upon terabytes of intermediate data. In contrast, PageRank is computed using values just from immediate neighbors, and is therefore more suitable for parallelization with Hadoop/MapReduce. The other main technical challenge of MapReduce graph algorithms is that the graph structure must be available at each iteration, but in the design above we only distribute the messages (partial PageRank values, partial search paths, etc). This challenge is normally resolved by “passing along” the graph structure from the mappers to the reducers. In more detail: the mapper reads in a vertex as input, emits messages for neighboring vertices using the neighboring vertex ids as the keys, and also reemits the vertex tuple with the current vertex id as the key. Then, as usual, the shuffle phase collects key-value pairs with the same key, which effectively collects together a vertex tuple with all the messages destined for that vertex (remember, this happens in parallel on multiple reducers). The reducer then processes each vertex tuple with associated messages, computes an updated value, and saves away the updated vertex with the complete graph structure for the next iteration. But wait, you might ask: doesn’t this entail the mappers emitting two different types of values (messages destined for neighboring vertices and the graph structure)? Yes, this is handled by “tagging” each value to indicate which type it is, so that the reducer can process appropriately. For more details about such graph algorithms, you be interested in Jimmy Lin and Chris Dyer’s recent book on MapReduce algorithm design. This basic design works and with it we can compute PageRank, solve the Kevin Bacon game, assemble together genomes, and attack many other large-scale graph problems. However, it has several inefficiencies that needlessly slow it down, such as the poor use of locality and substantial unnecessary computation. In part two we will explore the causes of those inefficiencies, and present a set of simple techniques called Schimmy that we developed that can dramatically improve the runtime of virtually all Hadoop/MapReduce graph algorithms without requiring any changes to the underlying Hadoop implementation.</snippet></document><document id="585"><title>Integrating Apache Hadoop in your Existing DW and BI Environment</title><url>http://blog.cloudera.com/blog/2010/11/integrating-hadoop-in-your-existing-dw-and-bi-environment/</url><snippet>Organizations are looking for a cost-effective way to deal with data that are now arriving in an unprecedented variety, velocity and volume. Apache Hadoop provides a way to capture, store and analyze this deluge of complex data. Data integration platforms, data warehousing technologies and business intelligence tools have traditionally been the core of most enterprises’ data management infrastructure. To take full advantage of the power offered by Hadoop, our customers have requested integration with their existing enterprise data systems and so we have formed partnerships with Informatica, Teradata, Aster Data, Pentaho, Talend and more. One of our customers, eBay, is using Hadoop to dramatically improve the buyer’s search experience and better match them to sellers, thereby improving sales and profits. You can hear about how eBay has integrated Hadoop into their existing data warehouse and business intelligence environment in our upcoming webinar. Join us for this webinar with Anil Madan of eBay and Jeff Hammerbacher of Cloudera on: Wednesday, November 17 at 10:00am PT, 1:00pm ET Presenters: Anil Madan, eBay, Director of Engineering, Analytics Product Development Anil leads the team at eBay that is leveraging its data assets to do advanced insights and analytics, by sourcing huge volumes of data into their Hadoop cluster and running clickstream and transactional data analysis for user behavior, search quality and research use cases. Jeff Hammerbacher, Cloudera, Founder and Chief Scientist Jeff conceived, built, and led the Data team at Facebook. The Data team was responsible for inventing and building powerful data analysis applications on Hadoop. That system is the core data platform at Facebook for improving the user experience and driving revenue.</snippet></document><document id="586"><title>Better Workflow Management in CDH with Oozie 2</title><url>http://blog.cloudera.com/blog/2010/11/better-workflow-management-in-cdh-with-oozie-2/</url><snippet>Oozie version 2.2.1 is now bundled with Cloudera Distribution for Hadoop (CDH3 Beta 3). This major upgrade includes new functionality such as time and date-driven workflow jobs, and an embedded Tomcat server that makes deploying Oozie much easier.  Oozie 2.2.1 also includes several bug fixes, while preserving backward compatibility with Oozie 1.6. Time and Data Triggered Workflow Jobs Oozie 2 introduces support for coordinator jobs. A coordinator job is a time and data-driven job that starts a workflow job every time a set of events happen. For example, a coordinator job could start a workflow job every hour, or start a workflow job at the end of the day when all of the hourly data of the day is available in HDFS. Coordinator jobs are expressed in terms of a time frequency, and a set of time-bound input and output data. A Use-Case for Coordinator Jobs Suppose a website has a high volume of traffic and rolls over its logs every 30 minutes. After the log files are rolled over, they are copied to HDFS in a directory named with the rollover timestamp (“logs-YYYY-MM-DD-HH-mm”).  At the end of the day, when the 48 daily logs are available in HDFS, a workflow job processes the logs to generate an aggregated log-report for the day. A coordinator job consists of three characteristics: the frequency of the workflow job, the time-bound input, and the time-bound output. The frequency dictates the nominal time for a workflow job to run. The time-bound input data must exist or else the workflow job will not start. The time-bound output data is produced by the workflow job. In our daily logs example, the characteristics of the coordinator job are: Frequency of the workflow job: daily Time-bound input data: the 48 log rollovers of the day Time-bound output: the aggregated log-report for the day One important property of a coordinator job is that even if the workflow job is started late (because the input data is not available, for example), the time boundaries of the input data do not change. That is, the coordinator job uses the input data within the specified time boundaries, not the input data from a time period relative to its execution time.  For example, in the use case for aggregating a daily log, the time boundaries of the input data are from the first 30 minutes to the last 30 minutes of a calendar day.  If the coordinator job starts six hours after the end of the day (at 6:00 AM) because the input data is not available until then, the coordinator job uses the data from the first 30 minutes to the last 30 minutes of the previous calendar day, not the data from the 24-hour period preceding the job start time at 6:00 AM. Oozie Runs Out of the Box Starting with Cloudera Oozie 2.2, there is no need to install a Tomcat server to run Oozie. Oozie bundles an embedded and preconfigured Tomcat server. You simply invoke ‘oozie-start.sh’ to run Oozie. Support for Multiple Databases Oozie now can run with HSQLDB, MySQL or Oracle databases. What’s Next? For the next release of CDH3, we will make Oozie easier to configure and use, add Kerberos security to the Oozie HTTP API as well as support for Derby databases, and include a few other incremental features and bug fixes.</snippet></document><document id="587"><title>Tackling Large Scale Data in Government</title><url>http://blog.cloudera.com/blog/2010/11/tackling-large-scale-data-in-government/</url><snippet>This is a guest post provided by Booz Allen Hamilton data analysis consultant, Aaron Cordova.  Aaron specializes in large-scale distributed data processing systems. Working within the U.S. federal government arena provides plenty of opportunities to encounter large-scale data analysis. Projects ranging from massive health studies to high-velocity network security events to new sources of imagery and video represent a huge increase in the amount of data that must be not only stored but processed quickly and efficiently. These challenges are at once a daunting and exciting chance to turn data into a positive impact for the country. The large-scale data processing technologies created and supported by Cloudera play a big part in answering that challenge. Often our clients have an immediate need to analyze the data at hand, to discover patterns, reveal threats, monitor critical systems, and make decisions about the direction the organization should take. Several constraints are always present: the need to implement new analytics quickly enough to capitalize on new data sources, limits on the scope of development efforts, and the pressure to expand mission capability without an increase in budgets. For many of these organizations, the large data processing stack (which includes the simplified programming model MapReduce, distributed file systems, semi-structured stores, and integration components, all running on commodity class hardware) has opened up a new avenue for scaling out efforts and enabling analytics that were impossible in previous architectures. We have found this new ecosystem to be remarkably versatile at handling various types of data and classes of analytics. When working to help solve clients’ large-scale data analysis problems we first take a comprehensive look at their existing environment, resources available, the nature of data sources, and immediate questions that must be answered of the data. Usually a large data processing solution will be composed of several pieces that are composed into a system that provide the desired capability. This can range from real-time tipping to vast index and query capabilities to periodic and deep analysis of all the data available. Constructing a solution almost always requires one or more new and highly scalable components from the large-scale data analysis software stack, and integration with conventional data storage and processing software. Having the ability to pick and choose which elements of the stack to include and having well-defined interfaces and in some cases interoperability standards is essential to making the system work. This is a major reason that we value the open source community and concept of the large-scale data analysis ecosystem. Perhaps the most exciting benefit, however, from moving to these highly scalable architectures is that after we’ve solved the immediate issues, often with a system that can handle today’s requirements and scale up to 10x or more, is that new analytics and capabilities are now incredibly easy to develop, evaluate, and integrate thanks to the speed and ease of MapReduce, Pig, Hive, and other technologies. More than ever the large-scale data analysis software stack is proving to be a platform for innovation. The response to the challenge of large-scale data analysis continues to emerge and there is room for ongoing innovation. One example of this is evident as large-scale data systems or clouds become more numerous; the task of integrating analysis across those clouds remains an area of open research. Even integrating data sources, existing systems, and delivery mechanisms within departments of the same enterprise can be a challenge and may require new solutions. Recently when researching the problem of large-scale Biometric search, we at Booz Allen realized the need for a highly scalable and low-latency method of performing fuzzy matching, i.e. returning the most similar items when there is no exact match, in response to growing databases of fingerprints and requirements to identify individuals quickly. It was clear that Hadoop would provide a good platform on which to build a new distributed fuzzy matching capability for several reasons. The ability to run MapReduce over all stored biometrics would help in clustering the data to reduce the search space. The data distribution and replication features of HDFS would provide a reliable storage system with enough parallelism to support fast queries running on multiple machines. The result of our research is a system we developed called FuzzyTable. FuzzyTable is a large-scale, low-latency, parallel fuzzy-matching database built over Hadoop. It can use any matching algorithm that can compare two often high-dimensional items and return a similarity score. This makes it suitable not only for comparing fingerprints but other biometric modalities, images, audio, and anything that can be represented as a vector of features. Fuzzy matching involves extracting features from the data of interest, and running a given fuzzy matching,  distance, or similarity algorithm over the resulting feature vectors to produce the numerical score. Our work involved developing two major components – a clustering process that we use to reduce the total search space for each query, and a client-server system for performing fuzzy matching on demand and in parallel across a Hadoop instance. In the first step, we use two clustering algorithms – canopy clustering and k-means – from the Apache Mahout project to assign each biometric into a bin.  Each bin contains biometrics that are statistically similar. Each bin also has a ‘mean biometric’ that represents an average of the biometrics contained in that bin. When performing queries looking for the best match we first find the bin that a given biometric scores closest to by comparing it to the list of ‘mean biometrics’ from our k-means processing. This allows us to avoid searching a large portion of the data set and only search the bin (or small number of bins) that contains the most similar items to the biometric in question. The FuzzyTable client then looks up the location of all blocks or chunks that contain biometrics for that bin by querying the FuzzyTable master server. Blocks are not replicas, rather they contain different sets of data (although using replication to improve the number of concurrent queries is possible). These blocks live on several HDFS DataNodes so that a single bin can be searched by several machines at once. Finally the FuzzyTable client submits the biometric query and the ID of the closest bin to FuzzyTable query servers running alongside HDFS DataNodes which sift through the blocks of the closest bin comparing the query biometric to each stored biometric and return the most similar matches. The client collects all the matches from FuzzyTable query servers and displays a ranked list of matches to the user. All this takes place in a few seconds. Because Hadoop can distribute our data automatically across multiple machines we only had to write the code to run comparisons on local data. HDFS exposes the location of data blocks making it possible to run the comparisons only on local data, which is also the key to the speed of MapReduce. The following graph shows how query times (in milliseconds) improved dramatically as we added machines. After about seven machines, our query times became dominated by fixed costs that did not increase with the number of machines. Our test cluster could easily store several times more than our test data before we would see query times increase again. ? Large-scale data stack components such as Hadoop and Mahout proved to be a perfect platform on which to create this new capability. Undoubtedly the trend of innovation will continue as we further explore the possibilities of these new reliable and easy-to-use software platforms that achieve scalability through parallel computation over distributed shared-nothing systems. Booz Allen Hamilton has been at the forefront of strategy and technology consulting for nearly a century. Providing a broad range of services in strategy and organization, technology, operations, and analytics, Booz Allen is committed to delivering results that endure. To learn more, visit www.boozallen.com. Aaron Cordova is a data analysis consultant at Booz Allen Hamilton, who specializes in large-scale distributed data processing systems.</snippet></document><document id="588"><title>Cloudera Fun &amp; Frightful Halloween Festivities</title><url>http://blog.cloudera.com/blog/2010/11/5226/</url><snippet>Here at Cloudera we embraced the holiday spirit with the light heartedness that is Halloween by hosting several activities including an engineering hack-a-thon, a hack-a-pumpkin-a-thon, and a costume competition. Cloudera Corporate &amp; a chicken, aka Cloudera engineers at their finest. Hadoop came and joined the festivities dressed in a very realistic pumpkin costume. To keep Hadoop’s pumpkin costume up to earthquake code specifications, several paper-clips were deemed necessary in the pumpkin’s structure. The Cloudera fun continued into the weekend as part of the Cloudera team gathered October 30th in Hollister, CA to take part in the Northern California Warrior Dash. Cloudera has a very friendly and accepting atmosphere, filled with great people that are passionate about their work. If this sounds like an environment you would like to be a part of check out the careers page on our website. There are numerous job openings here at head quarters (Palo Alto), along with rumors of a San Francisco office to be coming in the very near future. Check out the careers page to learn more.</snippet></document><document id="589"><title>Apache Hadoop Lab at JavaOne</title><url>http://blog.cloudera.com/blog/2010/10/hadoop-lab-at-javaone/</url><snippet>Guest post by Daniel Templeton, Product Manager at Oracle. Aside from JavaOne ’10 having a new home as part of the greater�Oracle OpenWorld conference, it was business as usual this year. Lots�of great sessions, lots of interesting labs, and lots and lots of�excited developers. (I think there may have even been more attendees�than the conference planners expected.) This year Apache�Hadoop joined the ranks of the�JavaOne hands-on labs with a lab co-produced by Oracle and�Cloudera. JavaOne Hands-on Lab S314413: Extracting Real Value from Your�Data With Apache Hadoop was offered as a two-hour interactive lab�designed to introduce attendees to the Hadoop environment, including�writing a MapReduce program, writing a custom input reader, running,�monitoring, and managing Hadoop jobs, and working with the Hive data warehousing�platform. The lab was designed for participants with at least�some Java programming experience but not necessarily any prior�exposure to Hadoop. In case you missed the lab at JavaOne, Oracle and Cloudera are both�making the lab materials available online. Oracle will post the�materials as part of the greater JavaOne presentations posting.�Cloudera has already posted�the lab materials online in the training section of the�website. When you download the zip file, in it you will find a lab workbook�as a PDF in the root directory. At the back of the workbook, you will�find an appendix that describes how to set up your own lab�environment. I highly recommend that you grab the Cloudera Distribution for�Hadoop (v2) to use as an environment for the lab. Cloudera even�makes a prebuilt Linux/Hadoop environment available as a�virtual machine. The lab was written for Solaris 11 Express and�NetBeans, but you should still be able to do the lab on another OS�with another IDE. At JavaOne, the lab was very successful. Turnout was good and the�comments were great! I’ve already incorporated lots of great feedback�from that session into the set of lab materials that Cloudera is now�hosting, but I’m always happy to hear any additional comments and/or�feedback. Happy coding!</snippet></document><document id="590"><title>Hadoop World 2010: An Unqualified Success</title><url>http://blog.cloudera.com/blog/2010/10/hadoop-world-2010-an-unqualified-success/</url><snippet>Hadoop World 2010 brought together a diverse group of attendees from techies working in the web 2.0 world to traditional enterprises to open source community members to IT industry vendors. The excitement at Hadoop World kicked off the night before with the welcome reception on the rooftop of the Ava Lounge sponsored by Accel Partners. For all that attended you surely remember the sudden lightning and thunder followed by the mad dash indoors. On the morning of Hadoop World attendees arrived as early as 6:30am and you could feel the buzz vibrating throughout the day—evidenced by the Hadoop World hashtag (#hw2010) trending to one of the top 10 tags on Twitter in NYC October 12th. The thirst for Hadoop knowledge is further evidenced by the fact that all but one of the New York Cloudera training sessions sold out. We thank everyone—all 923 of you—who participated in making it a great event and we hope you got value out of the 42 sessions in addition to the networking opportunities. YES! the presentations and videos will be available. We’re in the process of collecting them and we hope to get those posted within the next 2 weeks and we will communicate when they’re available. Our friends at Silicon Angle provided a live broadcast feed that reached close to 100,000 unique viewers throughout the day. Silicon Angle is currently recycling the footage so go to siliconangle.tv to catch their analysis of the event along with individual interviews from presenters, sponsors and other Hadoop heavyweights. More updates coming soon!</snippet></document><document id="591"><title>CDH3 beta 3 now available</title><url>http://blog.cloudera.com/blog/2010/10/cdh3-beta-3-now-available/</url><snippet>Today, Cloudera is excited to release the third beta release of Cloudera’s Distribution for Hadoop version 3 — CDH3b3. In this post, I’ll cover the major changes since CDH3b2, and give some insight on what’s coming down the pipe in the next couple of months. Platform Enhancements Many of you may have read about some of the recent announcements of partnerships between Cloudera and some of the leading data management software companies like Teradata, Netezza, Greenplum (EMC), Quest and Aster Data. We established these partnerships because Hadoop is increasingly serving as an open platform that many different applications and complimentary technologies work with. Our goal is to to make this as easy and as standardized as possible. As part of this beta update, we’ve added several platform enhancements that make it easier for complimentary technologies to run or work with the Hadoop platform. The Sqoop framework was enhanced with a plugin framework that allows for different swappable adapters that are optimized for integrations with database technologies. Some of these first adapters will be available in beta form as early as this week with many more to come in the next months. In addition Sqoop can now cover a broader range of integration use cases thanks to support for incremental database updates to and from Hadoop. We’ve also been hard at work on providing better integration opportunities for different analytical tools and applications. As of CDH3b3, this now includes an ODBC driver that can be used in conjunction with Hive to enable users to query Hadoop using their favorite BI tool. Security Enhancements As one of the primary contributors and largest production users of Hadoop, Yahoo! publishes the source tree for the version of Hadoop that they run on their production clusters. We are pleased to announce that we have merged Yahoo’s source tree into CDH3b3. This merge brings many improvements developed at Yahoo! into CDH, including improvements for MapReduce scalability on 1000+-node clusters and several new tools for benchmarking and testing Hadoop. The largest new feature, though, is the introduction of a strong authentication system based on Kerberos. Kerberos is an industry-standard authentication system supported both by completely open source software like MIT Kerberos as well as by common enterprise authentication systems like Microsoft Active Directory. The integration of Kerberos authentication into CDH enables enterprises to use their existing authentication infrastructure to manage user identities, and allows more sensitive data to be stored and analyzed within a cluster. Some new authorization features have also been added to CDH in this release. For example, if the security features are enabled, MapReduce jobs can specify access control lists (ACLs) that specify which users and groups may view job details or prematurely kill the job. Additionally, the tasks of MapReduce jobs may now run as the UNIX user who submitted them, improving the ability to isolate resources and protect confidentiality of intermediate data and logs. In addition to integrating these new features into the MapReduce and HDFS components, we have also updated the rest of the components of CDH to operate in an authenticated environment. This marks the first time where it has been possible to run a secure Hadoop while continuing the use of other Hadoop components like Hive, Hue and HBase. The work to integrate these new security features across the distribution is still continuing — we are currently aware of some places in which the current implementation is incomplete and vulnerable to certain exploits, and will fix these issues before we declare CDH3 stable. We are also hard at work on a comprehensive guide that will detail setup instructions and best practices for operating a secured Hadoop cluster. If you have security requirements in your organization, we hope you will find this beta release useful as a preview of what’s to come. Of course, CDH3b3 also contains several bug fixes and improvements based on our experiences deploying clusters for customers with a wide range of use cases. Please check the release notes for the full list. Whirr We are happy to include Apache Whirr as the newest member of the CDH family. Whirr is a tool for quickly starting and managing clusters running on cloud services like Amazon EC2. Stay tuned for an upcoming post with more information about Whirr. Improvements in Performance and Stability CDH3b3 includes updates to all of the other components in the platform. Most of these updates are fixes to eliminate bugs or to improve performance. One notable enhancement is the support for calendar and event based scheduling of Hadoop jobs via the Oozie workflow engine. Upgrading to CDH3b3 In order to upgrade an existing cluster from CDH2 or CDH3b2 to CDH3b3, you’ll have to perform a few extra manual steps. Please check out our CDH3 upgrade guide for detailed instructions. What’s up next While we’re done adding major new features for CDH3, we expect to do at least one more beta release before declaring it stable for critical production use. Here’s a sneak peak of what’s to come: New upstream versions of some components, including Hive 0.6.0 and HBase 0.90. Further integration of security features, including improved authentication support in Hive, ACLs for Fair Scheduler Pools, SPNEGO support for Oozie, and easier deployment. Further bug fixes based on our experiences deploying CDH3b3 in QA and in the field. As always, the CDH team is excited to hear your feedback. Please join the cdh-user mailing list and let us know what you think!</snippet></document><document id="592"><title>Hadoop: The Definitive Guide, Second Edition</title><url>http://blog.cloudera.com/blog/2010/10/hadoop-the-definitive-guide-second-edition/</url><snippet>The second edition of my book “Hadoop: The Definitive Guide”, published by O’Reilly, is now available. The first edition was launched at the Hadoop Summit in June 2009, and has gone on to sell well. Less than a year later I was asked to write the second edition. The Hadoop ecosystem has been growing fast (and continues to), and the bulk of the extra 100 pages in the second edition are devoted to three new projects: Hive, Avro, and Sqoop. The major changes are as follows: A chapter on Hive (Chapter 12). Hive is a data processing platform that provides a SQL interface to Hadoop. At the time I started out on the first edition Hive was a relatively new Hadoop contrib project from Facebook. Since then it has grown into an Apache Top-Level Project with a vibrant community and a wide user base spread across many organizations. A chapter on Sqoop (Chapter 15), written by Aaron Kimball, the project founder. Sqoop is a Cloudera-sponsored open-source tool for efficiently moving data between relational databases and HDFS. A section on Avro (in Chapter 4, “Hadoop I/O”). Avro was just starting out (at Yahoo!) at the time of the first edition, but is growing in importance, both for data serialization (which is what is covered in the book) and for RPC (which will likely be used for the foundations of Hadoop someday). Avro is now an Apache Top-Level Project. You can read the Avro section for free online. A section on security (in Chapter 9, “”Setting Up a Hadoop Cluster”). Adding Kerberos authentication to Hadoop has been a major undertaking by the Yahoo! engineering team, and this section gives an introduction to the topic and explains the changes that a user can expect to see. A new case study “Using Pig and Wukong to Explore Billion-edge Network Graphs” (in Chapter 16) by Philip (“flip”) Kromer of Infochimps. The second edition continues to target the Hadoop 0.20 release family (which includes all the major distributions), although there have been many small updates and clarifications made throughout the text. The content for Pig, HBase, and ZooKeeper has been revved to reflect the latest versions, some of which involved significant updates (such as the new Load and Store UDF interfaces in Pig 0.7.0). There is a companion website to the book where you can find example code and other information about the book. You can get a copy from O’Reilly’s website (in paperback and Ebook formats) or from Amazon. Note that all attendees at this week’s Hadoop World in New York will receive a free copy. Finally, I’d like to thank my editor Mike Loukides and the production team at O’Reilly for turning the second edition around so quickly, and John Kreisa at Cloudera who kept the process running smoothly.</snippet></document><document id="593"><title>Afternoon Hadoop World — Possible Path Through Great Content</title><url>http://blog.cloudera.com/blog/2010/10/afternoon-hadoop-world-possible-path-through-great-content/</url><snippet>It’s been repeated over and over again that Hadoop World is packed with great content, and I will again reaffirm this fact. Take a glance at the agenda to see for yourself all the presentations you surely will not want to miss. This post will take you on a stroll down a possible Hadoop World afternoon breakout session path. SRA, International Inc. is presenting “SIFTing Clouds” at 1:45pm with Paul Burkhardt. He will describe the SRA’s MapReduce implementations of the Scale-Invariant Feature Transform (SIFT) algorithm, a well-known computer vision algorithm used for object recognition. The SIFT MapReduce application enables fast object identification in distributed image datasets. Next, at 2:20pm Kevin Weil will be presenting “The Hadoop Ecosystem at Twitter.” This presentation will dive into how Twitter uses applications such as Pig, HBase, and Hive on-top of Hadoop to solve critical business and engineering problems. Then, at 2:55pm Charles Zedlewski of Cloudera is giving a presentation highlighting updates to Cloudera’s Distribution for Hadoop (CDH) and to Cloudera Enterprise. This presentation is titled “Cloudera Roadmap Review” and will also include valuable insights into development plans for the next 12 months. For those who will be attending, I strongly suggest you map out your Hadoop World path early using the agenda and program guide. If you wait until the last minute you will surely find yourself caught between breakout sessions trying to decide which to attend. There are over 800 people registered for the conference and space is filling fast. If you still need to register you can by clicking here.</snippet></document><document id="594"><title>One Possible Hadoop World Morning Path</title><url>http://blog.cloudera.com/blog/2010/10/one-possible-hadoop-world-morning-path/</url><snippet>Hadoop World will kick off with keynote presentations by Mike Olson, Cloudera CEO, and Tim O’Reilly, Founder and CEO of O’Reilly Media. Both are engaging speakers that will fill our fresh morning minds with valuable insight and anticipation for what the rest of the day has in store. Simply taking a brief glimpse at the Hadoop World morning presentations, here  is one way you might traverse through the great content. At 11am is a presentation by Tim Estes from Digital Reasoning on how the US Army is using Hadoop. This talk will discuss the type and scale of analysis that the new “Army Cloud” is doing using Synthesys—a new system for understanding and integrating structured and unstructured data—where Hadoop and CDH3 play a critical role. Then at 11:35am, you can learn more about security in Hadoop by checking out Todd Lipcon’s and Aaron Myers’s presentation on making Hadoop security work within an existing IT environment. For anyone worried about the sensitivity of their data with Hadoop, this is a must-attend presentation. Next you might sit-in on Lalit Kapoor’s presentation from Booz Allen Hamilton at 12:10pm on using Hadoop for indexing biometric data, high resolution images, voice/audio clips, and video clips. They do this using “Fuzzy Table,” created over Hadoop for content that cannot be easily indexed or ordered. Again, this is only one possible route you could take in the morning. At each time slot there are five different presentations to choose from and they all sound insightful, educational, and so exciting that it may be difficult to make the decision as to what path to take. There is still time to register for Hadoop World but it is filling up fast. You may want to bring more than one colleague to cover all the great content and get the most out of your Hadoop World experience.  Click here to register.</snippet></document><document id="595"><title>Hadoop World: More is better!</title><url>http://blog.cloudera.com/blog/2010/09/hadoop-world-more-is-better/</url><snippet>Yesterday I was asked to help proofread the Hadoop World program guide and as I was reading through it I thought “Whoa! This conference is jam packed with so many valuable sessions!” As a new Cloudera employee I knew that the team and the presenters have been working hard to put together a solid conference, but I was still surprised to see the depth and breadth of the content. If you haven’t registered for Hadoop World yet, I suggest you register quickly because it’s less than 2 weeks away and it’s filling up quickly. You will not want to miss it. And guess what? A 5th track containing 5 more excellent sessions was just added to the conference agenda! I generally agree with the famous designer who said “less is more” but this is a case where more is more… and better. You must be wondering “How can I check out the program guide?” Well you’re in luck because we’ve just posted it on the Hadoop World page of the web site – feel free to download it. The program guide gives attendees a complete view into all activities and content that will be presented during the conference sessions and it’s an excellent tool to help you plan your day at Hadoop World. When you begin creating your path through the sessions, note the letter “T” at the beginning of some of the session titles – it indicates a more technical presentation. Feel free to share the program guide with your colleagues, management and anyone who would be interested in the content. While you can learn a lot just reading the guide, you could learn even more if you came out to the Big Apple and joined us for the conference. Register for Hadoop World to save your spot at this not-to-be-missed Hadoop event of the year!</snippet></document><document id="596"><title>Top 10 Reasons to Attend Hadoop World</title><url>http://blog.cloudera.com/blog/2010/09/top-10-reasons-to-attend-hadoop-world/</url><snippet>Hadoop World 2010 is fast approaching, are you ready? On October 12th Hadoop novices and experts will gather in NYC to hear from industry leading organizations on how they are using Hadoop and related technologies to solve real-world business problems. Whether you are just exploring Hadoop or a long time Hadoop user, you won’t want to miss this opportunity to share ideas with experts and network with your peers. Register now! 10. Receive a free book! Every attendee of Hadoop World will receive a free copy of the Second Edition of Tom White’s Hadoop: The Definitive Guide. This indispensable book is required reading for anyone who has any interest in Hadoop. Be one of the first to get your hands on the Second Edition, free with your entry to Hadoop World. 9. Learn about Hadoop-related projects Throughout the day multiple presentations will feature Hadoop-related projects such as HBase, Hive, and Flume. You can listen to use-case presentations to learn how these projects have solved new and existing business problems or you can attend technical sessions to get the inside scoop on the technologies. If you are not familiar with these Hadoop-related projects, the conference is a great place to find out more. 8. Meet the Cloudera team Many Cloudera engineers and solutions architects will be present at Hadoop World. We have committers and contributors to Hadoop and related projects on staff, and our solutions architects are working on some of the largest Hadoop clusters in the world. If you need help or advice you’ll be talking to the right people! 7. Network with your peers With such a large number of attendees and sponsors at Hadoop World, the networking opportunities are boundless. We have arranged ample time for chatting during breaks, lunch, as well as an evening reception. Whether you’re looking for future business partners, colleagues, Hadoop consultants or just like-minded people, Hadoop World offers unparalleled access to the best and brightest in the industry. 6. Meet Doug Cutting, a Hadoop founder Doug is one of the foremost advocates and architects behind open source search technology. His resume includes the creation of Lucene, Nutch–with Mike Cafarella–, and the main attraction, Hadoop. Come shake hands and exchange a few words with this brilliant leader in the open source community. 5. Hear from Tim O’Reilly, keynote speaker! As the founder of the technology transfer company, O’Reilly Media, Tim O’Reilly’s mission has always been to spread the knowledge of technology innovators. Come lend your ears to O’Reilly’s message and absorb the insightful wisdom Tim always imparts when he speaks. You will not be disappointed! 4. Get trained on Hadoop Surrounding Hadoop World are several training sessions from Cloudera designed to help you get the very best out of Hadoop and related projects. Whether you’re a manager who needs an introduction to the subject; a developer who needs to learn how to write MapReduce code; a systems administrator who needs to know how to specify, deploy and manage Hadoop clusters; or a data analyst who wants to leverage Hive and Pig to manipulate huge amounts of data; we have just the right training available. Sign up for any of these training sessions and receive complimentary entry into Hadoop World! 3. See how Hadoop is used in your industry Presentations at Hadoop World represent a range of industries including financial services, web services, government, telecommunications and more. Attend Hadoop World to find valuable ideas for your business, and see for yourself how users within your industry have improved their data management and analysis with Hadoop. 2. Choose from thirty-six great presentations Hadoop World is packed with great content for you. There are four tracks during the conference, each containing nine different presentations. Choose those that interest you most and consider bringing along colleagues in order to get as much exposure to the material as possible. 1. See how Hadoop solves real-world business problems Hadoop World will feature presentations by organizations using Hadoop to solve real-world business problems. By attending, you’ll gain real insight into how you can use Hadoop to solve your own problems, and how Hadoop can be beneficial to your enterprise. Register for Hadoop World Now! Come to Hadoop World! You have to see it for yourself! With so much great content and networking opportunities space is filling fast, don’t miss your opportunity to be a part of the event.</snippet></document><document id="597"><title>Twitter Analytics Lead, Kevin Weil, and a Presenter at Hadoop World Interviewed</title><url>http://blog.cloudera.com/blog/2010/09/twitter-analytics-lead-kevin-weil-and-a-presenter-at-hadoop-world-interviewed/</url><snippet>Kevin Weil, Analytics Lead at Twitter and a featured presenter at Hadoop World was gracious enough to sit down with us and answer a few questions in this Q&amp;A format interview. From the information Kevin has provided you will gain a larger understanding into the Hadoop ecosystem at Twitter, Kevin’s presentation at Hadoop World, and what he expects to gain from attending Hadoop World. Q: What can attendees expect learn about Hadoop from your presentation at Hadoop World? Twitter uses Hadoop for a very broad set of applications. We have Hadoop jobs doing everything from user and product analysis, to social graph analysis, to generating indices for people search, to machine learning, and natural language processing. Nearly every team at Twitter uses Hadoop in some way. We even have folks from our product marketing and sales team running Hadoop jobs! In addition, we are very active in the Hadoop community; we employ committers on Hadoop, Pig, Hive, Avro, and Mahout. And we make the vast majority of our work on top of Hadoop open source: see our LZO compression work or our Elephant Bird project, both of which have seen great contributions from the community. Hadoop plays a critical role in helping us understand the Twitter ecosystem, and I’m going to touch on many of these use cases. Q: Do you have Hadoop in production use today? Yes, we have a cluster of about 100 nodes, which is actually far too small for the 8 terabytes of data generated each day across our systems. As we move to our new data center, we expect the size of our cluster to grow significantly, reflecting its importance to our company. Q: Describe use cases for Hadoop at Twitter. Hadoop is our data warehouse; every piece of data we store is archived in HDFS. We use HBase for data that sees updates frequently, or data we occasionally need low-latency access to. Every node in our cluster runs HBase. We use Java MapReduce for simple jobs, or jobs which have tight performance requirements. We use Pig for most of our analysis jobs, because its flexibility helps us iterate rapidly to arrive at the right way of looking at the data. Our Hadoop use is also evolving: initially it was primarily used as an analysis tool to help us better understand the Twitter ecosystem, and that’s not going to change. But it’s increasingly used to build parts of products you use on the site every day such as People Search, the data for which is built with Hadoop. There are many more products like this in development. Q: How do you support Hadoop? We are fortunate to have a team of folks with significant Hadoop experience, and many others across the company eager to dive in and learn more. We use Puppet at Twitter to distribute configuration to all nodes, we use Cloudera’s Hadoop RPMs to maintain this configuration, and we use Ganglia and Nagios to monitor and alert on issues. Q: What benefits do you see from Hadoop? The Twitter ecosystem is growing at fantastic rates, and there is a corresponding growth in the size of our dataset. Hadoop has been instrumental in helping us analyze and understand that data for a few reasons. Its horizontal scalability has meant that as our dataset has grown, we have been able to simply add new nodes to our cluster. The fact that Hadoop takes care of scaling this way has meant that we can concentrate on the hard parts of the analysis, rather than having to update our code every time our data set grows by an order of magnitude. Additionally, Hadoop’s flexibility in analyzing semi-structured data has been important to us again and again. As our team grows, more people and teams are writing data into Hadoop, and we see fast evolution in the type and amount of data logged. Many storage systems would have choked innovation by requiring teams to wait for a schema update. Hadoop works well with rapidly evolving data, and we think projects like Howl will only make this easier. Q: What did you use before Hadoop? Hadoop has quickly become an essential part of our data storage, analytics, product development, and research efforts. Prior to Hadoop, we had a MySQL-based data warehouse and ETL system, like many companies start with. It worked for a while, but over time the daily job began taking 16, 18, 20 hours. That’s never been an issue since we switched to Hadoop because it allows us to scale our cluster horizontally as Twitter usage grows.  It would probably take 2 weeks to run a day’s worth of numbers today if we had to go back to our old system. Q: How has Hadoop improved your work at Twitter? You can judge a lot about the way your knowledge of a system is evolving by the types of questions you’re asking. Before Hadoop, we were straining to answer simple questions like “how many unique users tweeted over the last 6 months?” or “What is the distribution of users across our website, our mobile site, mobile Twitter applications, and desktop-based Twitter applications?” With Hadoop, those kinds of questions are simple to answer. Answers to these questions become part of the daily lexicon, and the questions get replaced by better, harder, more informative questions involving AB testing, cohort analysis, and machine learning. It’s no longer hard to find the answer to a given question; the hard part is finding the right question. And as questions evolve, we gain better insight into our ecosystem and our business. Q: What are you hoping to get out of your time at Hadoop World? I’m looking forward to hearing about innovative uses of Hadoop from the other companies in attendance, catching up with old friends, and getting to know more of the great Hadoop community! Kevin is one of thirty-six different presentation that will be given at Hadoop World. Check out the Agenda and register now!</snippet></document><document id="598"><title>More on Cloudera Enterprise</title><url>http://blog.cloudera.com/blog/2010/09/more-on-cloudera-enterprise/</url><snippet>A few months back Cloudera announced two product updates: a significant expansion of our distribution for Hadoop and the availability of Cloudera Enterprise.  Over the course of the past two months we’ve had several blog posts talking about Cloudera’s Distribution for Hadoop (CDH) including what’s in it, news about different components and examples of how it is being used.  This was intentional.   Cloudera’s Distribution for Hadoop is the company’s main platform and we think it’s the most functional, reliable and consumable way to get Hadoop. At the same time many people have asked to learn more about Cloudera Enterprise and so this blog post provides an overview of CDH’s sister product. Cloudera Enterprise is: What companies need to run Hadoop in production This is the essence of Cloudera Enterprise.  Most companies working with Hadoop have learned there is a big difference between trying Hadoop, executing a single use case with Hadoop and deploying Hadoop as a long lived piece of a company’s internal infrastructure.   Cloudera Enterprise is aimed at that third case. When Hadoop moves into production, the expectations of internal customers grow and the operations of Hadoop gets professionalized. Operations teams need Hadoop to behave like any other component in their IT landscape.  They need Hadoop to: Conform to service level agreements established with internal customers. This may include things like uptime, slot availability or job completion time. Integrate with established IT processes and policies. Come with a reasonable administrative overhead in terms of the number of people who have to care for Hadoop and the amount of expertise they require. Cloudera Enterprise helps teams accomplish these aims A combination of services and software The product includes what we’ve learned that organizations need to run Hadoop in production for which both software and support is essential. Consequently Cloudera Enterprise includes: Production support for Cloudera’s Distribution for Hadoop Production support for certified interfaces to Cloudera’s Distribution for Hadoop (these either already include or have been announced to include MySQL, Oracle, Netezza and Teradata). Software that helps automate the administration of user and group permissions including prepackaged integration to Active Directory Software that supports the monitoring of different aspects of the performance and reliability of the Hadoop stack Software that help track the usage of cluster resources for purposes of interdepartmental billing and capacity planning Cloudera’s commercial offering (i.e. something that you pay for) Cloudera charges for Cloudera Enterprise and it is available as an annual subscription. Since its announcement, customer response to Cloudera Enterprise has been extremely positive and a number of companies are already live with Enterprise.   If you’d like to learn more about Enterprise, you can e-mail info@cloudera.com or you can come to Hadoop World in New York City this October 12th http://www.cloudera.com/company/press-center/hadoop-world-nyc/.</snippet></document><document id="599"><title>What’s Going On Surrounding Hadoop World</title><url>http://blog.cloudera.com/blog/2010/09/what%e2%80%99s-going-on-surrounding-hadoop-world/</url><snippet>We’re only 3 weeks away from the event date—October 12th— so we have decided to save you time and provide a short synopsis of events surrounding Hadoop World. These events include Hadoop-related activities, as well as some fun and interesting activities, which cannot be found in places other than the Big Apple. Register now for Hadoop World! October 11th, Monday: Introduction to Hadoop Training, 9am – 5pm: This session will provide attendees with the foundation of Hadoop knowledge necessary for anyone looking for further Hadoop understanding prior to the conference. Hadoop Essentials for Managers Training, 9am – 5pm: This session will provide Hadoop insight that will come in handy when the time comes to make informed decisions about using Hadoop. Cloudera Hue SDK Training, 9am – 5pm: Attendees will build an application with Hue SDK under direction from the Cloudera team. HBase User Group Meetup, 4 – 6pm: Hosted in the StumbleUpon Soho office with beer and snacks provided by Facebook. Accel Partners Welcome Reception, 6 – 9pm: Accel is sponsoring this welcoming event on the penthouse/rooftop deck of The Dream Hotel at a place called Ava Lounge. Not to be missed! Rock of Ages, 8pm: A Broadway musical featuring hits from Foreigner, Styx, Pat Benatar, and Journey. A retro adventure for those wishing it was still the 80’s. Ping-Pong in Bryant Park, 11am: Two tables will be set-up at Bryant Park for anyone who wants to learn, show off, or develop a phenomenal spinning serve. October 12th, Tuesday: Hadoop World, 8 – 5pm: The main event! Hear from more than 36 presentations revolving around Hadoop. NTT Data Hadoop World Networking Reception, 6 – 7:30pm: Following Hadoop World NTT Data has offered an hour of drinks and Hor D’oeuvres while we network and chat about what we have seen throughout the day. October 13th &amp; 14th, Wednesday &amp; Thursday: Developer Training and Certification, 9 – 5pm (2 days): Learn the MapReduce framework and how to write programs against its API. In addition learn design techniques for larger workflows, and advanced skills for debugging MapReduce programs and optimizing their performance. Administrator Training and Certification, 9 – 5pm (2 days): Learn aspects of Hadoop from installation and configuration to load balancing and tuning, to diagnosing and solving problems in your deployment. Analyzing Data with Hive and Pig, 9 – 5pm (2 days): This course will teach you how to process data by using filters, joins, user-defined functions and more. 4Play, 2pm (both days): The flying Karamazov Brothers combine circus arts, comedy, music, dance, and more in this loopy circus show. Be sure to smuggle in something heavy and awkward for them to juggle. Dead or Alive, All Day (both days): This show features artists that work with a wide range of materials such as feathers, bugs, bones, silkworm cocoons, greenery, and hair to create intricate installations and sculptures. A little weird, yet not to be feared. October 15th, Friday: HBase Training, 9 – 5pm: Learn how to use HBase as a distributed data store to achieve low-latency queries and highly scalable throughput. This course covers HBase architecture, data model, and Java API. Light reading, All day: The re-creation of classic literature books using LED devices to portray them in bright neon colors by artist Airan Kang. Again, this is a short summary of the dates surrounding Hadoop World. Obviously we have not included everything happening in NYC, we have condensed and included what we believe will be of most interest to you. If you feel we have left something out—especially a Hadoop-related item—please let us know, we would love to add your event to our calendar. Get ready for an exciting couple of Hadoop days! And a big thank you to Apache Hadoop for giving us a reason to put this event together! If you have yet to register for Hadoop World sign up here.</snippet></document><document id="600"><title>What is in our Kitchen?</title><url>http://blog.cloudera.com/blog/2010/09/what-is-in-our-kitchen/</url><snippet>If there is one thing that chefs are proud of, it’s their kitchens. Whether cavernous top-of-the-line affairs or cramped New York apartments, kitchens are the place where raw ingredients are combined with talent and hard work to produce results. The only difference in the world of software is what you will find in our kitchens.  In an interview with CNET, Google’s Hal Varian attributed Google’s success to the “kitchen” in which their products are developed: “I also think we have a better kitchen. We’ve put a lot of effort into building a really powerful infrastructure at Google, the development environment at Google is very good.” The goal of the Kitchen team at Cloudera is to create a powerful infrastructure for developing, building, testing, shipping, and supporting our software. Kitchen contributes its expertise to every product Cloudera builds, while also building out new infrastructure and tools to facilitate future development. Everyone on the Kitchen team writes software. While the Kitchen team’s culture was initially inspired by Google’s infrastructure, we agree with Piaw Na who recently provided some words of caution for companies looking to follow this example: “In short, I think startups have to be very careful about building generic infrastructure just because that’s the way Google did things.” The Kitchen team builds the infrastructure that is needed to solve our company’s problems. For example, our build system must be capable of coalescing many disparate open source projects into a unified platform. If there is an existing open source tool or framework that meets our needs we use it, improve it, and contribute it back to the project rather then “rolling our own” We use many of the open source tools you might expect, such as Hudson for continuous integration. Our Hudson instance manages tens of hosts running over seventy projects: Unit tests running on every commit, across multiple platforms, and flavors of Java or Python Hadoop clusters running on EC2 using Apache Whirr Various code improvement tools such as jcarder, Cobertura, Clover, FindBugs, CheckStyle and others If a tool does not exist the Kitchen team tries to leverage existing frameworks to build what is required. For example, our automated build and release system, which is at the heart of the Cloudera Distribution for Hadoop (CDH) platform, is built on top of boto. From a single git repository, we use crepo (another Kitchen project) to check out the latest source of each project within CDH. Then we build source artifacts for all of the projects, which get uploaded to S3. We then spin up an EC2 cluster to build everything for all the supported CentOS releases, Ubuntu, and Debian releases, including both 32 and 64-bit architectures. The resulting packages are stored back in S3, and then staged to a fresh EC2 instance of archive.cloudera.com for testing. Additional EC2 instances follow and run end-to-end package tests for each package that was built. We turn the crank nightly, not just for each release. The Kitchen team is in the process of building a status, dashboard, radiator, single-pane-of-glass to prominently display Hudson’s status, nightly builds, JIRA stats, CDH download statistics, and many other metrics we use daily. No software company is complete without a cluster or two. Kitchen maintains a development cluster, a long-lived CDH cluster, a security-enabled CDH cluster, and a “dog-food” cluster. We’re currently building out a Eucalyptus cluster so we can also run our build and test infrastructure in house. We have a large scale cluster in the works and we are busy building out our infrastructure to accommodate it.  We use Cobbler, run Ganglia (bias alert, we employ one of the original authors), debate Chef and Puppet. Our Kitchen team is growing. If this sounds like a team you would like to be a part of, get in touch with me on twitter or IRC (#cloudera on freenode.net) or apply directly. Stay tuned for more blog posts about what’s cooking in our Kitchen. Image courtesy of Chef Olive at Kitchen On Fire</snippet></document><document id="601"><title>Using Flume to Collect Apache 2 Web Server Logs</title><url>http://blog.cloudera.com/blog/2010/09/using-flume-to-collect-apache-2-web-server-logs/</url><snippet>Flume is a flexible, scalable, and reliable system for collecting streaming data.   The Flume User Guide describes how to configure Flume, and the new Flume Cookbook contains instructions (called recipes) for common Flume use cases.  In this post, we present a recipe that describes the common use case of using a Flume node collect Apache 2 web servers logs in order to deliver them to HDFS. Using Flume Agents for Apache 2.x Web Server Logging To connect Flume to Apache 2.x servers, you will need to: Configure web log file permissions Tail the web logs or use piped logs to enable Flume to get data from the web server This section will step through basic setup on default Ubuntu Lucid and default CentOS 5.5 installations. Then it will describe various ways of integrating Flume. If You are Using CentOS / Red Hat Apache Servers By default, CentOS’s Apache writes web logs to files owned by root and in group adm in 0644 (-rw-r–r–) mode. Flume is run as the flume user, so the Flume node is able to read the logs. Apache on CentOS/Red Hat servers defaults to writing logs to two files: /var/log/httpd/access_log /var/log/httpd/error_log The simplest way to gather data from these files is to tail the files by configuring Flume nodes to use Flume’s tail source: tail(“/var/log/httpd/access_log”) tail(“/var/log/httpd/error_log”) If You are Using Ubuntu Apache Servers By default, Ubuntu servers write web logs to files owned by root and in group adm in 0640 (-rw-r—–) mode. Flume is run as the flume user and by default will not be able to read the files. One approach to allow the flume user to read the files is to add it to the adm group. Apache servers on Ubuntu defaults to writing logs to three files: /var/log/apache2/access.log /var/log/apache2/error.log /var/log/apache2/other_vhosts_access.log The simplest way to gather data from these files is by configuring Flume nodes to use Flume’s tail source: tail(“/var/log/apache2/access.log”) tail(“/var/log/apache2/error.log”) tail(“/var/log/apache2/other_vhosts_access.log”) Getting Log Entries from Piped Log Files The Apache 2.x’s documentation describes using piped logging with the CustomLog descriptor. Their example uses the rotatelogs program to periodically write data to new files with a given prefix. Here are some example directives that could be in the httpd.conf/apache2.conf file. LogFormat “%h %l %u %t \”%r\” %&gt;s %b” common CustomLog “|/usr/sbin/rotatelogs /var/log/apache2/foo_access_log 3600? common TIP: In Ubuntu Lucid, these directives are in /etc/apache2/sites-available/default.  In CentOS 5.5, these directives are in /etc/httpd/conf/httpd.conf. These directives configure Apache to write log files in /var/log/apache2/foo_access_log.xxxxx every hour (3600 seconds) using the “common” log format. You can configure a Flume node to use Flume’s tailDir source to read all files without modifying the Apache settings: tailDir(“/var/log/apache2/”, “foo_access_log.*”) The first argument is the directory, and the second is a regex that should match against the file name.  tailDir will watch the directory and tail all files that have matching file names. Using Piped Logs Instead of writing data to disk and then having Flume read it, you can have Flume ingest data directly from Apache.  To do so, modify the web server’s parameters and use its piped log feature by adding some directives to the Apache server’s configuration: CustomLog "|flume node_nowatch -1 -n apache -c \'apache:console|agentBESink(\"collector\");\'" common CustomLog "|flume node_nowatch -1 -n apache -c \'apache:console|agentDFOSink(\"collector\");\'" common WARNING: By default, CentOS does not have Java required by the Flume node in user root‘s path. You can use alternatives to create a managed symlink in /usr/bin/ for the Java executable. Using piped logs can be more efficient, but is riskier because Flume can deliver messages without saving on disk. Doing this, however, increases the probability of event loss. From a security point of view, this Flume node instance runs as Apache’s user which is often root according to the Apache manual. NOTE: You could configure the one-shot mode node to deliver data directly to a collector. This can only be done at the best effort or disk-failover level. The prior examples use Flume nodes in one-shot mode which runs without contacting a master. Unfortunately, it means that one-shot mode cannot directly use the automatic chains or the end-to-end (E2E) reliability mode. This is because the automatic chains are generated by the master and because E2E mode delivers acknowledgements through the master. However, you can have a one-shot Flume node deliver data to a Flume local node daemon where the reliable E2E mode can be used. In this setup we would have the following Apache directive: CustomLog “|flume node_nowatch -1 -n apache -c \’apache:console|agentBESink(\”localhost\”, 12345);\’” common Then you can have a Flume node setup to listen with the following configuration: node : rpcSource(12345) | agentE2ESink(“collector”); Since this daemon node is connected to the master, it can use the auto*Chains. node : rpcSource(12345) | autoE2EChain; NOTE: End-to-end mode attempts to ensure of delivery of data that enters the E2E sink. In this one-shot-node to reliable-node scenario, data is not safe until it gets to the E2E sink. However, since this is a local connection, it should only fail when the machine or processes fails. The one-shot node can be set to disk failover (DFO) mode in order to reduce the chance of message loss if the daemon node’s configuration changes. Recently, we have committed a lightweight flume logger called flogger that is implemented in C++ by Cloudera Intern, Dani Rayan. This utility can be used in place of the one-shot Flume node to reduce the required resource footprint. This recipe is one of many from the growing Flume Cookbook. Currently we have written recipes for collecting data from syslog services, from scribe nodes, as well as techniques for testing Flume’s sources and sinks using the command line. If you have a Flume recipe you would like to share or would like to improve some our existing recipes, please contact us. We can add it to the Cookbook and help other users in the community! You can find us on IRC channel #flume at irc.freenode.net, on the flume-users mailing, or meet us in person in New York at Hadoop World 2010!</snippet></document><document id="602"><title>HUE SDK Training – NYC</title><url>http://blog.cloudera.com/blog/2010/09/hue-sdk-training-nyc/</url><snippet>Cloudera is offering several training sessions in New York City this October. These sessions not only provide you with the opportunity to accelerate your Hadoop education, they qualify you for complimentary entry to Hadoop World. Cloudera’s Hue SDK Training is one of these New York sessions and will be held on October 11th. Hue is an open source, Apache licensed unified user interface for Hadoop. Hue’s SDK provides back-end APIs to simplify interacting with Hadoop and front-end APIs to deliver rich, web-based, graphical user experiences. The session is designed for engineers with experience building web applications using AJAX  and modern MVC frameworks. Hue is implemented using Python and Django, so experience with these toolkits will be helpful. There are many interesting applications that can be built using Hue which will improve your overall Hadoop user experience and much more. Listed are some examples: You can create interfaces for complex components of CDH such as Hive, Zookeeper, Flume and Oozie; Create a single point-of-entry to your cluster making it easier to secure; Or build a visualization application that could visually present utilization in a Hadoop cluster similarly to how iTunes or Winamp does visualization of audio information. Cloudera’s Hue SDK Training will teach you: Hue architecture Architecture of a Hue application Hue configuration and deployment Hue APIs Back end APIs (interacting with Hadoop) Front end APIs (building graphically rich apps in the browser) To help you further absorb this Hue lesson, throughout the session, the Cloudera Hue Team will jointly build a fully-functional application with session attendees using the Hue framework. Learn more by clicking here.</snippet></document><document id="603"><title>CDH2 Update 2 Now Available</title><url>http://blog.cloudera.com/blog/2010/09/cdh2-update-2-now-available/</url><snippet>Cloudera is happy to announce the availability of the second update to version 2 of our distribution for Hadoop (CDH2). While new features are coming in version 3, we regularly update version 2 with improvements and bug fixes. CDH2 Update 2 contains a number of important fixes for Hadoop and Pig. Check out the release notes, Hadoop change log, and Pig change log. You can find the packages and tarballs on our website, or simply update your systems if you are already using our repositories. More instructions can be found in our CDH documentation. We appreciate feedback! Get in touch with us on the CDH user list, twitter or IRC (#cloudera on freenode.net) and let us know how the update is working for you.</snippet></document><document id="604"><title>Hadoop World Presentation Track Release</title><url>http://blog.cloudera.com/blog/2010/09/hadoop-world-presentation-track-release/</url><snippet>If you are already registered or plan to attend Hadoop World, then you must be excited for the release of the presentation track posted now on the Hadoop World webpage. Check out the 36 presentations—not including keynote speaker Tim O’Reilly— which will encompass multitudes of Hadoop-related information in a wide range of industries. We have speakers sharing Hadoop experiences in financial services, telecommunications, education, web and more. Below is listed only a few of the topics these presentations will cover: MapReduce and parallel database systems, Hadoop image processing for disaster relief, Sentiment analysis, Multi-channel behavioral analytics, And Hadoop-related projects such as HBase, Hive, Hue and more. Note that all of these presentations will be sharing real use-case examples and scenarios from their own experiences with Hadoop. So check out the presentation track and plan your line of attack. There is a plethora of information to digest along the Hadoop World presentation track so be sure to plan your path in advance and coordinate with a colleague or friend if needed. If you have yet to register, you can sign up now with the below link. We’re excited to share this Hadoop New York experience with you!</snippet></document><document id="605"><title>A Summer Internship with Cloudera</title><url>http://blog.cloudera.com/blog/2010/09/a-summer-internship-with-cloudera/</url><snippet>As the summer comes to a close it is time to thank and congratulate all of our summer interns for all their hard-work and contributions. This post was written by Lisa Chen, as she describes her Cloudera experience, and the development of Sledgehammer. When I first came to Cloudera, I only had a very vague notion of what I’d be working on during my eleven week internship.  Alex Loddengard, who was my mentor and guidance throughout the internship, had told me early on that I’d probably be building a tool used for community status reporting and lead generation.  The details of what I would be implementing was left up to me to decide. So I began asking around within the company, for what people might like to see.  I quickly learned that Jeff Hammerbacher, a co-founder of Cloudera, sends emails regularly to the sales team regarding potential customer leads at all hours of the day.  The general consensus was that, it would be really cool if I could build something that automates some of what Jeff does, by crawling a few of the sites that he frequents.  (Of course, nothing I could build in 11 weeks or so would even come close to the extent of the work Jeff actually does, but I could try, right?) From there, I had some idea of what I wanted to build, and next I needed to figure out how to build it.  In the first two weeks of my internship, I attended two training sessions on Hadoop and development by Sarah, read about half of Tom’s book, and learned how to use git, Hadoop, Flume, and write map-reduce code. With all the new knowledge I learned, I spent the next several weeks implementing the first stages of the project, until I eventually had something that could run without crashing every other time, and I could submit my code for review.  When reviews came back to me, I made changes as needed, committed the code, and moved onto something else.  Most of the time, it meant back to step one: figuring out what I wanted to build next.  It was like an infinite cycle, and even now, Sledgehammer still has a long ways to go to maximize it’s full potential. So, what does Sledgehammer actually do?  Well, right now it polls data from the Apache Hadoop and Hbase archives and sub-archives, and outputs a list of potential customers in comma separated value format, in order of potential.  And this potential is based off a scoring system that I’ll go into with more depth later.  The results list is integrate-able with Sales Force.  It also creates two other reports, one each for mbox files (from the Apache mailing lists), and twitter.  All these reports together provide a rich mass of data that can be back-referenced, or used for other purposes.  It’s really easy to look up more information about someone if you want to know more about them by googling some part or all of their provided info. Scoring is based off of whatever the user provides, and how active they are where, and the age of their activity.  For example, if someone uses their company email address to send mail to the relevant Apache mailing lists, they’ll get +50 points for providing company information.  Someone that uses twitter gets +15 points for each of the following fields, if provided : biography, website, location.  If someone has both a company name and a twitter username, that means that this person is active in both places, and gets a huge bonus of 2500 points, to propel them higher on the priority list.  A person also gets 3 points for every tweet, and 3 for each mail sent, and the total of everything is the final score that they are ranked by.  Of course right now, the results are far from ideal.  HDFS has data from 2007 until the present from the Apache mailing lists, and from Twitter, it only has data from about five weeks ago, when I learned how to use Flume.  That means right now priority is skewed heavily in favor of mailing list users, but over time it should even out, and more people will have a more complete information set in the final report. Now that I’ve covered the basics of Sledgehammer and what it does, I’ll elaborate more on the technical parts of the project.  Sledgehammer currently basically works in four distinct phases: locate a source, retrieve data into HDFS, parse and process the retrieved data, and output the results in a consumable format.  I’ve currently written 19 JAVA source files for it up to date, and the entire project can be built from the root directory of the repository with ant.  In total, the project consists of 3 map-reduce jobs, and one custom made crawler that fetches mbox files, and Twitter feeds are fetched with Flume.  Sledgehammer is currently running on the customers.sf.cloudera.com cluster, which is also accessible through HUE. The MboxCrawler that I wrote for Sledgehammer is a tool that finds and downloads all relevant mbox archives to a location I specify in HDFS.  It begins at the http://mail-archives.apache.org/mod_mbox/, the general mailing list archive home page, where it picks out and builds the URLs of Hadoop and Hbase related archives and sub-archives.  From there it polite queries the source code of each of the sub-archives into a String object, and parses the String to build the URL for the actual .mbox file for each archive  Finally it downloads the source of the .mbox files into HDFS.  The MboxCrawler avoids redundancy each run-through, by comparing the size of an existing file in HDFS with the size of the new file it may or may not want to pull, and if the new file is larger, the crawler will replace the existing file with the new one.  Otherwise, it just skips it. Twitter is polled much more simply.  All I had to do, was figure out which tags I wanted (and I picked Hadoop, Hbase, and Cloudera), and configure Flume to poll from their RSS feed.   Flume is amazing!  All I had to do was specify the source sink, which was the URL of the twitter RSS feed, and the collector sink, which was HDFS, and it automatically pulled the JSON file straight into HDFS for me.  I then set up a cron job, to have Flume fetch the feeds for me hourly, automatically. After I had all the data I wanted collected, it was time to figure out how to pick out only what I wanted, and discard everything else.  I began writing map-reduce jobs to do this, and in the process, ran into various problems that would inevitably end up crashing my job.  One of the biggest obstacles in my way, was figuring out how to ideally parse mbox files.  For some reason, every other Apache mailing list is structured slightly differently, and sometimes two emails in the same mbox file could be formatted differently.  This made parsing a nightmare.  I ended up writing code that would parse almost every type of possible format, and if any one email tried to crash the job, that email would simply be caught and discarded, and would increase a ‘skipped’ counter.  The end result worked out pretty well, and while about 1% of the results still end up with failed formatting, it wasn’t consequential enough to dwell on. For Twitter, I originally thought that all I had to do was use the JSON parsers that were readily available to me, and I’d be done.  Unfortunately that wasn’t the case.  JSON escaping isn’t exactly the greatest thing ever, and every time I ran into an out of place curly bracket, the entire job would crash.  What I ended up doing was completely removing all unwanted curly brackets, at the risk of formatting failure, and ignoring the tweets that failed. That wasn’t the end of fetching information from Twitter however.  From the JSON files that Flume pulled for me, I could get a user’s username and tweets, but not their profile information, which was the most valuable to me.  I also couldn’t fetch their profiles in the map-reduce process, because hitting Twitter with 10 queries simultaneously per second was probably going to get me banned from their site.  To implement polite querying, I had the map-reduce job write out a list of usernames and their respective tweets, and when the job was done, read them into two different queues.  I then built each user’s profile URL from their username, and, with a timed delay between queries, fetched each user’s profile separately, parsed it, and queued it into a third queue.  Finally when all three of my queues were full, I had the MR driver dequeue each item into a report in HDFS.  I should probably also note here, that map-reduce really dislikes non-English characters.  I was forced to remove all users with non-English characters anywhere in their profile or tweets. With preliminary reports generated, and ready to go, I sought out more feedback for next steps.  I had about two weeks left in my internship at this point, and that was still plenty of time to add cool, additional features on top of what I already had.  After hearing from several people, I decided to merge the results from the two separate sources into one final output report. I wrote a third map-reduce job to parse both mbox and twitter reports, and which wrote the information into a custom writable.  The reduce part of my MR job then combined all the mapped custom writables into one, which then generated a score when asked to, based off the information it had, and wrote a .csv file to HDFS. The last step was to get the recently created .csv file to sort itself in order of score.  I wrote a custom JAVA bean object that separated the score for each line/person from everything else (which got stored in a String), and wrote a custom CompareTo() method for the bean.  Arrays.sort() then took care of everything else, and I had the merge map-reduce job’s driver write the new, sorted array back out to HDFS, and that’s about all that Sledgehammer currently does. There are so many unimplemented possibilities that, had I more time, would definitely try to add to Sledgehammer.  Amongst all the feedback I got, these things stood out to me the most.  I think it’d be really nice to have a third source, from blogs, also feeding the Sledgehammer information database.  I’d also like to see it automatically cross-referencing Google or Linked-In, or maybe even both.  A cool, non-lead generation related add-on could be to use the existing data and run a status/activity analysis tool through it and generate a report related to that.  Also, sorting the current output by company activity might also be something that could be looked into.  There’s just so much data, all in one place, that people could do really cool things with it – possibly even at a hackathon. But anyway, winding down, I should probably mention that Sledgehammer is far from perfect right now, and even the initial setup process could use a little more work.  Installation and deployment setup varies from system to system, and the provided README may not cover every possible installation issue.  There are also several minor inconveniences required of the user, before everything will run properly, and those are specified in the README. All in all, I had a great time working on this project, and with all the brilliant people I’ve had the pleasure to meet at Cloudera.  It was an amazing experience, and I can honestly say that I learned so much more than I’d ever expected to, and loved every minute of it.</snippet></document><document id="606"><title>New York Training Session for Managers Interested In Hadoop</title><url>http://blog.cloudera.com/blog/2010/09/new-york-training-session-for-managers-interested-in-hadoop/</url><snippet>Hadoop Essentials for Managers is a one-day course provided October 11th—the day prior to Hadoop World—that will provide decision-makers with the information they need about Apache Hadoop. In this session we will answer questions such as: When is Hadoop appropriate? What are people using Hadoop for? How does Hadoop fit into our existing environment? What do I need to know about choosing Hadoop? You will hear use cases and case studies that help to explain what sort of problems Hadoop can solve. We will explain the Hadoop ecosystem, and how Hadoop-related projects such as Hive, Flume, Hue and others can help developers and administrators take best advantage of Hadoop’s power. We’ll help you understand how Hadoop can integrate into your existing systems architecture, and provide information on how to manage your Hadoop cluster: the people resources required, the hardware required, the overall costs, and how it can scale for growth. By attending any of the Cloudera training sessions surrounding Hadoop World—which includes Hadoop Essentials for Managers—you gain complimentary entry to the conference! Take advantage of this opportunity to learn about Hadoop as well as network with potential partners, clients and friends at the conference. Register for Hadoop Essentials for Managers now! See list of trainings. Sign up for Hadoop World!</snippet></document><document id="607"><title>Flume community update: September 2010</title><url>http://blog.cloudera.com/blog/2010/09/flume-community-update-september-2010/</url><snippet>The past month has been exciting and productive for the community using and developing Cloudera’s Flume!  This young system is a core part of Cloudera’s Distribution for Hadoop (CDH) that is responsible for streaming data ingest.  There has been a great influx of interest and many contributions, and in this post we will provide a quick summary of this month’s new developments. First, we’re happy to announce the availability of Flume v0.9.1 and we will describe some of its updates. Second, we’ll talk about some of the exciting new integration features coming down the pipeline. Finally we will briefly mention some community growth statistics, as well as some recent and upcoming talks about Flume. Flume v0.9.1 Flume v0.9.1 is now available both in tarball and packaged forms. This version resolves 63 issues and contains several key improvements and bugs fixes. Much of this release is focused on improving the stability of Flume’s internals to help users quickly get Flume up and running and to help developers build extensions to Flume. You can download the new release as an update to your Redhat RPM or Debian DEB based package managers. Or, you can download it in tarball form from Cloudera’s archive, or as always from the Cloudera’s github repository . The key functional highlights include: Support for gzip compressed output files. New and improved sources: scribe, syslog, tailDir (tail all files in a directory) Significant robustness improvements when using in the disk fail-over and end-to-end reliability modes. Significant robustness improvements when reconfiguring, commissioning, and decomissioning logical nodes. To improve the documentation and enhance debugging support, we have added: A new section of the manual that explains how to build your own flume source, sink, and decorator plugins by example. An ‘ant eclipse’ option to automatically build project files for developing in the Eclipse IDE. Improved error messages in logs, exposed Flume internals such as current configuration properties, and source/sink catalogs to ease operator and developer debugging and verification. For more details, read the full release notes. Up and coming Flume features One of Flume’s key design principles is extensibility. We are happy people are taking advantage of this to integrate Flume with other systems. Some new features currently being developed will enable the next release of Flume to have greater integration with CDH’s core components as well as other systems in the Hadoop ecosystem. Here are some of the new major contributions near completion or actively in the works: Flume + Hive integration plugin. Mozilla’s Anurag Phadke has been working with Cloudera’s Carl Steinbach to automatically import data ingested by Flume into Hive warehouses. Flume + HBase integration plugin. Several guests at the recent Cloudera Hackathon improved upon our initial Flume/HBase connector and posted it so the community could continue improving it. Since then, a more generic design was proposed and Cloudera’s new intern, Dani Rayan, has volunteered to implement it. Flume + Cassandra integration plugin. Tyler Hobbs contributed a first version of this plugin.  It is blocked by some Thrift compatibility and dependency issues. Secured data transport via TLS. Kim Vogt and Ben Standefer from SimpleGeo, with some feedback from David Zuelke of Bitextender have been working on adding TLS-based wire encryption to the RPC sources and sinks to provide secure data center communications. Flume + Kerberized HDFS integration. Flume takes its first steps to support the newer versions of HDFS that require Kerberos authentication in order to read from and write to HDFS. Generic compression codec support for output files. This enables users to choose from all of the codecs Hadoop supports: gzip, bzip2, and deflate.  It should also enable the LZO codec with a little extra work. Documentation improvements galore. Currently in the works are a semantics specification for sources and sinks, and step-by-step instructions for connecting Flume to common sources such as  Apache web servers, syslog, and existing scribe loggers. Community We are really grateful to the folks who have been exploring and talking about the project!  The guests (Dustin Sallings of NorthScale and Ron Bodkin among others…) who tried out Flume at the Cloudera’s Hackathon day gave us valuable feedback.  In the past month, Cloudera’s Henry Robinson presented “Inside Flume” at Hadoop Day in Seattle.   It is also great to see that some folks are slated to present at Hadoop World 2010 about integrating and using Flume.  Otis Gospodnetic of Sematext will be talking about analytics with Flume and HBase. Also, Anurag Phadke from Mozilla will be presenting a talk about of his experiences integrating Flume-collected data automatically into Hive. He recently posted some details in his blog. It is great to see the community growing and we love hearing from all of you as well! It has been two months since Flume was open sourced, and our main github repository now has 136 watchers and 24 forks.  Our user mailing list has 102 members and our developers mailing list has 41 members. Please join us! If you are using Flume and want to keep up with where it is going, join the mailing lists and follow us on Twitter at @cloudera and #flume.  If you need help, just send questions to the mailing lists or chat with us directly in IRC on channel #flume at irc.freenode.net.  To meet the Flume Team and contributors in person, you should join us in New York City at Hadoop World on October 12th! It has been a lot of fun so far, and we’re really looking forward to the following months! Thanks from everyone on the Cloudera Team.</snippet></document><document id="608"><title>Purdue University’s Saptarshi Guha Interviewed Regarding Hadoop, R and Hadoop World</title><url>http://blog.cloudera.com/blog/2010/09/purdue-university%e2%80%99s-saptarshi-guha-interviewed-regarding-hadoop-r-and-hadoop-world/</url><snippet>In anticipation of Hadoop World 2010 in New York – October 12th, we continue our Q&amp;A series with Hadoop World presenters to provide a taste of what attendees can expect. We’re excited about the 36 presentations that are planned (see agenda) including talks from eBay, Twitter, GE, Facebook, Digg, HP and more. Tim O’Reilly, founder of O’Reilly Media is keynoting, which should be inspiring as well as thought provoking. Everyone who registers for Hadoop World will receive a free copy of the second edition of Tom White’s Hadoop: The Definitive Guide. Hadoop World 2010 presenter Saptarshi Guha works in the Department of Statistics at Purdue University. His presentation for Hadoop World is titled “Using R and Hadoop to Analyze VoIP Network Data for QoS.” Guha has been developing with Hadoop and R for over a year. Q: What can attendees expect learn about Hadoop from your presentation at Hadoop World? The quality of VoIP calls are suspect to the queuing effects introduced by the network gateways. The jitter between two consecutive packets is the deviation of the real inter-arrival time from theoretical. We use the R environment for the statistical analysis of data to show jitter follows desired properties and is negligible, which demonstrates that the measured traffic is close to the offered traffic.  Data sets used to study the departure from offered load can be massive and require detailed study of several complex data structures. Using an environment that integrates R and Hadoop, we hope to demonstrate the effectiveness of R and Hadoop for the comprehensive statistical analyses of massive data sets. Q: Describe use cases for Hadoop at Purdue. Our team works with large amounts of network traffic data collected for VoIP and network security projects. Our language of analysis is almost exclusively R and we need a way to store the 190 gigabytes of VoIP related data, create data structures for analysis and compute across these. The R and Hadoop combination allows us to do all of this in a manner that scales with the size of the data and returns results within acceptable time frames. Despite not having HBase installed, we use Hadoop map files and R to query data structures from a database of 14 million objects spanning 21GB within seconds. Q: What benefits do you see from Hadoop? The biggest win is the reduction in computing time, the ease of programming in the R and Hadoop environment and the Hadoop Distributed Filesystem. We have stopped worrying about disk space and freely store as many databases of objects as required. It must be mentioned, that Hadoop DFS and MapReduce are both very easy to setup and return very impressive results. For our approach to analysis, the Hadoop MapReduce paradigm fits very well. We partition the data into many subsets (usually by the levels of categorical variables), compute across these and recombine the results. We also visualize a subset of these and recombine the results in to multi panel multi page displays, which are viewed across large 30″ monitors. Q: What did you use before Hadoop? Some of the things we have done were impossible without Hadoop. Before this we used a tree hierarchy of directories of flat files containing R objects and index files to locate objects with in these flat files. Distributing computation across our cluster was a laborious, manual and very project specific affair but now using the R and Hadoop system the we have sufficiently abstracted the workflow to span a multitude of data sets. Q: How has Hadoop improved your work at Purdue? Hadoop has certainly improved our workflow, allowing the researchers to think about studying the data rather than how to distribute code and data, how to maintain a cluster, or how to tackle tedious but vital things such as computer failure. Because the time to compute is substantially less the researchers have the flexibility to implement their ideas and interactively analyze the data. We hope to increase our cluster size and bring more people into the fold. Q: What are you hoping to get out of your time at Hadoop World? To demonstrate that it is indeed possible to comprehensively analyze gigabytes of data with a level of detail that was only possible with small data sets and to learn of new Hadoop related technologies that might benefit our workflow. Hear more from Guha at Hadoop World in New York! For more information regarding the conference its-self click here.</snippet></document><document id="609"><title>A Look Back at August Posts</title><url>http://blog.cloudera.com/blog/2010/09/summary-of-august-posts/</url><snippet>Migrating to CDH – August 2 You will learn everything you need to know about migrating with CDH3b2 ranging from why migrate to testing. Flume community update - August 3 In this blog we address Flume issues, talk about new features, and the improvement of the platform. Hadoop World: early-bird rate ends on August 11 – August 9 The early-bird registration window may have passed, however, it is not too late to register for Hadoop World. Register Now! Cloudera’s Henry Robinson to speak at Hadoop Day in Seattle - August 10 At Seattle’s Hadoop Day Cloudera’s Henry Robinson gave a speech entitled “Inside Flume.” Click the above title to read a short abstract, and the slides of his presentation are available by clicking here. CDH3b2 Release Recap – August 11 This post briefly explains CDH3b2 and contains links to the components of this package. Avoiding Common Hadoop Administration Issues – August 12 Cloudera’s support services see many issues on a regular basis. This post runs through some common administration issues we have come across and how to avoid them. Hadoop/HBase Capacity Planning – August 17 This blog provides guidance in sizing your first Hadoop/HBase cluster. Hadoop World: NYC – Training – August 19 This post contains brief summaries of the trainings available surrounding Hadoop World. Space is still available, sign up now! Improving Hotel Search: Hadoop @ Orbitz Worldwide – August 23 Jonathan Seidman provides a use-case from Orbitz, which will be further covered at Hadoop World on October 12th. Hadoop Administrator Training Comes to London – August 24 There will be Hadoop Training in London for Administrators and Developers alike. These sessions are September 9th, so sign up ASAP, use the promotion code in this post for a discount! Using Hadoop for Fraud Detection and Prevention – August 24 Hadoop has been proven very useful in fraud detection and prevention. This post covers how Hadoop can help solve these problems. What’s New in Apache Hadoop 0.21 – August 26 Tom White, author of Hadoop: The Definitive Guide gives us an overview of the changes and improvements with Apache Hadoop 0.21. Hadoop World 2010: Speaker Highlights – August 30 A brief glimpse into three of the thirty-six presentations that will be given at Hadoop World. This blog has presentation abstracts for General Electric and eBay. This concludes the summary of Cloudera blog posts for August 2010. Be sure to follow Cloudera as we will continue to provide updates on Apache Hadoop and Hadoop-related projects. Be sure to attend Hadoop World October 12th in New York City!</snippet></document><document id="610"><title>Tracing with Apache Avro</title><url>http://blog.cloudera.com/blog/2010/09/tracing-with-avro/</url><snippet>Written by Patrick Wendell, an amazing summer intern with Cloudera and an Avro Committer.   In my summer internship project at Cloudera, I added RPC tracing as a first-order feature of Apache Avro. Avro is a platform for data storage and exchange that caters to data-intensive, dynamic applications. My project focused on Avro�s RPC functionality. It is common knowledge that tracing in distributed systems can be difficult. In user-facing web services, a front-end function may recursively trigger several function calls to mid and back-tier services. In offline processing, data-center storage layers may distribute data across several hosts, querying one or many of them when a client requests a file. In either case, the inter-dependency of components makes it difficult to pinpoint the source of a slowdown or hang-up when they inevitably occur. AvroTrace is designed as a first responder for diagnosing problems in distributed systems that use Avro for RPC transport. It has two components, a real-time monitoring dashboard and an offline trace analyzer. Both run as low-overhead Avro plugins which store and propagate tracing meta-data among RPC clients and servers. The monitoring dashboard is accessible via a web interface on any Avro server, delivering a �snapshot� of the most recent RPC activity. The offline analysis tool offers a basic interface for collecting, aggregating, and analyzing this data to identify problem spots. It is largely based on Google�s Dapper tracing infrastructure, which is itself inspired by X-Trace and other academic tracing research. Below is an example trace analysis of a recursive RPC call pattern. In the example application, �one remote call, getFile() triggers two other RPC�s, getFileContents() and getFileMeta(). Avro�s tracing has detected this particular pattern and offers a dashboard view summarizing average timing and payload data. It is also showing detailed graphs for one of the specific nodes in this pattern, getFileContents() presenting a visual history of timing (top) and payload (bottom) analytics. Turnkey tracing is just one of many reasons to use Avro. �I recently became a committer on the Avro project and I look forward to supporting and improving trace functionality in the coming months! *Click on any of the graphs or stats for a larger version Learn more about Avro and other Hadoop projects at Hadoop World!</snippet></document><document id="611"><title>Infochimp’s President, Philip Kromer, Interviewed Regarding Hadoop and Hadoop World</title><url>http://blog.cloudera.com/blog/2010/09/infochimps-president-philip-kromer-interviewed-regarding-hadoop-and-hadoop-world/</url><snippet>Excitement is building as Hadoop World nears and we are sitting down with some of our presenters to ask them a few questions regarding their presentations and how they are using Hadoop within their organization. Here we speak with Philip Kromer, President of Infochimps, who  answers  questions regarding his presentation, how Hadoop is used in his business, and what he aims to get out of Hadoop World. Philip’s presentation at Hadoop World is about the development of a data marketplace and commoditization, and their chimpanzee-style approach to data processing. Attend Hadoop World October 12th in New York to hear more from and to talk to Philip. What can attendees expect learn about Hadoop from your presentation at Hadoop World? We’re now able to quantify aspects of human behavior never before accessible. Twitter, the News stream, the Smart Grid, are exquisite lab instruments for measuring ‘Conversation’, ‘Interest’, ‘Activity’. What’s more, with enough data machine-learning algorithms and big data tools let us expose insight using only the *structure*, not the content of the data. The massive quantity and connectivity required demands industrial-strength tools such as Hadoop. We do *all* our data processing in high level tools (chiefly Pig and Wukong) — “black boxes with flexible glue”. We use ‘programmer fun’ + ‘programmer time’ as our primary development  metrics. Together, writing simple loosely coupled scripts lets us run the fast experiment-driven design cycles that a lean startup demands. It has also let us grow our own talent and recruit outside CS (physicists, in particular, dream in map reduce). I think this approach should have strong appeal to small- and medium-sized businesses, or anyone looking for low barrier-to-adoption of Hadoop. Do you have Hadoop in production use today? We have Hadoop in heavy production use for ad-hoc analysis and for automated processes digesting terabytes of data. Can you describe some use cases for Hadoop in your business? We have scraped data from around the web, principally Social Networks. We use Hadoop for processing it on its own and to mash it up with other open &amp; commercial datasets. Examples: We have a collection of 3 billion tweets (twitter messages) from 60+million users that we tokenize into 16B+ usages of 65M terms — more than a terabyte of data on its own. Using Pig and Wukong we can identify whom to follow, to understand how events and news stories resonate, and even to find dates. MLB has released a dataset describing the trajectory and full game state for every pitch of every game for the past several seasons.  Smashing this against the hourly weather data produces a laboratory able with the potential to describe the physics of a knuckleball or the performance for pitcher’s age vs. game-time temperature. How do you support Hadoop? Operationally,  we use the Amazon cloud and a collection of Chef recipes (that we’ve open-sourced). These let us spin up, use, and spin down clusters of one to hundreds of machines, using either local (persistent) HDFS or just push/pull from Amazon S3. We have also been supporting Hadoop by giving back to the Hadoop open-source community. Wukong (our Ruby-language toolkit for Hadoop), which we believe is the easiest and most fun way to write map-reduce programs. At Hadoop World we’ll be announcing Chimpmark, a target benchmark for implementers and users of big data tools. It’s a collection of large scale datasets, accompanying challenges, and reference implementations that let you profile, tune and more deeply understand your hadoop system. ClusterChef, the cluster management toolkit I described above. How has Hadoop improved your business? Most of the stuff we use Hadoop for would be otherwise impossible. What are you hoping to get out of your time at Hadoop World? Learn Ideas. Popularize and receive feedback on the development of a data marketplace. Hear where the world of Big Data is going. At Hadoop World you can hear more from Philip Kromer as well as any of the thirty-five other presenters! Click here to register right away!</snippet></document><document id="612"><title>Register for Hadoop Training in New York and Get into Hadoop World for Free!</title><url>http://blog.cloudera.com/blog/2010/09/register-for-hadoop-training-in-new-york-and-get-into-hadoop-world-for-free/</url><snippet>That’s right, sign up for any of the training courses surrounding Hadoop World 2010, and receive a complimentary pass to the conference! There are seven different courses on offer, so whether you are new to Hadoop or looking to deepen your skills, you’ll find something to fit your needs. If you are a manager trying to decide whether Hadoop is an appropriate technology for your organization, Hadoop Essentials for Managers will answer your questions. We will show you when using Hadoop is appropriate, what Hadoop is being used for in a range of industries, how Hadoop fits into your existing environment and what you need to know in order to deploy it within your organization. Why not turn your Hadoop World trip into a multiple day Hadoop learning extravaganza by attending one of our two-day sessions? Both the developer and administrator training courses culminate in an exam which, when passed, confers Cloudera Certified Hadoop Developer or Administrator status. For the developer with an existing understanding of Hadoop and ready to utilize Hive and Pig for their data analysis, there is a two-day class teaching you how to process data using filters, joins, user-defined functions and more. For those looking to deploy HBase, consider our one-day HBase training session. Learn how to use HBase as a distributed data store to achieve low-latency queries and highly scalable throughput. This class covers HBase architecture, data modeling, and the Java API as well as some advanced topics and best practices. If you’re a developer who is completely new to Hadoop, we have put together a course that will provide you with a solid foundation in large scale data processing using MapReduce and Hadoop. This course is purposely offered the day before Hadoop World, so that while in attendance you will be able to better grasp the topics at the conference with your fresh Hadoop knowledge. Once you have taken this course and are comfortable with Hadoop, feel free to also enroll in a training course followed by Certification to document your new-found Hadoop knowledge. For developers who wish to simplify interacting with Hadoop, Cloudera HUE provides back- and front-end APIs to deliver a rich, web-based, graphical user experience. This class covers using the HUE APIs to develop your own rich, graphical applications built on top of the HUE platform. Once again, you will receive free entry to Hadoop World if you are registered in any of the training sessions surrounding the event! Don’t miss out on this opportunity to broaden your knowledge, and we hope to see you there!</snippet></document><document id="613"><title>Hadoop World 2010: Speaker Highlights</title><url>http://blog.cloudera.com/blog/2010/08/hadoop-world-2010-speaker-highlights/</url><snippet>Hadoop is increasingly being adopted by many Fortune 500 enterprises. Some of the speakers featured at Hadoop World this year include leading companies who have been able to create new value for their business using Hadoop. The presentations at Hadoop World are focused on how Hadoop is solving business problems for these enterprises.  Below are three examples of leading enterprises that will present how Hadoop has impacted their businesses. GE, Product Manager, Linden Hillenbrand, will be talking about how Hadoop has improved GE’s Marketing &amp; Communications functions.  One capability GE has implemented is assessing the external perception of GE–positive, neutral, or negative–through various marketing campaigns. eBay Engineering director of Analytical Platform Development, Anil Madan, is presenting “Hadoop at eBay.” One of eBay’s largest assets is the large amount of user data they have collected. By sourcing huge volumes of this data into the HDFS cluster and running click stream and transactional data analysis eBay gets a better understanding of user behavior as well as search quality. Hadoop World is a great way to learn how Hadoop is being used to power today’s modern enterprises. These presentations will help you understand how Hadoop improves your data storage and processing environment and directly impacts your business. Don’t miss out! Register now</snippet></document><document id="614"><title>What’s New in Apache Hadoop 0.21</title><url>http://blog.cloudera.com/blog/2010/08/what%e2%80%99s-new-in-apache-hadoop-0-21/</url><snippet>Apache Hadoop 0.21.0 was released on August 23, 2010. The last major release was 0.20.0 in April last year, so it’s not surprising that there are so many changes in this release, given the amount of activity in the Hadoop development community. In fact, there were over 1300 issues fixed in JIRA (Common, HDFS, MapReduce), the issue tracker used for Apache Hadoop development. Bear in mind that the 0.21.0 release, like all dot zero releases, isn’t suitable for production use. With such a large delta from the last release, it is difficult to grasp the important new features and changes. This post is intended to give a high-level view of some of the more significant features introduced in the 0.21.0 release. Of course, it can’t hope to cover everything, so please consult the release notes (Common, HDFS, MapReduce) and the change logs (Common, HDFS, MapReduce) for the full details. Also, please let us know in the comments of any features, improvements, or bug fixes that you are excited about. You can download Hadoop 0.21.0 from an Apache Mirror. Thanks to everyone who contributed to this release! Project Split Organizationally, a significant chunk of work has arisen from the project split, which transformed a single Hadoop project (called Core) into three constituents: Common, HDFS, and MapReduce. HDFS and MapReduce both have dependencies on Common, but (other than for running tests) MapReduce has no dependency on HDFS. This separation emphasizes the fact that MapReduce can run on alternative distributed file systems (although HDFS is still the best choice for sheer throughput and scalability), and it has made following development easier since there are now separate lists for each subproject. There is one release tarball still, however, although it is laid out a little differently from previous releases, since it has a subdirectory containing each of the subproject source files. From a user’s point of view little has changed as a result of the split. The configuration files are divided into core-site.xml, hdfs-site.xml, and mapred-site.xml (this was supported in 0.20 too), and the control scripts are now broken into three (HADOOP-4868): in addition to the bin/hadoop script, there is a bin/hdfs script and a bin/mapreduce script for running HDFS and MapReduce daemons and commands, respectively. The bin/hadoop script still works as before, but issues a deprecation warning. Finally, you will need to set the HADOOP_HOME environment variable to have the scripts work smoothly. Common The 0.21.0 release is technically a minor release (traditionally Hadoop 0.x releases have been major, and have been allowed to break compatibility with the previous 0.x-1 release) so it is API compatible with 0.20.2. To make the intended stability and audience of a particular API in Hadoop clear to users, all Java members with public visibility have been marked with classification annotations to say whether they are Public, or Private (there is also LimitedPrivate which signifies another, named, project may use it), and whether they are Stable, Evolving, or Unstable (HADOOP-5073). Only elements marked as Public appear in the user Javadoc (Common, MapReduce; note that HDFS is all marked as private since it is accessed through the FileSystem interface in Common). The classification interface is descibed in detail in Towards Enterprise-Class Compatibility for Apache Hadoop by Sanjay Radia. This release has seen some significant improvements to testing. The Large-Scale Automated Test Framework, known as Herriot (HADOOP-6332), allows developers to write tests that run against a real (possibly large) cluster. While there are only a dozen or so tests at the moment, the intention is that more tests will be written over time so that regression tests can be shared and run against new Hadoop release candidates, thereby making Hadoop upgrades more predictable for users. Hadoop 0.21 also introduces a fault injection framework, which uses AOP to inject faults into a part of the system that is running under test (e.g. a datanode), and asserts that the system reacts to the fault in the expected manner. Complementing fault injection is mock object testing, which tests code “in the small”, at the class-level rather than the system-level. Hadoop has a growing number of Mockito-based tests for this purpose (MAPREDUCE-1050). Among the many other improvements and new features, a couple of small ones stand out: the ability to retrieve metrics and configuration from Hadoop daemons by accessing the URLs /metrics and /conf in a browser (HADOOP-5469, HADOOP-6408). HDFS Support for appends in HDFS has had a rocky history. The feature was introduced in the 0.19.0 release, and then disabled in 0.19.1 due to stability issues. The good news is that the append call is back in 0.21.0 with a brand new implementation (HDFS-265), and may be accessed via FileSystem‘s append() method. Closely related—and more interesting for many applications, such as HBase—is the Syncable interface that FSDataOutputStream now implements, which brings sync semantics to HDFS (HADOOP-6313). Hadoop 0.21 has a new filesystem API, called FileContext, which makes it easier for applications to work with multiple filesystems (HADOOP-4952). The API is not in widespread use yet (e.g. it is not integrated with MapReduce), but it has some features that the old FileSystem interface doesn’t, notably support for symbolic links (HADOOP-6421, HDFS-245). The secondary namenode has been deprecated in 0.21. Instead you should consider running a checkpoint node (which essentially acts like a secondary namenode) or a backup node (HADOOP-4539). By using a backup node you no longer need an NFS-mount for namenode metadata, since it accepts a stream of filesystem edits from the namenode, which it writes to disk. New in 0.21 is the offline image viewer (oiv) for HDFS image files (HADOOP-5467). This tool allows admins to analyze HDFS metadata without impacting the namenode (it also works with older versions of HDFS). There is also a block forensics tool for finding corrupt and missing blocks from the HDFS logs (HDFS-567). Modularization continues in the platform with the introduction of pluggable block placement (HDFS-385), an expert-level interface for developers who want to try out new placement algorithms for HDFS. Other notable new features include: Support for efficient file concatenation in HDFS (HDFS-222) Distributed RAID filesystem (HDFS-503) – an erasure coding filesystem running on HDFS, designed for archival storage since the replication factor is reduced from 3 to 2, while keeping the likelihood of data loss about the same. (Note that the RAID code is a MapReduce contrib module since it has a dependency on MapReduce for generating parity blocks.) MapReduce The biggest user-facing change in MapReduce is the status of the new API, sometimes called “context objects”. The new API is now more broadly supported since the MapReduce libraries (in org.apache.hadoop.mapreduce.lib) have been ported to use it (MAPREDUCE-334). The examples all use the new API too (MAPREDUCE-271). Nevertheless, to give users more time to migrate to the new API, the old API has been un-deprecated in this release (MAPREDUCE-1735), which means that existing programs will compile without deprecation warnings. The LocalJobRunner (for trying out MapReduce programs on small local datasets) has been enhanced to make it more like running MapReduce on a cluster. It now supports the distributed cache (MAPREDUCE-476), and can run mappers in parallel (MAPREDUCE-1367). Distcp has seen a number of small improvements too, such as preserving file modification times (HADOOP-5620), input file globbing (HADOOP-5472), and preserving the source path (MAPREDUCE-642). Continuing the testing theme, this release is the first to feature MRUnit, a contrib module that helps users write unit tests for their MapReduce jobs (HADOOP-5518). Other new contrib modules include Rumen (MAPREDUCE-751) and Mumak (MAPREDUCE-728), tools for modelling MapReduce. The two are designed to work together: Rumen extracts job data from historical logs, which Mumak then uses to simulate MapReduce applications and clusters on a cluster. Gridmix3 is also designed to work with Rumen traces. The job history log analyzer is another tool that gives information about MapReduce cluster utilization (HDFS-459). On the job scheduling front there have been updates to the Fair Scheduler, including global scheduling (MAPREDUCE-548), preemption (MAPREDUCE-551), and support for FIFO pools (MAPREDUCE-706). Similarly, the Capacity Scheduler now supports hierarchical queues (MAPREDUCE-824), and admin-defined hard limits (MAPREDUCE-532). There is also a brand new scheduler, the Dynamic Priority Scheduler, which dynamically changes queue shares using a pricing model (HADOOP-4768). Smarter speculative execution has been added to all schedulers using a more robust algorithm, called Longest Approximate Time to End (LATE) (HADOOP-2141). Finally, a couple of smaller changes: Streaming combiners are now supported, so that the -combiner option may specify any streaming script or executable, not just a Java class. (HADOOP-4842) On the successful completion of a job, the MapReduce runtime creates a _SUCCESS file in the output directory. This may be useful for applications that need to see if a result set is complete just by inspecting HDFS. (MAPREDUCE-947) What’s Not In Finally, it bears mentioning what didn’t make it into 0.21.0. The biggest omission is the new Kerberos authentication work from Yahoo! While a majority of the patches are included, security is turned off by default, and is unlikely to work if enabled (certainly there is no guarantee that it will provide any level of security, since it is incomplete). A full working security implementation will be available in 0.22, and also the next version of CDH. Also, Sqoop, which was initially developed as a Hadoop contrib module, is not in 0.21.0, since it was moved out to become a standalone open source project hosted on github.</snippet></document><document id="615"><title>Using Apache Hadoop for Fraud Detection and Prevention</title><url>http://blog.cloudera.com/blog/2010/08/hadoop-for-fraud-detection-and-prevention/</url><snippet>Fraud has multiple meanings and the term can be easily abused.� The definition of fraud has undergone multiple changes throughout the years and is elusive as well as fraud itself.� The modern legal definition of fraud usually contains a few elements that have to be proven in court and depends on the state/country.� For example, in California, the elements of fraud, which give rise to the fraud cause of action in the California Courts, are: (a) misrepresentation (false representation, concealment, or nondisclosure); (b) knowledge of falsity (or scienter); (c) intent to defraud, i.e., to induce reliance; (d) justifiable reliance; and (e) resulting damage.� A more general definition may contain up to 9 elements. From the statistical or technical perspective, fraud is a rare event that results in a significant financial impact to the organization. Both definitions emphasize that the event is rare (assuming that most of the population is law-abiding citizens), is intentional (there is no �accidental� fraud), as well as imply a significant damage caused to the defrauded party (otherwise why bother).� Fraud detection is difficult from statistical point of view for exactly these reasons: (a) the events are rare and it is difficult to build a predictive model and (b) fraud assumes a real human being behind it and incorporates elements of game theory since the fraudster is often an insider who knows how to game the system. Fraud and Rare Events By definition, fraud is an unexpected or rare event with significant financial or other damage.� Fraud assumes that the fraudster has some prior information how the current system works including previous successful and unsuccessful fraud cases and possibly the fraud detection mechanisms.� The above breaks the standard statistical modeling assumption, the variable independence or i.i.d. assumption, making building a reliable statistical model difficult.� Often the fraudster is working in the same industry that the fraud detection is supposed to protect, is intimately familiar with the fraud detection methods, and is actively trying to avoid detection by masquerading. Rare event detection problem is also applicable to online advertising and marketing, particularly with predicting �long tail� events and terrorism detection. One common example of fraud is associated with Taleb distribution where a seemingly high probability of a small gain shadows a small probability of a large loss that more than outweighs the gains.� Relatively long periods of slightly better than moderate gains are interrupted by a rare event of large losses.� It is easy to defraud investors by presenting the results of partial analysis excluding the �rare events�. Fraud Prevention Since fraud is so hard to prove in courts, most organizations and individuals try to prevent fraud from happening by blanket measures.� This includes limiting the amount of damage the fraudster can impact on the organization as well as early detection of fraud patterns.� For example, credit card companies can cut the credit card limit across the board in anticipation of a few negative fraud cases.� Advertisers can prevent advertising campaigns with low number of qualifying events.� And anti-terrorism agencies can prevent people with bottles of pure water from boarding the planes.� These actions are often in contrast with the company efforts to attract more customers and result in general dissatisfaction.� To the rescue are new technologies like Hadoop, Influence Diagrams and Bayesian Networks which are computationally expensive (these are NP-hard in computer science terminology) but are more accurate and predictive. Why Hadoop? Apache Hadoop is a distributed system for processing large amounts of data.� In a recent Hadoop Summit 2010 Yahoo, Facebook, and other companies announced that they currently process a few TBs of data per day and the volumes are growing at exponential rates.� Hadoop can be vital for solving the fraud detection problem because: Sampling does not work for rare events since the chance of missing a positive fraud case leads to significant deterioration of model quality. Hadoop can solve much harder problems by leveraging multiple cores across thousands of machines and search through much larger problem domains. Hadoop can be combined with other tools to manage moderate to low response latency requirements. Let�s go through these reasons one by one.� Sampling is a common technique for modeling rare events.� One of the problems with sampling is that we cannot afford to throw away rare positive cases.� Even in a stratified or proportional sampling scheme one has to retain all positive cases since the model accuracy heavily depends on them (one can usually discard some negative cases though).� Given the above, the system still has to go through the whole dataset to sieve through the positive and negative cases. Hadoop is known for its gnawing power.� Nothing can compare with the throughput power of thousands of machines each of which has multiple cores.� As was reported recently at the Hadoop Summit 2010, the largest installations of Hadoop have 2,000 to 4,000 computers with 8 to 12 cores each, amounting to up to 48,000 active threads looking for a pattern at the same time.� This allows either (a) looking through larger periods of time to incorporate events across a larger time frame or (b) taking more sources of information into account. �It is quite common among social network companies to comb through twitter blogs in search of relevant data. Finally, one of the fraud prevention problems is latency.� The agencies want to react to an event as soon as possible, often within a few minutes of the event.� Yahoo recently reported that it can adjust its behavioral model in a response to a user click event within 5-7 minutes across several hundred of millions of customers and billions of events per day.� Cloudera has developed a tool, Flume, that can load billions of events into HDFS within a few seconds and analyze them using MapReduce. Often fraud detection is akin to �finding a needle in a haystack�.� One has to go through mountains of relevant and seemingly irrelevant information, build dependency models, evaluate the impact and thwart the fraudster actions.� Hadoop helps with finding patterns by processing mountains of information on thousands of cores in a relatively short amount of time. Where to look next? Techniques for fraud detection are industry-specific as a rule and often are guarded since they obviously represent valuable information for potential fraudsters.� They are often kept confidential for this reason.� Moreover, the fraud detection techniques are usually a moving target since the fraudsters quickly adjust to the new fraud detection mechanisms. One of the most publicized technical frauds is click fraud in on-line advertising.� Since advertisers are often charged on the per-click basis � so called PPC campaigns; there is a way to charge advertisers on a per-conversion basis, which we will cover shortly, but a different type of fraud emerges there where the advertiser tries to conceal the conversions � the traffic provider like a search web site has a clear incentive to inflate the number.� Additionally, an advertiser competitor may be incentivized to inflate the number to skew the original advertiser margin.� This can be achieved by a human or software agent that generates extra traffic and clicks on the competitor site.� Fraud management companies like Anchor Intelligence and Click Forensics estimate that approximately 20% to 30% of all clicks are fraud.� How do we know that a click is a fraud? Decline in the number of conversions � first and most important, if your conversion rate is normally positive (that is, you are making a profit on your ad), and all of a sudden, conversion dives into negative numbers, this could be a sign of click fraud in action.� Click fraud causes extra clicks on your ad with no actual purchases, and your conversion rate will fall accordingly. An abnormal number of clicks from the same IP address or a pattern in the access times � although this is the most obvious and easily identified form of click fraud, it is amazing how many fraudsters still use this method, particularly for quick attacks.� They may choose a to strike over a long weekend when they figure you may not be watching your log files carefully, clicking on your ad repeatedly so that when you return to work on Tuesday, your account is significantly depleted.� Part of this fraud might be unintentional when a user tries to reload a page. Large �abandonment rate�, or numbers of visitors who leave your site quickly � another indication of click fraud can be a pattern of visitors clicking on your ad, spending the minimum amount of time on your site required by your PPC search engine to establish it as a valid click (usually 30 seconds or more), and then leaving without having left the landing page at all. A large number of impressions, without the follow-through clicks or click on your ad � if you notice that there are a lot more impressions (views) of your website; this could indicate the impression fraud we discussed earlier. Artificial inflation of your ad impressions may cause your clickthrough rates to drop below the Google minimum, and your ad will be disabled.� Until you realize this, your competitors have free reign to use your keywords, sometimes at bargain prices.� As well, your relevancy ratings for search engines may drop as they record numerous impressions, but no interest shown via visits to other parts of your website, which could lead to a shutdown of your campaign. Abnormally high clicks and impressions on affiliate websites � although affiliates themselves are sometimes involved in conducting click fraud schemes, they can be victims of click fraud themselves.� If one of their competitors uses this same method of excessive clicks and impressions on an affiliate�s site, the PPC search engine will soon notice an abnormally high payment to a certain affiliate and perhaps go as far as canceling that affiliate�s account, even though he or she was not engaging in any form of click fraud. A large number of clicks coming from countries outside of your normal market area � using IP geo-location services, you can identify which country an IP address is probably coming from. In the case of performance-based advertising, the advertiser himself is interested in concealing some of the traffic, not inflating it.� Since most of the performance-based measurements is based in beacons or pixels placed on the advertiser conversion page, advertiser has an incentive to (temporarily) block the traffic from the beacon or to completely remove it from their web-site. Fraud is prevalent in telecom industry.� One of the leading commercially available fraud detection products is HP FMS system on which the author had a pleasure to work personally.� The types of telecom fraud include: Subscription fraud � involves the acquisition of telecommunications services using stolen or false credentials and/or identity with no intention of paying. With subscription fraud, not only do service providers lose revenue, but also individual consumers are vulnerable to having their identity stolen and credit rating tarnished. Technical/network fraud � occurs when someone uses equipment or technology to gain access to a service without paying. Fraudulent calls are typically billed to the legitimate owner of the line or service.� Wireless examples include cloning of cell phones or subscriber identity module (SIM) cards. Fixed line examples include clip on or line tapping, private branch exchange (PBX) hacking and calling card fraud. Prepaid services also have a large exposure to fraud with terminal tampering via magnetic strips or SIM chips, or recharging with stolen credit card numbers. Insider fraud � occurs when individuals inside the operator provide fraudulent access to networks or otherwise thwart the ability of the operator to be paid for services used. Handset abuse � is what takes place when stolen or lost handsets are used to consume telecommunications services that are in turn paid for by the service provider.� This is an expensive liability for carriers who absorb the costs. Social engineering � is an effective fraud technique in which people unwittingly help perpetrators by providing sensitive data, illicit access or simply forwarding their calls without ever knowing they have done anything wrong. All these patterns can be detected with special MapReduce pattern detection techniques. Flume offers low-latency stream processing capabilities. Needless to say, the fraudsters also explore the potential market and invent new innovative ways to generate fraud.� One of them is deployed by Click Monkeys which deploys a vessel with animals next to the coast of California to generate seemingly random traffic.</snippet></document><document id="616"><title>Hadoop Administrator Training Comes to London</title><url>http://blog.cloudera.com/blog/2010/08/london-hadoop-administrative-training-certificatio/</url><snippet>Cloudera�s Apache Hadoop Training and Certification for System Administrators has made it across the Atlantic to London for the first time! This two-day course covers planning, deploying, maintaining, monitoring, and troubleshooting your Hadoop cluster. We�ll talk about HDFS, MapReduce, Apache Hive, Apache Pig, Apache HBase, Flume and more, from the System Administrator�s point of view. Take the certification exam at the end of your training and go home with a valuable validation of your Hadoop knowledge. Enter the code “london_10pct” when�registering and receive a 10% discount! Hadoop is a rapidly growing field. Prove your expertise by attaining certification from the world�s foremost Hadoop training and consulting company. .</snippet></document><document id="617"><title>Improving Hotel Search: Apache Hadoop @ Orbitz Worldwide</title><url>http://blog.cloudera.com/blog/2010/08/improving-hotel-search-hadoop-orbitz-worldwide/</url><snippet>This post was contributed by Jonathan Seidman from Orbitz. Jonathan is a Lead Engineer on the Intelligent Marketplace/Machine Learning team at Orbitz Worldwide�. You can hear more from�Jonathan at Hadoop World October 12th in NYC. Orbitz Worldwide (NYSE:OWW) is composed of a global portfolio of online consumer travel brands including Orbitz, Cheaptickets, The Away Network, ebookers and HotelClub, Additionally, the company operates business-to-business service: Orbitz Worldwide Distribution provides third parties such as Amtrak, Delta, LAN, KLM, Air France and a number of other leading airlines hotel booking capabilities, and Orbitz for Business provides corporate travel services to a number of Fortune 100 clients. The Orbitz Worldwide sites process millions of searches and transactions every day, which not surprisingly results in hundreds of gigabytes of log data per day. Not all of that data necessarily has value, but much of it does. Unfortunately storing and processing all of that data in our existing data warehouse infrastructure is impractical because of expense and space considerations. Apache Hadoop was selected to provide a solution to the problem of long-term storage and processing of these large quantities of un-structured and semi-structured data. We deployed our first Hadoop clusters in late 2009 running Cloudera�s Distribution for Hadoop (CDH), and in early 2010 deployed Hive to provide structure and SQL-like access to Hadoop data. In the short period of time since our initial deployment we�ve seen Hadoop rapidly adopted as a component in a wide range of applications across the organization due to its power, ease of use, and suitability for solving big data problems. One of the applications that Hadoop facilitates is an effort to improve the hotel search results. Currently, when a user performs a hotel search on the Orbitz site the ranking of the search results returned (at least for larger markets) is influenced by a set of parameters manually tuned by an administrator. This leads to the question: can we use automation to optimize the ranking of hotels in order to increase bookings? In other words, can we identify consumer preferences in order to determine the best performing hotels to display to users, thus leading to more bookings? Further, for markets that are too small to be manually managed, can we implement a method to automatically rank hotel search results? To answer this question, it was decided to turn to machine learning techniques, specifically using a trained classifier to determine a ranking of hotels that more closely follows consumer preferences. Performing this analysis requires having data on consumer interactions when shopping for hotels. Fortunately, we have a rich source of this session data in web analytics logs that are collected as users browse the sites. Unfortunately, although parts of this data are loaded into the data warehouse, it turned out that the specific fields we require are not loaded because of space restrictions. Our only alternative was to turn to the raw logs to extract the required fields. Just to further complicate things, the available archive of these logs only went back several days � not nearly enough data to perform the required analysis. Hadoop of course provided a solution to the storage problem by providing a repository where we could download and archive logs. The next step was to extract the data we needed from the raw logs. We began with a set of shell and Perl scripts that were run manually to serially process logs on the local file system. This process worked fine for a while, but as the size of the data grew it was obvious that this process wouldn�t scale. Once again Hadoop provided a solution. Since we were already storing the logs in HDFS, by moving the most time-consuming portions of the data extraction into MapReduce, we were able to dramatically decrease processing time. �A test run against a small subset of data showed a greater than four time improvement for the MapReduce processing vs. the scripts. Now that we�ve accumulated several terabytes of data the performance disparity would be even more dramatic, assuming we even had access to a storage system large enough to hold all of the data for manual processing. After the data is extracted through MapReduce, we load the resulting records into a set of Hive tables. Hive allows us to perform ad hoc querying and further analysis of this data, such as: Obtaining useful metrics, many of which were unavailable with our existing data stores. Creating data exports for further analysis with R scripts, allowing us to derive more complex statistics and visualizations of our data. Aggregating data for import into our data warehouse for creation of new data cubes, providing analysts access to data unavailable in existing data cubes. �In addition to assisting with hotel rank optimization, a few examples of other ways Hadoop is being applied at Orbitz Worldwide are: Measuring page download performance: using web analytics logs as input, a set of MapReduce scripts are used to derive detailed client side performance metrics which allow us to track trends in page download times. Searching production logs: an effort is underway to utilize Hadoop to store and process our large volume of production logs, allowing developers and analysts to perform tasks such as troubleshooting production issues. Data aggregation for the data warehouse: further exploration is being done to expand the use of Hadoop and Hive as a means to aggregate previously unavailable data for import into our data warehouse, making it available for access by our existing data analysis tools. Cache analysis: extraction and aggregation of data to provide input to analyses intended to improve the performance of data caches utilized by our web sites. �Again, these are just a few examples of how Hadoop is being utilized at Orbitz Worldwide, and we�re still just scratching the surface. Each week seems to bring a new team with a big data challenge to be solved by Hadoop, a trend which I expect to continue as more teams discover the possibilities that Hadoop provides to store and process data. I�d like to thank my co-workers who have all made significant contributions to the work discussed here, including Rob Lancaster, Ramesh Venkataramaiah, Wai Gen Yee, Steve Hoffman, Matt Haddock and Andrew Yates. �Also a big thanks to Vice President of Technology Roger Liew, who was an early and enthusiastic champion of Hadoop.</snippet></document><document id="618"><title>Hadoop World: NYC – Training</title><url>http://blog.cloudera.com/blog/2010/08/hadoopworld-training/</url><snippet>Our vision for Hadoop World is a conference where both newcomers and experienced Hadoop users can learn and be part of the growing Hadoop community. We are also offering training sessions for newcomers and experienced Hadoop users alike. Whether you are looking for an Introduction to Hadoop, Hadoop Certification, or you want to learn more about related Hadoop projects we have the training you are looking for. Included with our top-notch Hadoop training you will have full access to Hadoop World free of charge. Available Training Sessions include: Oct 11: Introduction to Hadoop: http://www.eventbrite.com/event/762326138 This one-day course provides a solid foundation for those seeking to understand large scale data processing with MapReduce and Hadoop. This session is designed for developers, analysts or system administrators that are new to Hadoop. This course provides the pre-requisite knowledge for the later classes: Developer Training, Administrator Training or Analyzing Data with Hive and Pig. Hadoop Essentials For Managers: http://www.eventbrite.com/event/762237874 This one-day course will give decision-makers the information they need to know about Apache Hadoop, answering questions such as: When is Hadoop appropriate? What are people using Hadoop for? How does Hadoop fit into our existing environment? What do I need to know about choosing Hadoop? Cloudera HUE SDK Training: http://www.eventbrite.com/event/764021208 Cloudera Hue provides developers with back end APIs to simplify interacting with Hadoop and front end APIs to deliver rich, web based, graphical user experiences. For this training, developers should have experience building web apps using modern MVC frameworks and Ajax. Experience with Python and Django is a strong plus. In this session we spend half the day covering the following topics, and the other half of the day interactively building applications with the Cloudera Hue team. Oct 13 &amp; 14: Developer Training &amp; Certification: http://www.eventbrite.com/event/762320120 In this two-day hands-on session, developers learn the MapReduce framework and how to write programs against its API. In addition to learning how to write individual MapReduce jobs, we discuss design techniques for larger workflows. This course also covers advanced skills for debugging MapReduce programs and optimizing their performance. At the end of the course, attendees have the option to take a certification exam documenting their understanding of the concepts taught during the training session. Administrator Training &amp; Certification: http://www.eventbrite.com/event/762677188 This two-day hands-on session covers the system administration aspects of Hadoop from installation and configuration to load balancing and tuning including diagnosing and solving problems in your deployment. At the end of the course, attendees have the option of taking a certification exam documenting their understanding of the concepts taught at the training session. Analyzing Data with Hive and Pig: http://www.eventbrite.com/event/762318114 Cloudera’s two-day hands-on course on Hive and Pig is designed for people who have a basic understanding of how Hadoop works and want to utilize these languages for analysis of their data. Hive makes Hadoop accessible to users who already know SQL; Pig is similar to popular scripting languages. This course teachs you how to process data by using filters, joins, user-defined functions and more. Oct 15: HBase Training: http://www.eventbrite.com/event/762317111 This one-day hands-on course gives you the necessary knowledge for using HBase as a distributed data store to achieve low-latency queries and highly scalable throughput. This class covers the HBase architecture, data model, and Java API as well as advanced topics and best practices. This course is for developers who already have a basic understanding of Hadoop (Java experience is recommended).</snippet></document><document id="619"><title>Hadoop/HBase Capacity Planning</title><url>http://blog.cloudera.com/blog/2010/08/hadoophbase-capacity-planning/</url><snippet>Apache Hadoop and Apache HBase are gaining popularity due to their flexibility and tremendous work that has been done to simplify their installation and use. �This blog is to provide guidance in sizing your first Hadoop/HBase cluster. �First, there are significant differences in Hadoop and HBase usage. �Hadoop MapReduce is primarily an analytic tool to run analytic and data extraction queries over�all of your data, or at least a significant portion of them (data is a plural of datum). �HBase is much better for real-time read/write/modify access to tabular data. �Both applications are designed for high concurrency and large data sizes. �For a general discussions about Hadoop/HBase architecture and differences please refer to Cloudera, Inc. [https://wiki.cloudera.com/display/DOC/Hadoop+Installation+Documentation+for+Cloudera+Enterprise, http://blog.cloudera.com/blog/2010/07/whats-new-in-cdh3-b2-hbase], or Lars George blogs [http://www.larsgeorge.com/2009/10/hbase-architecture-101-storage.html]. �We expect a new edition of the Tom White’s Hadoop book [http://www.hadoopbook.com] and a new HBase book in the near future as well. Hadoop core is a file system, called HDFS, and the actual MapReduce implementation that can be used to compute on top of the HDFS. �Since we are talking about data, the first crucial parameter is how much disk space we need on all of the Hadoop nodes to store all of your data and what compression algorithm you are going to use to store the data. �For the MapReduce components an important consideration is how much computational power you need to process the data and whether the jobs you are going to run on the cluster is CPU or I/O intensive. �An example of a CPU intensive job is image processing while an I/O intensive job is a simple data loading or aggregation. �Finally, HBase is mainly memory driven and we need to consider the data access pattern in your application and how much memory you need so that the HBase nodes do not swap the data too often to the disk. �Most of the written data end up in memstores before they finally end up on disk, so you should plan for more memory in write-intensive workloads like web crawling. �A good application for HBase is a low latency key-based retrieval and storage of semi-structured data like web crawls or dimensional data for joining with a DW fact table, particularly if the data�need update time tracking and�can be easily grouped into column families. General�Cloudera hardware recommendations are given here.� This blog will focus on more detailed capacity planning issues. Network While the subject of network latency, throughput and bandwidth is very often overlooked when starting to work with Hadoop, it is bound to become a limiting factor as your cluster grows. �Each node in a Hadoop cluster needs to be able to communicate with each other with low latency and high throughput at least to grab the relevant data. �Besides, if the the nodes are not able to communicate with the master node, the master node will automatically think that they are dead and delist them, which will lead to an increased load on the rest of the nodes. �Hadoop will work with off-the-shelf TCP/IP network. Network load depends on the nature of analytical computations in the cluster. �One simple application that requires a lot of communication between nodes is sorting. �In fact, TeraSort is a good test to detect network issues in the cluster. A typical configuration is to organize the nodes into racks with a 1GE Top Of Rack (TOR) switch. The racks are typically interconnected by one or more low-latency high-throughput dedicated Layer-2 10GE�core switches. �Many customers are happy with ~40 node clusters that can fit onto one rack with a typical 48-port switch. �Even if all of your nodes can fit into one rack but you plan to scale beyond one rack, Cloudera recommends to go with at least two racks from the start to enforce proper practices and network topology scripting. Network problems can manifest themselves indirectly. �A good practical test is to run a network intensive application like terasort,�which sorts 10B 100 byte records (the specific parameters can be adjusted to your cluster size),��on your cluster. �On a 100-node cluster with a quad dual-core CPU hardware the running time should be roughly within 10 minutes (one of our customers sorted 1TB in 6 minutes on a 76-node cluster, the numbers are likely to go down with new 12-core CPU machines). �If you see “Bad connect ack with firstBadLink”, “Bad connect ack”, “No route to host”, or “Could not obtain block” IO exceptions under heavy loads, chances are these are due to a bad network. �Even one slow network card on one of the nodes can slow total job execution as much as a factor of 3-4 since the job completion is limited by the the slowest task. �This problems can also manifest themselves as ‘intermittent’ under heavy loads, but usually go away with proper network configuration and tuning. Network connection to outside systems is important for loading data into the HDFS and interoperability. �Some companies prefer to have a dedicated high-bandwidth network for loading the data (as opposed to just using VLAN). Memory HBase is a very memory hungry application. �Each node in HBase installation, called RegionServer, keeps a number of regions, or chunks of your data, in memory (if caching is enabled). �Ideally, the whole table would be kept in memory but this is not possible with a TB dataset. �Typically, a single RS can handle a few 100s of regions with each 1 or 2GBs (these are configurable parameters). �The number of HBase nodes and memory requirements should be planned accordingly. �From our experience, the memory requirement is at least 4GB/RS for any decent load, but depends significantly on your application load and access pattern. For Hadoop MapReduce, you want to allocate somewhere between 1GB and 2GB of memory per task on top of the memory allocated for HBase for large clusters: �As the cluster grows, you should plan for a slight overhead in both the tasks memory and the number of simultaneously opened tasktracker connections, controlled by�tasktracker.http.threads and�mapred.reduce.parallel.copies, to be able to serve more node-to-node connections. Both Hadoop and HBase memory problems will manifest in slowness of the whole system since both systems were not designed to rely on swapping. �It is recommended to discourage swapping on HBase nodes (set�vm.swappiness to 0 or 5 in�/etc/sysctl.conf) and to enable GC logging�(add “-Xloggc:/var/log/hbase/gc-hbase.log�-verbose:gc -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime ” to the JVM opts) to look for large GC pauses in the log. �GC pauses longer than 60 seconds can cause RS to go offline (even worse problems can occur if you run a ZK on the same node and it becomes unresponsive), but pauses as long as 1 second usually lead to noticeable responsiveness problems.� For HBase daemons, RS and ZK, Cloudera also recommends to switch to CMS GC (add “-XX:+UseConcMarkSweepGC -XX:-CMSIncrementalMode” to the JVM opts).� There is also work to develop pauseless JVMs. If a Hadoop node is running an HBase RS daemon together with a Hadoop TT daemon, Cloudera recommends to reduce the maximum number of map/reduce tasks via configuring �mapred.tasktracker.{map,reduce}.tasks.maximum parameter. �You can start with 1-2 map/reduce tasks per tasktracker and slowly increase the number until you see a degradation in the HBase performance. Often network and memory problems manifest themselves first in ZK [http://wiki.apache.org/hadoop/Hbase/Troubleshooting#A15].� ZK is a distributed lock system and is often called a “canary” of HBase. A�vmstat or Ganglia tool should be used to monitor memory status on the RS nodes. �Some VM GC information can be gathered via metrics interface accessible via Jetty interface at�&lt;hadoop/hbase-web-ui&gt;/metrics, for example�http://node:50060/metrics, if this is properly configured in�hadoop-metrics.properties. One should also keep in mind that even though the system does not get OOM exceptions, the OS and disk I/O performance may be compromised if the system is low on available memory since the system is under GC pressure and less memory is available to OS to buffer I/O (“memory cached”) to speed up other operations. Disk First, Hadoop requires at least two locations for storing it’s files:�mapred.local.dir, where MapReduce stores intermediary files, and�dfs.data.dir, where HDFS stores the HDFS data (there are other locations as well, like�hadoop.tmp.dir, where Hadoop and components stores its temporary data). �Both of them can cover multiple partitions. �While the two locations can be placed on physically different partitions, Cloudera recommends to configure them across the same set of partitions to maximize disk-level parallelism (this might not be an issue if the number of disk is much larger than the number of cores). The sizing guide for HDFS is very simple: each file has a default replication factor of 3 and you need to leave approximately 25% of the disk space for intermediate shuffle files. �So you need 4x times the raw size of the data you will store in the HDFS. �However, the files are rarely stored uncompressed and, depending on the file content and the compression algorithm, on average we have seen a compression ratio of up to 10-20 for the text files stored in HDFS. �So the actual raw disk space required is only about 30-50% of the original uncompressed size. �Compression also helps in moving the data between different systems, e.g. Teradata and Hadoop. HBase stores the regions in HFiles. �However, during the major compaction the data may be doubled for a given region temporarily. �In addition to HFile storage, there is a small overhead due to WALs, which ideally should be a small portion of the total data size. �Cloudera�recommends a 30-50% overhead in terms of free space for HFiles. While you can run Hadoop MapReduce with only 5-10% of the disk space left, the performance will be compromised due to fragmentation. �Disk performance can be up to 77% slower due to fragmentation and other issues compared to the “empty disk” [http://www.eecs.harvard.edu/vino/fs-perf/papers/keith_a_smith_thesis.pdf]. �With a disk more than 80% full you also run the risk of running out of disk space on an individual mount. CPU Cloudera recommends total 8 or 12 cores per node, and typically one would have the number of cores equal or slightly larger than the number of spindles. �One would like have the total number of mappers and reducers to be total number of hyperthreads – 2 (2 is for daemons and OS processing) and the ratio of mappers to reducers slightly skewed towards mappers as the reducers tend to spend more time waiting for the mappers. �The importance of CPU power increases with CPU intensive jobs and when using more compute-intensive compression like BZip2. A typical configuration may be found here. Summary � Network Memory Disk CPU # of nodes HDFS 1GE TOR, 10GE core � 8-10 spindles/node � enough nodes to fit the data Hadoop MapReduce 1GE TOR, 10GE core 1-2 GB/task # of spindles = # of cores 8-12 cores/node, # of tasks = # of hyperthreads – 2 � HBase 1GE TOR, 10GE core at least 4GB/node � 8-12 cores/node, reduce # of tasks if running with Hadoop DN/TT enough nodes to fit all regions and serve requests</snippet></document><document id="620"><title>Avoiding Common Hadoop Administration Issues</title><url>http://blog.cloudera.com/blog/2010/08/avoiding-common-hadoop-administration-issues/</url><snippet>It’s easy to get started with Hadoop administration because Linux system administration is a pretty well-known beast, and because systems administrators are used to administering all kinds of existing complex applications. However, there are many common missteps we’re seeing that make us believe there’s a need for some guidance in Hadoop administration. Most of these mistakes come from a lack of understanding about how Hadoop works. Here are just a few of the common issues we find: Lack of configuration management It makes sense to start with a small cluster and then to scale out over time as you find initial success and your needs grow. Without a centralized configuration management framework, you end up with a number of issues that can cascade just as your usage picks up. For example, manually ssh-ing and scp-ing files around by hand is a great way to effectively manage a small handful of machines, but as soon as your cluster gets to 5 or more nodes (let alone tens or hundreds, where Hadoop really shines), it becomes very cumbersome to manage and confusing to keep track of which files go (and have gone) where. Also, as the cluster evolves and becomes more heterogeneous, you have different versions of config files to manage, with each version changing over time. This adds a version control requirement to your configuration management. Such a solution might include a parallel shell and other established routines for starting and stopping cluster processes, copying files around the cluster, and making sure cluster configurations are kept in sync. It hasn’t quite worked so far to normalize this within a Hadoop distribution. Even though it’s a necessary component of a successful Hadoop deployment, there are many different ways to do it. It’s important not to overlook. Cloudera can help you to determine how Hadoop fits into your existing configuration management framework, we can make recommendations on how to accommodate Hadoop, and down the road we’ll be addressing this area with better software. Poor allocation of resources One of the most common questions we get is, how many map slots and reduce slots should we allocate on a given machine? If the administrator puts the wrong values here, there are a number of unfortunate potential consequences. You might see excessive swapping, long-running tasks, out-of-memory errors, or task failures. The number of “slots” on a machine seems like a simple enough configuration to tweak, but the optimal value is subject to many different factors, such as CPU power, disk capacity, network speed, application design, and the other processes that are sharing a node. For customers with CPU-intensive MapReduce application, we encourage our customers to guess high values and then tweak down as we notice how resources max out. But with an I/O intensive-application using HBase, we tell customers to start with a smaller allocation of slots and then tweak up as you can. There’s no hard and fast rule and it’s important to work with your existing jobs. Using your specific application, you should be able to iterate, tweak, compare, and repeat, asking yourself at every step of the way: does this help or hurt? With a new cluster, you can use manufactured benchmarks (TestDFSIO, teragen / sort) but there’s no point in tweaking if you have nothing to which you can compare. If you don’t have an understanding of how these factors affect each other, in the context of your use cases, you’re going to spend too much time tweaking variables at random, you’re not going to be able to compare methodically, you might not see the performance you expect, and it might sour your whole experience of Hadoop. Lack of a dedicated network While Hadoop doesn’t require a dedicated network to install and to run, it definitely requires one in order to run as designed and perform well. Data locality is central to the design of HDFS and MapReduce. What’s more, the shuffle-sort operation between the map and reduce phases of a job causes a good chunk of network traffic which can adversely affect and be affected by constraints on a shared network. Without a dedicated network, Hadoop has no way of determining how to allocate data blocks or where to schedule tasks to maximize network locality and ensure fault tolerance. The picture becomes more complicated as you add components such as HBase, which gets unhappy when it can’t access HDFS data due to a slow network. It’s easy to take the network architecture for granted, and equally easy to assume network architecture doesn’t matter. Rather than scatter a Hadoop cluster across a data center without thought to rack awareness or data locality considerations, it’s better to get it right the first time with our assistance. When we discuss “a dedicated network”, we’re referring to dedicated network hardware. Your Hadoop cluster deserves a dedicated switch (or set of switches) corresponding to a dedicated rack (or set of racks). This ensures optimal performance when nodes communicate with one another, and optimal power allocation such that a power circuit failure is equivalent to a switch failure. It’s hard for any administrator to know when to virtualize and abstract, and when to think about the bare metal. When you apply expertise from Cloudera, you can be sure you’re getting optimal understanding of both. Lack of monitoring and metrics Aside from the web UI, Hadoop doesn’t provide much in the way of built-in monitoring, so it’s tempting to think this isn’t that important. However, it’s critical to roll out some solution for capturing Hadoop’s metrics as well as monitoring and alerting the general OS and network health of the cluster. Tools like Ganglia and Nagios give you the ability to see the health of the entire cluster in a single dashboard, and be alerted when a health value exceeds a threshold, respectively. Without these monitoring tools in place, it becomes very difficult to determine what’s going on when something goes awry in a cluster of any considerable size. One very simple real world example from a recent consulting engagement: the Hadoop start scripts drop a pid file in a directory and report “OK” when they start. In this case, the processes encountered a simple permissions problem during startup, and exited gracefully, leaving the inexperienced administrator to wonder (some time later) when the process died and what killed it, even though it never really started. We’ve also seen occurrences where the local disk of one or a few nodes in a cluster fills up, leading to what appears as a flaky cluster, with sporadic task failures. A monitoring/alert process tells you about these problems before they became costly. And it’s important to monitor for both performance and health, not either/or. Ignorance of what log files contain what information You can read all the documentation you want, but knowing which log file to examine and what to look for when something goes wrong is a skill that only comes with a lot of experience with Hadoop. Customers often wonder where their stdout/stderr streams went when they deploy their first jobs on a large cluster. They’ll also ask us what caused a task to run longer than usual, or what caused it to fail when it hasn’t before. Most of this information is logged, but it’s hard to know where to look. Also, when you have enough experience, you start to recognize common patterns in log files for cross-process interactions like a busy data node causing HBase to crash or a bad network switch causing a map task to appear to time out. Our philosophy is to help people get up to speed as quickly as possible. We blog (http://blog.cloudera.com/blog/2009/09/apache-hadoop-log-files-where-to-find-them-in-cdh-and-what-info-they-contain/), train (http://www.cloudera.com/hadoop-training/) and of course provide individualized services (http://www.cloudera.com/hadoop-services/). Drastic measures to address simple problems When one node experiences a problem with one Hadoop process, it’s tempting to take drastic measures, such as killing additional processes on other nodes or taking entire nodes offline until you’ve killed the cluster. If you experience a problem with a data node or a group of data nodes, it’s tempting to assume that you’ve lost data and try to restore your filesystem from a namenode snapshot. If you’re unaware or untrusting that Hadoop’s replication ensures that you probaby lost no data, you’re tempted to take a measure that assures that you will lose data: restoring from an older snapshot. If you are experiencing task failures or disk errors due to log file proliferation, it’s tempting to assume you’re working with unstable software, wipe the cluster clean and start over. Our customers have made all of these mistakes, and we’ve worked with them to set things straight. When complicated distributed systems go wrong, it’s easy to react too drastically. With proper troubleshooting and a good run book from us, you can get back and going with less pain. Inadvertent introduction of single points of failure Hadoop is designed to be a highly redundant distributed system. But as you deploy it, sometimes, it’s difficult to keep it that way. For example, a common misunderstanding of Hadoop is that because it’s the only source of filesystem metadata, the namenode represents a single point of failure. This is only the case if you don’t configure the namenode to store its metadata in multiple places, preferably over an NFS store on the network. Other single-points of failure introduced by erroneous setups include: Having $HADOOP_HOME be an NFS mount (causing a distributed denial of service attack on the NFS server), running an HBase Zookeeper on the same node as an HBase region server, or non-existence or improperly configured domain name service (DNS) in the cluster. Over reliance on defaults If you buy a machine with a lot of disk and RAID support, you might not be aware that RAID is turned on by default, though it is not recommended for use with Hadoop. Likewise, if you take the defaults of a Hadoop configuration, you might take a default partitioning scheme that doesn’t apply to your disk allocation, or a dfs.name.dir setting that makes no sense whatsoever. We saw one cluster with an entire disk dedicated for log files and temp files, but because the defaults didn’t get changed in the configuration files, the disk was going unutilized while other disks were filling up. Unlike a lot of applications, you want to be very wary of Hadoop defaults, and give a lot of thought to key configuration settings that are available to you. Because Hadoop has a hierarchical configuration system, where the user provides files that overlay instead of editing existing ones, it’s easy to overlook or forget about defaults that affect you. Hadoop has a lot of knobs, and in some cases the defaults are OK, but our customers commonly hit issues when they accept a default they shouldn’t have. We’ve also blogged about which configuration details you should pay attention to (http://blog.cloudera.com/blog/2009/03/configuration-parameters-what-can-you-just-ignore/). Cloudera can help now and in the future. From day one we’ve been putting a lot of work into CDH to make it easier to administer (http://blog.cloudera.com/blog/2009/04/clouderas-distribution-for-hadoop-making-hadoop-easier-for-a-sysadmin/). If you haven’t taken a look at Cloudera Enterprise, we’re building tools on top make Hadoop more accessible and simplify administration and maintenance of the cluster as a whole. Having a partner to help you manage the logistics of deployment, administration and maintenance will save you time, money and headaches.</snippet></document><document id="621"><title>CDH3b2 Release Recap</title><url>http://blog.cloudera.com/blog/2010/08/cdh3b2-release-recap/</url><snippet>Just over a month ago, our CEO, Mike Olson, announced the availability of Cloudera’s Distribution for Hadoop (beta 2), or CDH3b2. As Charles, our head of Product Management, explained in a subsequent blog post, this release of CDH removes a lot of the complexity we’ve seen organizations encounter when deploying Hadoop within an existing data management infrastructure. By packaging Hadoop core together with a suite of additional projects for data collection, workflow management, and low-latency access, and putting all of those components behind a single user interface, CDH3b2 is the first distribution for Hadoop that puts tools for data collection, curation, and analysis into a single, supported software package. Others have noticed that a “LAMP stack” for working with big data is emerging from the Hadoop ecosystem; CDH3b2 delivers this stack in an integrated and well-tested fashion. For your convenience, we’ve collected our series of blog posts about what’s new for each component of CDH3b2 below. When you’re ready to upgrade your cluster to CDH3b2, be sure to follow the detailed instructions provided by Eric Sammer, a solutions architect at Cloudera. Hadoop Core Sqoop Flume Pig Hive Oozie HBase HUE HUE SDK ZooKeeper</snippet></document><document id="622"><title>Cloudera’s Henry Robinson to speak at Hadoop Day in Seattle</title><url>http://blog.cloudera.com/blog/2010/08/clouderas-henry-robinson-to-speak-at-hadoop-day-in-seattle/</url><snippet>On August 14th, Seattle will host it’s first Hadoop Day: a day-long community-organized event where people gather to discuss and learn the Apache Hadoop ecosystem. Cloudera’s very own Henry Robinson, will be giving a talk entitled “Inside Flume”. In Henry’s words “Flume is Cloudera’s new open-source data-aggregation framework – designed with getting data into Hadoop in mind. In this talk I’ll describe Flume’s architecture and its guiding design principles, and show through a worked example how easy it is to start collecting your data with Flume. Topics I’ll cover include Flume’s fault-tolerance model, managing Flume through its centralised configuration service and adding features to Flume’s extensible core. “ What: Seattle Hadoop Day 2010 When: Saturday, August 14, 2010 Where: Amazon Pac-Med, Seattle, WA Full agenda and event details can be found here. We hope to see you at Hadoop Day!</snippet></document><document id="623"><title>Hadoop World: early-bird rate ends on August 11</title><url>http://blog.cloudera.com/blog/2010/08/hadoop-world-early-bird-rate-ends-on-august-11/</url><snippet>They say the early bird catches the worm. The early bird also gets a discounted rate on a ticket to Hadoop World. For only $195 you will get access to an awesome line up of speakers including a keynote from Tim O’Reilly and talks by Facebook, Twitter, Yahoo, eBay, GE and many more. Take a look at the event agenda here for details and abstracts. Join your colleagues and community members for an engaging and educational day, plus terrific networking at the closing cocktail reception. But hurry, the early-bird rate ends on August 11th after which the rate will increase. Get your ticket here and we look forward to seeing you at Hadoop World in October.</snippet></document><document id="624"><title>Flume community update – the first 30 days!</title><url>http://blog.cloudera.com/blog/2010/08/flume-community-update-the-first-30-days/</url><snippet>?It’s been just over a month since we announced Flume at the Hadoop Summit, releasing the code as open source and distributing on github. For additional background see?? CDH3 and Cloudera Enterprise and What’s New in CDH3b2: Flume. Since that milestone the Flume community has been at work addressing issues, adding features, building community, and generally working hard to improve the platform. You can see the issues that have already been resolved for the next release — 25 so far! Talking about releases, we’re working hard to close out a few remaining blockers after which we plan to release a 0.9.1 version. This release should also contain a few features from new contributors, currently we have a number of significant features being worked on by: Anurag Phadke is adding Gzip compression to HDFS output files Nick Verbeck is working on a date extractor and multi group regex extractor Vibhor Bhatt is adding throttling support to Flume flows Patrick Wendell is looking at supporting Avro for Flume communication See this page if you are interested in contributing. There are many interesting areas to work on, both wrt new feature development and closing out issues that users have been finding (detailed on the JIRA): We’d love to hear from you! If you are using Flume feel free to update the newly added Powered By page. or contact us at: User mailing list Developer mailling list IRC channel #flume on irc.freenode.net for updates follow us on twitter (I’m there too :-) ) If I’ve missed anything you think is important, or would like to highlight, please feel free to reply to this email thread. Regards, The Cloudera Team.</snippet></document><document id="625"><title>Migrating to CDH</title><url>http://blog.cloudera.com/blog/2010/08/migrating-to-cdh3/</url><snippet>With the recent release of CDH3b2, many users are more interested than ever to try out Cloudera’s Distribution for Hadoop (CDH). One of the questions we often hear is, “what does it take to migrate?”. Why Migrate? If you’re not familiar with CDH3b2, here’s what you need to know. All versions of CDH provide: RPM and Debian packages for simple installation and management. Clean integration with the host operating system. Logs are in /var/log, common binaries in /usr/bin, and configuration in /etc. A Cloudera support-ready distribution. As Hadoop becomes a mission critical component of your production infrastructure, you’ll want the option of engaging Cloudera for support or consulting services. Running CDH makes this process simple. CDH3b2 additionally is: A complete platform with smooth integration of popular projects such as Hive, HBase, Pig, Zookeeper, Flume, Sqoop, Oozie, and HUE. HDFS and Hadoop Map Reduce are only two parts of a larger system. CDH3b2 brings together tools frameworks to get data in and out of HDFS, coordinate complex processing pipelines, as well as process and analyze your data. Learn more about this. Based on Apache Hadoop 0.20.2 with 320 patches worth of feature back ports, stability enhancements, and bug fixes. Overview The migration process does require a moderate understanding of Linux system administration. You should make a plan before you start. You will be restarting some critical services such as the name node and job tracker, so some downtime is necessary. Given the value of the data on your cluster, you’ll also want to be careful to take recent back ups of any mission-critical data sets as well as the name node meta-data. Backing up your data is most important if you’re upgrading from a version of Hadoop based on an Apache Software Foundation release earlier than 0.20. There were changes in the open source HDFS implementation prior to 0.20 that force this upgrade. See the section below on compatibility for more details. The process I’ll outline here is as follows: CDH version selection Options for installation Installation process Migration of configuration data Testing your cluster Selecting a Branch One of the first questions you should ask yourself is what level of stability versus new features you require from Hadoop. If you’re managing a production Hadoop cluster with jobs with SLAs, you need a rock solid, production-proven Hadoop distribution. This is Cloudera’s stable or production branch. At the time of this writing, this is CDH2 based on Hadoop 0.20.1+169.89. In certain cases, features may be of greater priority, in which case, CDH3 0.20.2+320 is appropriate. It’s important to note that both CDH2 and CDH3 pass all functional and unit tests at Cloudera. The real difference between them is that CDH2 has been in the field longer. We generally promote a release to stable when we’ve seen it running production workloads for a substantial period of time, and when the rate of issues opened against the distro in our support group tails off. We have customers running in production today on both CDH2 an CDH3. On Compatibility Before we dive into the installation process I’ll highlight some points on compatibility. When upgrading to CDH from an older version or another distribution of Hadoop, it’s possible that HDFS data needs to be taken through an upgrade process. This is relatively simple, but as with any upgrade of critical data, it is absolutely necessary to back up your data. Currently, it is not necessary to perform an HDFS upgrade if you’re upgrading to CDH3 from CDH2 or Apache Hadoop versions 0.20.0 or later. In fact, any distribution of Hadoop based on Apache 0.20.0 is likely to be a clean transition without an update to HDFS required, but you should always check with the distributor. During RPC operations, all Hadoop daemons will check to ensure they are speaking to the same exact version as themselves. This means that you cannot, at present, perform a rolling upgrade of CDH. There has been some discussion about relaxing this requirement so compatible versions of Hadoop can communicate, but this has not yet been implemented. Installation Options CDH is available in three forms: RPMs, debs, and tarball distributions. The preferred method of installation is usually the RPM or deb packages as they automate a lot of the work required to get CDH up and running quickly. Tarballs of CDH are useful for users on systems that do not use yum/rpm or apt/dpkg, or where you do not have root access to the host operating system. Installing CDH When installing CDH from from RPMs or Debian packages you will definitely want to take advantage of Cloudera’s yum or apt repository support. If you’re on a system that is not rpm or deb format packages, you can still use Cloudera’s binary tarball packages. You should follow the normal process for installing CDH on your systems. The CDH packages should be installed on all nodes in the cluster. The rpm and deb packages of CDH will automatically create a hadoop user and group as well as SYSV init scripts as part of the install process. The CDH tarballs do not contain the init scripts and obviously do not create the hadoop user and group. Detailed installation instructions for all formats of CDH are available. After the packages are installed, you’ll want to make sure you set the proper daemons to start on the proper machines upon boot. There is a separate init script for each Hadoop daemon so only what is necessary is started. Redhat example: % chkconfig --level 3 hadoop-0.20-namenode on Debian example: % update-rc.d hadoop-0.20-namenode start 80 3 . Make sure you specify the correct run level. While run level 3 is common for multiuser Linux servers, this may not be the case in your installation. You can use the runlevel command to find the currently active run level. For now, do not start any of the Hadoop daemons. Migrating Your Configuration If you’re coming from older version of CDH, your configuration should already be setup with alternatives. If not, now is a good time to bring your configuration layout in line with CDH by moving your conf directory to /etc/hadoop-0.20/conf.mycluster. You should also configure alternatives to know about your new configuration. The CDH documentation covers this in detail. For now, register your new configuration with alternatives and set it to be the preferred configuration. % alternatives --install /etc/hadoop-0.20/conf hadoop-0.20-conf /etc/hadoop-0.20/conf.mycluster 100 % alternatives --set hadoop-0.20-conf /etc/hadoop-0.20/conf.mycluster Users who are on systems that don’t have alternatives or who are installing CDH from tarballs should simply update the configuration files in $HADOOP_HOME/conf. Normally, $HADOOP_HOME is /usr/local/hadoop-$VERSION or /opt/hadoop-$VERSION but you can put it wherever it makes sense. This includes running CDH from your home directory if you don’t have root access. Testing CDH Now that CDH is installed and you’ve migrated your cluster configuration it’s time to fire up a few nodes and make sure everything is working as expected. Rather than bring up all the daemons at once, let’s focus on the name node first. Start by logging on to the name node machine. You may want to manually rotate the log file just to minimize the noise during testing. You can do this by simply moving today’s log file to a different name. % mv /var/log/hadoop/hadoop-hadoop-namenode-nn.mycompany.com.log \ /var/log/hadoop/hadoop-hadoop-namenode-nn.mycompany.com.log.old Next, start the CDH name node daemon using the provided init script. If an HDFS upgrade is required, you can use the upgrade argument in place of start below. This will be your last chance to grab a backup of the name node’s metadata prior to starting the daemon. % /etc/init.d/hadoop-0.20-namenode start Note that the CDH init scripts require you to be root whereas the Apache Hadoop start-all.sh / stop-all.sh scripts should not be run as root. It’s a good idea to check the contents of the name node log file now to ensure it has come up cleanly. You should see a warning about the name node being in safe mode due to missing blocks. This is OK because we haven’t brought up any data nodes yet. If something doesn’t look right, jump ahead to the getting help section before proceeding. Before you start any of your data nodes, you’ll want to place the name node in safe mode manually. This will prevent the name node from “panicking” and trying to repair missing block replicas as data nodes begin to register themselves. You’ll need to run this command as the hadoop user. % hadoop dfsadmin -safemode enter Next start one of the data nodes and watch its logs as you did for the name node. % /etc/init.d/hadoop-0.20-datanode start If everything is setup correctly, you should see the data node start up, register with the name node, and start its periodic block scanner thread. You should also check the name node logs to confirm you see the data node registration message there as well. Once you’ve confirmed that things look good, you should move on to starting additional data nodes checking them in batches as you go. After all data nodes are up and running, you can use the Hadoop fsck tool to confirm that the file system is healthy. % hadoop fsck / Your cluster should still be in safe mode. If the file system is healthy, you can go ahead and take it out of safe mode. % hadoop dfsadmin -safemode leave Follow this with a quick test of HDFS by copying a file into the file system. % date &gt; now.txt % hadoop fs -put now.txt /now.txt % hadoop fs -cat /now.txt % hadoop fs -rm /now.txt % rm now.txt Congratulations! You now have HDFS running on CDH. If you had to upgrade the HDFS data – that is, you started the init script with the upgrade option – you should do some more extensive testing of your data. Once you’ve confirmed everything is working as expected, finalize the HDFS upgrade. % hadoop namenode -finalize Starting and testing the map reduce daemons follows a similar procedure but is a bit simpler. Start the job tracker daemon on the proper machine and monitor the logs as you did with the name node. Once you’ve confirmed the job tracker is running, proceed with starting the task tracker daemons in groups checking the job tracker UI as you go. You should see the map and reduce task capacity increasing with each node you start. Don’t panic if the job tracker doesn’t see the nodes immediately; it can take a few seconds. Don’t forget to start the secondary name node daemon as well. It’s usually a good idea to wait an hour or so and check the modification time on the files in the configured fs.checkpoint.dir. You should see that the files have been updated within the last hour. You can also check the secondary name node logs; you’ll see an indication things are working there as well in the form of some log messages about performing the checkpoint. Documentation and References In addition to the community articles and blog posts on Hadoop, Cloudera provides CDH-specific documentation at docs.cloudera.com. Here you can find information on CDH including all of its components like Hadoop, Hive, Flume, Sqoop, HUE, and others. How to Get Help There are a number of ways to get help if you run into trouble during your migration or if you just have questions. Cloudera Documentation Cloudera mailing lists Cloudera videos IRC users can join #cloudera on freenode</snippet></document><document id="626"><title>How to Get a Job at Cloudera</title><url>http://blog.cloudera.com/blog/2010/07/how-to-get-a-job-at-cloudera/</url><snippet>We’re doing a lot of hiring at Cloudera — we have jobs open in operations, sales, engineering and elsewhere. Hiring well is hard work. We spend a lot of time on it, and have learned a lot about the kind of people we want to bring in. One of the best ways for us to do a good job of hiring is to help you do a good job of applying for a job here. I’ll begin the post, though, by telling you what doesn’t work. Several times a day, we get an unsolicited email or phone message from a contingency recruiter like this one: I specialize in the industry and wanted to contact you to let you know that I have a strong candidate for your [deleted] position, and wanted to know if you would like to review the resume that I have? My candidate is interested in interviewing as soon as possible. If you’re that candidate, bad news: Your resume goes straight to the bottom of the pile, and it’s a big pile. Contingency recruiters get paid by the hiring company when the candidate they introduce gets the job. The amount they get paid varies a little bit, but is generally a quarter to a third of the annual salary of the person they place. That means that a software developer who earns $80K a year shows up with a $20,000 price tag, minimum, due and payable to the recruiter, up front. If you’re that software developer, you need to be not merely as good as the rest of the candidates we’re looking at. It’s not even enough to be a little bit better. You need to be so astonishingly good that you’re worth our writing a fat check to a third party on your very first day in the office. And, honestly, if you’re that good, how come we don’t know about you already? Now, we do — occasionally, for a few positions that are especially tough to fill — work with contingency search firms. If a recruiter talks to you about Cloudera, you should ask: Do you have a signed engagement letter with Cloudera already? If the answer is yes, then you’re talking to someone we trust (but whom we’ll have to pay if we hire you, so we’d still prefer to hear from you directly). If the answer is no, or if they waffle, then you shouldn’t waste your time with them. They’re not going to help you get a job at Cloudera. If you want a job with us, we absolutely want to talk to you. Here’s some advice on how to reach us on your own. First, don’t just email us your resume with a cover letter. It’s not that we don’t read those; it’s that it’s very, very hard to stand out from the crowd that way. Every company that’s hiring and posts jobs on its web site gets a lot of resumes by email. Reading all of them is hard work, and (shame on us) we might be a little bit hurried or distracted when yours comes in. It’s much better to come in by way of an introduction from someone we already know. We know all the people who work here very well, of course. If one of them sends your name and resume along to a hiring manager, you can bet that it gets special attention. It turns out that Cloudera people are easy to find: We speak at conferences, attend trade shows, hang out in IRC and browse public forums about topics that matter to the company. We blog here, and many of us post on Twitter regularly — see @cloudera/cloudera. Check out our events page. Look for us on-line or in person at shows. Reach out, person to person, in those places. You should really try to engage with the Cloudera person, of course. Don’t just ask for an intro; talk to us us about the topics that you know matter to us. Show us, first, that you know a little bit about what we’re doing. If you’re a developer especially, making contributions to the open source projects we work on, or building cool applications on Cloudera’s Distribution for Hadoop, is a great way to show your chops. We’ve got a referral program in place — when an employee brings us a candidate we hire, that person gets some extra Cloudera stock (we would much rather grant equity to our employees than write checks to recruiters!). As a result, if you’re a good fit for a position and can establish a personal rapport with one of our current employees, you’ll have an enthusiastic champion who’ll make sure we take a hard look at you. Everybody wins. Before you reach out, though, do your homework. Figure out what we do. We put a lot of work into the web site — read the stuff there. Understand our products and customers. Know the role you’re interested in and why you’re a good fit. It’s really surprising to me how many people send us unsolicited resumes asking if we have any jobs open that would match their backgrounds. If you can’t do simple homework, you’re just not the kind of person we’re going to hire. If you can’t get to us directly, take a look at our customers, partners and investors. Do you have contacts there? Personal introductions really do get noticed, and your effort in making them happen demonstrates, all by itself, that you’re motivated and clever and willing to do a little extra work.</snippet></document><document id="627"><title>Notes From the Hackathon at Cloudera</title><url>http://blog.cloudera.com/blog/2010/07/notes-from-the-hackathon-at-cloudera/</url><snippet>I was positively blown away by the enthusiasm, creativity, and productivity exhibited by the participants in the CDH3b2 Hackathon. We had over twenty participants from established companies like Oracle and Akamai, stealth-mode startups and one-man consulting shops. At one point we had 9 simultaneous hacking projects going, with groups of one to five people. At the end of the day, participants voted on the most interesting project, which won a prize – an iPod Nano for each participant on that project. The winning project used Apache Hadoop, MapReduce, Apache Pig, Apache Hive and other tools to analyze the White House Visitor Logs. Using their sharp sleuthing efforts and mad Hadoop skillz, this team of hackers was able to come to some interesting conclusions about who is up to no good. Cross referencing their findings with the News reveals that, indeed, the most frequent White House visitors are also mentioned in some of the biggest political and business news stories of the year. Ground breaking journalism? Maybe not. A fantastic demonstration of the power of Hadoop to pull value and meaning out of even the most unstructured data? Absolutely! The second place project involved using Hadoop, Flume, Hbase and Avro to do in-depth analysis of 30 minutes of data from twitter using bigrams. This project was notable just for the sheer muscle of the hackers involved. In one day, not only did they make effective use of four (or more) major Hadoop toolsets, but they did it in a way that was effective and made sense. From scratch, mind you. SCRATCH! If this isn’t a good illustration of the power of CDH when placed in the right hands, I don’t know what is. Another notable project analyzed twitter data, cross-referenced with data from Yelp and Foursquare. These guys assumed that tweets with locations in them are an accurate indication of human behavior, and used CDH3 to come to some interesting conclusions. Other projects included: Nonnegative matrix factorization in MapReduce using Pig and Hive Analyzing product reviews from Toys R Us and other retailers for products that are harmful to kids Determining “interesting links” (using retweets, follows, etc) from twitter and storing them in Hive for query Integrating HTTP and Flume Integrating Perl and Avro The CDH3B2 Hackathon was a great time for all involved, and an impressive display! Thanks to Accel Partners for the pizza, thanks to Rackspace for the remote cluster, and thanks to the Cloudera engineers for supporting the hackers. Most of all, thanks to the participants!</snippet></document><document id="628"><title>Upcoming webinar: 10 Common Hadoop-able Problems</title><url>http://blog.cloudera.com/blog/2010/07/upcoming-webinar-10-common-hadoop-able-problems/</url><snippet>At Cloudera we find that organizations often have trouble recognizing how Hadoop can help solve some of the large scale data problems they may be facing. Through our engagements with customers we have seen a range of problems, a number of which come up time and time again. We thought it would be a good idea to share our findings and we’d like to invite you to attend this free and exclusive webinar to hear our Chief Scientist, Jeff Hammerbacher, as he describes ten common problems that can be solved with Hadoop. Some of these are problems that span multiple industries whilst others are more industry specific. In all examples, Hadoop provides a solution that allows the organization to reduce costs and extract more value from its data. In this webinar you will learn what are ten common business problems being solved with Hadoop; what industries are benefiting from these solutions; and specific examples of customers benefiting from Hadoop. For example, we will look at how Hadoop is used for threat detection in organizations from financial services to IT security to government. We will generalize the smart grid example we discussed in a previous blog post to look at how Hadoop can help you analyze readings across a sensor network in order to help predict failure. More broadly we will look at how Hadoop can be used by all types of organizations to analyze complex data from multiple sources to find patterns and relationships that allow them to extract additional value from their data. Also if you attend the webinar you’ll receive a free copy of our latest whitepaper: “10 Common Hadoop-able Problems”. So, what are you waiting for – go register to learn how Hadoop could be used to solve a problem within your organization.</snippet></document><document id="629"><title>Announcing Two New Training Classes from Cloudera: Introduction to HBase and Analyzing Data with Hive and Pig</title><url>http://blog.cloudera.com/blog/2010/07/announcing-two-new-training-classes-from-cloudera-introduction-to-hbase-and-analyzing-data-with-hive-and-pig/</url><snippet>Cloudera is pleased to announce two new training courses: a one-day Introduction to HBase and a two-day session on Analyzing Data with Hive and Pig. These join a recently-expanded two-day Hadoop for Administrators course and our popular three-day Hadoop for Developers offering, any of which can be combined to provide extensive, customized training for your organization. Please contact sales@cloudera.com for more information regarding on-site training, or visit www.cloudera.com/hadoop-training to view our public course schedule. Cloudera’s HBase course discusses use-cases for HBase, and covers the HBase architecture, schema modeling, access patterns, and performance considerations. During hands-on exercises, students write code to access HBase from Java applications, and use the HBase shell to manipulate data. Introduction to HBase also covers deployment and advanced features. Our Hive and Pig course is designed for developers who are skilled with SQL or scripting languages, but who are not Java experts. Hive and Pig are two approaches which allow non-Java programmers to access and manipulate massive amounts of data while abstracting away the complexities of MapReduce. Hive offers an SQL-like interface, while Pig’s scripting language, named PigLatin, is very easy for developers learn. This course covers both technologies, and includes multiple hands-on exercises to reinforce key concepts. Cloudera’s Hadoop for System Administrators course has recently been expanded from one day to two, and covers the important issues for System Administrators charged with looking after Hadoop clusters. Topics include planning and deploying the cluster, managing MapReduce jobs, scheduling jobs using the Fair Scheduler, cluster monitoring and troubleshooting, populating HDFS from existing relational database management systems with Sqoop, and using Flume to import logs and other files into HDFS. Our most popular course, Hadoop for Developers, is a three-day offering which covers everything from an introduction to HDFS and MapReduce right through to advanced MapReduce APIs and algorithms. Students learn to build MapReduce jobs through a combination of instructor-led training and hands-on exercises; the course includes an exam offering students the chance to earn Cloudera Certified Hadoop Developer credentials. A complete list of events including upcoming training is available here: http://www.cloudera.com/company/events/</snippet></document><document id="630"><title>What’s New in CDH3b2: Apache Hive</title><url>http://blog.cloudera.com/blog/2010/07/whats-new-in-cdh3b2-hive/</url><snippet>CDH3 beta 2 includes Apache Hive 0.5.0, the latest version of the popular open source Apache Hadoop data warehouse platform. Hive allows you to express data analysis tasks in a dialect of SQL called HiveQL, and then compiles these tasks into MapReduce jobs and executes the jobs on your Hadoop cluster. Hive is a natural entry point to Hadoop for people who have prior experience with relational databases, but even those who have never written a line of SQL should give it a chance since it is currently the only Hadoop dataflow programming platform to provide built-in facilities for managing metadata. This unique feature of Hive allows you to access your data through a Table abstraction, making it possible to cleanly separate your analysis logic from the details of how your data is formatted and parsed. This results in scripts that are easier to write and much easier to maintain. While Hive is great it on its own, it’s even better when you connect it to other tools in the Hadoop ecosystem. Users can currently use Sqoop to import data from relational databases into Hive, run Hive jobs inside Oozie workflows, and design queries in the Beeswax query editor that comes included with Hue. Hive 0.6.0 will include new features that make it possible to seamlessly access HBase tables from Hive, and there is also work afoot to provide an integration point between Hive and Flume. The 0.5.0 release of Hive includes a variety of feature enhancements and bug fixes that improve the usability and stability of the Hive platform. These changes include extensions to HiveQL such as support for the CREATE TABLE AS SELECT statement, LEFT SEMI JOINs, and LATERAL VIEWs, as well as support for User Defined Table Generating Functions. The 0.5.0 release also includes enhancements that improve the performance of GROUP BY aggregations and Hive’s RCFile columnar storage format. Readers who are new to Hive should check out our Hive training videos and tutorial notes, as well as an earlier blog post from Peter Skomoroch in which he explains how he used Hive and Hadoop to identify trending topics on Wikipedia. Experienced users looking to upgrade to the new version of Hive will want to consult the CDH Quick Start Guide and the CDH Hive Installation Guide.</snippet></document><document id="631"><title>Developing Applications for HUE</title><url>http://blog.cloudera.com/blog/2010/07/developing-applications-for-hue/</url><snippet>Yesterday’s post gave an overview of the HUE (aka. Hadoop User Experience) project which was released in CDH3b2 and available on github. HUE is a graphical “desktop” style web application that runs in modern browsers (Firefox, Chrome, Safari, and IE8+) that allows users to interact with a Hadoop installation as if it were just another computer. They browse the file system, create and manage user accounts, view and edit files, upload files, and then use some Hadoop-specific applications like the Job Browser and Beeswax (our Hive app). Here’s a quick demo from yesterday’s post running through Beeswax. It’s about 10 minutes long, but even if you only watch the first 2 or 3 you’ll get an idea of what HUE is and what it can do. This post is focused on what it means to develop for HUE, the source of which is available on github for all your forking pleasure. Developing For HUE On The Server Side Applications for HUE are usually implemented in Django, a popular MVC web framework that understands the application namespaces. On top of that, the SDK lets the application bundle and start helper daemons which might, for example, talk to various interfaces in Hadoop, HDFS, or one of the numerous other applications that ship with CDH3b2 (such as Hive in the video example above). To develop applications for HUE developers need only be familiar with basic web development practices. Learning Django is pretty straight forward; there are a lot of terrific resources online, including a free online book, not to mention the excellent documentation on the Django project site itself. Working with Web 1.0 When we develop applications for HUE here at Cloudera, we often talk about the “Web 1.0″ view. This is because the rich user interface you see when you look at applications in HUE are always built on top of plain old HTML with as little custom JavaScript and styling for each app. A typical HUE application returns tables of data, standard HTML form controls, lists of links for navigation and other simple web components. This makes building, debugging, and testing these web applications easier and also makes life easier for developers who want to build these apps. Pulling Web 1.0 into HUE With JFrame These “Web 1.0″ views aren’t that fun to use though. To convert these raw-looking “Web 1.0″ apps into slick user experiences with sortable tables, forms that validate inputs as you enter values, graphs, pretty navigation links and so on, developers pepper their HTML with semantic instructions that HUE knows how to convert into these highly interactive components. HUE allows users to launch windows that fetch these Web 1.0 views via an AJAX class that we call JFrame. JFrame gets its name because it’s essentially a JavaScript implementation of an IFrame – when the user clicks a link or submits a form, the request is routed back into the same DOM element as if it were a little web browser. Before showing that response to the user, HUE applies filters to the HTML and transforms them into these nice looking widgets: Developing On the Client Side Let’s take a simple example and illustrate what we’re talking about here. Let’s say we want our application to have a nice looking table of data with truncated text when things get too verbose as well as sortable columns. These patterns are already defined in the HUE development environment, so developers don’t need to write any JavaScript or CSS to get an output that looks like this: Instead, we require only that they decorate the HTML they return to have certain properties. Here’s what the HTML returned in the example above looks like:   &lt;table class="sortable" data-filters="HtmlTable FitText-Children"
  data-fit-text="td" cellpadding="0" cellspacing="0"&gt;
    &lt;thead&gt;
      &lt;tr&gt;
        &lt;th&gt;ID&lt;/th&gt;
        &lt;th&gt;TimeZone&lt;/th&gt;
        &lt;th&gt;Name&lt;/th&gt;
        &lt;th&gt;GEO Latitude&lt;/th&gt;
        &lt;th&gt;GEO Longitude&lt;/th&gt;
      &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;22&lt;/td&gt;
        &lt;td&gt;New York City, New York, United States of America&lt;/td&gt;
        &lt;td&gt;America/New_York&lt;/td&gt;
        &lt;td&gt;40.7255&lt;/td&gt;
        &lt;td&gt;-73.9983&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;23&lt;/td&gt;
        &lt;td&gt;San Francisco, California, United States of America&lt;/td&gt;
        &lt;td&gt;America/Los_Angeles&lt;/td&gt;
        &lt;td&gt;37.7587&lt;/td&gt;
        &lt;td&gt;-122.433&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
 Viewed on its own, it’s not that pretty nor is it interactive. To turn this plain HTML into a nice looking interactive interface, HUE uses a MooTools plugin we’ve developed called Behavior (the name being a nod to an old JavaScript library by that name, released back in 2005). MooTools Behaviors convert standard HTML components into attractive, interactive UI components. Things like ajax forms, tool tips, graphs and more. In the example above, the key to turning this table into a sortable table with truncated text is in the table tag:   &lt;table class="sortable" data-filters="HtmlTable FitText-Children"
  data-fit-text="td" ... These HTML5 data tags tell Behavior to run specific filters against these elements. These filters transform it into the screenshot above. The HUE SDK UI Library Currently the SDK library in HUE features roughly three dozen of these UI patterns and we add more every day. If you download and install HUE you’ll find an application called “JFrame Gallery” that demos all of them. It’s important to note that applications built in HUE don’t have to use these patterns. It’s easy to add your own if we’re missing something or if you don’t like the way the default one works. Whenever we write new applications we reuse these default patterns, but occasionally we find a need for a pattern that only makes sense for that application. When this happens we still write these HTML transformers, but they remain private to that specific application. Thus developers who want to use the patterns we have already can crank out applications that look nice without writing CSS or JavaScript (or perhaps writing only a little), while others who want to roll things their own way can customize everything they like. In today’s post I’ve only scratched the surface of how all this works. If you’d like to get a richer explanation I suggest you read through the SDK walkthrough and maybe download HUE and follow along, creating a simple web app using our tools. An Invitation We’re hoping to see people in the Hadoop community jump in and develop more applications for HUE and contribute to the ones already there. One of the benefits to this consolidated environment, aside from usability, the ability to cross-link apps, and not having to re-invent the wheel every time you want to browse the file system, is that anyone can jump in and contribute. If you think HUE could solve your problems if only it had feature X then we hope you’ll consider forking the repository on github and getting involved. If you have an idea for a killer app to make Hadoop better, we hope HUE is the right place for you to develop it. Sign up for the HUE Developer Group on Google Groups and let us hear from you. If you would like to get started with HUE, download it, and read through the installation guide and user manual. If you’re looking for ideas of things you might develop on HUE, give one of these a shot: An HBase explorer A Yahoo! Pipes-style UI for Oozie A social gaming app that gives people points for doing interesting things to data (e.g. running a Hive query, using a Combiner, etc.) so you can quickly figure out the data experts in your organization. Surprise us. Go nuts. Look for more posts in the future that focus more on the server side of HUE development.</snippet></document><document id="632"><title>What’s New in CDH3b2: HUE</title><url>http://blog.cloudera.com/blog/2010/07/whats-new-in-cdh3b2-hue/</url><snippet>The HUE (aka. Hadoop User Experience) project [download|installation|manual] started as Cloudera Desktop about a year ago. The old name “Desktop” really refers to a desktop look-and-feel, since HUE is a web UI for Hadoop. Beyond delivering a suite of web applications, it is also a platform for building custom applications with a nice UI library. Gradually, we realized how much value such a UI platform would bring to the community, and I am very excited that Cloudera contributed HUE as an open source project. HUE: a Web UI for Hadoop A still image does not do HUE justice. The following screencast, as an example, shows the rich user interaction in the Hive UI application, called Beeswax, and how HUE simplifies Hadoop for the average user. Applications Currently, HUE comes with a suite of applications. To highlight some of them: User Admin: Account management for HUE users. File Browser: Browse HDFS; change permissions and ownership; upload, download, view and edit files. Job Designer: Create MapReduce jobs, which can be templates that prompt for parameters when they are submitted. Job Browser: View jobs, tasks, counters, logs, etc. Beeswax: Wizards to help create Hive tables, load data, run and manage Hive queries, and download results in Excel format. Help: Documentation and help. Deployment Story Administrators like HUE especially for its ease of deployment. The one and only step is to set up HUE on a server machine with Hadoop installed. There is no client-side software — other than the web browser. And upon upgrade, there is no client compatibility problems to worry about. Unforeseen at first, we have come across cases in which administrators prefer HUE as a security tool. They set up HUE as the only gateway for normal users to access Hadoop. By leveraging its user management capability, HUE can enforce permission much more than the cooperative trust model in Hadoop command line. HUE submits MapReduce jobs as the logged-in user, and talks to HDFS as the logged-in user. It can also integrate with existing authentication systems. In addition, some IT administrators find comfort in letting corporate users access Hadoop by opening only a web port to the cluster. HUE: a Web Application Platform An important goal of HUE is to make it easy to add new applications so that they can take advantage of the functionality already provided by the existing applications. For example, if I want to write a Pig UI (not in HUE today), I would likely need to support multiple users (provided by HUE with the User Admin app), link Pig executions to MapReduce jobs (provided by the Job Browser app), let users select their UDF jars (provided by the File Chooser app), and download results in CSV or Excel format (also provided). The cross linkage of the different applications makes it powerful, and reusing existing components saves developers time. The engineers at Cloudera have spent tremendous effort in cleanly separating the core HUE framework (aka. the SDK) from the applications. And as the HUE team here develops new applications for Cloudera Enterprise, we ourselves use the SDK. These applications include a Flume UI to manage and monitor data flows, an advanced user management application with hierarchical group support and fine-grained permission control, and tools for cluster resource accounting. The HUE SDK is worth its own blog post, which is coming in the next few days. To whet your appetite, I will briefly describe the SDK framework. The backend of HUE uses Django, a popular MVC web framework that understands the application namespaces. On top of that, the SDK lets the application bundle and start helper daemons. The frontend uses MooTools, and allows the developer to reuse common interactive UI elements in a declarative way (via CSS), as opposed to writing lots of JavaScript. Things like sortable table, tabbed pages, search filter, growl notification, custom right click menu, and many more are available with no effort. Get HUE If you would like to get started with HUE, download it, and read through the installation guide and user manual. Drop us a note on the HUE user list. We would love to hear your feedback. If you are considering writing an application on HUE, or contribute to HUE itself, please also join our developer list.</snippet></document><document id="633"><title>Rackspace’s OpenStack shows the way for public cloud vendors</title><url>http://blog.cloudera.com/blog/2010/07/rackspace%e2%80%99s-openstack-shows-the-way-for-public-cloud-vendors/</url><snippet>Ed Albanese leads business development for Cloudera. He is responsible for identifying new markets, revenue opportunities and strategic alliances for the company. Rackspace’s OpenStack announcement is an important step in expanding the utility of public clouds. It will push other cloud vendors to stop “forking around” and enable the standards customers are actually using. Rackspace’s move is a win for consumers and for Rackspace – it creates the opportunity to run industry standard software in truly optimized ways. Open source software business models and public cloud business models, thus far, have been somewhat incompatible. In some cases, public cloud vendors compete with open source software vendors by offering the software vendors product as a service and delivering implied or actual support as a bundled component. There has been a fair bit of discussion on this topic. Of course, that’s a business problem – not a user problem. Increasingly, however, the precedent has been for public cloud vendors to take open source software and “fork” it to optimize it for their specific infrastructure. A “fork” of open source software is when code is changed, and those changes are kept private, creating multiple incompatible versions of the original package. In the process, they deviate from the open standard that application vendors, system integrators and in-house developers write against. Critics would argue that public cloud vendors do it to create “lock-in”. Public cloud vendors argue that it allows them to optimize for their infrastructure. Either way, this is a user problem. A fork from the standard forces everyone – ISVs, system integrators, in-house developers – to account for these differences in their respective applications and either modify them or develop against only part of the standard – the absolute lowest common denominator. This is the wrong approach. It stifles the ability of customers to get applications optimized by the vendors that know it best. It locks them into deployments and commercial relationships that may not be in their best interests as requirements and the market evolve. I believe this has been an inhibitor to real production deployments and wider adoption of public clouds. The benefits of open source for cloud customers are no secret. Some forward-looking vendors – Eucalyptus, for example – recognized those benefits early, and announced fully open source implementations of popular cloud infrastructure and APIs. Today, Rackspace Hosting takes it one step further. The company has announced the OpenStack project, which combines the Cloud Files and Cloud Servers software used within Rackspace’s internal hosting infrastructure (about 300,000 servers) with technology from NASA’s Nebula Cloud platform. OpenStack is now freely available and distributed under the Apache 2.0 open source license. OpenStack is a positive move for both prospective and active users of public clouds, for other cloud providers and for software developers everywhere. No doubt, the ongoing success of the Eucalyptus platform, which is aimed more directly at private and hybrid clouds, caught the eye of Rackspace. Eucalyptus has it right; if you want to deliver a cloud infrastructure, you need to work well with a wide variety of applications. The key to doing this not just well – but the best ways possible – is to engage the experts; the software vendors and developers of these applications and services. There are multiple ways of doing this – but open source is the most effective in my opinion. In preparing for the announcement and roll-out of OpenStack, Rackspace has invited Cloudera to participate by making Cloudera’s Distribution for Hadoop work well on the platform. I’m excited about this collaboration for three reasons: No one knows how to create and commercially enable Hadoop better than Cloudera. Cloudera will absolutely make sure our customers who elect to run on Rackspace’s platform have a great experience and that our tools do what they should. This should be much simpler with access to the complete source code that powers the infrastructure. Hadoop is a critical component to many successful PaaS environments. If you are going to build a new web application, you are surely going to want to understand how it is performing and analyze the data you are collecting and generating in new ways. There is no shortage of public and private cloud environments that leverage Hadoop as a key analytics component. Those that run on OpenStack will no doubt benefit from the optimizations Cloudera and others will contribute to ensure the standard works well A new vector in public cloud competition has been introduced. By delivering a platform where the widest variety of applications has been / can be optimized for deployment by application vendors themselves, closed source infrastructure vendors will be at a disadvantage as application vendors optimize where they can and run slower and less reliably where they can’t. Disclosure: Rackspace is a customer of Cloudera’s. They use our software as part of their infrastructure to monitor and track performance and troubleshoot issues.</snippet></document><document id="634"><title>What’s New in CDH3b2: Sqoop</title><url>http://blog.cloudera.com/blog/2010/07/what%e2%80%99s-new-in-cdh3b2-sqoop/</url><snippet>Cloudera customers usually have two major sources of data: log files, which can be imported to Hadoop via Flume, and relational databases. Throughout the previous releases of CDH2 and CDH3, Cloudera has included a package we’ve developed called Sqoop. Sqoop can perform batch imports and exports between relational databases and Hadoop, storing data in HDFS and creating Hive tables to hold results. We described its motivation and some use cases in a previous blog post a while ago. In CDH3b2, we’ve included a greatly-expanded version of Sqoop which has had a major overhaul since previous releases. This version is important enough that we’re deeming it the “1.0″ release of Sqoop. In this blog post we’ll cover the highlights of the new features available in Sqoop. New Interface The biggest change you’ll notice is that the Sqoop command-line interface has completely changed. Users who have been embedding Sqoop in scripts may be frustrated by this incompatible change, but we think that given the amount of functionality available in Sqoop now, some refactoring is necessary, and this is the correct opportunity to do it. Sqoop is now arranged as a set of tools. If you type sqoop help, you’ll see the list of tools available. Most of the original funtionality is contained in a tool called import; running sqoop help import will list the options available to this tool. Improved Export Performance In CDH3b1 we provided basic support for exports: the ability to take results from HDFS and insert them back into a database. CDH3b2 features a completely rewritten export pipeline which demonstrates considerably greater throughput and scalability. You can now export gigabytes of data with high performance. For MySQL users, we’ve added a separate “direct mode” channel that uses mysqlimport to perform this job even faster. Large Object Support Sqoop now has the ability to import CLOB and BLOB columns and store them in a separate file format in HDFS designed for these large records. If you have been accumulating large volumes of unstructured data in your database for a long period of time, Sqoop can now help you get this data into a format that you can more easily process with MapReduce. Append to an Existing Dataset Sqoop can now append new results to an existing dataset in HDFS. Users who perform periodic imports to synchronize a copy of a dataset in HDFS with a continually-updated copy in a database will now find that this process has been made much smoother. Documentation Overhaul We’ve completely rewritten the Sqoop user manual. You can browse it online, and it’s also included in the Sqoop installation package. Oozie Integration As Arvind mentioned in yesterday’s blog post about Oozie, Sqoop is now a supported component of the workflow engine. You can import source data from a database, run a MapReduce pipeline, and export your results back to a database entirely inside the Oozie framework. On the Horizon… We’re continually working on improving Sqoop at Cloudera. Here’s a short list of new features we’re actively working on for the next release: HBase integration – Import from a database to a table in HBase. Free-form query support – Sqoop’s existing import model is table-driven. You can now import data from an arbitrary SELECT statement against your database. UPDATE support – Export a set of UPDATE statements against an existing database, rather than a set of new records. We’re also building in support for additional database vendors. We’ve seen a lot of interest recently in integrating with Sqoop. In June, we announced a partnership with Quest to develop high-performance Oracle support. We’re also pleased to announce another partnership with Netezza, to develop tools for fast connectivity between Hadoop and their enterprise data warehouse. Longer term, we’re looking at some deeper enhancements to the system, such as adding a public Java API as well as support for pluggable serialization and storage formats, allowing users to process records with Avro or Protocol Buffers. These features (and more!) are coming soon. For more information Get started with CDH3 and Sqoop by configuring the installation packages. Sqoop is open source! Get the code at github.com/cloudera/sqoop. Read the user guide to learn more about how it works. Join the mailing list to get help with Sqoop and participate in the user community. Browse the issue tracker to see outstanding issues, or to file a bug report.</snippet></document><document id="635"><title>Hacking with Cloudera on CDH</title><url>http://blog.cloudera.com/blog/2010/07/hacking-with-cloudera-on-cdh/</url><snippet>The amount one can learn from data is often immense, and CDH makes it easier for analysts, researchers, programmers, and organizations to extract meaningful information from their data. We’d like to invite you to come build, play, and experiment with our newest release of CDH, CDH3, at our hackathon on July 27th. The CDH hackathon will be an opportunity to team up with Cloudera employees and other community members to use CDH3 to solve fun and interesting problems. Each attendee is encouraged to come to the hackathon with an idea about what to build. Then, at the beginning of the day, we’ll find groups to work with and start hacking. Some example projects might be to collect Twitter data and perform social graph analysis. Or maybe to analyze the Enron email data to find friend circles and categorize them into fraudulent or innocent. At the end of the day we’ll do an anonymous poll and vote on the best project. There will be a prize for the winning group! We’ll get started at 9:30am on July 27th at Cloudera Headquarters. We’ll finish sometime around 7:30pm. Lunch will be provided by our investor Accel. In summary: What: Build applications using CDH to do cool things; collaborate with the community and Cloudera employees Where: Cloudera HQ — 210 Portage Ave., Palo Alto, CA 94306 When: Tuesday July 27, 2010, 9:30am Please send an email to hackathon -at- cloudera -dot- com if you’d like to attend. Spots are limited!</snippet></document><document id="636"><title>What’s New in CDH3b2: Oozie</title><url>http://blog.cloudera.com/blog/2010/07/whats-new-in-cdh3-b2-oozie/</url><snippet>Hadoop has emerged as an indispensable component of any data-intensive enterprise infrastructure.  In many ways, working with large datasets on a distributed computing platform (powered by commodity hardware or cloud infrastructure) has never been easier. But because customers are running clusters consisting of hundreds or thousands of nodes, and are processing massive quantities of data from production systems every hour, the logistics of efficient platform utilization can quickly become overwhelming. To deal with this challenge, the Yahoo! engineering team created Oozie – the Hadoop workflow engine. We are pleased to provide Oozie with Cloudera’s distribution for Hadoop starting with the beta-2 release. Why create a new workflow system? You might wonder why a new workflow system is necessary for Hadoop, given that there are quite a few existing commercial and open-source systems available.  While it is possible to use existing general-purpose workflow systems with Hadoop, it is anything but simple. Intricacies such as monitoring long running jobs and interfacing with the distributed file system require extensive work to port general workflow systems to the Hadoop environment. Oozie, on the other hand, is designed specifically for the Hadoop platform and uses it as its execution environment. It has built-in support for Hadoop tasks and integrates with this environment cleanly. Oozie itself is fairly light-weight, requires minimal configuration, and scales linearly – thus offering a sustainable approach to building workflows in the Hadoop environment. Still not convinced about Oozie? Consider these numbers for a moment: According to the Oozie presentation during Hadoop Summit in June – there are over 4800+ workflow applications deployed within Yahoo! at the moment, with largest workflow containing 2000 actions. There were roughly 55,000 workflow jobs that Yahoo! infrastructure team executed in the month of May 2010 alone, with workflows that could run up to many hours. A Simple Use-Case Consider the example of web log analysis. For a typical operation that deals with a few gigabytes of log data every day, the steps involved in analyzing it can be many. First, the files have to be moved into a certain location. Next, the files are used to create new tables or partitions in Hive which are then queried to see if certain criteria have been met. For instance, if the number of accesses for a particular resource exceeds a certain threshold, some notifications must be generated. Regardless of the outcome of this analysis, certain other queries need be run in order to populate other tables that record rolled-up information. While these steps are not difficult to execute, they are repetitive and time consuming. Ideally, such steps should be automated in a manner that notifications are raised to operators when something interesting is discovered by the system or if there is a failure of some sort. That is exactly what Oozie does. Using Oozie, all the steps outlined in this example can be modeled as a workflow which can be executed with a single command. Once the workflow takes off, you can sit back and relax while Oozie runs through each step of the flow. Oozie Highlights Oozie workflow bundles the workflow definition, any libraries necessary for the execution of workflow actions, and properties that are necessary to resolve parameterized values in the workflow. Together, this bundle is referred to as an Oozie application and informally – a workflow. These are deployed to the Oozie server using a command line utility. Once deployed, the workflows can be started and manipulated as necessary using the same utility. The web console for Oozie server can be used to monitor the progress of various workflow jobs being managed by the server. Scalability Oozie is a server-based web application that uses a transactional store to manage workflow metadata and execution states. It relies on HTTP-based notifications and polling mechanism to monitor the progress of workflows and to manage its runtime state. The Oozie server itself does not do any particular work other than this state management. All of the work is delegated to worker nodes within the cluster on which the workflow executes. This allows Oozie to scale horizontally by adding more Oozie servers pointing to the same workflow metadata store. Resilience When a workflow execution encounters a transient error condition, Oozie automatically attempts to execute the action again. In some situations, when the error requires user intervention, Oozie can suspend the workflow indefinitely allowing the administrator to step in, take corrective action, and resume the workflow. For long running workflows that fail, Oozie provides a mechanism by which the workflow can be restarted from the point of failure to avoid redoing the steps that may have already completed earlier. Simple and Intuitive Workflows in Oozie are expressed in a simple XML representation that is inspired by process definition language – JPDL. However, compared to the overal JPDL schema, the Oozie schema is extremely simplified and intuitive. The key concepts in a Oozie workflow is that of action and control flow nodes. Action nodes do the workflow tasks – such as moving files, running Map/Reduce jobs, running Hive Queries etc. Control-flow nodes govern the progress of the workflow from action to action, enabling things like error handling, conditional execution and branching logic. Together, the action and control-flow nodes are arranged in a directed acyclic graph (DAG), which represents the overall workflow. This DAG is executed by the Oozie server in a controlled-dependency manner – implying that a node shall be executed if and only if all of the nodes that it depends upon have been executed successfully. This is very similar to how one would manually implement the workflow – by initiating actions and starting follow-up actions when the previous ones are complete with expected outcome. The bottom line is that if you know the steps you need to take for managing data in your Hadoop environment, you can easily express them as a workflow and hand it off to Oozie for execution. Rich Set of Features The core feature set of Oozie is designed to take care of the most commonly-exercised functionality for the Hadoop platform. The key objective behind these features is to ensure that anything done manually can be implemented as a workflow task to the last detail. The following list, while far from being exhaustive, lists out some of the many features that Oozie has. 1. Parameterization: With parameterization support, workflows can be written once and executed many times with different parameter bindings. This allows reuse of workflows in a manner that promotes ease of maintenance and management. 2. Fine-Grain and Coarse-Grain Notification Support: Workflows in Oozie can be configured to notify external systems at varying degree of granularity. Notifications can be raised when a workflow changes its overall state, or when an individual action within it changes state. These notifications are implemented as HTTP GET requests which can pass extra information to the receiver such as job identifier. Using this mechanism, external systems can be integrated at various stages of the workflow as necessary. 3. User Propagation: The user and group information associated with the workflow job is propagated by Oozie to the underlying action execution and cannot be overwritten.  This allows Oozie to work together with Hadoop security to ensure that actions are authorized to access and manipulate data where applicable. 4. Java Client API: A programmatic client API is provided by Oozie that can be called from external systems to better integrate with the Oozie system. This API provides equivalent functionality as provided by the command line client utility. 5. Web Services API: Oozie also provides a rich REST/JSON API for web-services integration. Clients that prefer to access Oozie via this API can directly access and manipulate workflows running on Oozie server using this interface. 6. Built in actions: The default actions  provided by Oozie cover a vast majority of the use-cases for workflows including: Map/Reduce action: Allows you to model Map/Reduce jobs. Streaming Map/Reduce action: Allows you to specify executable mapper and reducers that can be plugged in using the streaming support. Pig action: Allows you to run custom Pig scripts and tasks. FS action: Allows you to manipulate the Hadoop file system as necessary. SSH action: Allows you to securely execute commands over SSH connections. Sub-Workflow action: Allows a workflow to be executed within another workflow. Java action: Allows you to plug in any Java program that has a main method for direct execution. Hive action: Allows you to run Hive commands from within the workflow. This action is contributed by Cloudera. Sqoop action: Allows you to run Sqoop commands from within the workflow. This action is contributed by Cloudera. 7. Built in control-flow nodes: The control-flow nodes provided by Oozie allow you to create sophisticated workflow graphs. Together with built-in support for JSP Expression Language functions, the control-flow nodes can be used for creating conditional executions where necessary. These include the following: Start node: Indicates the starting point of a workflow. End node: Indicates a completion point of a workflow. Kill node: Allows a workflow action outcome to cause workflow termination where necessary. Decision node: Allows a branching construct similar to traditional switch-case statement using JSP Expression Language expressions as predicates. Fork and Join node: Fork node allows the splitting of a workflow execution path into multiple concurrent paths of execution. Join nodes allow the branches created by fork node to merge back into a single execution path. Fork and join nodes must be used in pairs and allow efficient parallel execution of tasks that do not have a direct-controlled dependency order. 8. Custom Extensions: Oozie provides an extension mechanism that allows the implementation of custom actions. This mechanism should be used when the extension cannot be modeled as a regular Java action. Get up and running! Now that you have a good feel for what Oozie is and how simple it is, it is time for you to get your instance of Oozie installed and configured. Follow our Quick Start Guide to get up and running with Oozie in a matter of minutes. You can reach out to the Oozie user group by sending a mail to oozie-users@yahoogroups.com.</snippet></document><document id="637"><title>What’s New in CDH3b2: Pig</title><url>http://blog.cloudera.com/blog/2010/07/whats-new-in-cdh3-beta-2-pig/</url><snippet>CDH3 beta 2 includes Apache Pig 0.7.0, the latest and greatest version of the popular dataflow programming environment for Hadoop. In this post I’ll review some of the bigger changes that went into Pig 0.7.0, describe the motivations behind these changes, and explain how they affect users. Readers in search of a canonical list of changes in this new version of Pig should consult the Pig 0.7.0 Release Notes as well as the list of�backward incompatible changes. Load-Store Redesign The biggest change to appear in Pig 0.7.0 is the complete redesign of the LoadFunc and StoreFunc interfaces. The Load-Store interfaces were first introduced in version 0.1.0 and have remained largely unchanged up to this point. Pig uses a concrete instance of the LoadFunc interface to read Pig records from the underlying storage layer, and similarly uses an instance of the StoreFunc interface when it needs to write a record. Pig provides different LoadFunc and StoreFunc implementations in order to support different storage formats, and since this is a public interface users may provide their own implementations as well. The primary motivation for redesigning these interfaces is to bring them into closer alignment with Hadoop’s InputFormat and OutputFormat interfaces, with the goal of making it much easier to write new LoadFunc and StoreFunc implementations based on existing Hadoop InputFormat and OutputFormat classes. At the same time the new interfaces were also made a lot more powerful by providing direct access to configurations as well as the ability to selectively read individual columns. In the short span of time since these new interfaces appeared the Pig community has responded by writing a variety of custom Loaders including ones for�Cassandra, Voldemort, and Hive’s RCFile columnar storage format. It is important to note that these new plugins were written without any direct involvement from the Pig core team, which is a significant validation of the work that went into the redesign effort. A list of third-party Pig Loaders is maintained on the Pig Intoperability page. Users who are interested in writing their own LoadFuncs or StoreFuncs should first read the updated Load-Store HowTo. If you are upgrading from an earlier version of Pig you need to be aware that the new Load/Store interfaces are not backward compatible with the old interfaces. Users who have written custom LoadFuncs or StoreFuncs that work with an earlier version will need to upgrade these functions to use the new interfaces. For more details about this process please consult the Load-Store Migration Guide on the Pig wiki. Use the Distributed Cache to Improve Performance Pig 0.7.0 includes a set of important performance enhancements that aim to make queries run faster by leveraging Hadoop’s Distributed Cache. The key observation that motivated these changes is that Pig query plans often involve directing a large number of tasks to read the same sample of data. One can observe this access pattern in the Fragment-Replicate Join, SkewedJoin, and GroupBy operators. Earlier versions of Pig read this data directly from the underlying distributed file system, an approach that is inefficient, but also has the potential to cause a cluster-wide failure if a large number of concurrent Map tasks swamp the NameNode with read requests. PIG-872 and PIG-1218 remedy this problem by loading the common data into the Distributed Cache. This allows tasks to perform a local disk read instead of having to wait while the data is retrieved from HDFS, and also allows tasks that run on the same node to share the same data. Use Hadoop’s Local Mode for Pig Local Mode One of things that has made Pig especially easy for new users to pick up is its support for a local mode that does not require an Hadoop installation. Unfortunately, maintaining this feature has turned into a major headache for the Pig developers as it requires a large body of custom code and execution paths that are not shared with the rest of the system. A direct consequence of this is that many of the new features that have been added to Pig do not work in local mode, and this has caused a lot of confusion within the Pig user community. Based on these factors the Pig developers decided that it made sense to replace Pig’s custom local mode implementation with one that depends on Hadoop’s local mode. This change benefits Pig users since they can now test a script in local mode and be confident that it will run correctly in distributed mode, or vice-versa. However, users should be aware that there is one unfortunate side-effect of this change: Pig now runs roughly an order of magnitude slower in local mode. Making Pig 0.7.0 Even Better Pig 0.7.0 was released in mid-May, and since that time several important patches have appeared for bugs that were found in the original release. These patches include a fix that allows UDFs to access counters, as well as another fix that adds a counter to track the number of output rows in each output file. I think you’ll be glad to hear that we have included these patches as well as others in the version of Pig 0.7.0 that is included in CDH3 beta 2. For More Information We hope you’ll give CDH and Pig a spin. The CDH Quick Start Guide is the best place to begin, followed with the Pig installation instructions in the Pig Installation Guide.</snippet></document><document id="638"><title>What’s New in CDH3b2: Flume</title><url>http://blog.cloudera.com/blog/2010/07/whats-new-in-cdh3b2-flume/</url><snippet>As part of our series of announcements at the recent Hadoop Summit, Cloudera released two of its previously internal projects into open source. One of those was the HUE user interface environment, which we’ll be saying a bit more about later this week. The other was our data movement platform Flume. We’ve been working on Flume for many months, and it’s really exciting to be able to share the details of what we’ve been doing. In this blog post I’d like to introduce Flume to the world, and say a little about how it might help problems with data collection that you might be facing right now. What is Flume? We’ve seen our customers have great success using Hadoop for processing their data, but the question of how to get the data there to process in the first place was often significantly more challenging. Many customers had produced ad-hoc solutions with complicated shell scripts and periodically running batch copies. Such solutions, while minimally effective, don’t allow the user any insight into how they were running, whether or not they were succeeding and whether or not any data were being lost. Changing or reconfiguring the scripts to collect more or different data was hard. They were, at best, ‘hit and hope’ solutions. At the same time, we observed that much more data are being produced than most organisations have the software infrastructure to collect. We are very keen to allow our users to take advantage of all the data that their cluster is generating. Looking around, we saw no solutions that supported all the features that we wanted to provide to our customers, incuding reliable delivery of data and an easy configuration system that didn’t involve logging in to a hundred machines to restart a process, as well a powerful extensibility solution for easy integration with a wide variety of data sources. As a result, we designed and built Flume. Flume is a distributed service that makes it very easy to collect and aggregate your data into a persistent store such as HDFS. Flume can read data from almost any source – log files, Syslog packets, the standard output of any Unix process – and can deliver it to a batch processing system like Hadoop or a real-time data store like HBase. All this can be configured dynamically from a single, central location – no more tedious configuration file editing and process restarting. Flume will collect the data from wherever existing applications are storing it, and whisk it away for further analysis and processing. Four design goals We designed Flume with four main goals for data collection in mind. Reliability – We recognise that failures regularly occur in clusters – machines crash, networks fail, cables get unplugged – and we designed Flume to be robust to that possibility. We also recognise that different kinds of data have different levels of importance, and therefore designed Flume to have fine-grained tunable reliability guarantees that dictate how much effort Flume goes to to ensure that your data are delivered when those failures happen. Scalability – Flume is designed to capture a lot of data from a wide variety of sources. As such we want to be able to run Flume on hundreds of machines with no scalability bottleneck. Flume is horizontally scalable, which broadly means that performance is roughly proportional to the number of machines on which it is deployed. Manageability – Flume is designed to be managed centrally. A medium sized Flume deployment is too large to configure machine-by-machine. We want Flume to enable flexible data collection, and that means making it easy to make changes. Extensibility – We want Flume to be able to collect all your data. We’ve written a number of different ‘sources’ and ‘sinks’ for Flume ourselves, but in the spirit of teaching a man to fish we’ve made sure that the API for doing so is extremely simple. That way, anyone can prototype and build new connectors for Flume to almost any data source very quickly indeed. Flume’s architecture Flume’s architecture is very simple. Data passes through a simple network of logical nodes, which are lightweight entities that know how to do exactly one thing: read data from some source and send it on to a sink. A source might be a file, process output or any other of the many sources that Flume supports – or it might be another Flume logical node. Similarly, a sink might be another Flume logical node, or it might be HDFS, S3 or even IRC or an e-mail. Structure of a Flume logical node We can link logical nodes together into a chain, or flow. At the beginning of a flow is the original source of the data, and the sink of the final logical node in the chain defines where the data will eventually be delivered. We call the first logical node in a flow the agent, and the last logical node the collector. The intermediate nodes are called processors, and their job is to do some light transformation and filtering on the events as they come through in order to get them ready for storage. A single Flume process can house many logical nodes. This makes it very easy to configure, create, delete and restart individual logical nodes without having to restart the Flume process. All of this configuration is controlled by the Flume master, which is a separate distributed service which takes care of monitoring and updating all the logical nodes in a Flume deployment. Users can make configuration changes, such as changing a logical node’s source or creating a brand new logical node, through the Flume master, either by using the shell that we’ve provided or by using the simple web-based administration interface. An example flow A web server log flow (click for larger) The diagram above shows the layout of a simple flow for capturing web server logs. Three logical nodes are involved, each with different source and sink configurations. The agent is continually watching the tail of an Apache HTTP server log file. Flume’s tail source correctly deals with log rotation, so you don’t have to change your logging policy to integrate with Flume. Each line of the log is captured as one event, and then sent downstream to… The processor receives individual events from the agent, and uses regular expressions to identify and extract the name of the user’s browser (such as ‘Firefox’ or ‘Internet Explorer’) from the log event. This metadata is then attached to the event so that it’s available for later processing by… The collector receives the annotated events from the processor, and writes them to a path in HDFS that is determined by the browser string that was extracted earlier. Flume’s HDFS sink uses a simple template language to figure out where to write events. In this case, all logs from users using Firefox are saved to /weblogs/Firefox/ in HDFS. This simple example shows how Flume is easily able to capture your data, and to perform some non-trivial processing on it. This entire flow can be constructed and configured from the Flume Master, without any need to log on to the individual machines involved. For more information Download and get started with Flume here. Flume is open source! Grab the source code from Cloudera’s Github page here. The Flume User Guide is a one-stop resource for a lot more detail on Flume’s architecture, usage and internals. There have recently been some good conversations on the Flume user list comparing Flume to a message queue, to Chukwa and to Scribe. We have a series of technical blog posts on using Flume planned for the near future. Stay tuned for much more Flume!</snippet></document><document id="639"><title>What’s New in CDH3b2: ZooKeeper</title><url>http://blog.cloudera.com/blog/2010/07/whats-new-in-cdh3-b2-zookeeper/</url><snippet>CDH3 beta 2 is the first version of CDH to incorporate Apache ZooKeeper. ZooKeeper is a highly reliable and available coordination service for distributed processes. It is a proven technology and a well established open source project at Apache (sub-project of Hadoop). ZooKeeper is distributed coordination Often distributed applications need some way to coordinate across processes; locking resources, managing queues of events, electing a “leader” process, configuration, etc… Coordination operations such as these are notoriously hard to get right. ZooKeeper provides a relatively simple API which allows clients to correctly implement these and many other coordination mechanisms. ?ZooKeeper is itself a replicated service based on a quorum algorithm. One or more ZooKeeper servers form what’s called an “ensemble”, which are in constant communication. As the size of the ensemble increases the reliability of the service itself increases – as long as a majority of the configured ensemble servers are available the service is available. As an example, say you have an ensemble of size three (three ZooKeeper servers), if one of the three fail the service is still “up”. If two of the three fail the service is down. One could run with five servers, in which case if two servers fail the service as a whole would still be available. Seven server ensembles can survive three failures, and so on. Who should be interested in the ZooKeeper project? (I say “project/us” here because the team &amp; community are just as important as the software, if not more so) Well, developers appreciate us because we make it simple to implement some very difficult distributed communication problems. Operations teams like us because we ensure that they only need to learn, operate and maintain a single, sane, coordination mechanism that’s easy to manage. Business folks like the fact that we are a proven technology helping to ensure high availability, allowing the development/ops teams to focus on domain specific problems. For more detail on ZooKeeper see the overview page. Additional documentation relative to CDH releases is available here. Powered By ZooKeeper Yahoo!, Facebook, Twitter, Digg, Rackspace and a number of other companies are making use of ZooKeeper in their production environments today. Additionally technologies such as HBase, Solr, Katta, and Neo4j have all increased reliability/availability and extended their capabilities by adopting ZooKeeper. Here at Cloudera we recently open sourced Flume, a distributed, real-time event collection service which is also part of CDH3 beta 2. Flume makes use of ZooKeeper to coordinate its various distributed components – for example to store and manage dynamically updated configuration information. You can find out more about Flume here. ZooKeeper and CDH A significant amount of work has gone into ZooKeeper integration with CDH3. In particular the Cloudera team ensures that the CDH3 components relying on ZooKeeper (HBase and Flume) are fully compatible and will work together. CDH based ZooKeeper packages (tar, RPM, and DEB files) containing libraries as well as startup scripts for running as a service are available on the Cloudera website. Cloudera is an active member of the Hadoop and ZooKeeper communities – we have two active commiters working on ZooKeeper, myself and Henry Robinson. Henry recently contributed a major new feature called “observers” which greatly extends ZooKeeper’s read scalability, Henry also created and maintains the popular ZooKeeper python client binding. In addition to leading the ZooKeeper project at Apache I’ve personally been working on a number of projects for upcoming releases; I’m currently adding transport level security (encryption and authentication of communications) to the service. The team is constantly on the lookout for other areas where the technology may be applied. As I mentioned HBase and Flume are currently using ZooKeeper and we hope to extend this further, in particular around the idea of centralized configuration and monitoring for Hadoop. There are just too many configuration files floating around when setting up a Hadoop based service, it seems like ZooKeeper would be a perfect fit for this. A great example of this in use today is LinkedIn’s Norbert project, where ZooKeeper is used to maintain and manage cluster metadata. Join the ZooKeeper community Find out more about ZooKeeper on the official Apache project pages. On that page you’ll also find links for documentation, user and developer mailing lists, issue tracking, etc… we welcome new users and contributors. You might also follow me on twitter, where I frequently post on community related issues.</snippet></document><document id="640"><title>What’s New in CDH3b2: Apache HBase</title><url>http://blog.cloudera.com/blog/2010/07/whats-new-in-cdh3-b2-hbase/</url><snippet>Over the last two years, Cloudera has helped a great number of customers achieve their business objectives on the Apache Hadoop platform. In doing so, we’ve confirmed time and time again that HDFS and MapReduce provide an incredible platform for batch data analysis and other throughput-oriented workloads. However, these systems don’t inherently provide subsecond response times or allow random-access updates to existing datasets. Apache HBase, one of the new components in CDH3b2, addresses these limitations by providing a real time access layer to data on the Hadoop platform. HBase is a distributed database modeled after BigTable, an architecture that has been in use for hundreds of production applications at Google since 2006. The primary goal of HBase is simple: HBase provides an immensely scalable data store with soft real-time random reads and writes, a flexible data model suitable for structured or complex data, and strong consistency semantics for each row. As HBase is built on top of the proven HDFS distributed storage system, it can easily scale to many TBs of data, transparently handling replicated storage, failure recovery, and data integrity. In this post I’d like to highlight two common use cases for which we think HBase will be particularly effective: Analysis of continuously updated data Many applications need to analyze data that is always changing. For example, an online retail or web presence has a set of customers, visitors, and products that is constantly being updated as users browse and complete transactions. Social or graph-analysis applications may have an ever-evolving set of connections between users and content as new relationships are discovered. In web mining applications, crawlers ingest newly updated web pages, RSS feeds, and mailing lists. With data access methods available for all major languages, it’s simple to interface data-generating applications like web crawlers, log collectors, or web applications to write into HBase. For example, the next generation of the Nutch web crawler stores its data in HBase. Once the data generators insert the data, HBase enables MapReduce analysis on either the latest data or a snapshot at any recent timestamp. User-facing analytics Most traditional data warehouses focus on enabling businesses to analyze their data in order to make better business decisions. Some algorithm or aggregation is run on raw data to generate rollups, reports, or statistics giving internal decision-makers better information on which to base product direction, design, or planning. Hadoop enables those traditional business intelligence use cases, but also enables data mining applications that feed directly back into the the core product or customer experience. For example, LinkedIn uses Hadoop to compute the “people you may know” feature, improving connectivity inside their social graph. Facebook, Yahoo, and other web properties use Hadoop to compute content- and ad-targeting models to improve stickiness and conversion rates. These user-facing data applications rely on the ability not just to compute the models, but also to make the computed data available for latency-sensitive lookup operations. For these applications, it’s simple to integrate HBase as the destination for a MapReduce job. Extremely efficient incremental and bulk loading features allow the operational data to be updated while simultaneously serving traffic to latency-sensitive workloads. Compared with alternative data stores, the tight integration with other Hadoop projects as well as the consolidation of infrastructure are tangible benefits. HBase in CDH3 b2 We at Cloudera are pretty excited about HBase, and are happy to announce it as a first class component in Cloudera’s Distribution for Hadoop v3. HBase is a younger project than other ecosystem projects, but we are committed to doing the work necessary to stabilize and improve it to the same level of robustness and operability as you are used to with the rest of the Cloudera platform. Since the beginning of this year, we have put significant engineering into the HBase project, and will continue to do so for the future. Among the improvements contributed by Cloudera and newly available in CDH3 b2 are: HDFS improvements for HBase – along with the HDFS team at Facebook, we have contributed a number of important bug fixes and improvements for HDFS specifically to enable and improve HBase. These fixes include a working sync feature that allows full durability of every HBase edit, plus numerous performance and reliability fixes. Failure-testing frameworks – we have been testing HBase and HDFS under a variety of simulated failure conditions in our lab using a new framework called gremlins. Our mantra is that if HBase can remain available on a cluster infested with gremlins, it will be ultra-stable on customer clusters as well. Incremental bulk load – this feature allows efficient bulk load from MapReduce jobs into existing tables. What’s up next for HBase in CDH3? Our top priorities for HBase right now are stability, reliability, and operability. To that end, some of the projects we’re currently working on with the rest of the community are: Improved Master failover – the HBase master already has automatic failover capability – the HBase team is working on improving the reliability of this feature by tighter integration with Apache ZooKeeper. Continued fault testing and bug fixing – the fault testing effort done over the last several months has exposed some bugs that aren’t yet solved – we’ll fix these before marking CDH3 stable. Better operator tools and monitoring – work is proceeding on an hbck tool (similar to filesystem fsck) that allows operators to detect and repair errors on a cluster. We’re also improving monitoring and tracing to better understand HBase performance in production systems. And, as Charles mentioned, we see CDH as a holistic data platform. To that end, several integration projects are already under way: Flume integration – capture logs from application servers directly into HBase Sqoop integration – efficiently import data from relational databases into HBase Hive integration – write queries in a familiar SQL dialect in order to analyze data stored in HBase. Get HBase If HBase sounds like it’s a good fit for one of your use cases, I encourage you to head on over to the HBase install guide and try it out today. I’d also welcome any feedback you have – HBase, like CDH3 b2, is still in a beta form, and your feedback is invaluable for improving it as we march towards stability.</snippet></document><document id="641"><title>What’s New in CDH3b2: Core Hadoop</title><url>http://blog.cloudera.com/blog/2010/07/whats-new-in-cdh3-b2-core-hadoop/</url><snippet>In this post I’ll cover some of the larger or more significant changes that have gone into core Hadoop in CDH3 beta 2. The Hadoop in CDH3 is based on the latest Apache Hadoop core release – version 0.20.2 – which was released February 26th, 2010. Details of what changed in the Apache Hadoop dot release can be found in the release notes and change log. We’ve included hundreds of additional bug fixes, improvements and features atop the base Apache release. You can see these in the CDH3 beta 2 release notes and change log. The version (hadoop-0.20.2+320) indicates the base Apache Hadoop release version and the number of additional patches that have been applied to this release. The changes in CDH3 beta 2 are primarily focused on improving Hadoop’s internals as opposed to adding user-facing APIs. The biggest addition to CDH3 beta 2 is the incorporation of the 0.20 append branch to enable HBase. The 0.20 append branch is a version of Hadoop 0.20 that supports the sync method to provide durability for the HBase edits log. See the this page on the HBase wiki and look out for an upcoming post by Todd Lipcon for more detail. A handful of other key changes to HDFS are better handling of Data Node volume failure, better handling of Name Node replica failure, and stable performance in the face of heavy block deletion. We’ve also added a FUSE package for HDFS. The most notable addition to MapReduce is FIFO pool support in the fair share scheduler. Like CDH2 update 1, CDH3 now supports Ubuntu’s Lucid release. We’re busy incorporating Hadoop security from the Yahoo! security branch for the next beta – CDH3 beta 3 – which we expect to release in the fall. CDH3 beta 3 will be the final beta before CDH3 is declared stable. Get in touch if there are other changes you’d like to see.</snippet></document><document id="642"><title>More on Cloudera’s Distribution including Apache Hadoop 3</title><url>http://blog.cloudera.com/blog/2010/07/more-on-clouderas-distribution-for-hadoop-3/</url><snippet>A week ago we announced two significant product updates: a substantial functional update (doubling the number of components) to Cloudera’s Distribution including Apache Hadoop (CDH) and the launch of Cloudera Enterprise.  I wanted to delve a bit deeper into the first announcement regarding Cloudera’s Distribution including Apache Hadoop version 3 (CDH3).  This post will actually serve to kick off a series of posts that go into progressively more detail about different aspects of CDH3. Cloudera has been in the Hadoop business for nearly two years now, which doesn’t sound like a long time until you put it in context and realize Hadoop has only existed for twice that long. We did a tally recently and figured we employ more than 60 person-years of collective Hadoop experience.  Coupled with more than 40 paying customers and tens of thousands of downloads of our distribution each month, we’ve had a good vantage point from which to see how Hadoop gets used in the real world.   We learned a few things in this time that informed our latest update to CDH. We saw: an ever diversifying set of Hadoop applications Every week we see an expansion in the number and type of applications that are well suited to Hadoop: from simple query &amp; analysis to machine learning to click stream analysis to data transformation. Hadoop has already proven itself relevant in industries spanning high tech, financial services, web, telecommunications, manufacturing, pharmaceuticals, utilities and media.  We feel we have only scratched the surface of what is possible. We concluded: Hadoop is a multi-application platform for data intensive applications. More and more of Cloudera’s customers are moving from single use case applications to deploying Hadoop as general infrastructure to run multiple applications.  Hadoop is not a tool or add-on so much as it is a platform unto itself. We saw: a rapidly expanding ecosystem of Hadoop technologies and frameworks. Those who are familiar with Hadoop are familiar with popular job authoring frameworks like Hive and Pig, or high speed clients like HBase.  In fact, such frameworks generate the overwhelming volume of Hadoop workloads in most production situations.  These components also provide the most common interfaces by which other technologies integrate with Hadoop.  For example business intelligence tools interface with Hadoop via Hive drivers, and data warehouses and databases interface with Hadoop via the Sqoop framework. We concluded: Hadoop “core” (Mapreduce and HDFS) is the kernel of the Hadoop data platform. Like any kernel it has a central role and is responsible for the most vital functions of the platform.  But users and technologies rarely interact directly with the kernel.  It is too low level for most users to have a productive experience with it, and it is too central to allow dozens of adjacent technologies to access it directly.  The community has been fleshing out the Hadoop-based data management platform.  This latest update to CDH formalizes what had already been the case. We saw: piecing together the platform for each deployment was costly and distracting. The Hadoop community is innovative, fast moving and by design decentralized.  These are all very positive traits, but one byproduct has been a significant amount of complexity imposed on organizations who want to use Hadoop.  Every Hadoop component has its own schedule, with some components releasing 5 times as often as others.  Every component also has its own dependencies that are a non-trivial set of installation and upgrade possibilities when you are dealing with nearly a dozen components.  Several speakers of the Hadoop Summit noted there are often 3 of any one component.  For example, there are three job authoring clients, three database integration frameworks, three streaming data collectors, three RPC frameworks, etc.  This is well optimized for users who want to create technology but is very sub-optimal for users who want to adopt technology.  A kit car makes for a fun weekend project, but the next Monday, most people will go to work in a car they drove off a dealer’s lot, supported by an explicit warranty and maintained by a qualified mechanic. We concluded: CDH3 could represent a step forward for organizations who wanted to harness the Hadoop platform. By bringing together the Hadoop components already in prevalent use and packaging and testing them in an integrated manner, we’re able to give users a platform that has the functionality to satisfy mainstream use cases and the interfaces to tie Hadoop into mainstream enterprise technologies.  Most importantly we’re able to take a huge amount of complexity out of the lives of the typical Hadoop adopter. This update does not in any way detract from Cloudera’s commitment to open source and an open platform.  CDH3 is 100% Apache licensed, in beta and available for download here.   We hope you get the opportunity to try this latest version and look forward to your feedback.</snippet></document><document id="643"><title>CDH3 and Cloudera Enterprise</title><url>http://blog.cloudera.com/blog/2010/06/cdhv3-and-cloudera-enterprise/</url><snippet>Today’s a big day for us at Cloudera. We’re announcing, as part of our activity at Hadoop Summit, two major new releases that we believe substantially advance Apache Hadoop for both the open source community and our enterprise customers. First, we’re announcing a new release of Cloudera’s Distribution for Hadoop – CDH3 Beta 2. This release, built on more than a year and a half of extensive engagement with real customers in the market, is the most comprehensive, capable and usable package available. Of course the Apache Hadoop project is the heart of our distribution, but we’ve added eight other open source packages that provide critical infrastructure and tools that are required to use Hadoop effectively in production. The additional packages include HBase, the popular distributed columnar storage system with fast read-write access to data managed by HDFS, Hive and Pig for query access to data stored in a Hadoop cluster, Apache Zookeeper for distributed process coordination and Sqoop for moving data between Hadoop and relational database systems. We’ve adopted the outstanding workflow engine out of Yahoo!, Oozie, and have made contributions of our own to adapt it for widespread use by general enterprise customers. We’ve also released – this is a big deal, and I’m really pleased to announce it – our continuous data loading system, Flume, and our Hadoop User Environment software (formerly Cloudera Desktop, and henceforth “Hue”) under the Apache Software License, version 2. Flume provides continuous, high-performance, reliable data loading and data flow monitoring from feeds, logging systems and other sources into a Hadoop cluster. Hue lets developers build attractive, easy-to-use Hadoop applications by providing a desktop-based user interface SDK. These two new open source projects have been under development at Cloudera, and in use by our customers, for more than a year. They’re now available to the community at large. At the heart of the distribution is the 0.20 release of Apache Hadoop. As in previous versions of CDH, we’ve added important patches from the community and bug fixes critical for enterprise deployment. We’ve integrated the correct, stable versions of all of the projects, tested them together at scale and made the package easy to acquire, install, configure and run. There’s no other distribution available that is as comprehensive, complete or usable as is CDH3. We’re very proud to make the innovative work of the global Hadoop development community easy for everyone to use. The entire package is open source, distributed under ASLv2 and freely available for download, use and redistribution from Cloudera’s web site. In addition to the release of CDH3 and our new open source projects, we’re announcing today the general availability of Cloudera Enterprise. Cloudera Enterprise combines the open source CDH3 platform with critical monitoring, management and administrative tools that our enterprise customers have told us they need to put Hadoop into production. We’ve added dashboards for critical IT tasks, including monitoring cluster status and activity, keeping track of data flows into Hadoop in real time based on the services that Flume provides, and controlling access to data and resources by users and groups. We’ve integrated access controls with Active Directory and other LDAP implementations so that IT staff can control rights and identities in the same way as they do for other business platforms they use. Cloudera Enterprise is available by annual subscription and includes maintenance, updates and support. In case it’s not clear, yet: Cloudera is all in on Apache Hadoop. We believe that the mission-critical infrastructure at the base of business systems must be open source, these days. Enterprises simply aren’t adopting new proprietary technologies at scale at the heart of their operations. Older proprietary companies have established markets and installed bases and will survive for a long time, but a sensible customer will refuse to be tied to a single new vendor for core IT nowadays. A smart entrepreneur wouldn’t use a decades-old playbook in creating a platform company today. CDH3 reflects that conviction. We’re convinced that open source licensing is critical to drive widespread adoption of our comprehensive Hadoop-based distribution. We’re convinced that Cloudera, with the creator of Apache Hadoop, committers and contributors across the breadth of necessary projects, experienced support professionals and consultants, and a world-class technical and business team, is ideally positioned to drive enterprise adoption of the platform. Cloudera Enterprise allows companies that rely on Hadoop to get up and running faster. It lets them meet more stringent SLAs, reduce administrative costs and eliminate risks by making their systems more transparent and easier to maintain. It codifies the lessons that our company and our people have learned since 2006, designing, building and running Hadoop in production. We have real customers across many vertical markets in production on clusters ranging from terabytes to petabytes, solving a broad range of important business problems. If you’d like some of that, by the way, our software, our services team and our support staff are at your disposal. Give us a call. Of course, you shouldn’t simply take my word for it. Our announcements on Cloudera’s Distribution for Hadoop and Cloudera Enterprise today include quotes from some of our customers (eBay), from industry experts and from partners. We’ve built a tremendous partner network – hardware vendors, Hadoop analytics and tools companies, database vendors and others who are committed to CDHv3 and our vision for this critical new business platform. We’re working hard to expand the ecosystem to help other companies integrate their products with Hadoop. We’re pretty excited.</snippet></document><document id="644"><title>Upcoming webinar: Tackling Big Data Challenges with Vertica and Hadoop</title><url>http://blog.cloudera.com/blog/2010/06/upcoming-webinar-tackling-big-data-challenges-with-vertica-and-hadoop/</url><snippet>Are your systems struggling to absorb ever-increasing amounts of data being generated daily? Are you mired in lengthy ETL processes preparing data for analysis? Are you forced to summarize information and thus losing important details along the way? We’ve been working with Vertica at several large enterprise customers to solve these very problems. We’d like to invite you to  attend this free and exclusive webinar to hear our CTO, Amr Awadallah and Vertica’s VP of Products, Colin Mahony describe how both our customers are using revolutionary technologies to solve these challenges, reduce costs and extract more value from real world big data sets. Stuff that really matters. We’ve talked a lot about how Hadoop’s MapReduce framework provides a scalable platform for powerful data processing and data analysis including data mining, ETL, and complex processing. The great thing is that it complements well with Vertica’s Analytic DBMS which provides structured analysis and query of big data. In this webinar we’ll show you how Vertica and Cloudera offer a cost-effective methodology to manage massive unstructured and structured data volumes; how Vertica and Cloudera combine to offer unmatched flexibility and power, so you can apply infinitely scalable processing; and how our customers are solving real world big data challenges. Also if you attend the webinar you’ll receive a free report from Knowledge Integrity, Inc entitled “New Paradigms for High Performance Analytical Computing” (view abstract). So, what are you waiting for – go register now!</snippet></document><document id="645"><title>Cloudera Hosting Hadoop World 2010: Call for Speakers Now Open</title><url>http://blog.cloudera.com/blog/2010/06/cloudera-hosting-hadoop-world-2010-call-for-speakers-now-open/</url><snippet>Cloudera is once again hosting Hadoop World which will take place in New York City on October 12th. Last year’s event was a fantastic success with 500 very active attendees. The event generated a lot of buzz around Hadoop as organizations demoed and discuss real-world use cases.  This year we are planning for an even bigger and better event and are expecting 800 attendees with more presentations from people working on really important data storage and analysis problems across a range of industries including financial services, telecomunications, government, retail, academia and more. The event will also be an international affair drawing attendees from around the world. If you are an experienced user of Hadoop then you know you need to be at this event. However, if you are currently hamstrung by Hadoop, perplexed by Pig or overwhelmed by Oozie? Then get yourself along to the Hadoop World conference to hear the leaders in the industry share their insight and vision (and contact us in the meantime to see how we can help your organization with getting up to speed). Cloudera will also be offering training sessions at the conference. What should you do next? Well apart from signing up here you can also submit your proposal for a breakout session. This is your chance to contribute to the success of the conference and showcase your solution to the Hadoop community. Submit your proposal online until July 17, and please don’t hesitate to submit more than one idea. More information to come as we get additional details. What: Hadoop World Conference 2010 Registration: http://www.cloudera.com/company/press-center/hadoop-world-nyc/ When: October 12, 2010 Where: Hilton New York Hotel, 1335 Avenue of the Americas, New York City View Hilton New York Hotel in a larger map If you have any questions about the event, don’t hesitate to email hadoopworld@cloudera.com</snippet></document><document id="646"><title>Cloudera to participate at OSCON 2010</title><url>http://blog.cloudera.com/blog/2010/06/cloudera-to-participate-at-oscon-2010/</url><snippet>Will Cloudera be at OSCON this year? Of course, it’s only the premier event for OS technologies on the market! We expect there to be big crowds and you can find us there at booth #411 which is in the center near the O’Reilly booth. If you are there stop by and see us and enter for a chance to win a sweet prize. During the first two days of the conference Cloudera’s very own Aaron Kimball will be leading four Hadoop sessions ranging from an overview of the “what” and “how” of Hadoop to an introduction to Hive and HBase. Full details for the sessions can be found on the conference website What: O’Reilly OSCON Open Source Convention 2010 When: 19-23 July 2010 Where: Oregon Convention Center in Portland, Oregon We look forward to seeing you at OSCON! For more information check out the website</snippet></document><document id="647"><title>Integrating Apache Hive and Apache HBase</title><url>http://blog.cloudera.com/blog/2010/06/integrating-hive-and-hbase/</url><snippet>This post was contributed by John Sichi, a committer on the Apache Hive project and a member of the Data Infrastructure team at Facebook. As many readers may already know, Hive was initially developed at Facebook for dealing with explosive growth in our multi-petabyte data warehouse. �Since its release as an Apache project, it has been put into use at a number of other companies for solving big data problems. �Hive storage is based on Hadoop‘s underlying append-only filesystem architecture, meaning that it is ideal for capturing and analyzing streams of events (e.g. web logs). �However, a data warehouse also has to relate these event streams to application objects; in Facebook’s case, these include familiar items such as fan pages, user profiles, photo albums, or status messages. Hive can store this information easily, even for hundreds of millions of users, but keeping the warehouse up to date with the latest information published by users can be a challenge, as the append-only constraint makes it impossible to directly apply individual updates to warehouse tables. �Up until now, the only practical option has been to periodically pull snapshots of all of the information from live MySQL databases and dump them to new Hive partitions. �This is a costly operation, meaning it can be done at most daily (leading to stale data in the warehouse), and does not scale well as data volumes continue to shoot through the roof. That’s where Apache HBase comes in. �HBase is a scaleout table store which can support a very high rate of row-level updates over massive amounts of data. �It sidesteps Hadoop’s append-only constraint by keeping recently updated data in memory and incrementally rewriting data to new files, splitting and merging intelligently based on data distribution changes. �Since it is based on Hadoop, making HBase interoperate with Hive is straightforward, meaning HBase tables can be accessed as if they were native Hive tables. �As a result, a single Hive query can now perform complex operations such as join, union, and aggregation across combinations of HBase and native Hive tables. �Likewise, Hive’s INSERT statement can be used to move data between HBase and native Hive tables, or to reorganize data within HBase itself. Putting it all together, we can solve the incremental refresh problem by keeping a near-real-time replica of MySQL data in HBase�(moving only the data which has actually changed), and then combine it with the latest event data in Hive. �For static data, native Hive tables are significantly more efficient than HBase for both storage and access, so periodically, we can continue to take snapshots from HBase into Hive tables for use by queries where data freshness is not paramount. Of course, the scenario described here is just one possible usage, and we look forward to hearing about other innovative applications of the technology as they are discovered by the open source community. Also, since the HBase integration work involved adding a generic storage handler interface to Hive, we are expecting to see development of more storage handler plugins for systems such as Cassandra and HyperTable soon. The integration effort is still a work in progress, and we are only just now starting to prototype the approach at large data scale, filling in necessary features as we go. �However, initial results are encouraging, and we’ll be presenting some of them at the Hadoop Summit at the end of this month. �Meanwhile, HBase developers at Cloudera, Facebook, StumbleUpon, Trend Micro and elsewhere are busy adding awesome new features such as bulk load into existing HBase tables; these are likely to increase efficiency and scalability significantly.</snippet></document><document id="648"><title>One word more…</title><url>http://blog.cloudera.com/blog/2010/06/one-word-more/</url><snippet>I want to follow Christophe onto the stage, here, to say just a few things. Most importantly: Thanks. Sincerely. We’ve worked together closely since the very earliest days of Cloudera, long before we had a business plan or any funding. The progress we’ve made in the years since our first meeting about starting a company is remarkable. Cloudera would be vastly different without Christophe’s many contributions. All of us here are indebted to him for his hard work and enthusiasm. Of course the departure of a founder is significant. That said, I’m absolutely confident in our continued growth and success. We’ve accomplished a great deal over the last couple of years — not least, we’ve built a world-class team. We’ve got the depth we need to meet the challenges in front of us. We’re looking forward to it. Knowing Christophe, I’m confident that we’ll see and hear great things from him soon. Our heartfelt best wishes! – Mike Olson, CEO</snippet></document><document id="649"><title>A transition</title><url>http://blog.cloudera.com/blog/2010/06/a-transition/</url><snippet>For an entrepreneur, it’s an incredibly fulfilling experience to start companies and watch them “grow up.” Along the way, you learn invaluable lessons, form deep bonds with colleagues, and if you do things right, bring significant value to your customers. Cloudera has done things right. We’ve hired an incredible team and brought the power of Apache Hadoop to some of the most forward thinking enterprises in the world. But we couldn’t have done it alone. What has enabled Cloudera’s rapid growth goes beyond our team – it includes the entire community around Hadoop. Many companies invest vast resources in the project and together we have created a new industry standard that is accessible to traditional enterprises. This is pretty exciting. Cloudera has grown up quickly and I am proud to have played a role. When not in the Cloudera office, I spend a fair bit of time advising early stage companies. Some of these companies complement Cloudera and others have nothing to do with software at all. In the process, I have realized that, while I still have the frenetic energy many of you know, I can contribute the most value to companies whose path to success is far less obvious than Cloudera’s. The opportunity to be involved with a start-up as it defines a new way to address a market need, formulates a strategic plan, and staffs to execute against it at scale is invigorating in ways that are hard to match. It is what led me to co-found Cloudera in 2008 and it is what has led me to the decision to step back from day-to-day involvement at this time. This was a difficult decision for me as I think there is much left for Cloudera to achieve. I reached this point after much deliberation and likely would not have the confidence to focus my attentions elsewhere were it not for the strong management team that we have put in place. I have no doubt that they will knock it out of the park. I will be paying keen attention and I look forward to celebrating their future successes. In closing I want to express my gratitude to everyone who has helped Cloudera with its accomplishments to date. The proverb about success having many fathers holds true, thanks to you. While I�m not able to announce what I�ll be working on next I look forward to letting you know soon via my currently dormant Twitter account – @cbisciglia – so stay tuned. – Christophe</snippet></document><document id="650"><title>Reporting from the UK Hadoop Users Group</title><url>http://blog.cloudera.com/blog/2010/06/reporting-from-the-uk-hadoop-users-group/</url><snippet>We like to support the broader Hadoop community in a number of ways including providing great software, services and training but also by providing support for meetups and a forum for reporting on interesting events. In this guest blog post from Klaas Bosteels from last.fm he gives us his report from the recent UK Hadoop Users Group (HUG) which featured 25 or so folks (some would say geeks) at the Skills Matter central London training facility earlier this week. Thanks Klaas! About two years ago, Last.fm sent Martin and Johan to Yahoo’s first Hadoop Summit and they liked what they saw. Johan even liked it so much that he started organizing similar events in London, which is how the Hadoop User Group UKcame into existence. He successfully organized three meetups featuring many great speakers, including Doug Cuttingand several other big names from the Hadoop community. But then the inevitable happened: Johan got stolen by Twitterand left the country, leaving us without a main organizer for the HUGUK meetups. Ever since he left Johan kept pushing us to keep HUGUK alive though, and when I heard the Cloudera guys were coming to town for some training sessionsI saw my chance to finally obey his requests. It didn’t take much effort at all to convince Cloudera’s Aaron Kimball to present something, which was a great starting point that eventually led to a pretty awesome meetup with four very interesting talks: Aaron Kimball (Cloudera): Introduction to Sqoop (slides and video) Tim Sell (Last.fm): Hive at Last.fm (slides and video) Ben Mankin (Karmasphere): Introduction to Karmasphere Studio Gavin Heavyside (Journey Dynamics): Introduction to Cascalog All of these talks, even the latter two which were very short lightning talks, featured a nifty live demo at some point and Cloudera put the final cherry on the cake by providing free beer and pizza for all attendants! It was definitely a great meetup, and we’re very grateful to Cloudera for being so supportive. Now that we got the wheels rolling again I’d like to keep going and organize HUGUK meetups on a regular basis by the way. Please let me know if you have talk suggestions or want to help out in any other way.</snippet></document><document id="651"><title>Considerations for Apache Hadoop and BI (part 2 of 2)</title><url>http://blog.cloudera.com/blog/2010/06/considerations-for-hadoop-and-bi-part-2-of-2/</url><snippet>Just today we heard another question about integrating Apache Hadoop with Business Intelligence tools. This is one of the most common questions we receive from enterprises adopting or evaluating Hadoop. In the early stages of their projects, customers are generally not sure how to connect their BI tools to Hadoop, and when it makes sense to do so. As I wrote in BI Considerations and Hadoop Part 1, Cloudera encourages you to use your existing infrastructure wherever possible, and this includes your investments in Business Intelligence. BI tools traditionally were designed for small volumes of structured data where Hadoop generally stores data in complex formats at scale and processes data on read using MapReduce. We give our customers recommendations for when and how to integrate Hadoop with their existing Business Intelligence environment, as well as when organizations should look to new tools to solve a new class of problem. Here are some questions to consider when determining which tools to use: Are you dealing with a technically difficult or intractable problem? In the traditional Business Intelligence world, transactional data is stored in a database and then periodically loaded into a warehouse for query and analysis. The warehouse is designed and implemented ahead of time to facilitate a specific set of reports or ad hoc query. This model breaks if your data sets are growing faster than the ETL jobs that collect them. This model also breaks if you don’t have an understanding of how to utilize the data at the time you collect it. Do you need process data sets that are growing faster than your ability to transform them? Is it too expensive to model these data sets relationally? Where is the data growing? If it’s growing around complex data types, then a relational database is probably not the most interesting place to ask questions of complex plus relational data. Does most of your data conform to a known schema? If you have a known schema, or you have the time to model a new schema as well as develop the required ETL to populate that schema, Hadoop might not be a requirement for you. However, we find that increasingly our customers are coming to us because they have exploding volumes of data in complex formats and don�t know how to model an appropriate schema. They know there is value in this data, but they do not know where it is. If you are confronted with that situation, perhaps it’s best to stand up a Hadoop cluster alongside your data warehouse for complex data analysis.After you have both running together, then your primary challenge will be integrating the two in a way that makes sense. Cloudera is committed to providing the tools and infrastructure required to help address this challenge. Do you require real-time analysis or will batch-analysis suffice? Although there is plenty of work being done around making Hadoop more real-time and low-latency in projects such as HBase, Hadoop was designed for batch processing of complex data types at scale. If your business requires ad-hoc exploration of structured data, then a traditional OLAP approach or an Analytic DBMS might be best. Organizations still need to collect, use, and present relational data for real time online analysis. If this is the case, that data should absolutely be properly governed and moved into a new data warehouse. Hadoop is where organizations can start to collect atomic raw data and ask new questions without increasing the expenses of their data warehouse. If you don’t necessarily have defined dimensions and clear facts in your data, and you need to identify trends, then you want to look adopting a new interface to your Hadoop cluster.If you do adopt Hadoop and still have a requirement for real-time ad-hoc query of data, then you need to talk to us about how to populate a data mart, OLAP cube or an ADBMS from the output of a MapReduce job. Do you require a flexible data processing methodology, or is it more important to you that your data fit into a rigid, well-understood format at the time that you model it? Hadoop provides the ability to postpone formalizing data until you query it. Because Hadoop keeps data local to processing you can scan, extract, and transform the data at query time.Some problems that are modeled as star schemas in a data warehouse are easier in Hadoop. For example, rather than model dimensions and facts that support slowly changing dimensions and develop ETL to manage it, Hadoop can store historical data in its original form and provide access on demand. This makes the data warehouse simpler and reduces the complexity of your data management environment! How nimble is your organization? Do you have requirements around processes and governance that you must adhere to? If process, governance, and compliance is a requirement, then it will most likely be worth the increased investment in your data warehouse and BI infrastructure to model and transform data directly into a relationsal store. Rather than trying to integrate with data stored in Hadoop, think about processing data within Hadoop and then moving data post-aggregation into a data warehouse that meets your governance needs. When we’re asked whether or not Hadoop is intended to replace or supplement a BI system, the answer isn�t either or. Cloudera wants you to leverage as much of your existing infrastructure as makes sense. We are also making improvements to Hadoop to make it more familiar to someone accustomed to BI systems. Likewise, we are rolling out new tools to help existing BI systems interface with Hadoop. While Hadoop is a disruptive technology in that it allows you to store and to process data without rigidly modeling it first, it is not so disruptive as to require a new front-end to your data. There are a lot of situations that are going to be unique to your organization. While there are probably some cases where as new presentation layer is required, we’d encourage you to exhaust your existing options before simply concluding that you need to turn away from your preexisting investments, or adopt new applications that you may not really need.</snippet></document><document id="652"><title>The Second Apache Hadoop HDFS and MapReduce Contributors Meeting</title><url>http://blog.cloudera.com/blog/2010/06/the-second-apache-hadoop-hdfs-and-mapreduce-contributors-meeting/</url><snippet>The second Apache Hadoop HDFS and MapReduce contributors meeting was held last Friday, May 28 at Cloudera’s offices in Palo Alto. Apache projects attract contributors from across the globe, and Hadoop is no exception, so the idea of holding face-to-face meetings may seem to run counter to the existence such a highly decentralized organization. However, the point of in-person meetings is not to make project decisions, but rather to start discussions that spur more in-depth, on-list decision making. Chris Douglas took excellent, detailed minutes of the meeting. The general theme of the meetings has been to discuss project process; in particular how does the Hadoop development community continue to move the platform forward while supporting the large user base that Hadoop has attracted? In this vein, Eli Collins presented his proposal for Hadoop to adopt a new mechanism for adding significant new features, analogous to Python’s PEP (Python Enhancement Proposal). A HEP (Hadoop Enhancement Proposal) would be used whenever a large new feature is being planned for Hadoop. By way of example, something the size of the backup namenode would need a HEP, but something like the pure Java CRC enhancement would probably not. At heart, a HEP is a consensus-building process for Hadoop changes. Some of the improvements that HEPs would bring include: discoverability (today, it’s too easy to miss umbrella JIRAs), achieving buy-in on use cases (focusing first on the problem that the HEP would solve, rather than diving straight into code, which can cause problems), and ensuring completeness (there would be a list of tasks that would need to be addressed by the HEP, such as backwards compatibility). HEPs would be reviewed by the PMC, and approval would mean that the authors could proceed with the implementation, although changes would still need the usual review and committer approval before being committed to the main line of development. Eli’s slides (along with Chris’s minutes) cover more of the details, and there’s some initial discussion on general@hadoop.apache.org. We also talked about feature branches as a tool for large changes, and how the choice of using a feature branch or a patch might be made on a HEP-by-HEP basis. Finally, we discussed contrib modules in Hadoop, and how it would be worth looking at whether some might be spun off to be hosted elsewhere, in the way HBase recently did. This topic will be taken up on the lists at a future date. The contributors meetings are open to anyone who contributes to the HDFS or MapReduce projects. So if you’re in the area, consider signing up for future meetings at http://www.meetup.com/Hadoop-Contributors/.</snippet></document><document id="653"><title>Upcoming Webinars From Cloudera</title><url>http://blog.cloudera.com/blog/2010/05/upcoming-webinars-from-cloudera/</url><snippet>Here at Cloudera we have deep knowledge and experience working with Hadoop and related technologies to solve a wide range of data challenges that organizations are facing today. Lately we’ve seen booming interest in tapping into that knowledge to gain a better understanding of the technology and to improve Hadoop deployments. We want to nurture the broader Hadoop community so we are offering two free Hadoop-focused webinars to share some of our insights. The first webinar is designed for technical audiences and is called “Top 10 Tips &amp; Tricks for Hadoop Success“. This webinar will be given by senior members of our customer solutions team and they will draw from years of experience in the trenches implementing Hadoop for small and large enterprises. During the webinar they will provide actionable steps to help you successfully deploy Hadoop. Top 10 Tips &amp; Tricks Our second webinar will be given by Hadoop expert and Cloudera co-founder and CTO, Dr. Amr Awadallah and is called “Hadoop: An Industry Perspective“. In this webinar he will provide insights into the next generation data platform that is revolutionizing the enterprise IT environment. Dr. Awadallah will discuss an overview of how the core technologies in Hadoop, MapReduce and HDFS work. And he will explore how Hadoop augments the most commonly used tool for data management today — an RDBMS. Finally, he will look at the various business requirements for Hadoop and how some companies are using it. This webinar is appropriate for both technical and business audiences and you can register by clicking the button below. Hadoop: An Industry Perspective</snippet></document><document id="654"><title>Considerations for Apache Hadoop and BI (part 1 of 2)</title><url>http://blog.cloudera.com/blog/2010/05/considerations-for-hadoop-and-bi/</url><snippet>We recently met with a customer at Cloudera�s new offices and asked if he had any specific use cases in mind for the Apache Hadoop cluster that we are helping him to roll out. He replied, quite honestly, that he didn’t know. His baseline understanding is that there is value in the data that his organization is collecting today, but he’s not sure where it is. He said, “I would like to have all of this data stored forever” and then proceeded to explain that as his business expands and matures, he wants the ability to go back and analyze this data in ways he cannot foresee today. This is an ideal use case for Hadoop and a prime example why Hadoop is such a disruptive technology. Historically, before analyzing any data set, organizations needed to model and transform the data. This requires a lot of effort to make sure the data is properly loaded, correctly structured, well-defined and typed, complete and conforms to organizational standards. Moreover, the organization must design, model and expose corresponding metadata using business intelligence and analysis tools. The key to making this all work is that an organization has not only a good understanding of current business needs, but ideally a pretty good view of what they�re going to want down the road. As data volumes grow, business needs change, and data comes from unexpected places. It becomes more difficult to keep up the transformation, structuring and modeling for all this new data. Hadoop is a platform for capturing all this data a very low cost per byte in raw form, before it is transformed and structured. �Hadoop is used to capture and consolidate data before modeling it to fit any given process as well as to keep a data that has been processed. With Hadoop data is structured as it is accessed. This means that when new data is introduced or as the relationships change, new queries can extract value from old and new data alike without requiring a complete redesign. A MapReduce job can quickly and efficiently churn through unbounded volumes of complex data and extract the needed intelligence. This freedom from constraint at load time can revolutionize an organization’s relationship with and understanding of their data. We find that as customers start understanding this revolution and begin exploring new relationship within their data, they’re struck by a common set of observations and subsequent questions. Although loading data is easier in Hadoop, without the need modeling complex data from heterogeneous sources, our customers start to become concerned that writing MapReduce jobs, Pig scripts and Hive queries is harder than using existing tools for pre-modeling data. Because the BI ecosystem is more mature, today’s analysts have a rich interface to their data using a variety of means. Whether using an OLAP tool that can �slice and dice data, a reporting framework for building and consuming dashboards, or Microsoft Excel for constructing spreadsheets out of human-sized chunks of the data, these tools make structured data much more accessible. Customers also start asking questions about how Hadoop output should be consumed. Should it be consumed using existing BI infrastructure, or is their relationship with their data now so fundamentally different that they need to start anew with a fresh set of applications for analyzing their data? We’re often asked if it’s a goal of Cloudera’s to replace relational databases and business intelligence tools. Hadoop is not a wholesale replacement for BI and we�re not replacing relational databases. Cloudera is helping our customers marry the power of Hadoop to existing tools. While the storage and analysis of data using Hadoop differs from highly structured formats preferred by RDBMS’s and the BI tools, Hadoop does not required throwing out existing business intelligence suites in order to perform meaningful analysis on data. Sometimes, our customers use Hadoop to scale over large or loosely structured data sets and then export the results as needed into existing RDBMS. Business users are not eager to learn new tools for analysis and by integrating Hadoop, Cloudera helps shield users from this complexity.� Business users continue to access processed data using well-defined and established business intelligence frameworks. But export to an RDBMS is not always the appropriate result of an MR job.� The existing BI infrastructure is not the only means of accessing data in Hadoop and new tools are not always complex or foreign. It becomes a matter of situation and preference. Hive is a SQL engine that compiles queries in a familiar language and constructs into MapReduce programs. Pig Latin was developed at Yahoo! as a new way to address complex data transformations procedurally without having to write java MapReduce code. While both are useful abstractions of MapReduce, they reflect different ways of thinking about data that correspond to different business problems. Our customers now face an interesting set of challenges: understanding when and how to integrate Hadoop with �the existing Business Intelligence environment and when to look to new tools to solve a new class of problem. In our next post we look at the top five questions when deciding how and when to use Hadoop.</snippet></document><document id="655"><title>CDH2 Update 1 Now Available</title><url>http://blog.cloudera.com/blog/2010/05/cdh2-update-1-now-available/</url><snippet>Cloudera is happy to announce the availability of the first update to version 2 of our distribution for Hadoop. While major new features are planned for our release of version 3 we will regularly update version 2 with improvements and bug fixes. Check out the change log and release notes for details. You can find the packages and tarballs on our website, or simply update if you are already using our yum and apt repositories. A notable addition in update 1 is a FUSE package for HDFS. This package allows you to easily mount HDFS as a standard file system for use with traditional Unix utilities. Check out the Mountable HDFS section in the CDH docs and the hadoop-fuse-dfs manpage for details. We appreciate feedback! Get in touch with us on Get Satisfaction, twitter and IRC (#cloudera on freenode.net) and let us know how the update is working for you.</snippet></document><document id="656"><title>What to Do with Extra Space?</title><url>http://blog.cloudera.com/blog/2010/05/what-to-do-with-extra-space/</url><snippet>What can you do with a lot of extra space? No, I do not mean HDFS. I am talking about the new Cloudera Global Headquarters. Cloudera’s old office became so crammed that our CEO had to take calls from his car, and our Architect shared a desk. So the new space in Palo Alto was much anticipated (not to mention its extreme proximity to Fry’s). It features eight conference rooms, one quiet room, two server rooms, showers, kitchen, and so on. A month has passed and we managed to only fill about one-third of the office. The excitement has sparked some creative ideas of what to do with all the extra space: Throw lunch parties. This was our move-in celebration. Cloudera Olympics. Christophe started jumping over desks instead of going around. This is Eli's attempt. Nutron demonstrating the correct way to clear the desks. Carl brought in his remote controlled plane to fly down the unoccupied part of the office. Too many conference rooms? We turned one into a workshop table. This is Vinithra, Jon and bc assembling a MakerBot 3D printer. Bring your kid to work. This is Amr's daughter. (No, not Jeff's.) Goodbye, old office Our extra space will probably meet the same fate as that in your HDFS, i.e. the content will fill it up. I look forward to the day when Cloudera needs a new new office, and challenges that come with being triple the size. To me, this is the real perk of working for a startup — the opportunity to build a company, not just code.</snippet></document><document id="657"><title>Highlights from the First Hadoop Contributors Meeting</title><url>http://blog.cloudera.com/blog/2010/05/the-first-hadoop-contributors-meeting/</url><snippet>While the vast majority of the Hadoop development discussion takes place on the Apache Jira and various project mailing lists, it’s often useful to meet face to face for high bandwidth discussion. To that end, Facebook hosted the first Apache Hadoop contributors meeting yesterday at their campus in Palo Alto. Cloudera, Facebook, Yahoo! and the Apache HBase team were well-represented. It was great to see a broad cross section of Hadoop developers in one room. Contributor meetings will be held on a monthly basis, at a rotating location. While any Hadoop project contributor is welcome to attend, the current focus of the meetings is HDFS and MapReduce. The goal of the discussion is to surface and flesh out ideas rather than make decisions, which happens on the development lists. If you’ve got ideas to add check out the meeting notes and continue the discussion. Sanjay Radia kicked off the meeting with a discussion of development priorities. Hadoop has become a platform and industry standard for data storage and analytics. What advances are most important to users? How do we continue to innovate without disrupting the installed base? Development must maintain and improve the quality that has allowed companies to adopt Hadoop in their production environments. Fortunately there is broad agreement among contributors on development priorities: availability, compatibility, security, scalability and performance. The next topic was releases. Tom White gave an update on the upcoming 0.21 release. The release has branched and is now in feature freeze; the focus is now shifting to closing out issues that block the release. You can hear more about the 0.21 release at the next Bay Area Hadoop user group. Chris Douglas recently proposed Hadoop adopt release managers. Owen O’Malley led a discussion on the role and responsibilities of the release manager, and the Hadoop release process. The agenda for the next meetup is a more formal way to propose and adopt new features. We’d love to see new faces, so if you’re a Hadoop contributor and want to attend, sign up for the next meetup. If you’d like to contribute to Hadoop, the Hadoop wiki is a good place to start. See you next month at the new Cloudera HQ!</snippet></document><document id="658"><title>Exciting new Hadoop Training Offerings from Cloudera</title><url>http://blog.cloudera.com/blog/2010/04/exciting-new-hadoop-training-offerings-from-cloudera/</url><snippet>Around the globe, more and more companies are turning to Hadoop to tackle data processing problems that don’t lend themselves well to traditional systems. Users in the community consistently ask us to offer training in more places and expand our course offerings, and those who have obtained certification have reported great success connecting with companies investing in Hadoop. All of this keeps us pretty excited about the long term prospects for Hadoop. We recently announced our first international developer training sessions in Tokyo (sold out, waitlist available) and Taiwan, and we’re happy to follow up with sessions in the EU. We’ll be visiting London the first week of June, and Berlin the next. If you’ll be in Berlin that week, be sure to check out the Berlin Buzzwords conference – a two day event focused on Hadoop, Lucene, and NoSQL. We’ve also put together new offerings for this years upcoming Hadoop Summit, and we’ve worked out a special deal with Yahoo! to waive the conference registration fee for anyone who attends a Cloudera training session at the 2010 Hadoop Summit (you’ll get a discount code for training in your conference registration confirmation). In addition to our developer certification course, we’ll offer an extended version of our Systems Administration course, as well as new, full-day course on HBase. One particularly exciting new offering is our full-day course on Hive, which opens Hadoop up to anyone who knows SQL. All of these offerings are driven by direct customer feedback about what their organizations need to be even more successful with Hadoop, and we’re excited to help.</snippet></document><document id="659"><title>CAP Confusion: Problems with ‘partition tolerance’</title><url>http://blog.cloudera.com/blog/2010/04/cap-confusion-problems-with-partition-tolerance/</url><snippet>The ‘CAP’ theorem is a hot topic in the design of distributed data storage systems. However, it’s often widely misused. In this post I hope to highlight why the common ‘consistency, availability and partition tolerance: pick two’ formulation is inadequate for distributed systems. In fact, the lesson of the theorem is that the choice is almost always between sequential consistency and high availability. It’s very common to invoke the ‘CAP theorem’ when designing, or talking about designing, distributed data storage systems. The theorem, as commonly stated, gives system designers a choice between three competing guarantees: Consistency – roughly meaning that all clients of a data store get responses to requests that ‘make sense’. For example, if Client A writes 1 then 2 to location X, Client B cannot read 2 followed by 1. Availability – all operations on a data store eventually return successfully. We say that a data store is ‘available’ for, e.g. write operations. Partition tolerance – if the network stops delivering messages between two sets of servers, will the system continue to work correctly? This is often summarised as a single sentence: “consistency, availability, partition tolerance. Pick two.”. Short, snappy and useful. At least, that’s the conventional wisdom. Many modern distributed data stores, including those often caught under the ‘NoSQL’ net, pride themselves on offering availability and partition tolerance over strong consistency; the reasoning being that short periods of application misbehavior are less problematic than short periods of unavailability. Indeed, Dr. Michael Stonebraker posted an article on the ACM’s blog bemoaning the preponderance of systems that are choosing the ‘AP’ data point, and that consistency and availability are the two to choose. However for the vast majority of systems, I contend that the choice is almost always between consistency and availability, and unavoidably so. Dr. Stonebraker’s central thesis is that, since partitions are rare, we might simply sacrifice ‘partition-tolerance’ in favour of sequential consistency and availability – a model that is well suited to traditional transactional data processing and the maintainance of the good old ACID invariants of most relational databases. I want to illustrate why this is a misinterpretation of the CAP theorem. We first need to get exactly what is meant by ‘partition tolerance’ straight. Dr. Stonebraker asserts that a system is partition tolerant if processing can continue in both partitions in the case of a network failure. “If there is a network failure that splits the processing nodes into two groups that cannot talk to each other, then the goal would be to allow processing to continue in both subgroups.” This is actually a very strong partition tolerance requirement. Digging into the history of the CAP theorem reveals some divergence from this definition. Seth Gilbert and Professor Nancy Lynch provided both a formalisation and a proof of the CAP theorem in their 2002 SIGACT paper. We should defer to their definition of partition tolerance – if we are going to invoke CAP as a mathematical truth, we should formalize our foundations, otherwise we are building on very shaky ground. Gilbert and Lynch define partition tolerance as follows: “The network will be allowed to lose arbitrarily many messages sent from one node to another” Note that Gilbert and Lynch’s definition isn’t a property of a distributed application, but a property of the network in which it executes. This is often misunderstood: partition tolerance is not something we have a choice about designing into our systems. If you have a partition in your network, you lose either consistency (because you allow updates to both sides of the partition) or you lose availability (because you detect the error and shutdown the system until the error condition is resolved). Partition tolerance means simply developing a coping strategy by choosing which of the other system properties to drop. This is the real lesson of the CAP theorem – if you have a network that may drop messages, then you cannot have both availability and consistency, you must choose one. We should really be writing Possibility of Network Partitions =&gt; not(availability and consistency), but that’s not nearly so snappy. Dr. Stonebraker’s definition of partition tolerance is actually a measure of availability – if a write may go to either partition, will it eventually be responded to? This is a very meaningful question for systems distributed across many geographic locations, but for the LAN case it is less common to have two partitions available for writes. However, it is encompassed by the requirement for availability that we already gave – if your system is available for writes at all times, then it is certainly available for writes during a network partition. So what causes partitions? Two things, really. The first is obvious – a network failure, for example due to a faulty switch, can cause the network to partition. The other is less obvious, but fits with the definition from Gilbert and Lynch: machine failures, either hard or soft. In an asynchronous network, i.e. one where processing a message could take unbounded time, it is impossible to distinguish between machine failures and lost messages. Therefore a single machine failure partitions it from the rest of the network. A correlated failure of several machines partitions them all from the network. Not being able to receive a message is the same as the network not delivering it. In the face of sufficiently many machine failures, it is still impossible to maintain availability and consistency, not because two writes may go to separate partitions, but because the failure of an entire ‘quorum’ of servers may render some recent writes unreadable. This is why defining P as ‘allowing partitioned groups to remain available’ is misleading – machine failures are partitions, almost tautologously, and by definition cannot be available while they are failed. Yet, Dr. Stonebraker says that he would suggest choosing CA rather than P. This feels rather like we are invited to both have our cake and eat it. Not ‘choosing’ P is analogous to building a network that will never experience multiple correlated failures. This is unreasonable for a distributed system – precisely for all the valid reasons that are laid out in the CACM post about correlated failures, OS bugs and cluster disasters – so what a designer has to do is to decide between maintaining consistency and availability. Dr. Stonebraker tells us to choose consistency, in fact, because availability will unavoidably be impacted by large failure incidents. This is a legitimate design choice, and one that the traditional RDBMS lineage of systems has explored to its fullest, but it implicitly protects us neither from availability problems stemming from smaller failure incidents, nor from the high cost of maintaining sequential consistency. When the scale of a system increases to many hundreds or thousands of machines, writing in such a way to allow consistency in the face of potential failures can become very expensive (you have to write to one more machine than failures you are prepared to tolerate at once). This kind of nuance is not captured by the CAP theorem: consistency is often much more expensive in terms of throughput or latency to maintain than availability. Systems such as ZooKeeper are explicitly sequentially consistent because there are few enough nodes in a cluster that the cost of writing to quorum is relatively small. The Hadoop Distributed File System (HDFS) also chooses consistency – three failed datanodes can render a file’s blocks unavailable if you are unlucky. Both systems are designed to work in real networks, however, where partitions and failures will occur*, and when they do both systems will become unavailable, having made their choice between consistency and availability. That choice remains the unavoidable reality for distributed data stores. Further Reading *For more on the inevitably of failure modes in large distributed systems, the interested reader is referred to James Hamilton’s LISA ’07 paper On Designing and Deploying Internet-Scale Services. Daniel Abadi has written an excellent critique of the CAP theorem. James Hamilton also responds to Dr. Stonebraker’s blog entry, agreeing (as I do) with the problems of eventual consistency but taking issue with the notion of infrequent network partitions.</snippet></document><document id="660"><title>Get Hadoop Training from Cloudera at the Hadoop Summit</title><url>http://blog.cloudera.com/blog/2010/04/get-hadoop-training-from-cloudera-at-the-hadoop-summit/</url><snippet>We love getting together with other Hadoop fans and fanatics! We’ve put together new training offerings for this years upcoming Hadoop Summit in June, and we’ve worked out a special deal with Yahoo! to waive the conference registration fee for anyone who attends a Cloudera training session at the 2010 Hadoop Summit (you’ll get a discount code for training in your conference registration confirmation). In addition to our developer certification course, we’ll offer an extended version of our Systems Administration course, as well as new, full-day course on HBase. One particularly exciting new offering is our full-day course on Hive, which opens Hadoop up to anyone who knows SQL. All of these offerings are driven by direct customer feedback about what their organizations need to be even more successful with Hadoop, and we’re excited to help. We look forward to seeing you there.</snippet></document><document id="661"><title>Cloudera Hadoop Training Spreads Worldwide</title><url>http://blog.cloudera.com/blog/2010/04/cloudera-hadoop-training-spreads-worldwide/</url><snippet>Around the globe, more and more companies are turning to Hadoop to tackle data processing problems that don’t lend themselves well to traditional systems. Users in the community consistently ask us to offer training in more places and expand our course offerings, and those who have obtained certification have reported great success connecting with companies investing in Hadoop. All of this keeps us pretty excited about the long term prospects for Hadoop. We recently held our first international developer training sessions in Tokyo (they sold out and were a huge success!) and Taiwan, and we’re happy to follow up with sessions in the EU. We’ll be visiting London the first week of June, and Berlin the next week. If you’ll be in Berlin that week, be sure to check out the Berlin Buzzwords conference – a two day event focused on Hadoop, Lucene, and NoSQL. We are excited to see interest in Hadoop and our training offerings spread around the world!</snippet></document><document id="662"><title>Cloudera Has Moved!</title><url>http://blog.cloudera.com/blog/2010/04/cloudera-has-moved/</url><snippet>Over the past weekend Cloudera moved into some brand new digs in sunny Palo Alto CA; neighbors to VMware, Facebook, SAP, HP and a whole host of other great companies. We are also closer to some of our biggest and best customers. If you are wondering why we moved, one reason outweighed all the others – space. We were officially out of it.  When our most recent hire arrived this past week, there wasn’t a single spot for another desk anywhere in our Burlingame HQ – a  good sign that it’s time to move. We are excited to be in our new space and we love visitors.  We are directly behind Fry’s Electronics, the iconic Silcon Valley retailer. The next time you are picking up a cable or a camera, stop by and say hi!</snippet></document><document id="663"><title>Scaling Social Science with Apache Hadoop</title><url>http://blog.cloudera.com/blog/2010/04/scaling-social-science-with-hadoop/</url><snippet>This post was contributed by researcher Scott Golder, who studies social networks at Cornell University. Scott was previously a research scientists at HP Labs and the MIT Media Laboratory. The methods of social science are dear in time and money and getting dearer every day. � George C. Homans, Social Behavior: Its Elementary Forms, 1974. When Homans � one of my favorite 20th century social scientists � wrote the above, one of the reasons the data needed to do social science was expensive was because collecting it didn’t scale very well. If conducting an interview or lab experiment takes an hour, two interviews or experiments takes two hours. The amount of data you can collect this way grows linearly with the number of graduate students you can send into the field (or with the number of hours you can make them work!). But as our collective body of knowledge has accumulated, and the “low-hanging fruit” questions have been answered, the complexity of our questions is growing faster than our practical capacity to answer them. Things are about to change. We’re reaching the end of what philosopher Thomas Kuhn might call “normal science” in the social sciences � a period of time when scholarly progress grows incrementally using widely-accepted methods. This doesn’t mean an end to interviews, surveys, or lab experiments as important social science methods. Though questions about interpersonal behavior and small groups are undoubtedly still interesting, what we really want to know � what we’ve always wanted to know � is how entire societies work. The most interesting findings are going to have to come some other way. “Computational social science” [1] represents a turn toward the use of large archives of naturalistically-created behavioral data. These data come from a variety of places, including popular social web services like Facebook and Twitter, consumer services like Amazon, weblog and email archives, mobile telephone networks, or even custom-built sensor networks. What these data have in common is that they grow as byproducts of people’s everyday lives. People email, shop and talk for their own reasons, without thinking about how the digital traces of their activity provide naturalistic data for social scientists. That the data are created naturalistically is important both methodologically and theoretically. Though social scientists care what people think it’s also important to observe what people do, especially if what they think they do turns out to be different from what they actually do.When responding to survey or interviews, subjects might honestly mis-remember and mis-report the past. They might deliberately omit some things that embarrass them, or rationalize post-hoc and justify actions differently from how they reasoned about them at the time [2]. Collecting data on actual behavior is seen by many as the gold standard of social science, and experimental methods have had, and continue to have, many successes across the social sciences, including how people interpret probabilities in decision making, and how people develop beliefs about status hierarchies along racial, gender and other dimensions. But it was recognized long ago that findings within a lab might not generalize to the whole world. What we need to do now is measure the whole world in a controlled way. The web services named above do just that. Want to know how corporations really work? Look at their email [3]. Want to know about racial preferences in dating? Look at their online dating profiles (or even server logs) [4]. I believe that Hadoop is going to play a large role in analyzing these data and therefore in generating social science advances very soon. In the infancy of the social web, even successful systems had only thousands or tens of thousands of users (in contrast with tens of millions today), and creating an archive of all of the system’s data was as simple as doing SELECT *on each table in a MySQL database. But in an ironic twist, this method’s undoing would be the success of the social web itself. Though in Homans’ day, the questions grew faster than the data, today the data is growing faster than we can store and process it. Enter Hadoop. Last year, I decided to invest some time in learning to write my data analysis processing programs using MapReduce. Cornell is lucky enough to have a project called WebLab whose resources include a 50-plus node Hadoop cluster, and I am lucky enough to be allowed to use it. As soon as I ran some test cases on it � single-process implementations of computations that took 4.5 hours on my beefy workstation took 3 minutes when implemented in MapReduce � I was sold. In social network analysis, the research area I work in, the main questions of interest concern how patterns of social relationships affect individuals’ behavior and create social structure at the macro, or societal, level. Network analysts work in many areas of sociological interest, such as markets, employment, individual well-being, opinion formation, and others. Often, the structural properties of these networks are important predictors of individual behavior, but the computations required to calculate these measures is prohibitive. Right now, for example, I’m struggling to work with a comparatively large dataset comprising about 8 million people. I learned quickly, it’s not the size of the data that kills you, it’s the size of the metadata. Thought it’s relatively easy to count the number of friends or neighbors everyone has, other calculations, such as the average number of “steps” between each pair of people, have much more demanding computational requirements. Algorithms that are O(n2) or bigger in their space or time requirements become prohibitive � a network with 8 million members has a whopping 64 trillion relations between (all pairs of) members. No individual workstation, no matter how fancy, is equal to such a task. With enough disk space and RAM, and some fancy programming tricks that repeatedly swap to/from disk only the data necessary for parts of computations, you might be able to process all that data, once, and it might take several weeks to do even that. Distributing the same computation over a large number of Hadoop nodes and finishing the process in minutes or hours means that it’s possible to iterate rapidly. Iterating rapidly means fixing bugs rapidly, and trying variations rapidly. I can process weighted and non-weighted versions of the same graph in quick succession, with only a small code change, for example. Learning to process data using MapReduce is a skill that scales. The benefits of MapReduce over conventional programming is, in my opinion, equal to (or greater than) the benefits of conventional programming over analyzing data by hand. It takes a sizable initial time investment to learn to think in this way, especially if you haven’t been exposed to functional programming before (most non-computer scientists haven’t). But after getting the basic idea � the application of a series of transformations and compressions to data � the usefulness of the skill continues to grow naturally. The data is going to keep getting larger and more detailed, as more people experience more of their social and economic lives online. But the size of Hadoop clusters are going to get larger as well, and often increasing the number of nodes a job is processed on is as simple as changing one parameter in a configuration file. Academics can request access to the National Science Foundation’s TeraGrid system, and academics as well as recreational or corporate data crunchers can use MapReduce with their own clusters or cloud-based services like Amazon’s Elastic MapReduce. Another of my favorite scholars, British sociologist Anthony Giddens, once remarked that because we live in a modern world in which people naturally reflect on their own behavior and the behavior of others, the professional sociologist is “at most one step ahead of the enlightened lay practitioner” [5]. And that was before we lived in a world of gigantic datasets. Besides their use in web search at Yahoo and Google, Hadoop and MapReduce have been touted as having tremendous potential in the area of business intelligence. The Economist just recently focused on this very issue [6]. Though corporations are generally not releasing their internally-generated data, governments and the media are starting to get into the act, with data.gov in the U.S. and Guardian Datastore in the U.K. User-contributed sites like Swivel and ManyEyes contain data sets of many different kinds, though of relatively small sizes. Clearly, many people are interested in questions of social scientific importance and the stories these data can tell us. I think that’s a really good thing, and I’m excited for the long-term prospects of both “professional” and “amateur” data analysis. In the same way that the DIY movement and publications like Make Magazinehave inspired laypeople to become interested in some of the principles and practices of engineering, public datasets can perhaps inspire interest in the social sciences. Right now, the datasets are small and not particularly interoperable, but I have some confidence that will change over time. Imagine a world in which mashups aren’t just songs and videos, but terabytes of data, where the data input path specified in a Hadoop configuration file isn’t a local directory containing one’s own data, but rather a URI pointing to some stranger’s (or company’s or government’s) publicly-available archives. There’s a long way to go. Business practices, technologies and tools, and social science training each have years of advances to make before such a reality can become possible. Until now, scientific computing has largely been the domain of the natural sciences, fields like fluid dynamics, astrophysics and bioinformatics. The computational social science revolution that is just beginning is mostly attributed to the growth in data available from the sources I’ve mentioned and surely from scores more, and I agree; you can’t have data analysis without the data. Another important part of that story is computation on a cheap, pervasive, distributed cloud, and tools like Hadoop to process and analyze it all. Notes [1] For an overview see, this article published in Science last year. [2] They might also provide insightful but non-intuitive ideas that open up whole new lines of inquiry in your research. So these methods have many positive and indispensible qualities, too. [3] You can start with these two, very different, papers: Email as Spectroscopy or Communication (and Coordination) in a Modern, Complex Organization. [4] An excellent choice is Cynthia Feliciano’s Gendered Racial Exclusion among White Internet Daters. The dating service OkCupid also has a blog which is suggestive and interesting, but not quite controlled enough to be persuasive as social science. [5] Anthony Giddens, The Consequenecs of Modernity, 1990. [6] “The Data Deluge“, The Economist, 10 February 2010. [7] See the WebUse project to see the ways in which internet users are and are not representative of larger populations.</snippet></document><document id="664"><title>Pushing the Limits of Distributed Processing</title><url>http://blog.cloudera.com/blog/2010/04/pushing-the-limits-of-distributed-processing/</url><snippet>Lately we’ve been sharing stories about customers and how they’re using and benefiting from Hadoop.  For example, last week we saw how Raytheon Researchers are using Hadoop to build a scalable, distributed triple store.  This week’s war story comes from the inventor of MapReduce, Google, who is using MapReduce to reduce their map tile image files. Apache Hadoop has been making waves of excitement in the industry for several years, pushing the limits of distributed processing.  Founded on the premise that computation should move to data instead of the other way around, Hadoop has been deployed in hundreds of organizations, on clusters ranging from a few virtual nodes up to thousands. We recently learned of a team at Google that has pushed Hadoop to the limits by creating a cluster whose size is on the order of 100,000 nodes, running on the recently released Nexus One mobile phone hardware, powered by Android. By pushing computation out to these devices, the Nexus One team was able to solve the difficult rendering and scaling problems in situ. Since Google maps, street view, and directions were originally sized and scaled for display on the web, the team was faced with a difficult challenge when introducing mobile turn by turn directions. While the Google MapReduce cluster could easily be tasked with re-rendering for a smaller display, each image would have to be transmitted and tested on a device before it was served to account for proper scaling and pixel offsets. A direction that flowed off the screen would not only be an embarrassing quality issue, but raised safety concerns for drivers attempting to follow these directions.  The driver would drive off the screen!  A single iteration was certainly achievable but getting data back, adjusting the renderings and re-transmitting was obviously going to tax the network with unnecessary data movement. Following the philosophy of pushing computation to data, the team realized that they could construct a massive distributed MapReduce cluster and perform the rendering on each device.  When streaming down the images, each Nexus One would receive a high quality image that it then dynamically adjusted and measured, returning the final computations back to Google to produce the authoritative copy. Since Google’s MapReduce implementation is highly customized for their environment, the team set out to look for a portable MapReduce platform to distribute to the Nexus One devices.  Hadoop, which runs on a Java Virtual Machine, fit the bill. Now Google has created not only the largest MapReduce cluster but also the smallest and most distributed one as well. And because of the efficient power consumption of the Nexus One and reduced network traffic. The environmental cost of this solution is 1/100th the equivalent of running it within their data center.  Good thing Android supports background processes! Overall the Nexus One team at Google has declared the solution a complete success and rumor has it is considering using the Nexus One platform as the basis of a new general purpose virtual data center by harnessing the aggregate compute power during rush hour on the Tokyo Metro lines.</snippet></document><document id="665"><title>Cloudera’s Support Team Shares Some Basic Hardware Recommendations</title><url>http://blog.cloudera.com/blog/2010/03/clouderas-support-team-shares-some-basic-hardware-recommendations/</url><snippet>Refer to this post (Aug. 28, 2013) for state-of-the-art recommendations about hardware selection for new Hadoop clusters.</snippet></document><document id="666"><title>CDH3 Beta 1 Now Available</title><url>http://blog.cloudera.com/blog/2010/03/cdh3-beta1-now-available/</url><snippet>It’s official – Cloudera’s Distribution for Hadoop Version 2, which we often shorthand as CDH2, has been released. CDH2 is the product we recommend to our current production customers. It’s a stable version that has undergone a long cycle of time in the field with a variety of customers, in addition to Cloudera’s internal QA process. And with the CDH2 release, the Cloudera engineering team is excited to start the feedback and development process for the next version of Cloudera’s Distribution for Hadoop – Version 3. CDH3 includes an Apache�Pig package with additional bug fixes and performance improvements, and the Apache Hive package is now based on the latest Apache release. One of the most notable aspects of CDH3 beta 1 is what has not changed: CDH3 remains based on the Apache 0.20 release. However, we have already bundled many new improvements and bug fixes in CDH3. The release notes cover these changes in detail. What the release notes don’t share, though, is what we plan on putting into upcoming CDH3 releases. Cloudera is working hard with the rest of the Apache community to deliver additional features in CDH3 including the following noteworthy items: A new Pig package based on the latest Apache release. Apache HBase and ZooKeeper, previously only supported�as part of our contrib repository, will become first class packages in CDH3. The security work Yahoo! is contributing, which should significantly impact Hadoop adoption. As you’d expect,� we�will continue to test and integrate many other improvements, bug fixes and features throughout the release. Please check out the beta and tell us what you think!</snippet></document><document id="667"><title>CDH2 is released</title><url>http://blog.cloudera.com/blog/2010/03/cdh2-is-released/</url><snippet>We’re proud to announce that Cloudera’s Distribution for Hadoop Version 2 (CDH2) is officially released. We’ve come a long way to get to a production quality release. At the beginning of September we announced the first beta of CDH2. After 6 months of additional testing we announced a release candidate. The release candidate spent over a month hardening in Cloudera’s internal QA process and on a wide variety of customer clusters. CDH2 is now stable and ready for use – we are pleased to recommend it to all our production users. CDH2 is based on Apache Hadoop 0.20 – a release that has been available for almost a year. During this time, the Apache Hadoop community has produced hundreds of bug fixes, improvements and features. Cloudera is proud to have contributed many of these and incorporated them into CDH2.  For more information, please review the following resources: The release notes for CDH2. All bug fixes and improvements are covered in detail. For new features you’ll want to checkout CDH3, which is now in beta. For how to get started, please have a look at CDH documentation, which includes a helpful bit on determining which version is right for you. Hadoop is a community effort. We’d like to thank everyone who contributes to Hadoop, especially the substantial contribution made by the big team at Yahoo! and all the other users who have contributed to this release. We appreciate the feedback on Get Satisfaction, twitter and IRC (#cloudera on freenode.net). Keep it coming, and thanks for using Cloudera’s Distribution for Hadoop!</snippet></document><document id="668"><title>How Raytheon BBN Technologies Researchers are Using Hadoop to Build a Scalable, Distributed Triple Store</title><url>http://blog.cloudera.com/blog/2010/03/how-raytheon-researchers-are-using-hadoop-to-build-a-scalable-distributed-triple-store/</url><snippet>This post was contributed by Kurt Rohloff, a researcher in the Information and Knowledge Technologies group of Raytheon BBN Technologies, a wholly owned subsidiary of Raytheon Company. Using Hadoop to Build a Scalable, Distributed Triple Store The driving idea behind Semantic Web is to provide a web-scale information sharing model and platform.  One of the singular advancements over the past several years in the Semantic Web domain has been the explosion of data available in semantic formats.  Unfortunately, Semantic Web data processing technologies are deployed on a single (or a small number of) machine(s) at a time.  This is fine when data is small, but current methodologies create horrible data processing and analysis bottlenecks.  These scalability constraints are the biggest barriers in achieving the fundamentally web-scale Semantic Web vision of Tim Berners-Lee.  More importantly, these limitations have hindered the broader adoption of Semantic Web technologies. Some of my fellow scientists and engineers at BBN and I have started to address these scalability limitations in the Semantic Web by developing a work-in-progress cloud-based triple-store technology we call SHARD (Scalable, High-Performance, Robust and Distributed) that enables scalable data processing and analysis based on the Cloudera Distribution for Hadoop implementation of the MapReduce formalism.  My cowokers and I found that this formalism enables a game-changing approach to scalable, robust and lightly coordinated parallel data processing that avoids some of the coordination bottlenecks endemic in distributed computing environments. SEMANTIC WEB Our SHARD triple-store persists data as standard RDF triples and runs queries over this data using the standard SPARQL query-language.  We deployed an early version of SHARD into Amazon EC2 and ran the standard LUBM triple-store benchmark.  We found that SHARD already performs better than current industry-standard triple-stores for datasets on the order of a billion triples.  A small example graph can be seen in Figure 1. AN EXAMPLE Figure 1. Small Graph of Triple Data This small graph contains 7 triples – Kurt lives in Cambridge, Kurt owns an object car0, car0 is a car, car0 was made by Ford, car0 was made in Detroit, Detroit is a city and Cambridge is a city. The SPARQL query language is the standard query language for querying the triple data.  SPARQL semantics are remarkably similar to the more well-known SQL.  An example SPARQL query for the above graph data can be seen immediately below. SELECT ?person WHERE { ?person :owns ?car . ?car :a :car . ?car :madeIn :Detroit . } The above SPARQL query has three clauses and asks for all matches to the variable ?person such that ?person owns an entity represented by the variable ?car which is a car and was made in Detroit.  The above query can be represented as a directed graph as seen in Figure 2. Figure 2. Directed Graph Representation of a Query Processing of SPARQL queries such as the one above consists of identifying which variables in the query clauses can be bound to nodes in the data graph such that the query clauses align with the data triples.  An example of this alignment for our example query and data can be seen in Figure 3. Figure 3. Alignment of SPARQL Query Variables with Triple Data The Shard Triple Store Our functional design goals for the SHARD triple-store are to: 1.    Serve as a persistent store for triple data in RDF format. 2.    Serve as a SPARQL endpoint to process SPARQL queries. The SHARD triple-store is designed to persist graph data and process SPARQL queries.  Input (data, queries) and output (query results) are passed to/from SHARD through the HDFS file system.  We made this design decision with the understanding that the input data and output results are generally very large and not feasible to output directly to the user.  In light of our experience with large-scale Semantic Web users, we think it is most efficient to save query results to disk for eventual use by users. Triple Data Persistence We persist data in SHARD in flat files in the HDFS file system such that each line of the triple-store text file represents all triples associated with a different subject. Consider the following exemplar line saved in SHARD from the LUBM domain that represents three triples associated with the entity subject Pub1: Pub1 :author Prof0 :name “Pub1″ a :Publication This line represents that the entity Pub1 has an author entity Prof0, Pub1 has a name “Pub1” and that Pub1 is a publication. Although this approach to persisting triple data as flat text files is rudimentary as compared to other triple-store approaches, we found that it offers a number of important benefits for several general application domains.  For one, this approach to saving files in HDFS brings a level of automated robustness to the triple-store that would normally be difficult to develop using other distributed file systems.  The data is also stored in a simple, easy to read format that lends itself to easier drill-down diagnostics of query results returned by the triple-store.  Most importantly, however, although this approach to storing triples is inefficient for query processing that requires the inspection of only a small number of triples, this approach is very efficient in the context of Hadoop for scanning over large sets of triples to respond to queries that will generate a large number of results. Query Processing Overview The query processing engine in SHARD is designed to iterate over the clauses in the queries using the triple data and incrementally attempt to bind query variables to literals in the triple data while satisfying all of the query constraints.  Each step of the iteration consists of a MapReduce operation for a single clause in the query.  A schematic overview of this iterative query binding process can be seen in Figure 4. We made the design decisions that intermediate results of the SPARQL query operation are cached to disk to speed later similar queries.  We found that this capability is very useful in practice when users make frequent, similar queries of the triple data. Figure 4. Schematic Overview of the Iterative Algorithm to Process SPARQL Queries with Triple Data The first map MapReduce step maps the triple data to a list of variable bindings which satisfy the first clause of the query.  The key of the Map step is the list of variable bindings.  The Reduce step removes duplicate results and saves them to disk with the variable bindings as the key. The intermediate query binding steps continue to iteratively bind variables to literals as new variables are introduced by processing successive query clauses and/or filtering the previous bindings which cannot fit the new clauses.  The intermediate steps perform a MapReduce operation over both the triple data and the previously bound variables which were saved to disk. The ith intermediate Map step identifies all variables in the triple-data which satisfy the ith clause and saves this result with the key being any variables in the ith clause which appeared in previous clauses.  The value of this Map step is the bindings of other variables not previously seen in the query clauses, if any.  This iteration of the Map set also rearranges the results of the previous variable bindings saved to disk to the same of a variable key in the ith clause that appeared in previous clauses.  The value of this key-value pair are the list of variable bindings which occurred in previous clauses but not in the ith clause. The ith Reduce step runs a join operation over the intermediate results from the Map step by iterating over all pairs of results from the previous clause and the new clause with the same key assignment. This iteration of map-reduce-join continues until all clauses are processed and variables are assigned which satisfy the query clauses.  SHARD is designed to save intermediate results of the query processing to speed up the processing of similar later queries. The final MapReduce step consists of filtering bound variable assignments to satisfy the SELECT clause of the SPARQL query.  In particular, the Map step filters each of the bindings, and the Reduce step removes duplicates where the key value for both Map and Reduce are the bound variables in the SELECT clause. PROOF OF CONCEPT EXPERIMENTATION To test the performance of the SHARD triple-store design, we deployed an early version of SHARD onto an Amazon EC2 cloud environment of 20 XL compute nodes running the Cloudera distribution of Hadoop.  The version of SHARD we deployed for evaluation supports basic SPARQL query functionality (without support for prefixes, optional clauses or results ordering) over full RDF data.  Additionally, the deployed version of SHARD does not perform any query manipulation/reordering/etc… normally done for increased performance by SPARQL endpoints and the deployed version of SHARD does not take advantage of any possible query caching made possible by our design choices. LUBM Benchmark We used the standard LUBM triple-store benchmark to evaluate the performance of SHARD.  The LUBM benchmark creates artificial data about the publishing, coursework and advising activities of students and faculty in departments in universities.  We used code from the LUBM benchmark to generate triple data for 6000 universities which is approximately 800 million triples to parallel the performance evaluations made in a previous triple-store comparison study. After loading the triple data into the SHARD triple store, we evaluated the performance of SHARD in responding to queries 1, 9 and 14 of LUBM as was done in the previous triple-store study.  Query 1 is very simple and asks for the students that take a particular course and returns a very small set of responses.  Query 9 is relatively more complicated query with a triangular pattern of relationships – it asks for all teachers, students and courses such that the teacher is the adviser of the student who takes a course taught by the teacher.  Query 14 is relatively simple as it asks for all undergraduate students (but the response is very large).  Except for Query 1, SHARD performed better than other known technologies.  Also, due to the inherent scalability of the Hadoop and HDFS approach in SHARD, the SHARD triple-store could potentially be used for extremely large datasets (trillions of triples) without requiring any specialized hardware as required for other monolithic triple-stores. ONGOING WORK Development work is ongoing with SHARD.  Based on our experience with the initial SHARD deployment, we have several short- and long-term activities to further improve performance and applicability. First among the improvements to be made to SHARD is a more intelligent indexing capability to better optimize query performance on “small” queries.  Additional performance of SHARD in a targeted production environment could be provided by using cached partial results, as outlined above.  This will require additional capability for the triple store to track what partial results were previously cached and possibly to track which cached results could be thrown out to save disk space in the cloud (if this is a user concern.)</snippet></document><document id="669"><title>HBase User Group #9: HBase and HDFS</title><url>http://blog.cloudera.com/blog/2010/03/hbase-user-group-9-hbase-and-hdfs/</url><snippet>Last week, several Cloudera employees attended the Bay Area HBase User Group #9, kindly hosted by Mozilla at their headquarters in Mountain View. About 80 people attended, and it was a great chance to get together with the whole HBase community. I got a chance to chat with some community members who have been running HBase in their organizations for quite some time, and also several who are just beginning to investigate the project for new and exciting projects within their businesses. The user group organizers were kind enough to invite me to present, and I took the opportunity to discuss the integration between HBase and HDFS (the Hadoop Distributed File System). HBase utilizes HDFS for all of its underlying storage, and therefore understanding the performance and reliability characteristics of HDFS is key to a deep understanding of HBase. In my presentation, I talked about some of the original design goals of HDFS for batch processing, and enumerated some of the exciting new developments currently under way that will really improve it for online use cases like HBase. I also announced that Cloudera will be including these important patches in CDH3 after development and testing are finished; we’re committed to being the very best Hadoop and HBase distribution out there. Check out the slides above, and if you’re considering using HBase in your business, please feel free to get in touch with me at todd@cloudera.com. And if you’re in the area, be sure to register now for HBase User Group #10.</snippet></document><document id="670"><title>Natural Language Processing with Apache Hadoop and Python</title><url>http://blog.cloudera.com/blog/2010/03/natural-language-processing-with-hadoop-and-python/</url><snippet>This blog was co-written by Nitin Madnani andJimmy Lin, both researchers at the University of Maryland, who are sharing their thoughts and experiences with Apache Hadoop and Python for improving Natural Language Processing techniques. If you listen to analysts talk about complex data, they all agree, it’s growing, and faster than anything else before. Complex data can mean a lot of things, but to our research group, ever increasing volumes of naturally occurring human text and speech�from blogs to YouTube videos�enable new and novel questions for Natural Language Processing (NLP). The dominating characteristic of these new questions involves making sense of lots of data in different forms, and extracting useful insights. NLP is hot and getting hotter NLP is a highly interdisciplinary field of study comprising of concepts and ideas from Mathematics, Computer Science and Linguistics. Naturally occurring instances of human language, be it text or speech, are growing at an exponential rate given the popularity of the Web and social media. In addition, people are increasingly becoming more and more reliant on internet services to search, filter, process and, in some cases, even understand the subset of such instances they encounter in their daily lives. Whether you think about it or not, those services allowing you to do so much with language everyday are generally trying to solve well-understood NLP problems under active research. To put it into context, let us show you some examples. Let�s say that a blogger is trying to gather the latest information on the earthquake in Chile. Her workflow might consist of the following sequence of web-based tasks. With each task, we include the name of the specific NLP problem being solved by the service performing the task: � �Show me the 10 most relevant documents on the web about the earthquake in Chile� (Information Retrieval) � �Show me a useful summary of these 200 news articles about the earthquake in Chile� (Automatic Document Summarization) � �Translate this Spanish blog into English so I can get the latest information about the earthquake in Chile� (Machine Translation) We believe that NLP as an area of scholarly exploration has never been more relevant than it is today. One of the most successful trends in NLP has been that of using methods that are driven by naturally occurring language data instead of using purely knowledge-based or rule-based methods that are generally not as robust and are expensive to build. As this trend has continued, data-driven NLP techniques have become more and more sophisticated by borrowing heavily from the fields of Statistics and Machine Learning. These more sophisticated techniques now require large amounts of data in order to build a reasonably good model of what human language looks like. NLP needs lots of data to shine. Therefore, we use Hadoop. In order to do effective NLP research, we use very large bodies of text (or corpora) that are now becoming available to us. Examples include: � The Google n-gram corpus, a trillion word database containing phrases (up to 5 words long) occurring on public Web pages. � The USENET corpus, a 25 billion word (compressed) corpus containing public USENET postings on 47,680 English language, non-binary-file newsgroups between Oct 2005 and Jan 2010. It should be obvious by now why this post belongs on the Cloudera blog. In order to process language data sets of such sizes efficiently, we have turned to Hadoop and Cloudera. The Python and The Elephant However, one issue that we have encountered is that Hadoop is written entirely in Java and we regularly use Python in our research, particularly the excellent and open-source Natural Language ToolKit or NLTK. NLTK is a great tool in that it tries to espouse the same �batteries included� philosophy that makes Python a useful programming language. NLTK ships with real-world data in the form of more than 50 raw as well as annotated corpora. In addition, it also includes useful language processing tools like tokenizers, part-of-speech taggers, parsers as well as interfaces to machine learning libraries. NLTK is impressive enough that it now commands its own animal in the O�Reilly collection. In short, we would like to be able to continue using Python and NLTK for our research but also leverage the fantastic distributed processing capabilities of Hadoop. What�s a Pythonista to do? The Hadoop Streaming interface is the solution to our problem. From the webpage: �Hadoop streaming is a utility that comes with the Hadoop distribution and allows allows you to create and run map/reduce jobs with any executable or script as the mapper and/or the reducer�. However, wouldn�t it be nice if someone did the hard work of wrapping the streaming interface into a nice Pythonic API? Indeed, it would be and it is. Dumbo is a fledgling, yet already very capable, open source project that strives to do exactly this. In fact, Dumbo is so nice that we recently used it to do the NLP task of automatic word association with a very large corpus by using Hadoop on Amazon EC2. Word association is a very common task in psycholinguistics where the subject is asked to say the word X that immediately comes to mind when hearing word Y. The results of having a computer do it were both entertaining and informative and we presented it to the Python community at PyCon 2010 in Atlanta. The PyCon folks recorded the talk and have made it available here. The tools and resources that Cloudera provides were a really big help to us in doing this task. We used the Cloudera 0.20.1+152 hadoop distribution and the stock ec2 shell-scripts that come bundled with that distribution. Note that Cloudera has now made available better and more robust Python versions of the ec2 scripts that are even easier to use. The folks at Cloudera were also very helpful and gracious with their time when we had a question or two. Bottom line: If you are interested in NLP, and like us, think that Python and NLTK are useful tools, the world is a better place these days thanks to Dumbo. Some additional resources As long as we are talking about Hadoop and NLP together, it would be worth mentioning that in the last three years or so, one of us has worked really hard, with help from other smart people, to come up with Hadoopified versions of the most commonly used algorithms for many NLP problems and collected them into a book. It is an excellent resource for anyone who is interested in doing large scale text processing with MapReduce.</snippet></document><document id="671"><title>Why Europe’s Largest Ad Targeting Platform Uses Apache Hadoop</title><url>http://blog.cloudera.com/blog/2010/03/why-europes-largest-ad-targeting-platform-uses-hadoop/</url><snippet>Richard Hutton, CTO of nugg.ad, authored the following post about how and why his company uses Apache Hadoop. nugg.ad operates Europe�s largest targeting platform. The company�s core business is to derive targeting recommendations from clicks and surveys. We measure these, store them in log files and later make sense of them all. In 2007 up until mid 2009 we used a classical data warehouse solution. As data volumes increased and performance suffered, we recognized that a new approach was needed. This post tells the story of how we arrived at using a Hadoop-based solution � and how we took jobs that required five days to process down to one hour. Data Processing Platform Requirements The nugg.ad service is split into two parts. The online targeting platform accessed by HTTP provides real-time targeting recommendations in response to a users clicking behavior. The off-line data processing platform performs the analytics to make this possible. Currently our online platform creates on a daily basis just over a 100 GB of log data per day. The majority of data is for website clicks, ad clicks and targeting predictions. These are split into different files for each category and rotated on an hourly basis. The data which needs to be logged is sent in UDP packets to a series of log nodes implemented in Erlang. The logging of user interactions with our online platform creates considerable amounts of data. We need to use all of this data in order to: � Monitor the performance of ad campaigns � Track the precision of targeting predictions � Create reports used by our account managers and statisticians for decision making � Summarize the data for our customers to help sell ad space � Run ad hoc reports over historical data � Extract training data for our prediction algorithms � Build machine learning models When We Were Young and Energetic The initial solution for the data processing platform was built on the principles of classical data warehousing. The log files were collected from each of the log nodes and merged into one big file for each category. The contents were then aggregated. After that the results were used by the Pentaho Kettle ETL tool to populate the tables based on a star schema in PostgreSQL. At this point we could run SQL queries to use our data. In March 2008 we needed to process and use 30 GB of daily log data per day. The processing times for our most important events were: � 12 hours to summarize all daily log files for logged events � 6 hours to create training data samples � 2 days to create weekly reports � 2 days to summarize data accessed via a customer web based interface We were far from satisfied with these times. However, it was good enough in the early days and customer expectations of our service in 2008 were a lot lower than today. Headache Turning Into a Migraine One year later our situation had changed considerably. The graph below shows the amount of data we were logging from January 2009 until December 2009. 2009 Data Growth in GB In March 2009 compared to the previous year we were logging more than double the amount of data per day as the online platform went from receiving 3200 to over 8500 requests per second. The good news was our business was growing nicely. Our online platform was straight-forward to scale in tandem, we just added more machines. The bad news was our data processing platform was not as simple to scale. In March 2009 the processing times for the same events mentioned early were: � 23 hours to summarize all daily log files for all events (almost a day to process a day) � 18 hours to create training data samples (inhibiting our ability to refresh data mining models regularly) � 5 days to create weekly reports (our weekly reports were almost one week behind) � 4 days to summarize data accessed via a customer web based interface (prone to crash) We could no longer process our data in the time scales needed for our business. The root cause of our problem was clear. A significant increase in data volume resulted in a significant increase in processing times. Initially we thought maybe we could just improve our classical data warehouse. After analyzing our current solution and problem it was pretty clear that we could not avoid hitting scaling and performance problems. Searching for Relief, We Found Hadoop In March 2009 we started our investigation of possible technologies. As part of it we setup a Hadoop test cluster with three machines. A selection of log files was copied into the HDFS. We started writing simple Pig scripts and later MapReduce jobs using the standard Hadoop API. It took some practice to go beyond the classical word count example to writing solutions for our problems using MapReduce. Since our initial tests looked promising, we decided to try and build our solution on Hadoop. Besides scaling there were several additional reasons: � Relatively easy to administrate and monitor � Easy to use (when you are a small team everyone needs to be able to change anything at anytime) � No software licensing costs � Only expansion cost is hardware Development Begins In June 2009 we started to develop a full solution. At this point certain external factors played a very helpful role in our development. One was the publication of the book �Hadoop: The Definitive Guide� by Tom White, which helped us to understand how we could use Hadoop. The other was the emergence of a dynamic programming language called Clojure, which compiles directly to JVM byte code. After one month of development we were able to create our reports using Hadoop with our processing times going from five days to one hour. This was great for building confidence in our decision. For the following four months we progressively turned features off in the old data warehouse as they became available in Hadoop. By October 2009 we had completed the migration and also additional new features previously impossible to run. In the next section I will briefly explain how it actually works. A Closer Look at our Hadoop Setup Our cluster is located in one of our data centers and contains commodity machines with a total of 36 cores and 8 TB of disk space. The machines were provisioned using Chef from Opscode.com and we use the Cloudera CDH1 distribution. The log files are now copied to the HDFS. In order to make sense of our data we need it summarized by hours, days and weeks. Therefore, we organize our HDFS directory structure hierarchically by date to reflect this requirement. The path for handling days and hours follows the structure /event/years/months/days/hours. This way we can use simple file globs for a MapReduce job input file configuration. We wrote our own simple scheduler querying the HDFS to see, if input is available to create missing output. When it cannot find the output a configuration is created containing input and output path which is sent to our MapReduce server. The MapReduce server provides a JSON HTTP API for starting, querying and stopping jobs. It supports both scheduled and on-demand jobs. When the server receives a request to run a job, the event name is used to locate the associated chain of one or more Hadoop jobs to run. A unique identifier is returned which can be later used to query or stop the job. An example is the chain of events to run one of our daily reports. Customer-wise it contains a summary of page impressions and unique clients for each socio-demographic and product interest prediction our online platform produces. Therefore, we first fetch information stored in our customer database and add this to the distributed cache. The MapReduce phase sums up the page impressions for each user and counts the number of client ids for each prediction and possible outcome e.g. age class 30-39. The final phase is to perform a reduce side join where the internal customer ids are translated to account manager readable information by accessing the data previously stored in the distributed cache. At a later point we intend to use the MapReduce server API to build a web based interface fitting our purposes. Frequently-run jobs are implemented in Clojure and Java using the Hadoop MapReduce API with a number of performance optimizations. These are: � Compress map output to LZO, mainly to reduce disk IO during the shuffle phase � Apply a Combiner to perform initial aggregation before the data arrives at the reducer � Use our own developed Writable types which are written to be RawComparators � Add type hints with Clojure code to avoid the overhead of reflection Also we use tools like Pig to run ad hoc reports as well as streaming jobs e.g. to grep the contents of the web servers logs. Spreadsheets are often used by our statisticians and account managers as a tool to analyze data. Therefore, we wrote an OutputFormat class which generates Excel work books with a number of sheets summarizing customer data. Older and a Little Wiser The processing times for our most important events in December 2009 were: � 42 minutes to summarize all daily log files for all events � 1 hour to create training data samples � 1 hour to create weekly reports � 3 hours to summarize data accessed via a customer web based interface Hadoop has really helped us to reduce dramatically the time taken to process data. We can expand both our online and data processing platform in the same way by simply adding more machines. A recent interesting development in our market is to enable different customers to share their data with each other for variable time frames. Data shared by several willing customers involves finding and processing huge training sets for our prediction algorithms. If we had not migrated, we could have never made this possible. Looking Ahead A potential next step for us would be to use column-oriented stores with MapReduce integration. Some of the options in the Hadoop ecosystem include Zebra (Pig), RCFile (Hive), or HBase. If this proves to be successful I look forward to writing the follow-up post. Moving from one hour to one minute sounds good. About nugg.ad With its Predictive Behavioral Targeting solution nugg.ad operates Europe�s largest targeting platform. nugg.ad’s unique predictive algorithm reduces media loss, increases campaign efficiency and lowers target-group CPM. nugg.ad works with and assists its clients to increase turnover and win new advertising budgets as it delivers predicted values on socio-demographics, gender and product interests making it possible to target hard-to-reach target groups online. About the author Richard Hutton is the CTO of nugg.ad and has been working for the organization since October 2006.</snippet></document><document id="672"><title>Trip Report: Utah Java User’s Group</title><url>http://blog.cloudera.com/blog/2010/03/trip-report-utah-java-users-group/</url><snippet>One of the fun things about working at a company that’s involved in an exciting open-source project like Hadoop is that, surprisingly often, you get invited to talk about it. On February 18th, I presented Hadoop (slides) to the folks at the Utah Java User’s Group. A little over one hundred people were in attendance. There was pizza, a talk about Android development, my talk about Hadoop, a raffle for door prizes (software licenses, pens, t-shirts, books, etc.; Cloudera contributed two signed copies of Tom White’s Hadoop: The Definitive Guide), and breakout sessions. About half the audience had heard of Hadoop, but fewer than 10 people had ever written MapReduce jobs, so we went over the motivation and the basic ideas behind HDFS and MapReduce. I fielded a couple of great questions (how does Hadoop fit in with cloud offerings from Amazon and Google?) at the talk, and then, at the breakout session, about 10 of us (including folks from Overstock.com and Tynt) dug deeper. We discussed HDFS High Availability (a perennial favorite), using Hadoop for real-time serving (usually the answer is “don’t”), HBase, and Zookeeper. The next day, Michael Finger of Overstock and I took some turns at Snowbird (snow day!). Thanks to Chris Maki and the rest of the UJUG crew for having me! Download Slides (PDF)</snippet></document><document id="673"><title>Apache Avro 1.3.0</title><url>http://blog.cloudera.com/blog/2010/03/avro-1-3-0/</url><snippet>Apache Avro was added the to Hadoop family last April and last year there were three Avro releases: 1.0.0 in July, 1.1.0 in September and 1.2.0 in October. �After the 1.2.0 release, Doug Cutting introduced Avro: a New Format for Data Interchange on this blog and the Avro team went right to work building the next release of Avro. It’s a new year and there’s a new Avro: 1.3.0. Starting with Avro 1.3.0, the Avro team is releasing packages specially tailored to consumers of each language. �For example, Python users can download an egg, Java users can manage jars using Maven and C/C++ users can grab an autotools package ready to�`./configure; make`. �Speaking of languages, we’re thrilled to announce that there’s a Ruby implementation for Avro now! The Avro specification has been updated to include support for Avro RPC over HTTP. �Currently, only Java and Python support this new RPC specification but you can expect other languages to follow. �The Avro team also designed a test framework to ensure�interoperability between any mix of Avro RPC clients and servers. In Avro 1.3.0, there’s a new Avro data file format that is simpler, better suited for compression and provides support for streaming Avro data. �You’ll find support for this new file format in Ruby, Python, Java and C; giving you an array of languages to choose from for reading and writing Avro data. There have been more features added to Avro than can fit in a single blog post but here are some of the highlights. Java Substantial improvements to Reflection API. Now uses java.lang.String for Avro strings, either Java collections or arrays for Avro arrays, etc. New GenAvro tool provides a high-level syntax for schemas and protocols. Command-line tools jar for debugging. An RPC statistics system. Support for compression in data files Better Maven support including a mvn-install ant task to publish jar to local Maven repository, plus source and javadoc artifacts. Substantial performance improvements. Many bug fixes. Python Rewritten to be slightly more Pythonic, simpler, and with greater test coverage RPC over HTTP support RPC and data file interoperability New command-line utility for sending and receiving RPCs Python eggs created C++ The C++ implementation now uses autotools for its build, has a new API for checking schema resolution and provides a new tutorial to make it easier for you to get up and running with Avro in C++. Ruby Ruby hackers will be happy to hear that Ruby has been added to Avro 1.3.0 complete with support for the new data file format. C The C implementation has been completely rewritten from top to bottom and supports reading and writing the new Avro data file format adds a contact database example to make it easier for you learn the Avro C API provides schema validation, promotion and projection allows schema validation to be optional removes all dependencies on external libraries (e.g. APR, APR-util) embeds jansson for JSON parsing To download Avro 1.3.0, visit the Avro releases page. �Once you’ve downloaded Avro, you might want to �take a look at the Avro documentation pageas well. You can contact the Avro team by visiting the #avro irc channel on irc.freenode.net or through one of the Avro mailing lists. �The Avro team is always open to suggestions about future features and would love to hear about your experiences using Avro 1.3.0.</snippet></document><document id="674"><title>Cloudera’s Apache Hadoop Training Programs Expand Internationally</title><url>http://blog.cloudera.com/blog/2010/02/clouderas-hadoop-training-program-expand-internationally/</url><snippet>It’s been over a year now since we started offering Hadoop training in the Bay Area, and since then, we’ve put many of our introductory materials online (for free), and offer in-person public classes in cities around the US (click here for a full list of sessions). The response has been incredible, but one thing is painfully obvious: we’re not doing enough to meet the needs of the growing world-wide Apache Hadoop community. To that end, we’ve made investments in translating translating our materials into new languages and thinking about how to scale our training programs internationally. As a first step, we’ll offer our three-day developer training session outside the US later this spring. We’ll announce cities and dates in the EU soon, but we’re happy to announce our first two sessions in Asia now: Tokyo, Japan – April 6-8- Instruction in English, Materials in Japanese and English, with special thanks to NTT-Data and Preferred Infrastructure for local support. Taipei, Taiwan – April 12-14 – Instruction in English, Materials in Traditional Chinese and English, with special thanks to ChungHwa System Integration and the National Center for High-Performance Computing for local support. Going forward, we’ll explore partnership relationships with companies that are well-positioned to serve the needs of local Hadoop communities in their local language. Great partners have strong technical skills, experience delivering professional training, and business interests aligned with driving Hadoop adoption. We’ve identified some great partners already, and if you think you’d be a good fit, let us know.</snippet></document><document id="675"><title>CDH2: “Testing” Heading Towards “Stable”</title><url>http://blog.cloudera.com/blog/2010/02/cdh2-testing-heading-towards-stable/</url><snippet>In September 2009, we announced the first release of CDH2, our current testing repository. Packages in our testing repository are recommended for people who want more features and are willing to upgrade as bugs are worked out. Our testing packages pass unit and functional tests but will not have the same “soak time” as our stable packages. A testing release represents a work in progress that will eventually be promoted to stable. It’s a long road of feedback, bug fixes, QA and testing to move from testing to stable. As someone who tracks the maturity of a testing build throughout its life cycle, I’m pleased to say we’ve put a lot of polish into this release. CDH2 has reached the point where we are preparing to promote it to stable. One might even call this a “release candidate”. Cloudera engineers have been hard at work getting patches into CDH2 to make it the best 0.20 release available. Here are some of the highlights: Hadoop 0.20.1 – 73 more patches of extra Hadoop’y goodness (that is 225 total patches over vanilla 0.20.1) Lots of libhdfs and fusefs love resulting in stability and usability improvements currently in use at scale HDFS fixes that improve the write pipeline Lots of general stability fixes for Hadoop Pig 0.5.0 release – Working out of the box with our Hadoop 0.18 and 0.20 builds Hive 0.4.1 release – Works with both of our Hadoop 0.18 and 0.20 HBase 0.20.3 – We worked with the HBase team to bring the latest rpms to a yum repo near you We are excited about our CDH2 release. Its running at scale at some really great companies. We are looking forward to promoting it to stable shortly and moving on to the next big thing, CDH3. I’ll let you know as soon as this happens. When CDH2 becomes stable, it also means that CDH3 is ready to start its journey through testing. Stay tuned for more details as to what CDH3 will encompass; I’ll just say that I’m pretty excited about it. You can subscribe to our CDH mailing list (cdh-announce-subscribe@cloudera.com) to get information about new releases as we push them out. Check out the new release, and remember to let us know what you think!</snippet></document><document id="676"><title>Cloudera speaks VMware vCloud API, too.</title><url>http://blog.cloudera.com/blog/2010/01/cloudera-speaks-vmware-vcloud-api-too/</url><snippet>We’ve announced, with VMware, the ability to use third-party vCloud Express service providers and the vCloud API to run Cloudera’s Distribution for Hadoop. We think this is interesting; as cloud services proliferate, it’s important to be able to move easily among public and private clouds. vCloud makes that easier and VMWare is working hard to make the interfaces available across a wide variety of vendors. We started with Terremark, but expect the vCloud API to make moving to others straightforward. In addition to work we’ve done on our scripts for Hadoop, we’ve contributed to the open source libcloud project which also supports the VMware’s vCloud API. If you’d like to experiment, you can try it for yourself. These README instructions are a good place to start.</snippet></document><document id="677"><title>Hadoop World: Building Data Intensive Apps with Hadoop and EC2</title><url>http://blog.cloudera.com/blog/2010/01/hadoop-world-building-data-intensive-apps-with-hadoop-and-ec2/</url><snippet>Today’s Hadoop World Talk comes from Pete Skomoroch, and dives into detail about how he built TrendingTopics.org using Hadoop and EC2.</snippet></document><document id="678"><title>Hadoop World: Making Hadoop Easy on Amazon Web Services</title><url>http://blog.cloudera.com/blog/2009/12/hadoop-world-making-hadoop-easy-on-amazon-web-services/</url><snippet>Today’s Hadoop World talk comes from Peter Sirota, who leads Amazon Web Service’s Elastic MapReduce team. In this talk, Peter provides more detail on the platform, shares some new features, and shows how the AWS community, from customers to developers, are making things easier with Hadoop.</snippet></document><document id="679"><title>Hadoop World: Hadoop Applications at Yahoo!</title><url>http://blog.cloudera.com/blog/2009/12/hadoop-world-hadoop-applications-at-yahoo/</url><snippet>Today’s Hadoop World talk comes from Eric Baldeschwieler, Yahoo!’s VP of Hadoop Development. In this talk, Eric highlights Yahoo’s contributions to development and testing of Hadoop at scale, and goes into detail about how Yahoo! uses Hadoop to deliver several popular services. A major thanks to Eric, and everyone else at Yahoo! for their ongoing contributions to, and investment in, Apache Hadoop – it’s hard to imagine where the project would be today without such dedication.</snippet></document><document id="680"><title>7 Tips for Improving MapReduce Performance</title><url>http://blog.cloudera.com/blog/2009/12/7-tips-for-improving-mapreduce-performance/</url><snippet>One service that Cloudera provides for our customers is help with tuning and optimizing MapReduce jobs. Since MapReduce and HDFS are complex distributed systems that run arbitrary user code, there’s no hard and fast set of rules to achieve optimal performance; instead, I tend to think of tuning a cluster or job much like a doctor would treat a sick human being. There are a number of key symptoms to look for, and each set of symptoms leads to a different diagnosis and course of treatment. In medicine, there’s no automatic process that can replace the experience of a well seasoned doctor. The same is true with complex distributed systems — experienced users and operators often develop a “sixth sense” for common issues. Having worked with Cloudera customers in a number of different industries, each with a different workload, dataset, and cluster hardware, I’ve accumulated a bit of this experience, and would like to share some with you today. In this blog post, I’ll highlight a few tips for improving MapReduce performance. The first few tips are cluster-wide, and will be useful for operators and developers alike. The latter tips are for developers writing custom MapReduce jobs in Java. For each tip, I’ll also note a few of the “symptoms” or “diagnostic tests” that indicate a particular remedy might bring you some good improvements. Please note, also, that these tips contain lots of rules of thumb based on my experience across a variety of situations. They may not apply to your particular workload, dataset, or cluster, and you should always benchmark your jobs before and after any changes. For these tips, I’ll show some comparative numbers for a 40GB wordcount job on a small 4-node cluster. Tuned optimally, each of the map tasks in this job runs in about 33 seconds, and the total job runtime is about 8m30s. Tip 1) Configure your cluster correctly Diagnostics/symptoms: top shows slave nodes fairly idle even when all map and reduce task slots are filled up running jobs. top shows kernel processes like RAID (mdX_raid*) or pdflush taking most of the CPU time. Linux load averages are often seen more than twice the number of CPUs on the system. Linux load averages stay less than half the number of CPUs on the system, even when running jobs. Any swap usage on nodes beyond a few MB. The first step to optimizing your MapReduce performance is to make sure your cluster configuration has been tuned. For starters, check out our earlier blog post on configuration parameters. In addition to those knobs in the Hadoop configuration, here are a few more checklist items you should go through before beginning to tune the performance of an individual job: Make sure the mounts you’re using for DFS and MapReduce storage have been mounted with the noatime option. This disables access time tracking and can improve IO performance. Avoid RAID and LVM on TaskTracker and DataNode machines – it generally reduces performance. Make sure you’ve configured mapred.local.dir and dfs.data.dir to point to one directory on each of your disks to ensure that all of your IO capacity is used. Run iostat -dx 5 from the sysstat package while the cluster is loaded to make sure each disk shows utilization. Ensure that you have SMART monitoring for the health status of your disk drives. MapReduce jobs are fault tolerant, but dying disks can cause performance to degrade as tasks must be re-executed. If you find that a particular TaskTracker becomes blacklisted on many job invocations, it may have a failing drive. Monitor and graph swap usage and network usage with software like Ganglia. Monitoring Hadoop metrics in Ganglia is also a good idea. If you see swap being used, reduce the amount of RAM allocated to each task in mapred.child.java.opts. Benchmarks: Unfortunately I was not able to perform benchmarks for this tip, as it would involve re-imaging the cluster. If you have had relevant experience, feel free to leave a note in the Comments section below. Tip 2) Use LZO Compression Diagnostics/symptoms: This is almost always a good idea for intermediate data! In the doctor analogy, consider LZO compression your vitamins. Output data size of MapReduce job is nontrivial. Slave nodes show high iowait utilization in top and iostat when jobs are running. Almost every Hadoop job that generates an non-negligible amount of map output will benefit from intermediate data compression with LZO. Although LZO adds a little bit of CPU overhead, the reduced amount of disk IO during the shuffle will usually save time overall. Whenever a job needs to output a significant amount of data, LZO compression can also increase performance on the output side. Since writes are replicated 3x by default, each GB of output data you save will save 3GB of disk writes. In order to enable LZO compression, check out our recent guest blog from Twitter. Be sure to set mapred.compress.map.output to true. Benchmarks: Disabling LZO compression on the wordcount example increased the job runtime only slightly on our cluster. The FILE_BYTES_WRITTEN counter increased from 3.5GB to 9.2GB, showing that the compression yielded a 62% decrease in disk IO. Since this job was not sharing the cluster, and each node has a high ratio of number of disks to number of tasks, IO is not the bottleneck here, and thus the improvement was not substantial. On clusters where disks are pegged due to a lot of concurrent activity, a 60% reduction in IO can yield a substantial improvement in job completion speed. Tip 3) Tune the number of map and reduce tasks appropriately Diagnostics/symptoms: Each map or reduce task finishes in less than 30-40 seconds. A large job does not utilize all available slots in the cluster. After most mappers or reducers are scheduled, one or two remains pending and then runs all alone. Tuning the number of map and reduce tasks for a job is important and easy to overlook. Here are some rules of thumb I use to set these parameters: If each task takes less than 30-40 seconds, reduce the number of tasks. The task setup and scheduling overhead is a few seconds, so if tasks finish very quickly, you’re wasting time while not doing work. JVM reuse can also be enabled to solve this problem. If a job has more than 1TB of input, consider increasing the block size of the input dataset to 256M or even 512M so that the number of tasks will be smaller. You can change the block size of existing files with a command like hadoop distcp -Ddfs.block.size=$[256*1024*1024] /path/to/inputdata /path/to/inputdata-with-largeblocks. After this command completes, you can remove the original data. So long as each task runs for at least 30-40 seconds, increase the number of mapper tasks to some multiple of the number of mapper slots in the cluster. If you have 100 map slots in your cluster, try to avoid having a job with 101 mappers – the first 100 will finish at the same time, and then the 101st will have to run alone before the reducers can run. This is more important on small clusters and small jobs. Don’t schedule too many reduce tasks – for most jobs, we recommend a number of reduce tasks equal to or a bit less than the number of reduce slots in the cluster. Benchmarks: To make the wordcount job run with too many tasks, I ran it with the argument -Dmapred.max.split.size=$[16*1024*1024]. This yielded 2640 tasks instead of the 360 that the framework chose by default. When running with this setting, each task took about 9 seconds, and watching the Cluster Summary view on the JobTracker showed the number of running maps fluctuating between 0 and 24 continuously throughout the job. The entire job finished in 17m52s, more than twice as slow as the original job. Tip 4) Write a Combiner Diagnostics/symptoms: A job performs aggregation of some sort, and the Reduce input groups counter is significantly smaller than the Reduce input records counter. The job performs a large shuffle (e.g. map output bytes is multiple GB per node) The number of spilled records is many times larger than the number of map output records as seen in the Job counters. If your algorithm involves computing aggregates of any sort, chances are you can use a Combiner in order to perform some kind of initial aggregation before the data hits the reducer. The MapReduce framework runs combiners intelligently in order to reduce the amount of data that has to be written to disk and transfered over the network in between the Map and Reduce stages of computation. Benchmarks: I modified the word count example to remove the call to setCombinerClass, and otherwise left it the same. This changed the average map task run time from 33s to 48s, and increased the amount of shuffled data from 1GB to 1.4GB. The total job runtime increased from 8m30s to 15m42s, nearly a factor of two. Note that this benchmark was run with map output compression enabled – without map output compression, the effect of the combiner would have been even more important. Tip 5) Use the most appropriate and compact Writable type for your data Symptoms/diagnostics: Text objects are used for working with non-textual or complex data IntWritable or LongWritable objects are used when most output values tend to be significantly smaller than the maximum value. When users are new to programming in MapReduce, or are switching from Hadoop Streaming to Java MapReduce, they often use the Text writable type unnecessarily. Although Text can be convenient, converting numeric data to and from UTF8 strings is inefficient and can actually make up a significant portion of CPU time. Whenever dealing with non-textual data, consider using the binary Writables like IntWritable, FloatWritable, etc. In addition to avoiding the text parsing overhead, the binary Writable types will take up less space as intermediate data. Since disk IO and network transfer will become a bottleneck in large jobs, reducing the sheer number of bytes taken up by the intermediate data can provide a substantial performance gain. When dealing with integers, it can also sometimes be faster to use VIntWritable or VLongWritable — these implement variable-length integer encoding which saves space when serializing small integers. For example, the value 4 will be serialized in a single byte, whereas the value 10000 will be serialized in two. These variable length numbers can be very effective for data like counts, where you expect that the majority of records will have a small number that fits in one or two bytes. If the Writable types that ship with Hadoop don’t fit the bill, consider writing your own. It’s pretty simple, and will be significantly faster than parsing text. If you do so, make sure to provide a RawComparator — see the source code for the built in Writables for an example. Along the same vein, if your MapReduce job is part of a multistage workflow, use a binary format like SequenceFile for the intermediate steps, even if the last stage needs to output text. This will reduce the amount of data that needs to be materialized along the way. Benchmarks: For the example word count job, I modified the intermediate count values to be Text type rather than IntWritable. In the reducer, I used Integer.parseString(value.toString()) when accumulating the sum. The performance of the suboptimal version of the WordCount was about 10% slower than the original. The full job ran in a bit over 9 minutes, and each map task took 36 seconds instead of the original 33. Since integer parsing is itself rather fast, this did not represent a large improvement; in the general case, I have seen using more efficient Writables to make as much as a 2-3x difference in performance. Tip 6) Reuse Writables Symptoms/diagnostics: Add -verbose:gc -XX:+PrintGCDetails to mapred.child.java.opts. Then inspect the logs for some tasks. If garbage collection is frequent and represents a lot of time, you may be allocating unnecessary objects. grep for “new Text” or “new IntWritable” in your code base. If you find this in an inner loop, or inside the map or reduce functions this tip may help. This tip is especially helpful when your tasks are constrained in RAM. One of the first mistakes that many MapReduce users make is to allocate a new Writable object for every output from a mapper or reducer. For example, one might implement a word-count mapper like this: public void map(...) {
  ...
  for (String word : words) {
    output.collect(new Text(word), new IntWritable(1));
  }
} This implementation causes thousands of very short-lived objects to be allocated. While the Java garbage collector does a reasonable job at dealing with this, it is more efficient to write: class MyMapper ... {
  Text wordText = new Text();
  IntWritable one = new IntWritable(1);
  public void map(...) {
    ...
    for (String word : words) {
      wordText.set(word);
      output.collect(word, one);
    }
  }
} Benchmarks: When I modified the word count example as described above, I initially found it made no difference in the run time of the job. This is because this cluster’s default settings include a 1GB heap size for each task, so garbage collection never ran. However, running it with each task allocated only 200mb of heap size showed a drastic slowdown in the version that did not reuse Writables — the total job runtime increased from around 8m30s to over 17 minutes. The original version, which does reuse Writables, stayed the same speed even with the smaller heap. Since reusing Writables is an easy fix, I recommend always doing so – it may not bring you a gain for every job, but if you’re low on memory it can make a huge difference. Tip 7) Use “Poor Man’s Profiling” to see what your tasks are doing This is a trick I almost always use when first looking at the performance of a MapReduce job. Profiling purists will disagree and say that this won’t work, but you can’t argue with results! In order to do what I call “poor man’s profiling”, ssh into one of your slave nodes while some tasks from a slow job are running. Then simply run sudo killall -QUIT java 5-10 times in a row, each a few seconds apart. Don’t worry — this doesn’t cause anything to quit, despite the name. Then, use the JobTracker interface to navigate to the stdout logs for one of the tasks that’s running on this node, or look in /var/log/hadoop/userlogs/ for a stdout file of a task that is currently running. You’ll see stack trace output from each time you sent the SIGQUIT signal to the JVM. It takes a bit of experience to parse this output, but here’s the method I usually use: For each thread in the trace, quickly scan for the name of your Java package (e.g. com.mycompany.mrjobs). If you don’t see any lines in the trace that are part of your code, skip over this thread. When you find a stack trace that has some of your code in it, make a quick mental note what it’s doing. For example, “something NumberFormat-related” is all you need at this point. Don’t worry about specific line numbers yet. Go down to the next dump you took a few seconds later in the logs. Perform the same process here and make a note. After you’ve gone through 4-5 of the traces, you might notice that the same vague thing shows up in every one of them. If that thing is something that you expect to be fast, you probably found your culprit. If you take 10 traces, and 5 of them show NumberFormat in the dump, it means that you’re spending somewhere around 50% of your CPU time formatting numbers, and you might consider doing something differently. Sure, this method isn’t as scientific as using a real profiler on your tasks, but I’ve found that it’s a surefire way to notice any glaring CPU bottlenecks very quickly and with no setup involved. It’s also a technique that you’ll get better at with practice as you learn what a normal dump looks like and when something jumps out as odd. Here are a few performance mistakes I often find through this technique: NumberFormat is slow – avoid it where possible. String.split, as well as encoding or decoding UTF8 are slower than you think – see above tips about using the appropriate Writables Concatenating Strings rather than using StringBuffer.append These are just a few tips for improving MapReduce performance. If you have your own tips and tricks for profiling and optimizing MapReduce jobs, please leave a comment below! If you’d like to look at the code I used for running the benchmarks, I’ve put it online at http://github.com/toddlipcon/performance-blog-code/ Appendix: Benchmark Cluster Setup Each node in the cluster is a dual quad-core Nehalem box with hyperthreading enabled, 24G of RAM and 12x1TB disks. The TaskTrackers are configured with 6 map and 6 reduce slots, slightly lower than we normally recommend since we sometimes run multiple clusters at once on these boxes for testing.</snippet></document><document id="681"><title>Observers: Making ZooKeeper Scale Even Further</title><url>http://blog.cloudera.com/blog/2009/12/observers-making-zookeeper-scale-even-further/</url><snippet>As readers of our previous post on the subject will recall, ZooKeeper is a distributed coordination service suitable for implementing coordination primitives like locks and concurrent queues. One of ZooKeeper’s great strengths is its ability to operate at scale. Clusters of only five or seven machines can often serve the coordination needs of several large applications. We’ve recently added a major new feature to ZooKeeper to improve its scalability even further – a new type of server called Observers. In this blog post, I want to motivate the need for such a feature, and explain how it might help your deployments scale even better. Scalability means many things to many people – here I mean that a system is scalable if we can increase the workload the system can handle by assigning more resources to the system – a non-scalable system might see no performance improvement, or even a degradation as the workload increases. To understand why Observers have an effect on ZooKeeper’s scalability, we need to understand a little about how the service works. Broadly speaking, every operation on a ZooKeeper cluster is either a read or a write operation. ZooKeeper makes sure that all reads and all writes are observed by every client of the system in exactly the same order, so that there’s no confusion about which operation happened first. Along with this strong consistency guarantee, ZooKeeper also promises high availability, which can loosely be interpreted to mean that it can withstand a significant number of machine failures before the service stops being available to clients. ZooKeeper achieves this availability in a traditional way – by replicating the data that is being written and read amongst a small number of machines so that if one fails, there are others ready to take over without the client being any wiser. However, these two properties – consistency and availability – are hard to achieve together, as now ZooKeeper must make sure that every replica in its cluster agrees on the series of read and write operations. It does this by using a consensus protocol. Simplifying greatly, this protocol operates by having a designated Leader propose a new operation to all the other servers, all of whom vote and respond back to the Leader. Once the Leader has gathered more than half the votes outstanding from the other servers, it can deduce that the vote has passed, and sends a further message telling the servers to go ahead and commit the operation to their memory. This data flow is illustrated, from start to finish as the client sees it, in the diagram below. The client proposes a value to the server it is connected to. The server then relays that to the Leader, which initiates the consensus protocol, and once the original server has heard from the Leader it can relay its answer back to the client. Figure 1: Simplified Write Request Flow The need for Observers arises from the observation (no pun intended!) that ZooKeeper servers are playing two roles in this protocol. They accept connections and operation requests from clients, and also vote upon the result of these operations. These two responsibilities stand in opposition to each other when it comes to scaling ZooKeeper. If we wish to increase the number of clients attached to a ZooKeeper cluster (and we are often considering the case with 10000 or more clients), then we have to increase the number of servers available to support those clients. However, we can see from the description of the consensus protocol, that increasing the number of servers can place pressure on the performance of the voting part of the protocol. The Leader has to wait for at least half of the machines in the cluster to respond with a vote. The chance of one of these machines running slowly and holding up the entire vote process therefore gets bigger, and the performance of the voting step can decrease commensurately. This is something that we have seen in practice – as the size of the ZooKeeper cluster gets bigger, throughput of voting operations goes down. So there is a tension between our desire to scale the number of clients, and our desire to keep performance reasonable in terms of throughput. To decouple this tension, we introduced non-voting servers called Observers to the cluster. Observers can accept client connections, and will forward write requests to the Leader. However, the Leader knows not to ask the Observers to vote. Instead, Observers takes no part in the voting process, but instead are informed about the result of the vote in Step 3 along with all the other servers. This simple extension opens up new vistas of scalability to ZooKeeper. We may now add as many Observers as we like to the cluster without dramatically affecting write throughput. The scaling is not absolutely perfect – there is one step in the protocol (the ‘inform’ step) that is linear in the number of servers to inform, but the serial overhead of this step is extremely low. We would expect to hit other bottlenecks in the system before the cost of sending an inform packet to every server dominates the throughput performance of a ZooKeeper cluster. Figure 2: Observers Write Throughput Benchmark Figure 2 shows the results of one microbenchmark. The vertical axis measures the number of synchronous write operations per second that I was able to issue from a single client (a fully tuned ZooKeeper installation can significantly more operations per second – it’s the relative size of the bars we’re interested in here) . The horizontal axis denotes the size of the ZooKeeper cluster used. The blue bars are ZooKeeper clusters where every server is a voting server, where the green bars are ZooKeeper clusters where all but three servers are Observers. The chart shows that write performance stays approximately constant as we scale out the number of Observers, but falls off dramatically if we expand the size of the voting cluster. This is a win for Observers! Observers scale read performance too Scaling the number of clients is an important use case for Observers, but in fact there are significant other advantages to having them in your cluster. As an optimisation, ZooKeeper servers may serve read requests out of their local data stores, without going through the voting process. This puts read requests at a very slight risk of a ‘time-travel’ read, where an earlier value is read after a later value; but this only happens when a server fails. Indeed, in that case a client may issue a ‘sync’ request that ensures the next value it reads is the most up-to-date. Therefore Observers are a big performance improvement for read-heavy workloads. Writes go through the standard voting path, and so, by the same argument as for client scalability, increasing the number of voting servers in order to serve more reads will have a detrimental effect on write performance. Observers allow us to decouple read performance from write performance. This meshes well with many use cases for ZooKeeper, where most clients issues few writes but many reads. Observers enable WAN configurations There’s yet more that Observers can do for you. Observers are excellent candidates for connecting clients to ZooKeeper across wide-area networks. There are three main reasons for this. In order to get good read performance, it is necessary to have your clients relatively near to a server so that round-trip latencies aren’t too high. However, splitting a ZooKeeper cluster between two datacenters is a very problematic design, due to the fact that ZooKeeper works best when the voting servers are able to communicate with each other at low latency – otherwise we get the slowdown problem I described earlier. Observers can be placed in every datacenter that needs to access a ZooKeeper cluster. Therefore the voting protocol doesn’t take place across a high-latency intra-datacenter link, and performance is improved. Also, two fewer messages that are sent between Observers and the Leader during the voting process than between a voting server and the Leader. This can help ease bandwidth requirements on write-heavy workloads from remote datacenters. Finally, since Observers can fail without affecting the voting cluster itself, there’s no risk to the availability of the service if the link between datacenters is severed. This is much more likely than the loss of internal rack-to-rack connections, so it is beneficial not to rely on such a link. How to get started with Observers Observers are not yet part of a ZooKeeper release, so in order to start working with them you will have to download the source code from the Subversion trunk. The following is excerpted from the Observers user guide, found in docs/zooKeeperObservers.html in the source distribution. How to use Observers Note that until ZOOKEEPER-578 is resolved, you must set electionAlg=0 in every server configuration file. Otherwise an exception will be thrown when you try to start your ensemble. The reason: because Observers do not participate in leader elections, they rely on voting Followers to inform them of changes to the Leader. Currently, only the basic leader election algorithm starts a thread that responds to requests from Observers to identify the current Leader. Work is in progress on other JIRAs to bring this functionality to all leader election protocols. Setting up a ZooKeeper ensemble that uses Observers is very simple, and requires just two changes to your config files. Firstly, in the config file of every node that is to be an Observer, you must place this line: peerType=observer This line tells ZooKeeper that the server is to be an Observer. Secondly, in every server config file, you must add :observer to the server definition line of each Observer. For example: server.1:localhost:2181:3181:observer This tells every other server that server.1 is an Observer, and that they should not expect it to vote. This is all the configuration you need to do to add an Observer to your ZooKeeper cluster. Now you can connect to it as though it were an ordinary Follower. Try it out, by running: bin/zkCli.sh -server localhost:2181 where localhost:2181 is the hostname and port number of the Observer as specified in every config file. You should see a command line prompt through which you can issue commands like ls to query the ZooKeeper service. Future work There’s more to be done with the Observers feature. In the short term we are working on making Observers fully compatible with all leader election algorithms that ship with ZooKeeper – we expect this to be finished within the next few days. Longer term, we are hoping to investigate performance optimisations such as batching and off-line reads for Observer-based clusters, to take advantage of the fact that Observers have no strict latency requirement to meet unlike a normal ZooKeeper server. We hope that Observers will make it into the release of ZooKeeper 3.3.0, due early next year. We would be delighted to hear your feedback, either on the mailing lists, or via direct e-mail. ZooKeeper is always looking for contributors, and we’ve got plenty of interesting problems to solve, so do get in contact if you’d like to get involved and I’d be happy to help you get started.</snippet></document><document id="682"><title>Hadoop World: Sqoop – Database Import for Hadoop</title><url>http://blog.cloudera.com/blog/2009/12/hadoop-world-sqoop-database-import-for-hadoop/</url><snippet>At Cloudera, we’re always working to make it easier for you to work with Hadoop and integrate Hadoop-based systems in with your existing data sources. One example of how we accomplish this is Sqoop, a database import tool developed at Cloudera that allows you to easily copy data between databases and HDFS. We originally announced this tool in June, but we’ve been steadily improving it since then. It can now talk with several more databases than before, and performance has been improved considerably. Sqoop has demonstrated its usefulness pretty quickly; several open source projects and many of our clients use Sqoop as part of their data pipeline. Last summer our friend Pete Skomoroch demonstrated how to integrate it into his Wikipedia Trending Topics project (blog tutorial ). This talk at Hadoop World NYC by Cloudera engineer Aaron Kimball introduces Sqoop, describes its use cases, and gives some technical details of how it works.</snippet></document><document id="683"><title>Hadoop World: Security and API Compatibility</title><url>http://blog.cloudera.com/blog/2009/12/hadoop-world-security-and-api-compatibility/</url><snippet>Today’s Hadoop World talk comes from Owen O’Malley and talks about some of the biggest challenges facing Hadoop: Security and API Compatibility. Over the past several months, Yahoo! has been leading the charge in both areas. This work will enable wider use of Hadoop within Yahoo! as well as lower the barrier for new users – particularly those working with sensitive data. A big thanks to Yahoo! and everyone else in the community helping out.</snippet></document><document id="684"><title>Hadoop World: Hadoop for Bioinformatics</title><url>http://blog.cloudera.com/blog/2009/12/hadoop-world-hadoop-for-bioinformatics/</url><snippet>Today’s Hadoop World talk comes from Deepak Singh of Amazon Web Services. Prior to AWS, Deepak spent his entire career in the bioinformatics community. This talk goes into many of the challenges facing this community and shows how Hadoop and Elastic Computing change the game. Thanks Deepak!</snippet></document><document id="685"><title>Hadoop World: Practical HBase from Jonathan Gray and Ryan Rawson</title><url>http://blog.cloudera.com/blog/2009/11/hadoop-world-practical-hbase-from-jonathan-gray-and-ryan-rawson/</url><snippet>Today’s Hadoop World talk comes from Jonathan Gray at Streamy and Ryan Rawson at StumbleUpon. Jonathan and Ryan go into detail about how you can get the most from your HBase installation. Jonathan, Ryan, thank you! And watch this space –  have plenty more of the talks from Hadoop World coming your way!</snippet></document><document id="686"><title>Hadoop World: Hadoop + Vertica from Omer Trajman</title><url>http://blog.cloudera.com/blog/2009/11/hadoop-world-hadoop-vertica-from-omer-trajman/</url><snippet>Today’s Hadoop World talk comes from Omer Trajman at Vertica. He goes into detail about how Hadoop complements Vertica’s column-oriented analytic database. Our thanks to Omer — and stay tuned for more!</snippet></document><document id="687"><title>Hadoop World: Hadoop + Clojure from Stuart Sierra and Tim Dysinger</title><url>http://blog.cloudera.com/blog/2009/11/hadoop-world-hadoop-clojure-from-stuart-sierra-and-tim-dysinger/</url><snippet>Today’s Hadoop World talk comes from Stuart Sierra at Columbia University and Tim Dysinger at Sonian Networks. Unfortunately Tim couldn’t make the talk, so Stuart covered for him. Stuart goes into detail about how Hadoop can be used with Clojure, a neat and new language implemented on the Java Virtual Machine (JVM). Tim, Stuart, thank you! We’ll be posting more of the Hadoop World talks in the weeks to come.</snippet></document><document id="688"><title>Hadoop World: Protein Alignment from Paul Brown</title><url>http://blog.cloudera.com/blog/2009/11/hadoop-world-protein-alignment-from-paul-brown/</url><snippet>Today’s Hadoop World talk comes from Paul Brown at Booz Allen. He goes into detail about how Hadoop is used for bioinformatics and protein alignment. Our thanks to Paul — and stay tuned for more!</snippet></document><document id="689"><title>Hadoop at Twitter (part 1): Splittable LZO Compression</title><url>http://blog.cloudera.com/blog/2009/11/hadoop-at-twitter-part-1-splittable-lzo-compression/</url><snippet>This summer I sent the following tweet, “Had lunch today at Twitter HQ. Thanks for the invite, @kevinweil! Great lunch conversation. Smart, friendly and fun team.” Kevin Weil leads the analytics team at Twitter and is an active member of the Hadoop community, and his colleague Eric Maland leads Operations.  Needless to say, Twitter is doing amazing things with Hadoop.  This guest blog from Kevin and Eric covers one of Twitter’s open-source projects which provides a solution for splittable LZO for Hadoop. – Matt At Twitter we are significantly ramping up usage of Hadoop to help us analyze the massive amounts of data that our platform generates each day.  We are happy users of Cloudera’s free distribution of Hadoop; we’re currently running Hadoop 0.20.1 with Pig 0.4.  In this first of a small series of posts about our architecture and the open source software we’re working on around it, we’d like to focus on an infrastructure-level solution we use to make our cluster more efficient: splittable LZO for Hadoop.  Using LZO compression in Hadoop allows for reduced data size and shorter disk read times, and LZO’s block-based structure allows it to be split into chunks for parallel processing in Hadoop.  Taken together, these characteristics make LZO an excellent compression format to use in your cluster. Splittable LZO and Hadoop The original splittable LZO work was done by Johan Oskarsson, formerly of Last.fm and soon to be joining us at Twitter (welcome, Johan!).  Chris Goffinet also wrote a blog post on the topic for Cloudera a while back, but many people we’ve talked to don’t understand how the technique works, or why it’s effective.  It’s worth mentioning that there are three different versions of the LZO code for Hadoop floating around: the original Google Code project, Chris Goffinet’s version, and our version.  Chris’s version is a backport of the Google Code project to Hadoop 0.18.3 with a couple of fixes.  Our version works with Hadoop 0.20.1, has Chris’s fixes, and has other fixes as well from using the code at scale.  Our repository also has InputFormats for both the new and the old MapReduce API, since you generally want to write jobs against the new API, but Hadoop streaming still requires the old API.  Finally, we have refactored some of the core code to separate static inner classes into their own objects for better reuse.  This code is currently running 24 hours a day at Twitter, and we look forward to working with Owen O’Malley and Arun Murthy to get it integrated back into the main Google Code project soon. Let’s back up one step and talk briefly about compression and Hadoop.  Storing compressed data in HDFS allows your hardware allocation to go further since compressed data is often 25% of the size of the original data.  Furthermore, since MapReduce jobs are nearly always IO-bound, storing compressed data means there is less overall IO to do, meaning jobs run faster.  There are two caveats to this, however: some compression formats cannot be split for parallel processing, and others are slow enough at decompression that jobs become CPU-bound, eliminating your gains on IO.  The gzip compression format illustrates the first caveat, and to understand why we need to go back to how Hadoop’s input splits work.  Imagine you have a 1.1 GB gzip file, and your cluster has a 128 MB block size.  This file will be split into 9 chunks of size approximately 128 MB.  In order to process these in parallel in a MapReduce job, a different mapper will be responsible for each chunk. But this means that the second mapper will start on an arbitrary byte about 128MB into the file.  The contextful dictionary that gzip uses to decompress input will be empty at this point, which means the gzip decompressor will not be able to correctly interpret the bytes.  The upshot is that large gzip files in Hadoop need to be processed by a single mapper, which defeats the purpose of parallelism.  For an example of the second caveat in which jobs become CPU-bound, we can look to the bzip2 compression format.  Bzip2 files compress well and are even splittable, but the decompression algorithm is slow and cannot keep up with the streaming disk reads that are common in Hadoop jobs.  While Bzip2 compression has some upside because it conserves storage space, running jobs now spend their time waiting on the CPU to finish decompressing data, which slows them down and offsets the other gains. So is there something that balances these two extremes and solves both problems at once?  As you’ve probably guessed, the answer is LZO.  The LZO compression format is composed of many smaller (~256K) blocks of compressed data, allowing jobs to be split along block boundaries.  Moreover, it was designed with speed in mind: it decompresses about twice as fast as gzip, meaning it’s fast enough to keep up with hard drive read speeds.  It doesn’t compress quite as well as gzip — expect files that are on the order of 50% larger than their gzipped version.  But that is still 20-50% of the size of the files without any compression at all, which means that IO-bound jobs complete the map phase about four times faster.  Here’s a typical example, starting with an 8.0 GB file containing some text-based log data: Compression File Size (GB) Compression Time (s) Decompression Time (s) None some_logs 8.0 - - Gzip some_logs.gz 1.3 241 72 LZO some_logs.lzo 2.0 55 35 As you can see, the LZO file is slightly larger than the corresponding gzip file, but both are much smaller than the original uncompressed file.  Additionally, the LZO file compressed nearly five times faster, and decompressed over two times faster. We mentioned above that LZO files can be split as long as the splits occur on block boundaries, so how can we ensure that this occurs?  If we go back to our hypothetical 1.1 GB file, assuming now that it’s LZO compressed instead, the mapper that gets the second 128 MB chunk needs to be able to identify the beginning of the next LZO block boundary to start decompressing.  LZO does not not write any magic bytes in its block header, so a priori there is no way to identify the next header.  The solution is to perform a one-time indexing of the LZO file, writing a foo.lzo.index file for each foo.lzo file.  The index file simply contains the list of byte offsets for each block, and it’s fast to write because the indexing process is mostly seeking and thus moves at hard drive read speed.  Typically we see 90-100 MB/s indexing speed, i.e. 10-12 seconds per GB.  Once the index file has been created, any LZO-based input format can split compressed data by first loading the index, and then nudging the default input splits forward to the next block boundaries.  With these nudged splits, each mapper gets an input split that is aligned to block boundaries, meaning it can more or less just wrap its InputStream in an LzopInputStream and be done.  If you currently have a job that relies on TextInputFormat, for example, you can LZO-compress your data, make sure it’s indexed, rename TextInputFormat to LzoTextInputFormat, and your job will run just like before, only likely faster.  Incidentally, it’s worth noting that unindexed LZO files will still work with MapReduce jobs, they just won’t be splittable. Setting up LZO with Hadoop The steps to setting up LZO with Hadoop are simple. Get the lzop native libraries. Installing on Mac sudo port install lzop lzo2 Installing on Redhat based systems sudo yum install liblzo-devel Installing on Debian based systems sudo apt-get install liblzo2-dev Clone our github repo and build it according to the instructions in the README. Place the hadoop-lzo-*.jar somewhere on your cluster nodes; we use /usr/local/hadoop/lib Place the native hadoop-lzo binaries (which are JNI-based and used to interface with the lzo library directly) on your cluster as well; we use /usr/local/hadoop/lib/native/&lt;arch&gt;/ Add the following to your core-site.xml: &lt;property&gt;
&lt;name&gt;io.compression.codecs&lt;/name&gt;
&lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;
&lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;
&lt;/property&gt; Add the following property to mapred-site.xml.  We should note that the Cloudera guys backported the patch implementing this into their distribution for us to make configuration easier (thanks!). &lt;property&gt;
  &lt;name&gt;mapred.child.env&lt;/name&gt;
  &lt;value&gt;JAVA_LIBRARY_PATH=/path/to/your/native/hadoop-lzo/libs&lt;/value&gt;
&lt;/property&gt; If you would like to use LZO to compress map outputs as well, add the following to your mapred-site.xml.  This setting handles how and if the map output data, which must get sent over the network and then written to disk on the node running the reduce, is compressed.  Since this is an IO-heavy operation, it is another area where LZO compression can make your job significantly faster. &lt;property&gt;
  &lt;name&gt;mapred.map.output.compression.codec&lt;/name&gt;
  &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;
&lt;/property&gt; That’s it!  Now upload a bunch of LZO files to your cluster; imagine you put them in /lzo_logs.  Make sure that the hadoop jars are in your classpath (we export the directory, /usr/local/hadoop/lib, as part of HADOOP_CLASSPATH in hadoop-env.sh) and take any lzo file on your cluster.  Run the following command to index your log files: 
$ hadoop jar /path/to/hadoop-lzo.jar com.hadoop.compression.lzo.LzoIndexer /lzo_logs
 Take any job you used to have running over those files, say a WordCount, and run it just like before but using the hadoop-lzo LzoTextInputFormat in place of the TextInputFormat.  Voila, splittable LZO! All of our raw data at Twitter is stored in an LZO-compressed, splittable format.  Since MapReduce jobs are generally IO-bound, storing data in compressed formats yields significantly faster computations.  With splittable LZO, you can achieve the full parallelism offered by the MapReduce architecture in addition to saving on disk space.  We hope the explanation and code help you to test out splittable LZO compression in your own Hadoop cluster! In a followup to this post, we’ll examine the structure of Hadoop InputFormats and Pig LoadFuncs for use with LZO-compressed data. For more about the kinds of analyses we do with Hadoop and Pig at Twitter, please see the slides from my recent talk at NoSQL East.  If this sounds like interesting work, Twitter is hiring engineers in Analytics, Infrastructure, Frontend, App Services, and Operations, and we’d love to talk to you.</snippet></document><document id="690"><title>Hadoop World: Rethinking the Data Warehouse with Hadoop and Hive from Ashish Thusoo</title><url>http://blog.cloudera.com/blog/2009/11/hadoop-world-rethinking-the-data-warehouse-with-hadoop-and-hive-from-ashish-thusoo/</url><snippet>Today’s Hadoop World talk comes from Ashish Thusoo at Facebook and goes into detail about how Facebook uses Hadoop and Hive to expose massive volumes of data to their internal users familiar with traditional data warehousing tools. Thanks Ashish, and stay tuned for more!</snippet></document><document id="691"><title>Hadoop World: Monitoring Best Practices from Ed Capriolo</title><url>http://blog.cloudera.com/blog/2009/11/hadoop-world-monitoring-best-practices-from-ed-capriolo/</url><snippet>Today’s Hadoop World video comes from Ed Capriolo, and goes into details about how to effectively monitor Hadoop in production environments. Thanks Ed, and stay tuned for more!</snippet></document><document id="692"><title>Apache Avro: a New Format for Data Interchange</title><url>http://blog.cloudera.com/blog/2009/11/avro-a-new-format-for-data-interchange/</url><snippet>Apache Avro is a recent addition to Apache’s Hadoop family of projects.� Avro defines a�data format designed to support data-intensive applications, and provides support for this format in a variety of programming languages. Background We’d like data-driven applications to be dynamic: folks should be able to rapidly combine datasets from different sources.� We want to facilitate novel, innovative exploration of data.� Someone should, for example, ideally be able to easily correlate point-of-sale transactions, web site visits, and externally provided demographic data, without a lot of preparatory work.� This should be possible on-the-fly, using scripting and interactive tools. Current data formats often don’t work well for this.� XML and JSON are expressive, but they’re big, and slow to process.� When you’re processing petabytes of data, size and speed matter a lot. Google uses a system called Protocol Buffers to address this.� (There are other systems, like Thrift, similar to Protocol Buffers, that I won’t explicitly discuss here, but to which my comments about Protocol Buffers also apply.)� Google has made Protocol Buffers freely available, but it’s not ideal for our purposes. Generic Data With Protocol Buffers, one defines data structures, then generates code that can efficiently read and write them.� However, if one wishes, from a scripting language, to quickly implement an experiment with Protocol Buffer data, one must first: locate the data structure definition; generate code for it; and, finally, load that code before one can touch the data.� That might not be that bad, but if one wanted to have, for example, a generic tool that could browse any dataset, it would have to first locate definitions, then generate and load code for each such dataset.� This complicates something that should be simple. Avro’s format instead always stores data structure definitions with the data, in an easy-to-process form.� Avro implementations can then use these definitions at runtime to present data to applications in a generic way, rather than requiring code generation. Code generation in Avro is optional: it’s nice in some programming languages to sometimes use specific data structures, that correspond to frequently serialized data types.� But, in scripting systems like Hive and Pig, code generation would be an imposition, so Avro does not require it. An additional advantage of storing the full data structure definition with the data is that it permits the data to be written faster and more compactly.� Protocol Buffers add annotations to data, so data may still be processed even if the definition doesn’t exactly match the data.� However these annotations make the data slightly larger and slower to process.� Benchmarks have shown that Avro data, which does not need such annotations, is smaller and faster to process than that of other serialization systems. Avro Schemas Avro uses JSON to define a data structure’s schema.� For example, a two-dimensional point might be defined as an Avro record: {"type": "record", "name": "Point", �"fields": [ ��{"name": "x", "type": "int"}, ��{"name": "y", "type": "int"}, �] } Each instance of this is serialized as simply two integers, with no additional per-record or per-field annotations.� Integers are written using a variable-lengthed zig-zag encoding.� So points with small positive and negative values can be written in as few as two bytes: 100 points might require just 200 bytes. In addition to records and numeric types, Avro includes support for arrays, maps, enums, variable and fixed-length binary data and strings.� It also defines a container file format intended to provide good support for MapReduce and other analytical frameworks.� For details, see the Avro specification. Compatibility Applications evolve, and as they evolve their data structures can change.� We’d like new versions of an application to still be able to process data created by old versions, and vice versa.� Avro handles this in much the same way as Protocol Buffers.� When an application expects fields that are not present, Avro provides a default value, specified in the schema.� Avro ignores unexpected values that are present in data.� This doesn’t handle all back-compatibility issues, but it makes most common ones easy to handle. RPC Avro also lets one define Remote Procedure Call (RPC) protocols. While data types used in RPC are usually distinct from those in datasets, using a common serialization system is still useful.� Data-intensive applications require distributed RPC-based frameworks.� So, everywhere that we need to be able to process dataset files we also need to be able to use RPC.� Thus building these on a common base minimizes the chance that one would, e.g., be able to write code that will process the data, but unable to use a distributed framework to do so. Integration with Hadoop We’d like it to be easy to use Avro data in Hadoop’s MapReduce.� This is still a work in progress.� The issues MAPREDUCE-1126 and MAPREDUCE-815 track this. Note that Avro data structures can specify their sort order, so complex data created in one programming language can be sorted by another.� Sorting is also possible without deserialization, and is thus quite fast. We hope that Avro will replace Hadoop’s existing RPC.� Hadoop currently requires its clients and servers to run the exact same version of Hadoop.� We hope to use Avro to permit one to, e.g., have a single Hadoop application that can talk to multiple clusters running different versions of HDFS and/or MapReduce. Finally, we hope that Avro will permit Hadoop applications to be more easily written in languages besides Java.� For example, once Hadoop’s built on Avro, we hope to support native MapReduce and HDFS clients in languages like Python, C and C++. Talk from Hadoop World Many of these topics were covered during my recent talk at Hadoop World, and we’re happy to release that video along with this blog post.</snippet></document><document id="693"><title>Hadoop World: NYC – Let the Videos Roll</title><url>http://blog.cloudera.com/blog/2009/10/hadoop-world-nyc-let-the-videos-roll/</url><snippet>It has been almost a month since Hadoop World: NYC, and things are just starting to get back to normal here at Cloudera HQ. We were thrilled to see over 500 Apache Hadoop enthusiasts descend upon New York City for the first major Hadoop event on the East Coast. The variety of applications, and the number of companies involved, were mind-boggling. For those of you who weren’t able to join us, we hope to see you at another event soon! We have managed to get slides from all of the presentations on the conference website. Over the next few weeks, we’ll be releasing videos from talks across all three tracks: Applications, Development and Administration, and Extensions. This week, we have the opening address and the introduction of Cloudera Desktop. That leaves 34 more talks! To stay up to date, subscribe to our blog and follow us on twitter.</snippet></document><document id="694"><title>Apache Hadoop Get-Together in Berlin – Videos Online</title><url>http://blog.cloudera.com/blog/2009/10/apache-hadoop-get-together-in-berlin-videos-online/</url><snippet>Around the world, individuals contribute to Hadoop and build community around the technology. This kind of collaboration is at the heart of open source software, and here at Cloudera, we feel privileged to be a part of the Apache Hadoop community. Getting together in person is a great way to build community. On global projects, though, sharing information from those gatherings with people who are far way is a big challenge. Recently, Isabel Drost from Berlin approached us about helping out with their local get-together, and we were more than happy to pitch in to sponsor the video production, and to help get the word out on our blog. If you run a local Hadoop meetup and would like to share your event with the world-wide community, please let us know! Following is Isabel’s write up of the event, along with the videos. – Christophe Recently, the Apache Hadoop Get-Together took place in Berlin Mitte. For the sixth time, developers interested in large-scale information processing met in newthinking store: Over the course of one year the event grew from a small, spontaneous gathering of Hadoop enthusiasts to a meetup of 40 people from Germany, France and Denmark. Developers from various companies like Nokia Gate5, StudiVZ and others participated, as well as several freelancers specialized in providing support for Hadoop and Lucene. Students and researchers from local universities also joined the event. The meetup was kindly hosted by newthinking store, an event management / IT services company which provides rooms for free software meetings at no cost. The Get-Together was sponsored by Cloudera (video recordings) and O’Reilly (books). Thanks to all three of you. The Get-Together started late afternoon at 5p.m. with a talk on solving puzzles with MapReduce by Thorsten Schütt. After that Thilo Götz gave an introduction to JAQL. Finally Uwe Schindler explained the improvements that come with Lucene 2.9. After the official part we moved over to a bar close by for some food, drinks and (non-free) beer. A brief summary of each talk can be found below. The slides of the talks have been put online already. Videos will be available early next week. Notifications of future meetups and related events in Germany are announced on a public mailing list. Feel free to subscribe to stay up to date on Hadoop Berlin meetups. Solving puzzles with Map Reduce Thorsten Schütt gave a presentation on solving sliding puzzles with a MapReduce implemention. Thorsten is a researcher at Zuse Institute Berlin. Zuse Institute has been using high performance compute clusters working on scalable algorithms for decades. The talk did not focus on Hadoop in particular, but on applying MapReduce (the paradigm) to solve sliding puzzles. For HPC clusters Hadoop is not the best choice to implement distributed algorithms – developing for these clusters rather involves writing software in Fortran or C/C++. MPI provides the parallelization framework for distributed programs. In his presentation Thorsten explained the way he had implemented and optimized a breadth-first search algorithm to efficiently solve a 4×4 sliding puzzle in a reasonable amount of time. If you are interested in all the details, proofs and concepts have a look at his HPCS paper “Out-of-Core Parallel Heuristic Search with MapReduce.” I met Thorsten at the “Lange Nacht der Wissenschaften”, a recurring event where Berlin’s Universities open up for one night and present their fields of study to everybody. On these evenings, you can join presentations, take guided tours, and meet researchers. If you would like to take a look at the ZIB datacenters yourself, you might want to join this special night next year in summer and take the tour through the ZIB basement. An introduction to JAQL Thilo Götz introduced JAQL. JAQL is a higher level query language for JSON documents. It was developed at IBM’s Almaden research center. JAQL supports several operations generally known from SQL. It has support for grouping results, joining arrays on a common attribute, sorting and expansion. It also has built-in support for loops, conditionals and recursion. The language can be easily extended by custom Java methods. And the great thing about JAQL: The resulting scripts can be compiled to Hadoop MapReduce jobs. That way developers do not need to know all the gory details of Hadoop MapReduce, but can still get at those if the need arises. JAQL supports various I/O options: JSON data can be read from local disk, HDFS and HBase tables. If that does not fit your needs, there are easy interfaces to implement your own I/O adapters. Lucene 2.9 developments The last talk was given by Uwe Schindler. He gave an overview of the optimizations and new features that come with the recently released Lucene 2.9. Lucene 2.9 comes with a highly optimized implementation of range queries and filters. Uwe gave a demonstration of its performance by doing a range search in the geographical search engine Pangaea. A second large improvement is per-segment searching. Lucene indexes are split into segments that are written incrementally and merged during optimization. The index searcher now works directly on segments, and results are merged by collectors. As a result, FieldCaches to also work on segments as well. That way only the caches for changed segments need to be invalidated and recreated. Lucene now comes with near real-time search that permits low latency between indexing documents and that is able to retrieve documents through searches. The refactored TokenStream API supports adding attributes to terms. That way arbitrary information can be added to tokens during indexing time. A use case for this feature is adding POS tags to tokens that can be used at later analysis steps. Summary The general feedback from attendees was very positive: Getting developers and current and future users together at an informal meetup clearly fosters exchanging experience and ideas. Judging from the presentations and discussions at the Get-Together, people are starting to use Hadoop for a variety of processing and data mining tasks. The next Get-Together is scheduled to take place on December 16th. The date was set by the first presenter at the December meetup, Jörg Möllenkamp from Sun. Tuesday late evening, nurago from Hannover offered to submit a talk on their experiences with Hadoop. In addition StudiVZ offered to support the event by sponsoring video production. If you would like to submit a talk yourself or sponsor free beer for all attendees, please contact me at isabel@apache.org If you just cannot wait until December, the first NoSQL Meetup in Germany is scheduled for mid-October and will be hosted by newthinking store. For those of you who need an excuse to travel to California, Apache Con US features trainings, meetups and a lot of presentations on Lucene, Solr and Hadoop. Looking forward to seeing you in Oakland!</snippet></document><document id="695"><title>Cloudera Desktop and MooTools</title><url>http://blog.cloudera.com/blog/2009/10/cloudera-desktop-and-mootools/</url><snippet>At Hadoop World NYC Cloudera announced a new product: Cloudera Desktop. Over the past several months this product has been my principal concern here at Cloudera where I’m the UI lead (actually, until about a week ago, I was the only UI developer). If you aren’t familiar with Cloudera Desktop, you should check out this brief screencast: Cloudera Desktop Screencast from Cloudera on Vimeo. Making Desktop has been a fun, though difficult, task that involved a lot of trial and error (and in many ways we’re just getting started). In the screencast above you can see roughly half a dozen applications running on Desktop. Each of these has code conventions unique to it, and shared with the other applications. Those conventions evolved as we went from application to application, refining and changing direction, building and using software and learning lessons. MooTools Cloudera Desktop uses many technologies including Hadoop (obviously), Thrift, Python, Django, and others. On the client side, though, which is where I spend most of my time developing, we use HTML, CSS, and JavaScript (lots of JavaScript) — specifically, the MooTools JavaScript framework. MooTools is a robust framework that extends well past the basics of DOM manipulation and provides numerous low level utilities and extensions to the JavaScript language (enhancing the native prototypes of Function, Array, etc), as well as exposing JavaScript’s native inheritance model with psuedo-classes. Design Principles When we began working on this project to provide tools for Hadoop users, our goal was to make life easier for administrators, developers, and analysts. Hadoop’s principal interface to date has been the command line and a few basic web pages that allow you to see the output and status of jobs. Other projects have started work on making the interface easier. For instance, Yahoo! and Facebook have many non-programmers who use Hadoop to do analysis by way of custom, internal applications. Part of our consideration was that we didn’t want users to have to choose. Ideally, if there’s an awesome tool that help users accomplish a task out there, it should play well with whatever we develop. The File Browser This led us to consider a Desktop model with numerous windows, with the idea that other developers could expose their applications within our framework and make use of common tools. For instance, a familiar-looking file browser ships with Cloudera Desktop. Other applications can invoke this file browser to allow the user to select a file as input, or a directory as output, or even upload new files easily and quickly. This kind of integration meant making a multitasking environment on which third parties could develop and this lead us to the multi-windowed desktop metaphor. MooTools Contributions For the past several months as we’ve been building Cloudera Desktop, we’ve been busy also making a lot of contributions back to MooTools and we wanted to share them with you here. Some of these things are simple UI conventions while others are interesting for their technique. MooTools Depender MooTools ships with a dependency map that powers its download builder. The modular nature of the library yields itself to custom builds, putting together a library specific to the task at hand. This allows MooTools to power, for instance, a mobile application with only a small amount of JavaScript. For the Cloudera Desktop, we knew we were going to end up with a LOT of JavaScript, and loading it all on startup didn’t make much sense. Instead, we authored the Depender application. It’s an easy-to-deploy, real-time library builder and dependency mapper. This allows our application to load with a minimum of JavaScript. When users launch specific applications, Depender loads any dependencies for that app that aren’t loaded already, and then display the application. In addition to the server side component (available in both PHP and Python/Django), there are two client side components: a stand alone version to be released in MooTools 1.2.4 and a server side application that ships with a client that talks to the server for you, which lets you do this slickness: Depender.require({
    scripts: ['DatePicker', 'Logger'], //array or single string for one item
    callback: function() {
        //your code that needs DatePicker and Logger
    }
});
//later, you need to load more dependencies...
Depender.require({
    scripts: 'Fx.Reveal', //array or single string for one item
    callback: function(){
        //if, for some reason, Fx.Reveal is available already,
        //then this function will exeute immediately, otherwise it will
        //wait for the requirements to load
        $('someElement').reveal();
    }
});
 The Depender app is now part of the MooTools project and will be powering MooTools.net as well as the forthcoming user-created plugin repository’s dependency builder. MooTools ART ART Windows MooTools at the moment doesn’t have an official, public UI system, but that’s changing, and in no small part due to our contributions to the MooTools ART project. MooTools ART is an in-development UI library that currently outputs canvas. It’s an abstraction of the canvas API and it allows developers to make style-able UI elements like buttons, windows, and icons. At the moment it only outputs to canvas (limiting its support to browsers other than Internet Explorer), but we’re working on wrappers for VML and SVG. In addition to these drawing tools provided by the ART API is a widget-based system that has numerous features including keyboard management, event bubbling, custom styling, and more. This widget system is the foundation for many of our UI elements, though not all of them. While the basic ART API was developed by the core MooTools Team (of which I am a part), we’ve contributed most of the widgets available in the library built with that API, including a window manager, a history interface, pop-ups for alert, confirm, and prompt, split views and more. MooTools More The MooTools Core is the basic distribution of the framework. It is minimal and changes infrequently. In addition to this library are official MooTools plugins. Version 1.2.4, soon to be released, has numerous new features and bug fixes, many contributed to by Cloudera, including: Depender (the stand alone version of the afore mentioned dependency loader) Mask – masks elements (including the window) with a semi-opaque overlay Spinner – automates the creation of ajax spinners over DOM elements being updated Form.Request – automates creating ajax forms that update HTML in DOM elements Keyboard – a robust event manager for keyboard groupings In addition, there were numerous bug patches and other smaller features. It’s important to note that the MooTools developer community helped in the development and integration of these into the official MooTools code base. We contributed much of the code in these features, but they were tested, refined, and optimized in the spirit of open source collaboration. Third Party Plugin Contributions In addition to these contributions to the MooTools projects we also made a few other smaller contributions. We contributed to the SubtleTemplate data binding plugin by Thomas Aylott with optimizations and small bug fixes as well as a more robust manager for binding a single data object to numerous template instances. We also contributed to the Clientcide plugins (which isn’t saying much as these are my personal plugins), mostly with bug fixes and minor features. Future Open Source Plans Cloudera remains committed to open source and as such you should expect to see even more output to the MooTools community and JavaScript in general. We recently added another UI developer, Nathan White, to our ranks, which essentially doubles our capacity to crank out useful stuff for others. We hope to release more things from our Desktop environment for others, where it makes sense, and give back to and tap into the thriving MooTools Community. We have numerous little plugins that we want to share but haven’t had time to release. In the mean time, we look forward to continuing to support the things we have released and doing what we can to help the technologies we utilize prosper. If you’d like to try Cloudera Desktop, you can download it for free at http://cloudera.com/desktop. Other resources include: The google group for Desktop users (note: Desktop was rename to “Hue – Hadoop User Experience” in June of 2010) The google group for Desktop developers Get Satisfaction for Desktop (for support and feedback about using Desktop)</snippet></document><document id="696"><title>Analyzing Human Genomes with Apache Hadoop</title><url>http://blog.cloudera.com/blog/2009/10/analyzing-human-genomes-with-hadoop/</url><snippet>Every day, we hear about people doing amazing things with Apache Hadoop. The variety of applications across industries is clear evidence that Hadoop is radically changing the way data is processed at scale. To drive that point home, we’re excited to host a guest blog post from the University of Maryland’s Michael Schatz. Michael and his team have built a system using Hadoop that drives the cost of analyzing a human genome below $100 — and there’s more to come! Like Michael, we’re excited about the power that Hadoop offers biotech researchers. Thanks, Michael! -Christophe   Ben Langmead and I are very pleased to announce the release of Crossbow, an open-source, Hadoop-enabled pipeline for quickly, accurately, and cheaply analyzing human genomes in the clouds. DNA sequencing has improved tremendously since the completion of the human genome project in 2003, and it is now possible to sequence a genome in a few days for about 50 thousand dollars. The more-than-one-thousand-fold improvement in throughput and cost is spurring a new era of biomedical research, where genomes from many individuals are sequenced and studied over the course of one project. Human genomes are about 99.9% identical, explaining our overall similarity, but discovering differences between genomes is the key to understanding many diseases, including how to treat them. While sequencing has undoubtedly become an important and ubiquitous tool, the rapid improvements in sequencing technology have created a “firehose” problem of how to store and analyze the huge volume DNA sequence data being generated. The human genome is about 3 billion DNA nucleotides (characters), about the same as the English portion of the Wikipedia. Storing or searching one genome by itself is not too difficult, and standard tools are quite efficient for searching it on a single computer. However, because of the limitations of DNA sequencing technology, we cannot simply read an entire genome end-to-end. Instead the machine reports a very large number of tiny fragments called reads, each 25-500 letters long, collected from random locations in the genome. Then, much like how raindrops will eventually cover the whole sidewalk, we can sequence an entire genome by sequencing many billions of reads, with 20-fold to 30-fold oversampling to ensure each nucleotide is seen. Presently, this process generates about 100GB of compressed data (read sequences and associated quality scores) for one human genome. Once collected, we can map the billions of reads to the reference human genome using sequence alignment algorithms, and then scan the alignments to find differences between the newly sequenced genome and the reference genome. Again, the problem of mapping and scanning 100GB of data isn’t too onerous, especially for large sequencing centers with large compute grids, and recent studies of individual sequenced genomes have been able to do the analysis in about 1000 CPU hours of computation. The “problem” is sequencing technology is continuing to improve, and pretty soon a single sequencing machine will generate 100GB of data in a few hours. If our computational methods aren’t as efficient as our sequencing methods, we’ll only get further and further behind as more and more data arrives. Clearly we need very efficient and scalable methods if we hope to keep up, especially as sequencing moves from large sequencing centers, to smaller research centers, and perhaps eventually to hospitals and clinical labs. This is exactly the problem Crossbow aims to solve. Crossbow combines one of the fastest sequence alignment algorithms, Bowtie, with a very accurate genotyping algorithm, SoapSNP, within Hadoop to distribute and accelerate the computation. The pipeline can accurately analyze an entire genome in one day on a 10-node local cluster, or in about three hours for less than $100 using a 40-node, 320-core cluster rented from Amazon’s EC2 utility computing service. Our evaluation against a “gold standard” of known differences within the individual shows Crossbow is better than 99% accurate at identifying differences between human genomes. We set out to create a tool that could reproduce the analysis of a recent whole genome study, and we did exactly that, only it is much much faster, and runs in the clouds. As such, any researcher in the world can reproduce our results, or use our pipeline to analyze their own data. As sequencing reaches an ever wider audience and becomes used in small labs, Crossbow will enable the computational analysis without requiring researchers to own or maintain their own compute infrastructure. This is a compelling result from both a users and a systems perspective: it is an accurate, fast, and cheap way of squeezing 1000 hours of computation into an afternoon, all made possible with MapReduce/Hadoop. It is also noteworthy that Crossbow uses Hadoop Streaming so that we could reuse existing tools written in C rather than reimplementing their sophisticated algorithms in Hadoop’s native Java. In this way Hadoop was a good fit for our needs: it runs and monitors Bowtie and SOAPsnp in parallel on many nodes, adds fault tolerance, and takes care of the massive distributed sorts that are needed for the analysis. Now that we are starting to think MapReduce/Hadoop, several extensions to Crossbow are apparent, and we are thinking about how to apply these techniques to analyze copy number variations, RNA-seq data, Methyl-Seq, ChIP-seq, structural variations, and more. I’m also nearly done with a MapReduce/Hadoop based de novo assembler that scales to assemble mammalian genomes from short reads. I’m really excited about Crossbow, and about the role of Hadoop in Computational Biology. Crossbow solves one of the biggest problems in personalized genomics research, and I hope it will be used someday to understand or cure diseases. Furthermore, Crossbow shows how Hadoop can be a enabling technology for computational biology, and I foresee widespread use of it in the future. For more information see: http://bowtie-bio.sf.net/crossbow.</snippet></document><document id="697"><title>Introducing Cloudera Desktop</title><url>http://blog.cloudera.com/blog/2009/10/introducing-cloudera-desktop/</url><snippet>Today at Hadoop World NYC, we’re announcing the availability of Cloudera Desktop, a unified and extensible graphical user interface for Hadoop. The product is free to download and can be used with either internal clusters or clusters running on public clouds. At Cloudera, we’re focused on making Hadoop easy to install, configure, manage, and use for all organizations. While there exist many utilities for developers who work with Hadoop, Cloudera Desktop is targeting beginning developers and non-developers in an organization who’d like to get value from the data stored in their Hadoop cluster. By working within a web browser, users avoid the tedious client installation and upgrade cycle, and system administrators avoid custom firewall configurations. We’ve worked closely with the MooTools community to create a desktop environment inside of a web browser that should be familiar to navigate for most users. The desktop environment has other advantages: it’s extensible to hundreds of applications and allows for data to be shared between applications. Initial applications for Cloudera Desktop include: File Browser: The File Browser application lets you navigate your Hadoop Distributed File System as easily as your local file system. You can browse the directory structure, examine file contents, rename files and directories, and upload files from within your web browser. Job Browser: The Job Browser enables you to examine the past and present of your MapReduce cluster in detail. Follow the progress of running jobs, debug failed jobs, and get more details on historical job performance. Cluster Health: Learn the state of your cluster from a glance with the Cluster Health application. A wide variety of metrics are collected and displayed in real time to help you pin down problems with your cluster. Define custom checks for non-healthy states and ensure that you catch minor problems before they become major. Job Designer: Create MapReduce job designs for commonly run jobs and save them for reuse later or by other users. Submit jobs to your MapReduce cluster from your browser. We expect to deliver several more applications over the coming months, and we’re working with a few partners to standardize the API for building Desktop applications so that any software developer can build applications for Cloudera Desktop. If you’re interested in developing on Cloudera Desktop, drop us a note at desktop-api-subscribe@cloudera.com. Cloudera Desktop currently works with the latest testing release of Cloudera’s Distribution for Hadoop, and we’re working with the Apache Hadoop community to get our patches into the Apache project as well To keep up with new developments for Cloudera Desktop, send a note to desktop-announce-subscribe@cloudera.com. For more information, see the official product page. Check out the screencast below for more details on the product, and be sure to post any feedback you have to the Cloudera Desktop page on Get Satisfaction. We hope Cloudera Desktop makes Hadoop an even more critical tool for data management and analysis in your organization, and we look forward to delivering more quality software to you soon.</snippet></document><document id="698"><title>CDH2: Testing Release now with Pig, Hive, and HBase</title><url>http://blog.cloudera.com/blog/2009/09/cdh2-testing-release-now-with-pig-hive-and-hbase/</url><snippet>At the beginning of September, we announced the first release of CDH2, our current testing repository. Packages in our testing repository are recommended for people who want more features and are willing to upgrade as bugs are worked out. Our testing packages pass unit and functional tests but will not have the same “soak time” as our stable packages. A testing release represents a work in progress that will eventually be promoted to stable. We plan on pushing new packages into the testing repository every 3 to 6 weeks.  And it just so happens it is just about 3 weeks after we announced the first testing release. So it must be time for a new one. Here are some of the highlights: Hadoop 0.20.1 – Bumps the hadoop package up to the 0.20.1 release and adds 133 patches worth of extra goodness Alternatives for Hadoop – Now you can have both 0.18 and 0.20 installed and use the alternatives system to pick a default Pig 0.50 pre-release – We included some magic to get things working out of the box with both 0.18 and 0.20 Hive 0.40 pre-release - Integrated with the alternatives setup out the box works with 0.18 and 0.20 HBase 0.20 – We worked with the HBase team to bring rpms to a yum repo near you A project as large as Hadoop is a communal effort. Cloudera is proud to be part of that community and hope that our products and services make Hadoop even more accessible to a wider audience. We’d like to thank everyone who contributes to Hadoop, especially the Yahoo! team for all of their hard work on getting 0.20.1 released, the developers at Facebook and those working on Pig, Hive and HBase. We are just getting the ball rolling here. You can subscribe to our CDH mailing list (cdh-announce-subscribe@cloudera.com) to get information about new releases as we push them out. Check out the new release, and remember to let us know what you think!</snippet></document><document id="699"><title>Apache HBase Available in CDH2</title><url>http://blog.cloudera.com/blog/2009/09/hbase-available-in-cdh2/</url><snippet>One of the more common requests we receive from the community is to package Apache HBase with Cloudera’s Distribution for Apache Hadoop. Lately, I’ve been doing a lot of work on making Cloudera’s packages easy to use, and recently, the HBase team has pitched in to help us deliver compatible HBase packages. We’re pretty excited about this, and we’re looking forward to your feedback. A big thanks to Andrew Purtell, a Senior Architect at TrendMicro and HBase Contributor, for leading this packaging project and providing this guest blog post. -Chad Metcalf What is HBase? Apache HBase is an open-source, distributed, column-oriented store modeled after Google’s Bigtable large scale structured data storage system. You can read Google’s Bigtable paper here. “Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from back end bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products.” HBase extends the publicly shared aspects of the Bigtable architecture and design as described in the Bigtable OSDI’06 paper with community developed improvements and enhancements: Convenient base classes for backing Hadoop MapReduce jobs with HBase tables Query predicate push down via server side scan and get filters Optimizations for real time queries A high performance Thrift gateway A REST-ful Web service gateway that supports XML, Protobuf, and binary data encoding options Cascading source and sink modules A JRuby-based shell Support for exporting metrics via the Hadoop metrics subsystem to files or Ganglia; or via JMX This most recent version of HBase, 0.20.0, has greatly improved on its predecessors: No HBase single point of failure Rolling restart for configuration changes and minor upgrades Generally, one order of magnitude performance improvement for every class of operation Random access performance on par with open source relational databases such as MySQL We use ZooKeeper as a substitute for Google’s “Chubby” to enable hot fail over should a Master node fail. We do other interesting things with ZooKeeper as well and on our roadmap is increasingly distributed function via emergent behaviors with no central point of control. Unfortunately the HDFS NameNode is still a single point of failure in Hadoop 0.20, and HBase depends on HDFS. For more information on mitigating this risk, see this Cloudera blog post on NameNode High Availability. For more detail, please visit the HBase wiki. Links and references to additional information appear below. Why Would You Need HBase? Use HBase when you need fault-tolerant, random, real time read/write access to data stored in HDFS. Use HBase when you need strong data consistency. HBase provides Bigtable-like capabilities on top of Hadoop. HBase’s goal is the hosting of very large tables — billions of rows times millions of columns — atop clusters of commodity hardware. HBase is an answer for effectively managing terabytes of mutating structured storage on the Hadoop platform at reasonable cost. HBase manages structured data on top of HDFS for you, efficiently using the underlying replicated storage as backing store to gain the benefits of its fault tolerance and data availability and locality. HBase hides the gory details of how one would provide random real time read/write access on top of a filesystem tuned for MapReduce jobs that process terabytes of data, where file block sizes are huge, and where a file can be open for reading or for writing, but not both. At large scale traditional relational databases (RDBMSes) fall down. We are considering here big queries, typically range or table scans; and big tables, typically terabytes or petabytes. Such workloads generally exceed the ability of these systems to process them in a timely, cost-effective manner. Managing very large storage with them alone is an expensive proposition. Processing that data incurs other cost — in time, in productivity. Waits and deadlocks rise nonlinearly with transaction size and concurrency, the square of concurrency, the third power of the transaction size. In contrast, HBase table scans run in linear time, and row lookup or update times are logarithmic with respect to the size of the table. Features of the relational model get in the way as data volumes scale up and analytics get more complex and interesting. Expensive commercial RDBMS systems can deliver large storage capacities and they can execute some queries over all that data in reasonable time, but only at high dollar cost. Using open source RDBMSes at scale simply requires giving up all relational features (e.g. secondary indexes) for performance. Sharding is a brittle and complex non-solution to these scalability problems. It is something done when there are no better alternatives. What if we trade relational features for performance since using RDBMSes at scale often requires giving them up anyway? The first casualty of sharding is the normalized schema. We can avoid waits and deadlocks by restricting transaction scope to groups of row mutations only. What if we generalize the data model? Then we can provide transparent horizontal scalability without architectural limits — generic “self-sharding”. We can also provide fault tolerance and data availability by way of the same mechanisms which allow this scalability. Bigtable and HBase are able to avoid the scalability issues that trouble RDBMSes by eschewing the relational data model. HBase provides something else. It is like a large distributed map. It is a row indexed list of tags and data, values of variable length, bounded by a configuration setting.� It is a column based data store. Keys are arbitrary byte data and are multidimensional: row, column, optional column qualifier, and timestamp. Columns in HBase are multiversioned. You can store more than one version of a value in a particular row and column, and the timestamp provides an extra dimension of indexability.� This can be a particularly useful feature: Multiversioning and timestamps avoid edit conflicts caused by concurrent decoupled processes. Rows are stored in byte lexicographic sorted order. Lexicographically similar values are packed adjacent to one another into blocks in the column stores and are retrieved efficiently together. Column stores may optionally be compressed on disk. Tables are dynamically split into regions. Regions are hosted on a number of region servers. Adding additional capacity to a HBase cluster is a simple and transparent process: Provision another region server, configure it, and start it. Typically, HBase region servers are co-deployed with Hadoop HDFS DataNodes. The underlying storage capacity grows also. As regions grow, they are split and distributed evenly among the storage cluster to level load. Splits are almost instantaneous. A cluster master process manages region assignment for fast recovery and fine grained load balancing. The Master role falls over to spares as necessary for fault tolerance. The Master rapidly redeploys regions from failed nodes to others. Because the stores are in HDFS, all region servers in the cluster have immediate access to the replicated table data. When Would You Not Want To Use HBase? When your data access patterns are largely sequential over immutable data. Use plain MapReduce. When your data is not large. When the large overheads of the extract-transform-load (ETL) of your data into alternatives such as Hive is not an issue because you are purely operating on the data in a batching manner and can afford to wait, and some feature of the alternative is simply a must-have. If you need to make a different trade off between consistency and availability. HBase is a strongly consistent system. HBase regions can be temporarily unavailable during fault recovery. The HBase client API will suspend pending reads and writes until the regions come back online. Perhaps for your use case blocking of any kind for any length of time is intolerable. If you just can’t live without SQL. When you really do require normalized schemas or a relational query engine. However, this last point can use some additional detail. HBase supports random, real time read/write access to your data by way of a single index. However, secondary indexes can be emulated by managing additional index tables at the application level. To achieve fast query response times under real world conditions, “Web 2.0″ applications often denormalize and replicate and synchronize values in multiple tables anyway. Bigtable’s simpler data model is sufficient for many such use cases and furthermore does not support constructs that can get you into trouble. What you do get is: Fast (logarithmic time) lookup using row key, optional column and column qualifiers for result set filtering and optional timestamp; Full table scans; Range scans, with optional timestamp; Queries for most recent version or N versions; Partial key lookups: When combined with compound keys, these have the same properties as leading left edge indexes with the benefit of a distributed index; Server side filters, a form of query push down. And, while HBase does not support joining data from multiple tables, you can implement your data workflows using Cascading or a similar higher level construct on top of HBase to recover some relational algebraic operators; or you can simply do ?insert time joins? — denormalization, view materialization, and so on. How Do You Try Out HBase? Installing and configuring HBase on the CDH2 is fast and easy. Before we begin, note that HBase requires an available Zookeeper ensemble.� The CDH2 packages for HBase includes a Zookeeper package. You can install and configure it and then point HBase to it, or you can let HBase create and manage a private Zookeeper ensemble using the bundled Zookeeper jar. For new users who do not have Zookeeper already set up, it is easiest to just let HBase take care of it. The instructions below assume this is the case. Also, let’s consider what is a reasonable test deployment. Google aims for ~100 regions per region server, and each region is kept to around 200 MB. Large RAM per node and reasonable region counts and sizing means many tables can be cached and served entirely out of RAM. Bigtable is big because there are 100s if not 1000s of nodes participating. The performance numbers in the Bigtable paper are impressive because of the above. It is cheap (for Google) because they build their own hardware and buy components in bulk. HBase operates in a different world. Many evaluators or new users expect a lot more for a lot less. They don’t build their own hardware — but could and maybe should — and don’t make bulk purchases. Rather, test deployments of 3 or 4 standard type servers are common. Often the hardware is underpowered for the attempted load. Sometimes even smaller deployments are considered, or even virtual machines are used, but those do not make any sense except as programmer tools. While a “pseudo-distributed” configuration for HBase is included in the distribution, a single server deployment is suitable only for very limited testing. We recommend that three servers be considered a minimum test deployment. These can be Amazon EC2 instances, but use c1.xlarge instances. A reasonable physical server configuration could be: Dual quad core CPU 8 GB RAM or more (4 GB is passable, but constrain MapReduce to only 1 concurrent mapper and reducer per node) 4 x 250 GB data disk attached as JBOD (for the DataNode process) The reason for the resource demand is simple: The typical deployment combines HDFS, MapReduce, and HBase over all servers uniformly. A good rule of thumb here is each Hadoop and HBase daemon requires 1 available CPU core and 1 GB of heap. Each mapper or reducer task requires 1 CPU core and 200 MB of heap by default, more if asked for. For HBase to achieve best performance, the region servers must be given sufficient heap to buffer writes and cache blocks for repeated reads. The higher the write load or the larger the working set, the more larger heap allocations will be useful. Configuring 2GB or 4GB heap for HBase region servers is not uncommon. On to a quick install: 1) On each server, install the core HBase RPMs: hbase, hbase-native, hbase-master, hbase-regionserver, hbase-zookeeper, hbase-conf-pseudo, hbase-docs. 2) On each server, create the cluster configuration and use ‘alternatives’ to enable it. Create the configuration: % mkdir /etc/hbase-0.20/conf.my_cluster % cp /etc/hbase-0.20/conf.pseudo/* /etc/hbase-0.20/conf.my_cluster % vi /etc/hbase-0.20/conf.my_cluster/hbase-site.xml Set up the ZooKeeper quorum: &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;host1,host2,host3&lt;/value&gt; &lt;/property&gt; Point HBase root to a folder to be created in HDFS: &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://namenode:nnport/hbase&lt;/value&gt; &lt;/property&gt; Note: Do not create this folder yourself. Enable distributed operation: &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; On small clusters reduce DFS replication to speed writes: &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; Use the new configuration: % alternatives --install /etc/hbase-0.20/conf hbase-0.20-conf \ /etc/hbase-0.20/conf.my_cluster 50 3) Bring Hadoop HDFS up as you would normally. 4) On all cluster nodes, start zookeeper: % service hbase-zookeeper start 5) On the designated master, start the master process: % service hbase-master start 6) On the designated backup master, start another master process: % service hbase-master start 7) On the designated slaves, start the region server processes: % service hbase-regionserver start 8 ) Anywhere on the cluster, launch the HBase shell and create a table: % su - hadoop % hbase shell HBase Shell; enter ‘help&lt;RETURN&gt;‘ for list of supported commands. Version: 0.20.0~1-1.cloudera hbase(main):001:0&gt; create 'TestTable', {NAME=&gt;'test'} 0 rows(s) in 3.4460 seconds hbase(main):002:0&gt; 9) Somewhere else on the cluster, launch the HBase shell and describe your new table: % su - hadoop % hbase shell HBase Shell; enter ‘help&lt;RETURN&gt;‘ for list of supported commands. Version: 0.20.0~1-1.cloudera hbase(main):001:0&gt; describe 'TestTable' DESCRIPTION��������������������������������������������������� ENABLED {NAME =&gt; ‘TestTable’, FAMILIES =&gt; [{NAME =&gt; 'test',����������� true COMPRESSION =&gt; 'NONE', VERSIONS =&gt; '3', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]} 1 row(s) in 1.6820 seconds You are now ready to try out your new HBase installation! For More Information Visit the HBase Website and Wiki. Users and supporting projects. HBase Roadmap HBase Mailing List IRC Channel: #hbase on Freenode Committers and core contributors are here on a regular basis. More active than the Hadoop forums! Follow us on Twitter: @hbase</snippet></document><document id="700"><title>Grouping Related Trends with Hadoop and Hive</title><url>http://blog.cloudera.com/blog/2009/09/grouping-related-trends-with-hadoop-and-hive/</url><snippet>(guest blog post by Pete Skomoroch) In a previous post, I outlined how to build a basic trend tracking site called trendingtopics.org with Cloudera’s Distribution for Hadoop and Hive.  TrendingTopics uses Hadoop to identify the top articles trending on Wikipedia and displays related news stories and charts.  The data powering the site was pulled from an Amazon EBS Wikipedia Public Dataset containing 8 months of hourly pageview logfiles.  In addition to the pageview logs, the EBS data volume also includes the full text content and link graph for all articles.  This post will use that link graph data to build a new feature for our site: grouping related articles together under a single “lead trend” to ensure the homepage isn’t dominated by a single news story. Finding Related Trends Using Wikipedia Link Graph Data For several weeks this summer, the death of Michael Jackson dominated the news and drove a large number of pageviews on Wikipedia.   The hourly data in the following chart was downloaded from trendingtopics.org during the last week in June: This burst of interest in the Jackson family, Michael’s albums, and his medical conditions pushed most topics unrelated to MJ off the front page of our site: Ideally, TrendingTopics would continue to show a range of stories when this type of event happens.  Automated news sites like Techmeme or Google News display clusters of rising articles across a number of topics to provide a better mix of content on the homepage.   For example, Techmeme shows a ranked list of related blog posts that link back to the lead article for each story: We can build a simple version of this functionality with Hadoop by combining the article trend estimates we computed in the previous post with the Wikipedia link graph.  Wikipedia provides a periodic database dump which includes a file with the outgoing pagelinks for each article.  The June data dump includes a 12GB pagelink file named “enwiki-20090618-pagelinks.sql” containing ~ 13K SQL insert statements: INSERT INTO `pagelinks` VALUES (5588,0,'Pinar_del_Río'),(5588,0,'Planned_economy'),..
INSERT INTO `pagelinks` VALUES (5845,0,'Attorney_General_of_Colombia'),(5845,0,'Auditor_General_of_Colombia'),...
INSERT INTO `pagelinks` VALUES (6187,0,'Wallraf-Richartz_Museum'),(6187,0,'Wangen_im_Allgäu'),...
... The first step in our data preparation uses a Hadoop Streaming job to convert the Mediawiki SQL insert format into a tab delimited text file ready for further processing with Hive.  The job uses a Python mapper called “parse_links.py” to convert the SQL insert statements into a tab delimited format.  The “parse_links.py” script uses a regular expression to extract the graph edges from each SQL statement and emits tab-delimited record if the page_id belongs to namespace 0 (this indicates the page is an article). parse_links.py import sys, os, re

insert_regex = re.compile('''INSERT INTO \`pagelinks\` VALUES (.*)\;''')
row_regex = re.compile("""(.*),(.*),'(.*)'""")

for line in sys.stdin:
  match = insert_regex.match(line.strip())
  if match is not None:
    data = match.groups(0)[0]
    rows = data[1:-1].split("),(")
    for row in rows:
      row_match = row_regex.match(row)
      if row_match is not None:
        # &gt;&gt;&gt; row_match.groups()
        # (12,0,'Anti-statism')
        # # page_id, pl_namespace, pl_title
        if row_match.groups()[1] == '0':
          page_id, pl_title = row_match.groups()[0], row_match.groups()[2]
          sys.stdout.write('%s\t%s\n' % (page_id, pl_title)) You can test this script at the unix command line on a single machine before running it on Hadoop.  Here we pipe a sample of the file into the mapper script and then examine the results with grep.  The first column of the resulting output is the Wikipedia page_id and the second column contains other Wikipedia article titles linked to by that page_id. $ head -35 enwiki-20090618-pagelinks.sql | ./parse_links.py &gt; links.txt

$ grep 'Super' links.txt | more
303    Super_Outbreak
303    Talladega_Superspeedway
324    Super_Bowl
594    Tarquinius_Superbus
615    List_of_Super_Bowl_champions
615    List_of_Super_Bowl_records
615    Super_Bowl
615    Super_Bowl_XIX We ran this script on the entire pagelinks file using a temporary Cloudera Hadoop cluster on Amazon EC2, and saved the output of the conversion process in an Amazon S3 folder called “links”.  To work with these tab delimited link files on a new Hadoop cluster, we use distcp on the master node to pull the output data into HDFS from S3.   We also pull in an archived export of our trendingtopics “pages” data which contains the page_id -&gt; title mapping and monthly trend score for each article: $ hadoop distcp s3n://trendingtopics/wikidump/links links
$ hadoop fs -rmr links/_distcp_logs*
$ hadoop distcp s3n://trendingtopics/archive/20090816/pages pages
$ hadoop fs -rmr pages/_distcp_logs* We will use a set of Hive queries on the link graph data to generate the ranked list of top trending articles linking back to each Wikipedia.  After our data is in hdfs, we can start the Hive CLI on the Hadoop master node: $ hive In the Hive shell, we create a “links” table and load the raw data from hdfs: hive&gt; CREATE TABLE links (
 page_id BIGINT,
 pl_title STRING)
  ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '\t'
  STORED AS TEXTFILE;

hive&gt; LOAD DATA INPATH 'links' OVERWRITE INTO TABLE links; Now we have the “outlink” data for each page in Hive, but we want to reverse the keys so that we can find all trending page titles which link back to a given page_id.  We can map the original page_id’s to article titles and pull in the trend data using a Hive JOIN: hive&gt; CREATE TABLE temp_backlinks (
    pl_title STRING,
    page_id BIGINT,
    bl_title STRING,
    monthly_trend DOUBLE)
  ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '01'
  STORED AS TEXTFILE;

hive&gt; INSERT OVERWRITE TABLE temp_backlinks
select links.pl_title, pages.page_id,
  pages.redirect_title, pages.monthly_trend
from links JOIN pages ON (pages.page_id = links.page_id); A second join is used to convert the original outlink titles to page_ids for use in our web application’s MySQL database: hive&gt; CREATE TABLE backlinks (
  page_id BIGINT,
    bl_title STRING,
    monthly_trend DOUBLE)
  ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '01'
  STORED AS TEXTFILE;

hive&gt; INSERT OVERWRITE TABLE backlinks
select pages.page_id,
    temp_backlinks.bl_title,
    temp_backlinks.monthly_trend
from pages JOIN temp_backlinks
ON (pages.redirect_title = temp_backlinks.pl_title); The final stage of processing involves generating an array of the top trending backlink urls for each Wikipedia page_id.  Later on, we can export the entire lookup table “backlinks_reduced” to MySQL for use in the trendingtopics Rails app.  We use a Hive transform written in Python to rank the backlinks and concatenate the top 10 titles into a comma delimited text string for each page_id: hive&gt; CREATE TABLE backlinks_reduced (
    page_id BIGINT,
    backlinks STRING)
  ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '01'
  STORED AS TEXTFILE;

hive&gt; add FILE /mnt/hive_backlink_mapper.py;
hive&gt; add FILE /mnt/hive_backlink_reducer.py;

hive&gt; FROM (
  FROM backlinks
  MAP backlinks.page_id, backlinks.bl_title, backlinks.monthly_trend
  USING 'python hive_backlink_mapper.py'
  CLUSTER BY key) map_output
INSERT OVERWRITE TABLE backlinks_reduced
  REDUCE map_output.key, map_output.value
  USING 'python hive_backlink_reducer.py'
  AS page_id, backlinks; The streaming mapper and reducer we used are shown below: hive_backlink_mapper.py import sys, os

for line in sys.stdin:
  try:
    page_id, bl_title, score = line.strip().split("\t")
    sys.stdout.write('%s\t%s\t%s\n' % (page_id, bl_title, score))
  except:
    # just skip possible bad rows for now
    pass hive_backlink_reducer.py import sys, os

def top_backlinks(backlinks, scores):
  # return top 10 backlinks based on score metric (trend)
  scorevals,links = zip( *sorted( zip (scores,backlinks)))
  toplinks = list(links)
  toplinks.reverse()
  backlink_str = '[%s]' % ','.join(toplinks[:10])
  return backlink_str

#  For each page, emit backlinks sorted by score desc
last_page, backlinks, scores = None, [], []
for line in sys.stdin:
  try:
    (page, backlink, score) = line.strip().split("\t")
    if last_page != page and last_page is not None:
      backlink_string = top_backlinks(backlinks, scores)
      print "%s\t%s" % (last_page, backlink_string)
      backlinks = []
      scores = []
    last_page = page
    backlinks.append(backlink)
    scores.append(float(score))
  except:
    pass
backlink_string = top_backlinks(backlinks, scores)
print "%s\t%s" % (last_page, backlink_string) To examine the results, we run a join query against the pages table to find the top trending backlinks for the Wikipedia article on “Swine Flu”: hive&gt; SELECT backlinks_reduced.backlinks FROM backlinks_reduced
         JOIN pages ON (pages.page_id = backlinks_reduced.page_id)
         WHERE redirect_title="Swine_influenza";

[Influenza_A_virus_subtype_H1N1,1918_flu_pandemic,Guillain-Barré_syndrome,
Oseltamivir,Influenza_treatment,Influenza_vaccine,Zanamivir,
Orthomyxoviridae,Pig,Amantadine] This looks like a reasonable list of related pages.  Here are a few more trending topics from the same time period: Julia Child
[Julie_Powell,Julie_&amp;_Julia,August_13,August_15,Meryl_Streep,
Mastering_the_Art_of_French_Cooking,Le_Cordon_Bleu,Amy_Adams,
Simone_Beck,The_French_Chef] John Hughes
[The_Breakfast_Club,Molly_Ringwald,Sixteen_Candles,Pretty_in_Pink,
Weird_Science_(film),Some_Kind_of_Wonderful_(film),Curly_Sue,
Home_Alone_3,Uncle_Buck,Home_Alone_(film)] Quentin Tarantino
[Inglourious_Basterds,Machete_(film),Pulp_Fiction_(film),
Eli_Roth,Bruce_Willis,Rosario_Dawson,Robert_Rodriguez,
Grindhouse_(film),Julie_Dreyfus,Death_Proof] At this point, we have what looks like a reasonable approach for generating related links with a low amount of effort.  The Rails application can apply filters to remove titles like “August_15″ from each list and display the related links around each trend. Here are a few ideas for next steps with the Wikipedia link graph data and Hadoop: Show related articles by using the backlink ids as a feature vector to compute article similarity Use Pagerank to filter articles displayed in trends (see Running PageRank on Wikipedia) Compute TFIDF weights for Wikipedia articles and incorporate it into the similarity calculation If you are interested in digging deeper into the link data with Python and Hadoop, you should also check out Viraj Bhat and Jake Hofman’s talk “Cool Development Projects at Yahoo!: Automatic Tuning and Social Graph Analysis” next week at Hadoop World NYC.  I’ll also giving a talk that afternoon called “Building Data Intensive Apps: A closer look at TrendingTopics.org”</snippet></document><document id="701"><title>Apache Hadoop Log Files: Where to find them in CDH, and what info they contain</title><url>http://blog.cloudera.com/blog/2009/09/apache-hadoop-log-files-where-to-find-them-in-cdh-and-what-info-they-contain/</url><snippet>Apache Hadoop’s jobtracker, namenode, secondary namenode, datanode, and tasktracker all generate logs. That includes logs from each of the daemons under normal operation, as well as configuration logs, statistics, standard error, standard out, and internal diagnostic information. Many  users aren’t entirely sure what the differences are among these logs, how to analyze them, or even how to handle simple administrative tasks like log rotation.  This blog post describes each category of log, and then details where they can be found for each Hadoop component. The log categories are: Hadoop Daemon Logs These logs are created by the Hadoop daemons, and exist on all machines running at least one Hadoop daemon. Some of the files end with .log, and others end with .out. The .out files are only written to when daemons are starting. After daemons have started successfully, the .out files are truncated. By contrasts, all log messages can be found in the .log files, including the daemon start-up messages that are sent to the .out files. There is a .log and .out file for each daemon running on a machine. When the namenode, jobtracker, and secondary namenode are running on the same machine, then there are six daemon log files: a .log and .out for the each of the three daemons. The .log and .out file names are constructed as follows: hadoop-&lt;user-running-hadoop&gt;-&lt;daemon&gt;-&lt;hostname&gt;.log where &lt;user-running-hadoop&gt; is the user running the Hadoop daemons (this is always ‘hadoop’ with Cloudera’s distribution), &lt;daemon&gt; is the daemon these logs are associated (for example, namenode or jobtracker), and &lt;hostname&gt; is the hostname of the machine on which the daemons are running. For example: hadoop-hadoop-datanode-ip-10-251-30-53.log By default, the .log files are rotated daily by log4j. This is configurable with /etc/hadoop/conf/log4j.properties. Administrators of a Hadoop cluster should review these logs regularly to look for cluster-specific errors and warnings that might have to do with daemons running incorrectly.  Note that the namenode and secondarynamenode logs should not be deleted more frequently than fs.checkpoint.period, so in the event of a secondarynamenode edits log compaction failure, logs from the namenode and secondarynamenode will be available for diagnostics. These logs grow slowly when the cluster is idle. When jobs are running, they grow very rapidly. Some problems create considerably more log entries, but some problems only create a few infrequent messages. For example, if the jobtracker can’t connect to the namenode, the jobtracker daemon logs explode with the same error (something like “Retrying connecting to namenode [..]“). Lots of log entries here does not necessarily mean that there is a problem: you have to search through these logs to look for a problem. Job Configuration XML The job configuration XML logs are created by the jobtracker. The jobtracker creates a .xml file for every job that runs on the cluster. These logs are stored in two places:/var/log/hadoop and /var/log/hadoop/history. The XML file describes the job configuration. The /hadoop file names are constructed as follows: job_&lt;job_ID&gt;_conf.xml For example: job_200908190029_0001_conf.xml The /hadoop/history file names are constructed as follows: &lt;hostname&gt;_&lt;epoch-of-jobtracker-start&gt;_&lt;job-id&gt;_conf.xml where &lt;hostname&gt; is the hostname of the machine on which the jobtracker is running, &lt;epoch-of-jobtracker-start&gt; is the number of milliseconds that had elapsed since Unix Epoch when the jobtracker daemon was started, and &lt;job-id&gt; is the job ID. For example: ec2-72-44-61-184.compute-1.amazonaws.com_1250641772616_job_200908190029_0001_conf.xml These logs are not rotated. These files may be more interesting to developers than system administrators, because their contents are job-specific. You can clear these logs periodically without affecting Hadoop. However, consider archiving the logs if they are of interest in the job-development process. Make sure you do not move or delete a file that is currently being written to by a running job. Individual job configuration logs are created for each job that is submitted to the cluster. Each log file will be more or less the same size for each job. Job Statistics These logs are created by the jobtracker. The jobtracker runtime statistics from jobs to these files. Those statistics include task attempts, time spent shuffling, input splits given to task attempts, start times of tasks attempts and other information. The statistics files are named: &lt;hostname&gt;_&lt;epoch-of-jobtracker-start&gt;_&lt;job-id&gt;_&lt;job-name&gt; where &lt;hostname&gt; is the hostname of the machine creating these logs, &lt;epoch-of-jobtracker-start&gt; is the number of milliseconds that had elapsed since Unix Epoch when the jobtracker daemon was started, &lt;job-id&gt; is the job ID, and &lt;job-name&gt; is the name of the job. For example: ec2-72-44-61-184.compute-1.amazonaws.com_1250641772616_job_200908190029_0002_hadoop_test-mini-mr These logs are not rotated.  You can clear these logs periodically without affecting Hadoop. However, consider archiving the logs if they are of interest in the job development process. Make sure you do not move or delete a file that is being written to by a running job. Individual statistics logs are created for each job that is submitted to the cluster. The size of each log file varies. Jobs with more tasks produce larger files. Standard Error for a particular task attempt These logs are created by each tasktracker. They contain information written to standard error (stderr) captured when a task attempt is run. These logs can be used for debugging.  For example, a developer can include System.err.println(“some useful information”) calls in the job code. The output will appear in the standard error files. The parent directory name for these logs is constructed as follows: /var/log/hadoop/userlogs/attempt_&lt;job-id&gt;_&lt;map-or-reduce&gt;_&lt;attempt-id&gt; where &lt;job-id&gt; is the ID of the job that this attempt is doing work for, &lt;map-or-reduce&gt; is either “m” if the task attempt was a mapper, or “r” if the task attempt was a reducer, and &lt;attempt-id&gt; is the ID of the task attempt. For example: /var/log/hadoop/userlogs/attempt_200908190029_0001_m_000001_0 These logs are rotated according to the mapred.userlog.retain.hours property. You can clear these logs periodically without affecting Hadoop. However, consider archiving the logs if they are of interest in the job development process. Make sure you do not move or delete a file that is being written to by a running job. The size of these log files depends on the amount of stderr calls used in the job code. Standard Out for a particular task attempt These logs are very similar to the standard error logs, except they capture stdout instead of stderr. File size depends entirely on how much data the task writes to stdout. log4j informational messages from within the task process These logs contains anything that log4j writes from within the task process. This includes some Hadoop internal diagnostic info. If the job’s mapper or reducer implementations include call such as LOG.info(), then that output also get written here. Messages can include information about the task, such as how big its record buffer was or how many reduce tasks there are. The size of these log files depends on the number of log4j calls used in the job code. These logs are rotated according to the mapred.userlog.retain.hours property. You can clear these logs periodically without affecting Hadoop. However, consider archiving the logs if they are of interest in the job development process. Make sure you do not move or delete a file that is being written to by a running job. The Hadoop components produce the following logs when installing Hadoop from CDH RPM or DEB packages: On the jobtracker: /var/log/hadoop
              /hadoop-* =&gt; daemon logs
              /job_*.xml =&gt; job configuration XML logs
              /history
                     /*_conf.xml =&gt; job configuration logs
                     &lt; everything else &gt; =&gt; job statistics logs On the namenode: /var/log/hadoop
              /hadoop-* =&gt; daemon logs On the secondary namenode: /var/log/hadoop
              /hadoop-* =&gt; daemon logs On the datanode: /var/log/hadoop
              /hadoop-* =&gt; daemon logs On the tasktracker: /var/log/hadoop
              /hadoop-* =&gt; daemon logs
              /userlogs
                      /attempt_*
                               /stderr =&gt; standard error logs
                               /stdout =&gt; standard out logs
                               /syslog =&gt; log4j logs It’s probably clear now that Hadoop generates plenty of logs, each with a different purpose and audience.  I hope this post will help you debug your MapReduce jobs and Hadoop cluster.</snippet></document><document id="702"><title>CDH2: Cloudera’s Distribution for Apache Hadoop 2</title><url>http://blog.cloudera.com/blog/2009/09/cdh2-clouderas-distribution-for-hadoop-2/</url><snippet>In March of this year, we released our distribution for Apache Hadoop.� Our initial focus was on stability and making Hadoop easy to install. This original distribution, now named CDH1, was based on the most stable version of Apache Hadoop at the time:0.18.3. We packaged up Apache Hadoop, Pig and Hive into RPMs and Debian packages to make managing Hadoop installations easier.� For the first time ever, Hadoop cluster managers were able to bring up a deployment by running one of the following commands depending on your Linux distribution: # yum install hadoop
# apt-get install hadoop As proof of this, our easy-to-use Hadoop Amazon Machine Images (AMIs) use these commands at boot to install the latest release of CDH1 whenever a Hadoop cluster is launched on ec2. In addition, our packages followed the Filesystem Hierarchy Standard, used chkconfig and service for service management and even added a hadoop man page. CDH1 demonstrated that Hadoop, Pig and Hive could be tightly integrated into Linux allowing people to manage Hadoop using tools they are familiar with like service, chkconfig, alternatives, logrotate, man, etc.� Hadoop users benefited from the fact that hadoop scripts and environment variables were automatically managed for them, making it easier to focus on using Hadoop. Our distribution has been installed on a variety of clusters, from small clusters parsing web logs to large clusters processing protein sequences. We’ve received plenty of great feedback on CDH1 from our customers and the community. That feedback was really useful, and influenced the second major release: CDH2. CDH2 We released CDH2 on August 3rd as a testing release.� CDH1 is still considered our stable release.� More on that later in this post. A few of our customers and community members volunteered to serve as beta testers in July to get early access to CDH2 features.� With CDH2, Apache Hadoop is available in two packages: hadoop-0.18 and hadoop-0.20.� Hadoop cluster administrators can now choose to install one or both versions on their cluster.� Installing Hadoop is still as simple as running one of the following commands depending on your Linux distribution and which version of Hadoop you want: # yum install hadoop-0.18
# yum install hadoop-0.20
# apt-get install hadoop-0.18
# apt-get install hadoop-0.20 The hadoop-0.18 and hadoop-0.20 packages don’t conflict because they use different directory paths, e.g. /usr/lib/hadoop-0.18 and /usr/lib/hadoop-0.20.� To see all the files these packages install, run one of the following commands depending on your Linux distribution: $ rpm -ql hadoop-0.18
$ dpkg -L hadoop-0.18 These packages also allow you to upgrade from your existing CDH1 hadoop package installation. The hadoop-0.20 package is based on Apache Hadoop 0.20.0 but with many additions: Sqoop Updates Support for Oracle databases in addition to MySQL configurable format control (quoting, delimiters) better documentation (man sqoop) WHERE clause support user-defined class and package name support bugfixes like more secure treatment of passwords and eliminating an out-of-memory condition with MySQL Fair Share Scheduler Updates Support for scheduler preemption allows setting the default value of maxRunningJobs from all pools and much more (see our manifests for details) stable and testing releases Some people are happy to trade some stability in order to get the latest features.� To accommodate people’s differing needs, we are publishing our distribution to unique stable and testing repositories. Packages in our stable repository are deemed ready for production clusters. These packages have passed all unit tests, functional tests and have had a few months of “soak time” in production environments. You can trust that we’ll work hard to prevent any breaking changes to packages in the stable repository (e.g. changing interfaces). As such, we can’t always put the latest features into our current stable release. Packages in our testing repository are recommended for people who want more features. Our testing packages pass unit and functional tests but will not have the same “soak time” as our stable packages. A testing release represents a work in progress that will eventually be promoted to stable. For example, CDH2 is our current testing release. Over the next few months as our customers and community report their experiences with CDH2, we’ll work to craft it into a stable release. You can expect a new stable and testing release about every quarter. This doesn�t mean, however, that we�ll immediately discontinue support for previous stable releases.� We’ll support a stable release for at least one year after an alternative stable release is available. Here is a table showing our current repository state: Repository CDH Release Released Patched Source Apt Repository Yum Repository Stable CDH1 March 2009 /cdh/stable /debian /redhat/cdh/stable Testing CDH2 August 2009 /cdh/testing /debian /redhat/cdh/testing From CDH2 forward we’ll be using an improved version syntax which will allow package managers like yum and apt-get to correctly manage your package updates depending on whether you’ve subscribed to the stable and testing repositories. Here is a table with some examples to help familiarize you with the new version syntax: Full Package Version Component Branch Base Version Patch Level hadoop-0.18-0.18.3+76 hadoop 0.18 0.18.3 76 hadoop-0.20-0.20.0+45 hadoop 0.20 0.20.0 45 hadoop-0.18-0.18.3+76.3 hadoop 0.18 0.18.3 76.3 pig-0.4.0+14 pig - 0.4.0 14 The dot releases in the patch level, e.g. 76.3, are use to ensure software continuity as packages are promoted from testing to stable.� Once a package is promoted, the major patch level will never change. Pig and Hive There are currently no packages for pig and hive in CDH2 but we’re working hard to remedy this situation. Two Cloudera team members worked with the community to help make Pig and Hive work with Hadoop 0.20.0. Todd Lipcon submitted a patch for HIVE-487 which has been committed to trunk and will be included in Hive 0.4.0. Dmitriy Ryaboy opened PIG-924 and submitted a patch which provided “shims” to allow Pig 0.4 to run on Hadoop 0.20.0. This work will allow us to create packages over the next week or two for CDH2. Learning more This blog post really only touched on the highlights of the CDH2 release.� If you’re interest in learning more details, visit our Software Archive.� We’ve unified our documentation into a single manual with a clean style to make it easier for you to find the answers you are looking for. As always, we look forward to any feedback you might have regarding CDH2.</snippet></document><document id="703"><title>Hadoop World: NYC 2009: Speakers Announced</title><url>http://blog.cloudera.com/blog/2009/09/hadoop-world-nyc-2009-speakers-announced/</url><snippet>It’s been a crazy few weeks here at Cloudera, and while there is no sign of things letting up before Hadoop World: NYC 2009 on October 2nd, we wanted to take a minute to share the latest details about the speakers, and to say thanks to our sponsors who have recently come on board. We’re absolutely thrilled to have such a wide variety of organizations sharing their experiences with Apache Hadoop. In addition to a deeply technical track headlined by Cloudera, Yahoo! and Facebook, those new to Hadoop will appreciate an entire track focused on applications. There is also a track for extensions to highlight cool projects like HadoopDB, a mashup of Hadoop and a relational database, from Yale University. We’re excited about every talk on the schedule, but we wanted to call out just a few that highlight how Hadoop is being used by more traditional enterprise users beyond the web space. Hadoop Applications at Yahoo!, Eric Baldeschwieler, Yahoo! Rethinking the Data Warehouse with Hadoop and Hive, Ashish Thusoo, Facebook Large Scale Transaction Analysis, Joe Cunningham, VISA Enabling Ad-hoc Analytics at Web Scale, Rod Smith, IBM Cross Data Center Logs Analysis, Stu Hood, Rackspace Data Processing for Financial Services, Peter Krey and Sih Lee, JPMorganChase Protein Alignment, Aaron Cordova, Booz Allen Hamilton Analytics and Reporting at Scale, Gyanit Singh, eBay A Data Mining Platform for the Telecom Industry, Feng Cao, China Mobile There are over 20 additional talks across the tracks. Whatever your interest, Hadoop World has something for you. You can find more details and register on the conference website. In addition to these scheduled talks, we’ll host a Hadoop Futures Panel as well as a BOF session to explore some of the emerging projects that focus on Hadoop user interfaces. Lastly, we’ve gotten a lot of sponsorship interest since the first announcement, and we’re glad to see so many companies support Hadoop in this way. A big thank you to the following companies for joining our premium sponsorship tier: Amazon.com, Facebook, Rackspace, Softlayer and Supermicro. You can see the full list on the Hadoop World website.</snippet></document><document id="704"><title>Hadoop World: NYC 2009</title><url>http://blog.cloudera.com/blog/2009/08/hadoop-world-nyc-2009/</url><snippet>To say we were surprised by the quality and quantity of submissions we received for Hadoop World: NYC 2009 would be an understatement. We were amazed at how many “normal” companies have come to use Hadoop for everything ranging from business intelligence to protein alignment. It’s truly exciting to see how a system originally designed to process and index the web has evolved to support the data-driven workloads of so many industries. It’s with great joy that we invite you to come learn about what the following companies have done with Hadoop: About.com, Booz Allen Hamilton, China Mobile, ContextWeb, eBay, Facebook, IBM, Intel, JPMC, Microsoft, The New York Times, NexR, Rackspace, Vertica, Visa, Visible Measures, Yale, and Yahoo! If you have ever wondered what Hadoop might be able to do for you, this is your chance to learn  both from leaders in the webspace and within your own industry. We have posted the titles of talks on three separate tracks: Applications, Development and Administration, and Extensions. We’re still working out a few details, but we will host keynotes from Cloudera, Yahoo!, IBM, and Facebook, as well as BOF sessions to dig deeper into topics that are not addressed effectively with traditional talk formats. We’re also excited to offer three days of training with separate tracks for Developers, Administrators and Managers, plus certification for those who are interested. You can see the schedule and register here. As a special bonus, everyone who registers before September 21st will receive a free copy of O’Reilly’s Hadoop: The Definitive Guide, written by Cloudera’s own Tom White. Lastly, a big thanks to everyone who has helped make this happen, especially our premium sponsors: Yahoo!, IBM, Intel, and eHarmony.</snippet></document><document id="705"><title>Hadoop Default Ports Quick Reference</title><url>http://blog.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/</url><snippet>Editor’s note (Oct. 3, 2013): The information below has become quite dated since its original publication. We recommend that you consult this documentation for ports info instead. Is it 50030 or 50300 for that JobTracker UI? I can never remember! Hadoop’s daemons expose a handful of ports over TCP. Some of these ports are used by Hadoop’s daemons to communicate amongst themselves (to schedule jobs, replicate blocks, etc.). Others ports are listening directly to users, either via an interposed Java client, which communicates via internal protocols, or via plain old HTTP. This post summarizes the ports that Hadoop uses; it’s intended to be a quick reference guide both for users, who struggle with remembering the correct port number, and systems administrators, who need to configure firewalls accordingly. Web UIs for the Common User The default Hadoop ports are as follows: � Daemon Default Port Configuration Parameter HDFS Namenode 50070 dfs.http.address Datanodes 50075 dfs.datanode.http.address Secondarynamenode 50090 dfs.secondary.http.address Backup/Checkpoint node? 50105 dfs.backup.http.address MR Jobracker 50030 mapred.job.tracker.http.address Tasktrackers 50060 mapred.task.tracker.http.address ? Replaces secondarynamenode in 0.21. Hadoop daemons expose some information over HTTP. All Hadoop daemons expose the following: /logs Exposes, for downloading, log files in the Java system property hadoop.log.dir. /logLevel Allows you to dial up or down log4j logging levels. This is similar to hadoop daemonlog on the command line. /stacks Stack traces for all threads. Useful for debugging. /metrics Metrics for the server. Use /metrics?format=json to retrieve the data in a structured form. Available in 0.21. Individual daemons expose extra daemon-specific endpoints as well. Note that these are not necessarily part of Hadoop’s public API, so they tend to change over time. The Namenode exposes: / Shows information about the namenode as well as the HDFS. There’s a link from here to browse the filesystem, as well. /dfsnodelist.jsp?whatNodes=(DEAD|LIVE) Shows lists of nodes that are disconnected from (DEAD) or connected to (LIVE) the namenode. /fsck Runs the “fsck” command. Not recommended on a busy cluster. /listPaths Returns an XML-formatted directory listing. This is useful if you wish (for example) to poll HDFS to see if a file exists. The URL can include a path (e.g., /listPaths/user/philip) and can take optional GET arguments: /listPaths?recursive=yes will return all files on the file system; /listPaths/user/philip?filter=s.* will return all files in the home directory that start with s; and /listPaths/user/philip?exclude=.txt will return all files except text files in the home directory. Beware that filter and exclude operate on the directory listed in the URL, and they ignore the recursive flag. /data and /fileChecksum These forward your HTTP request to an appropriate datanode, which in turn returns the data or the checksum. Datanodes expose the following: /browseBlock.jsp, /browseDirectory.jsp, tail.jsp, /streamFile, /getFileChecksum These are the endpoints that the namenode redirects to when you are browsing filesystem content. You probably wouldn’t use these directly, but this is what’s going on underneath. /blockScannerReport Every datanode verifies its blocks at configurable intervals. This endpoint provides a listing of that check. The secondarynamenode exposes a simple status page with information including which namenode it’s talking to, when the last checkpoint was, how big it was, and which directories it’s using. The jobtracker‘s UI is commonly used to look at running jobs, and, especially, to find the causes of failed jobs. The UI is best browsed starting at /jobtracker.jsp. There are over a dozen related pages providing details on tasks, history, scheduling queues, jobs, etc. Tasktrackers have a simple page (/tasktracker.jsp), which shows running tasks. They also expose /taskLog?taskid=&lt;id&gt; to query logs for a specific task. They use /mapOutput to serve the output of map tasks to reducers, but this is an internal API. Under the Covers for the Developer and the System Administrator Internally, Hadoop mostly uses Hadoop IPC to communicate amongst servers. (Part of the goal of the Apache Avro project is to replace Hadoop IPC with something that is easier to evolve and more language-agnostic; HADOOP-6170 is the relevant ticket.) Hadoop also uses HTTP (for the secondarynamenode communicating with the namenode and for the tasktrackers serving map outputs to the reducers) and a raw network socket protocol (for datanodes copying around data). The following table presents the ports and protocols (including the relevant Java class) that Hadoop uses. This table does not include the HTTP ports mentioned above. Daemon Default Port Configuration Parameter Protocol Used for Namenode 8020 fs.default.name? IPC: ClientProtocol Filesystem metadata operations. Datanode 50010 dfs.datanode.address Custom Hadoop Xceiver: DataNode and DFSClient DFS data transfer Datanode 50020 dfs.datanode.ipc.address IPC: InterDatanodeProtocol, ClientDatanodeProtocol ClientProtocol Block metadata operations and recovery Backupnode 50100 dfs.backup.address Same as namenode HDFS Metadata Operations Jobtracker Ill-defined.? mapred.job.tracker IPC: JobSubmissionProtocol, InterTrackerProtocol Job submission, task tracker heartbeats. Tasktracker 127.0.0.1:0� mapred.task.tracker.report.address IPC: TaskUmbilicalProtocol Communicating with child jobs ? This is the port part of hdfs://host:8020/. ? Default is not well-defined. Common values are 8021, 9001, or 8012. See MAPREDUCE-566. � Binds to an unused local port. That’s quite a few ports! I hope this quick overview has been helpful.</snippet></document><document id="706"><title>Doug Cutting joins Cloudera</title><url>http://blog.cloudera.com/blog/2009/08/doug-cutting-joins-cloudera/</url><snippet>Back in October, I promised to keep marketing and sales out of this blog. We wanted to concentrate on technical topics and to choose signal over noise. Mostly, that’s meant that I let other people do the writing. I’m breaking that habit today so that I can announce — with great pleasure! — that Doug Cutting, co-founder of the Apache Hadoop project and creator of Nutch and Lucene, has agreed to join Cloudera beginning on September 1, 2009. Doug’s contributions to Hadoop over the years have been considerable. With Yahoo!’s backing, he split the data-parallel processing engine out of the Nutch crawler to create the Hadoop project. He’s remained an active contributor and commentator, providing guidance and advice to the growing community of Hadoop users and developers. Doug will join his project co-founder, Mike Cafarella, at Cloudera. Mike has a full-time appointment as a professor of Computer Science at the University of Michigan beginning in December. In the meantime, and part-time after he starts his academic work, Mike will be working as a consultant for us here. In the near term, we expect no change to the specific project work Doug is doing. Cloudera is excited about Avro. We intend to continue to contribute to Hadoop and related projects, and we all expect that Doug will be a critical part of both our thinking and our activity there. Besides, of course, we’re very pleased to add such a capable and experienced systems engineer to our team. We look forward to welcoming Doug on September 1! —Mike Olson, CEO, Cloudera</snippet></document><document id="707"><title>Tracking Trends with Hadoop and Hive on EC2</title><url>http://blog.cloudera.com/blog/2009/07/tracking-trends-with-hadoop-and-hive-on-ec2/</url><snippet>At Cloudera, we frequently work with leading Hadoop developers to produce guest blog posts of general interest to the community. We started a project with Pete Skomoroch a while back, and we were so impressed with his work, we’ve decided to bring Pete on as a regular guest blogger. Pete can show you how to do some pretty amazing things with Hadoop, Pig and Hive and has a particular bias towards Amazon EC2. With that, I’m happy to welcome Pete to the blog, and hope you enjoy his first post as much as we did. -Christophe Trendingtopics.org was built by Data Wrangling to demonstrate how Hadoop and Amazon EC2 can be used with Rails to power a data-driven website.  This post will give an overview of how trendingtopics.org was put together and show some basic approaches for finding trends in log data with Hive.  The source code for trendingtopics is available on Github and a tutorial is provided on the Cloudera site which describes many of the data processing steps in greater detail. The trendingtopics Rails application identifies recent trends on the web by periodically launching an EC2 cluster running Cloudera’s Distribution for Hadoop to process Wikipedia log files.  The cluster runs a Hive batch job that analyzes hourly pageview statistics for millions of Wikipedia articles, and then loads the resulting trend parameters into the application’s MySQL database. Application Features Ranked list of the most significant trends over the last 30 days along with total pageviews Ranked list of rising articles that have been trending in the last 24 hours Daily time series charts and sparklines for over 2.5 million Wikipedia articles Autocomplete functionality and search results ranked by article trend score Category based trends for People or Companies How Hadoop is Used in the Application Cleaning raw log data and joining article title strings with Wikipedia page IDs Aggregating hourly time series data for daily pageview charts and sparklines Running periodic trend estimation jobs and regressions Generating statistics to power search autocomplete and the ranking of search results Preprocessing the Raw Data The daily pageview charts on the site were created by running an initial MapReduce job on 1TB of hourly traffic logs collected from Wikipedia’s squid proxy by Domas Mituzas.  We made the first seven months of this hourly log data available as an Amazon Public Data Set which covers the period from October 1, 2008 to April 30, 2009.  We run a cron job on the trendingtopics.org server to fetch the latest log files every hour and store a copy on Amazon S3 for processing by Hadoop. The log files are named with the date and time of collection. Individual hourly files are around 55 MB when compressed, so eight months of compressed data takes up about 300 GB of space. Each line in a pagecount file has four fields: projectcode, pagename, pageviews, and bytes: $ grep ‘^en Barack’ pagecounts-20090521-100001 en Barack 8 1240112 en Barack%20Obama 1 1167 en Barack_H._Obama 1 142802 en Barack_H_Obama 3 428946 en Barack_H_Obama_Jr. 2 285780 en Barack_Hussein_Obama,_Junior 2 285606 en Barack_O%27Bama 1 142796 en Barack_Obama 701 139248439 en Barack_Obama%27s_first_100_days 2 143181 en Barack_Obama,_Jr 2 285755 Many records in the log file are actually Wikipedia redirects that point to other articles in the Wikipedia “Pages” table. As part of our Hadoop processing, we clean the page names and perform a join in Hive against the contents of a MySQL redirect table to find the true Wikipedia page_id for each page title. Aggregating the Hourly Data For the initial log file text normalization and filtering, we used a simple Hadoop Streaming job with Python.  The first MapReduce pass restricts pageviews to a subset of English Wikipedia pages, filters out bad records, and then sums hourly pageviews keyed by article-date. You will notice that the date is not actually in the raw log data itself, but is part of the filename. We can access this parameter in our streaming script using a Hadoop environment variable. You could fetch this job parameter in Java via job.get(“mapred.input.file”), but Hadoop also makes it available as an environment variable directly accessible to streaming jobs. For example, in Python you can access the file name as follows: filepath = os.environ["map_input_file"] filename = os.path.split(filepath)[-1] Our second MapReduce pass maps the daily aggregations by article name: Barack_Obama 20090422 129 Barack_Obama 20090419 143 Barack_Obama 20090421 163 Barack_Obama 20090420 152 These records are merged at the reducers to generate a daily time series for each article in serialized JSON format. Format: article\tdates\tpagecounts\ttotal_pageviews ‘Barack_Obama\t[20090419,20090420,20090421,20090422]\t[143,152,163,129]\t587′ These records are joined with the Wikipedia page ID using Hive, and the resulting output is loaded into MySQL where it is indexed for fast lookups by the web application.  For the first version of the site, we chose a simple key-value store approach where the time series for a single article is stored in JSON  format.   This format is simple to work with using R or Python and can be loaded quickly by the dynamic charts on the website. Daily Trend Estimation After the historical timeline aggregation was complete, we began running a daily batch job in Hive on EC2 to aggregate recent log data and detect topics trending over the previous 24 hours.  This MapReduce job only operates on the last 30 days of data for trend estimation, so it is less resource intensive than a pass over the full Wikipedia log dataset. For MapReduce operations that go beyond the basic cleaning and aggregation of data, using higher level tools, such as Hive, can accelerate the development process. Hive supports a range of SQL-like operators, random sampling, bucketing of tables, and calling of external functions through custom MapReduce scripts. To find trending articles, we load the latest timeline data into Hive and run a HiveQL query that applies a simple Python trend estimation algorithm to all 2.5 Million Wikipedia articles: hive&gt; INSERT OVERWRITE TABLE daily_trends
      SELECT u.page_id, u.daily_trend, u.error
        FROM ( FROM daily_timelines ndt
               MAP ndt.page_id, ndt.dates, ndt.pageviews, ndt.total_pageviews
               USING 'python hive_trend_mapper.py'
               AS page_id, daily_trend, error) u; The file hive_trend_mapper.py looks something like this: #!/usr/bin/env python
# encoding: utf-8

import sys, os, re
from math import log, sqrt
import simplejson

def calc_daily_trend(dates, pageviews, total_pageviews):
  # pageviews for most recent day
  y2 = pageviews[-1]
  # pageviews for previous day
  y1 = pageviews[-2]
  # Simple baseline trend algorithm
  slope = y2 - y1
  trend = slope  * log(1.0 +int(total_pageviews))
  error = 1.0/sqrt(int(total_pageviews))
  return trend, error  

for line in sys.stdin:
  (page_id, dates, pageviews, total_pageviews) = line.strip().split("\t")
  dates = simplejson.loads(dates)
  pageviews = simplejson.loads(pageviews)
  try:
    daily_trend, error = calc_daily_trend(dates, pageviews, total_pageviews)
  except:
    # skip bad rows
    daily_trend = 0
    error = 0
  sys.stdout.write('%s\t%s\t%s\n' % (page_id, daily_trend, error)) The nice thing about this approach is that analysts who use Python or R can easily substitute more advanced algorithms by replacing the calc_daily_trend method.  We describe the trend estimation process in more detail in the tutorial on building data intensive web applications with Hadoop. Running the Batch Jobs with Hadoop and Hive on EC2 One of the advantages of using the Cloudera Distribution for Hadoop on EC2 is that it gives you full control over the configuration of your Hadoop cluster and operating system environment.  To run batch jobs for trendingtopics.org, we use the Cloudera command line tools within cron or Rake tasks to launch Hadoop clusters on EC2: ./hadoop-ec2 launch-cluster –user-packages ‘r-base python-simplejson git-core s3cmd’ my-hadoop-cluster 10 By using a combination of command line options and simple configuration files, you can install packages and modify settings to meet the needs of your specific MapReduce jobs.  If you have any additional commands you want to run immediately after your Hadoop EC2 cluster launches, then you can add them to the bottom of the file hadoop-ec2-init-remote.sh. For trendingtopics, we added a few lines of code to fetch the trendingtopics source code from GitHub and start our daily batch job: function run_trendingtopics_batch() {
  MYBUCKET=trendingtopics
  MYSERVER=db.trendingtopics.org
  MAILTO=pete@datawrangling.com
  # set the default number of reducers using the following formula:
  # number of concurrent reducers per node * number of nodes * 1.75
  # for 10 c1.medium instances = 2 * 10 * 1.75 = 35
  NUMREDUCERS=35 

  cd /mnt
  # git clone the trendingtopics code
  git clone git://github.com/datawrangling/trendingtopics.git
  cd trendingtopics
  git checkout --track -b experimental origin/experimental
  cd ../

  bash trendingtopics/lib/scripts/run_daily_merge.sh \
 $MYBUCKET $MYSERVER $MAILTO $NUMREDUCERS
  # tail -f /var/log/syslog to see progress

}

# We want our cluster nodes to self terminate
# if the job takes longer than 108 minutes
shutdown -h +108 &gt;/dev/null &amp;

install_user_packages
install_hadoop
configure_hadoop

if $IS_MASTER ; then
  setup_web
  start_hadoop_master
  run_trendingtopics_batch
else
  start_hadoop_slave
fi Running the Rails Application If you want to explore the application further, you can on check out the code on Github. Note that the latest version of the trending topics code is in the “experimental” branch. Dependencies for local development: Ruby (1.8.7) Ruby Gems (1.3.1) Capistrano (v2.5.5) Rails (2.3.2) Running locally in development mode: Fetch the trending topics source code: git clone git://github.com/datawrangling/trendingtopics.git Navigate to the root of the source code directory and create the necessary configuration files from the provided examples: $ cd trendingtopics $ cp config/config.yml.example config/config.yml $ cp config/database.yml.example config/database.yml Run the normal Rails gem installations to get any missing dependencies: $ rake gems:install Note that we also use the following plugins (already included in /vendor): autocomplete annotated-timeline gc4r (modified a bit for custom sparklines) Create the database: $ rake db:create $ rake db:migrate Populate the app with 2 months of demo data from 100 wiki articles: $ rake db:develop Launch the Rails app itself: $ script/server =&gt; Booting Mongrel =&gt; Rails 2.3.2 application starting on http://0.0.0.0:3000 =&gt; Call with -d to detach =&gt; Ctrl-C to shutdown server Navigate to http://localhost:3000/ to access the application. Deploying the Rails App to EC2 The Rails app and MySQL database are deployed to Amazon EC2 using Paul Dowman’s EC2 on Rails.  To deploy the application to EC2, first install the EC2 on Rails gem as described at http://ec2onrails.rubyforge.org/: $ sudo gem install ec2onrails Find AMI ID of the latest 32-bit EC2 on Rails image (in our case this was ami-5394733a): $ cap ec2onrails:ami_ids Launch an instance of the latest EC2 on Rails AMI and note the returned instance address from ec2-describe-instances. It will be something like ec2-12-xx-xx-xx.z-1.compute-1.amazonaws.com. $ ec2-run-instances ami-5394733a -k gsg-keypair $ ec2-describe-instances Fetch the trendingtopics source code from Github as shown in the previous section. Create the necessary configuration files from the examples provided and edit them, filling in your EC2 instance address information, keypairs, and other configuration details as indicated in the comments of each file. See the ec2onrails documentation or source code for more details on each setting. $ cp config/deploy.rb.example config/deploy.rb $ cp config/s3.yml.example config/s3.yml $ cp config/config.yml.example config/config.yml $ cp config/database.yml.example config/database.yml Be sure to substitute your own AWS key and secret key in both config.yml and s3.yml (you can leave these out of s3.yml and ec2onrails will still work – it just won’t back up MySQL or the log files). aws_secret_access_key: YYVUYVIUBIBI aws_access_key_id: BBKBBOUjbkj/BBOUBOBJKBjbjbboubuBUB Deploy the app to your launched EC2 instance with Capistrano (this will take several minutes). $ cap ec2onrails:setup $ cap deploy:cold You should now be able to access your app from a web browser or as a web service at the URL of the instance you provided in deploy.rb: http://ec2-12-xx-xx-xx.z-1.compute-1.amazonaws.com. You can also SSH into your running EC2 instance as usual with your keypairs to debug any issues. See the EC2 on Rails forums for more help with debugging. To redeploy the app after making changes to the base trending topics code, just do the usual cap deploy: $ cap deploy No data will be populated in the production-deployed app until you run the included Hadoop jobs and import the resulting data to MySQL. To test the deployment, you can use Capistrano to run the db:develop task on the EC2 server; just wipe the dev data before loading real production data. Possible Next Steps with Hadoop and Trending Topics Implement alternate trend-detection algorithms Use the Cloudera EBS (beta) to persist the timeline data in HDFS between runs, appending each hour of data to partitioned Hive tables Explore using partitions with Hive for fast time window queries Generate smaller representative sample datasets for R&amp;D with Hive bucket sampling Use Wikipedia text content to find trends for individual words and phrases Use the Wikipedia link graph dataset to show related articles for each trend Find correlated Wikipedia articles based on page views</snippet></document><document id="708"><title>Advice on QA Testing Your MapReduce Jobs</title><url>http://blog.cloudera.com/blog/2009/07/advice-on-qa-testing-your-mapreduce-jobs/</url><snippet>As Hadoop adoption increases among organizations, companies, and individuals, and as it makes its way into production, testing MapReduce (MR) jobs becomes more and more important. By regularly running tests on your MR jobs–either invoked by developers before they commit a change or by a continuous integration server such as hudson–an engineering organization can catch bugs early, strive for quality, and make developing and maintaining MR jobs easier and faster. MR jobs are particularly difficult to test thoroughly because they run in a distributed environment.  This post will give specific advice on how an engineering team might QA test its MR jobs. Note that Chapter 5 of Hadoop: The Definitive Guide gives specific code examples for testing an MR job. As is the case with most testing scenarios, there are certain practices one can follow that have a low barrier to entry; such practices might do a fairly sufficient job of testing. There are also practices one can follow that are more complicated but perhaps result in more thorough testing. Let’s walk through some good QA practices, starting with the easiest and ending with the most complicated. Traditional Unit Tests – JUnit, PyUnit, Etc. Your MR job will probably have some functionality that can be tested in isolation using a unit-testing framework such as JUnit or PyUnit. For example, if your MR job does some document parsing in Java, your parse method can be tested using JUnit. Using a traditional unit-testing framework is perhaps the easiest way to get started testing your MR jobs for a few reasons. First, they are already used by a huge collection of developers. Second, they can be invoked by and integrated into most popular continuous integration servers. Finally, they are simple, effective, and don’t require Hadoop daemons to be running. Unit tests do a great job of testing each individual part of your MR job, but they do not test your MR job as a whole, and they do not test your MR job within Hadoop. To start using traditional unit tests to improve the quality of your MR jobs, simply write your map and reduce functions in such a way that functionality is extracted from these functions into “private” helper functions (or classes), which can be tested in isolation. For example, put all your parse code in a “parse” method. Then, choose a unit-testing framework that fits your use case. Most Python unit testing uses PyUnit, and most Java unit testing uses either JUnit or TestNG. Most popular programming languages have a standard unit-testing tool, so do some research to learn which framework is best for your needs. Finally, write tests in the framework of choice that thoroughly test each helper function you’ve defined for your map and reduce functions. Unit tests can then be run either on the local machine or on a continuous integration to ensure that the post conditions of the tested helper functions are met for a particular input. MRUnit – Unit Testing for MR Jobs MRUnit is a tool that was developed here at Cloudera and released back to the Apache Hadoop project. It can be used to unit-test map and reduce functions. MRUnit lets you define key-value pairs to be given to map and reduce functions, and it tests that the correct key-value pairs are emitted from each of these functions. MRUnit tests are similar to traditional unit tests in that they are simple, isolated, and don’t require Hadoop daemons to be running. Aaron Kimball has written a very detailed blog post about MRUnit here. Local Job Runner Testing – Running MR Jobs on a Single Machine in a Single JVM Traditional unit tests and MRUnit should do a fairly sufficient job detecting bugs early, but neither will test your MR jobs with Hadoop. The local job runner lets you run Hadoop on a local machine, in one JVM, making MR jobs a little easier to debug in the case of a job failing. To enable the local job runner, set “mapred.job.tracker” to “local” and “fs.default.name” to “file:///some/local/path” (these are the default values). Remember, there is no need to start any Hadoop daemons when using the local job runner. Running bin/hadoop will start a JVM and will run your job for you. Creating a new hadoop-local.xml file (or mapred-local.xml and hdfs-local.xml if you’re using 0.20) probably makes sense. You can then use the –config parameter to tell bin/hadoop which configuration directory to use. If you’d rather avoid fiddling with configuration files, you can create a class that implements Tool and uses ToolRunner, and then run this class with bin/hadoop jar foo.jar com.example.Bar -D mapred.job.tracker=local -D fs.default.name=file:/// (args), where Bar is the Tool implementation. To start using the local job runner to test your MR jobs in Hadoop, create a new configuration directory that is local job runner enabled and invoke your job as you normally would, remembering to include the –config parameter, which points to a directory containing your local configuration files. The -conf parameter also works in 0.18.3 and lets you specify your hadoop-local.xml file instead of specifying a directory with –config. Hadoop will run the job happily. The difficulty with this form of testing is verifying that the job ran correctly. Note: you’ll have to ensure that input files are set up correctly and output directories don’t exist before running the job. Assuming you’ve managed to configure the local job runner and get a job running, you’ll have to verify that your job completed correctly. Simply basing success on exit codes isn’t quite good enough. At the very least, you’ll want to verify that the output of your job is correct. You may also want to scan the output of bin/hadoop for exceptions. You should create a script or unit test that sets up preconditions, runs the job, diffs actual output and expected output, and scans for raised exceptions. This script or unit test can then exit with the appropriate status and output specific messages explaining how the job failed. Note that the local job runner has a couple of limitations: only one reducer is supported, and the DistributedCache doesn’t work (a fix is in progress). Pseudo-distributed Testing – Running MR Jobs on a Single Machine Using Daemons The local job runner lets you run your job in a single thread. Running an MR job in a single thread is useful for debugging, but it doesn’t properly simulate a real cluster with several Hadoop daemons running (e.g., NameNode, DataNode, TaskTracker, JobTracker, SecondaryNameNode). A pseudo-distributed cluster is composed of a single machine running all Hadoop daemons. This cluster is still relatively easy to manage (though harder than local job runner) and tests integration with Hadoop better than the local job runner does. To start using a pseudo-distributed cluster to test your MR jobs in Hadoop, follow the aforementioned advice for using the local job runner, but in your precondition setup include the configuration and start-up of all Hadoop daemons. Then, to start your job, just use bin/hadoop as you would normally. Full Integration Testing – Running MR Jobs on a QA Cluster Probably the most thorough yet most cumbersome mechanism for testing your MR jobs is to run them on a QA cluster composed of at least a few machines. By running your MR jobs on a QA cluster, you’ll be testing all aspects of both your job and its integration with Hadoop. Running your jobs on a QA cluster has many of the same issues as the local job runner. Namely, you’ll have to check the output of your job for correctness. You may also want to scan the stdin and stdout produced by each task attempt, which will require collecting these logs to a central place and grepping them. Scribe is a useful tool for collecting logs, though it may be superfluous depending on your QA cluster. We find that most of our customers have some sort of QA or development cluster where they can deploy and test new jobs, try out newer versions of Hadoop, and practice upgrading clusters from one version of Hadoop to another. If Hadoop is a major part of your production pipeline, then creating a QA or development cluster makes a lot of sense, and repeatedly running jobs on it will ensure that changes to your jobs continue to get tested thoroughly. EC2 may be a good host for your QA cluster, as you can bring it up and down on demand. Take a look at our beta EC2 EBS Hadoop scripts if you’re interested in creating a QA cluster in EC2. You should choose QA practices based on the importance of QA for your organization and also on the amount of resources you have. Simply using a traditional unit-testing framework, MRUnit and the local job runner can test your MR jobs thoroughly in a simple way without using too many resources. However, running your jobs on a QA or development cluster is naturally the best way to fully test your MR jobs with the expenses and operational tasks of a Hadoop cluster. Do you have any helpful advice on beneficial QA practices for MR jobs?  Leave a comment :).</snippet></document><document id="709"><title>Running the Cloudera Training VM in VirtualBox</title><url>http://blog.cloudera.com/blog/2009/07/cloudera-training-vm-virtualbox/</url><snippet>Update (May 1 2013): The post below, which is based on an outdated VM, is deprecated. Rather please see the Cloudera QuickStart VM, which runs on VirtualBox, VMware, and KVM. Cloudera’s Training VM is one of the most popular resources on our website. It was created with VMware Workstation, and plays nicely with the VMware Player for Windows, Linux, and Mac. But VMware isn’t for everyone. Thomas Lockney has managed to get our VM image running on Virtual Box, and has written a step-by-step guide for the community. Thanks Thomas! – Christophe I was quite pleased when I discovered that Cloudera had created a virtual machine image that could be used while working through their training material. It would make the process simpler, and it looked like a potentially useful environment for general Hadoop experimentation. However, their VM is built for VMware, which I stopped using a while back. However, as a heavy VirtualBox user, I knew that it would not be hard to get it running in my preferred desktop virtualization environment. Here’s a step-by-step guide for getting Cloudera’s virtual machine image up and running. I’ll include screenshots for most of the steps to make it as clear as possible. I’ll assume you already have at least some familiarity with running VirtualBox (if not, there are plenty of good tutorials and references available online) and some experience with Ubuntu or some other fairly modern Linux desktop system. 1. The first step is to download the virtual machine from the�Hadoop Training Virtual Machine page on Cloudera’s site. The version at the time of this writing is 3.1, and the filename you’ll end up with is cloudera-training-0.3.1.tar.bz2. Once you have downloaded the file (this may take a little while — it’s quite large), decompress it somewhere useful. On a Unix-based machine (e.g., Linux or OS X), you can do this by running the following command: tar xjf cloudera-training-0.3.1.tar.bz2 2. Next, start up VirtualBox. Once it loads, go to the File menu and select Virtual Media Manager. 3. The Virtual Media Manager is where you set up new drive images. An image needs to be created before you can use it with a virtual machine. In this case, you’re creating a new image by pointing to the existing image, which was supplied with the Cloudera VM download. It’s a VMware image (a .vmdk file), which VirtualBox can read. In the Virtual Media Manager window, click New to create a new image. 4. In the file dialog box that appears, browse to the directory where you extracted the download and select the file cloudera-training-0.2-cl3.vmdk. Please note that this name will likely change with later releases, so you might need to experiment to find the right file. If that is the case, you’ll be looking for files ending in .vmdk. Note that files with the s00# names are generally either snapshots or extensions to the base drive image (you can choose to have the image split up into multiple files). 5. After closing the Virtual Media Manager window, click the New button in the main VirtualBox window to create a new virtual machine. 6. From the Create New Virtual Machine dialog box, give your new machine a name. Select Linux as the operating system and Ubuntu as the version. 7. On the next screen, set the memory size. The VMware image that Cloudera created has 1024 MB assigned, but I’ve found I can get away with less for basic needs. If you plan to do full development in this VM, set it higher (if you have the space to spare). 8. Next, you’ll select the hard disk image, which we added earlier. 9. Double check the summary before clicking Finish. 10. After closing the Virtual Machine Wizard, you can select the Cloudera machine that you just created and click Start. 11. Assuming you’ve done everything correctly up to this point and your VirtualBox installation is working properly, you should see a window pop up with the boot-up messages for the new virtual machine. Watch this to make sure everything is booting fine. If you see error messages here or if your machine doesn’t boot up correctly, you may have missed a step earlier or selected the wrong file for the hard disk image. 12. After a few moments, you should see the desktop of your new image. If you’ve gotten this far, you can stop here if you want, but you’ll be missing out on the enhanced functionality that VirtualBox offers, such as better integration with your existing desktop, sharing of files, etc. 13. If you want full integration, open a terminal and run the following command: sudo apt-get install build-essential linux-headers-`uname -r` This will install the basics that you need before loading the VirtualBox additions. 14. Select Install Guest Additions from the Devices menu. 15. You should now see a pop-up window prompting you to run the installer for the guest additions. Click the Run button to continue. 16. If the dependencies installed correctly earlier, you’ll see a terminal window, which will show you the progress as the add-ons are installed. 17. At this point, you can select Shutdown from the system menu in the top menu bar, and then choose Restart to reboot your virtual machine. When the VM restarts and the desktop is fully loaded, you should be able to resize the window, use your mouse seamlessly between the virtual machine window and your desktop, and add a shared folder (see the VirtualBox documentation for instructions on this). One last thing: there is a call at the very end of /etc/init.d/rc.local to /usr/bin/vmware-user that you might want to remove. It won’t hurt anything if you leave it there, but you will occasionally see error messages at startup or shutdown due to its presence. I finally hunted it down just now after running this VM for a while, so it’s really not a big deal. That’s all folks!</snippet></document><document id="710"><title>Apache Hadoop HA Configuration</title><url>http://blog.cloudera.com/blog/2009/07/hadoop-ha-configuration/</url><snippet>Disclaimer: Cloudera no longer approves of the recommendations in this post. Please see this documentation for configuration recommendations. One of the things we get a lot of questions about is how to make Hadoop highly available. There is still a lot of work to be done on this front, but we wanted to take a moment and share the best practices from one of our customers. Check out what Paul George has to say about how they keep thier NameNode up at ContextWeb. – Christophe Here at ContextWeb, our Apache Hadoop infrastructure has become a critical part of our day-to-day business operations. As such, it was important for us to find a way to resolve the single-point-of-failure issue that surrounds the master node processes, namely the NameNode and JobTracker. While it was easy for us to follow the best practice of offloading the secondary NameNode data to an NFS mount to protect metadata, ensuring that the processes were constantly available for job execution and data retrieval were of greater importance.�We’ve leveraged some existing, well tested components that are available and commonly used in Linux systems today. Our solution primarily makes use of DRBD from LINBIT and Heartbeat from the Linux-HA project. The natural combination of these two projects provides us with a reliable and highly available solution, which addresses limitations that currently exist. While one could conceivably expand the use of these two projects to much deeper levels of protection, the goal of this post is to provide a basic working configuration as a starting point for further experimentation and tuning. There may be variations with regards to what works with your distribution or which requirements your organization has for SLAs and HA standards.�These instructions are most relevant to CentOS 5.3 combined with Cloudera’s Distribution for Hadoop, since that’s what we run in our production environment. Hadoop Environment Each of our master nodes has the following hardware specifications: Dell PowerEdge 1950, 2x Quad Core Intel Xeon E5405 CPUs @ 2.0GHz, 16GB RAM, 2x 300GB 15k RPM SAS disks, RAID 1.�100GB of the available drive space is reserved for the DRBD volume, which will contain Hadoop’s data. We use RAID and redundant hardware capabilities of the servers wherever possible to provide additional security for the master node processes.�Each master node is connected to a different switch on our network using multiple network connections. The following diagram represents our production cluster configuration: HA Setup and Configuration Our planned hosts: Node Hostname IP Address 1 master1.domain.com 192.168.4.116 2 master2.domain.com 192.168.4.117 Virtual hadoop.domain.com 192.168.4.115 Properties that will be defined as part of our hadoop-site.xml: Property Value Notes dfs.data.dir /hadoop/hdfs/data On DRBD replicated volume dfs.name.dir /hadoop/hdfs/namenode On DRBD replicated volume fs.checkpoint.dir /mnt/hadoop/secondarynamenode On NFS fs.default.name hdfs://hadoop.contextweb.prod:8020 Shared virtual name mapred.job.tracker hadoop.contextweb.prod:8021 Shared virtual name With this environment in mind, configuring the HA setup comes down to six parts: Install Sun JDK 6 Configure networking Install DRBD and Heartbeat packages Configure DRBD Install Hadoop RPMs from Cloudera Configure Heartbeat ** Except where noted, all procedures should be followed on both nodes. ** 1. Install Sun JDK The only version of Java JDK that should be used with Hadoop is Sun’s own. This has been well documented. Grab a copy of the latest JDK from http://java.sun.com/javase/downloads/index.jsp and install it on both of the nodes. At the time of this writing, the latest version is jdk-6u14-linux-amd64. We download the rpm.bin file and complete the installation: [root@master1 ~]# chmod +x jdk-6u14-linux-x64-rpm.bin [root@master1 ~]# ./jdk-6u14-linux-x64-rpm.bin 2. Configure Networking Each of our servers has two embedded gigabit ethernet ports, and we choose to bond them for HA and bandwidth purposes. We use LACP/802.3ad, which also requires changes to the switch configuration to support this mode. If you don’t have LACP enabled switches, or cannot modify their configurations, there are other bonding options available through the driver. You can read more about Linux network bonding from /usr/share/doc/kernel-doc-2.6.18/Documentation/networking/bonding.txt on your system (requires installation of the package kernel-doc). The following is an example from our systems. Edit the file /etc/modprobe.conf: #/etc/modprobe.conf alias eth0 bnx2 alias eth1 bnx2 alias bond0 bonding options bond0 mode=4 miimon=100 lacp_rate=1 Edit the file /etc/sysconfig/network-scripts/ifcfg-eth0: #/etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0 MASTER=bond0 SLAVE=yes BOOTPROTO=static DHCPCLASS= ONBOOT=yes Edit the file /etc/sysconfig/network-scripts/ifcfg-eth1: #/etc/sysconfig/network-scripts/ifcfg-eth1 DEVICE=eth1 MASTER=bond0 SLAVE=yes BOOTPROTO=static DHCPCLASS= ONBOOT=yes Edit the file /etc/sysconfig/network-scripts/ifcfg-bond0: #/etc/sysconfig/network-scripts/ifcfg-bond0 DEVICE=bond0 BOOTPROTO=static IPADDR=192.168.4.116 NETMASK=255.255.255.0 GATEWAY=192.168.4.1 BONDING_MODULE_OPTS=”mode=4 miimon=100 lacp_rate=1″ Finally, reboot the system or restart networking: [root@master1 ~]# service network restart We also mount the NFS export at this time. It will be used as the base directory for the secondary namenode: [root@master1 ~]# echo “filer:/hadoop /mnt/hadoop nfs rsize=65536,wsize=65536,intr,soft,bg 0 0″ &gt;&gt; /etc/fstab [root@master1 ~]# mkdir /mnt/hadoop [root@master1 ~]# mount /mnt/hadoop 3. Install DRBD and Heartbeat Packages DRBD (including its kernel module) and Heartbeat are part of the “extras” repository: [root@master1 ~]# yum -y install drbd82 kmod-drbd82 heartbeat [root@master1 ~]# chkconfig –add heartbeat Tip: Sometimes the installation of the Heartbeat package fails on the first try. Just try again; it may work for you the second time. 4. Configure DRBD Important Note: Before continuing with the DRBD configuration, I highly recommend reading through the documentation and reviewing examples to get a clear understanding of the architecture and intended goals: http://www.drbd.org/docs/about/. In our kickstart configuration, we reserved 100GB of space in the RAID set; this will be used for our Hadoop data. The following /etc/drbd.conf file is created on both nodes: # # drbd.conf example # global { usage-count no; } resource r0 { protocol C; syncer { rate 100M; } startup { wfc-timeout 0; degr-wfc-timeout 120; } on master1.domain.com { device /dev/drbd0; disk /dev/sda4; address 192.168.4.116:7788; meta-disk internal; } on master2.domain.com { device /dev/drbd0; disk /dev/sda4; address 192.168.4.117:7788; meta-disk internal; } } The following tasks are performed on BOTH nodes to set up the DRBD resource and start the DRBD service. (You should be prepared to perform these tasks at the same time, so that the peers locate each other when you start the DRBD service.) [root@master1 ~]# echo “/dev/drbd0 /hadoop � � ext3 � � defaults,noauto � � 0 0″ &gt;&gt; /etc/fstab [root@master1 ~]# mkdir /hadoop [root@master1 ~]# yes | drbdadm create-md r0 [root@master1 ~]# service drbd start Tip: If the destination partition or disk that you are using for your DRBD volume previously had a file system created on it, you may receive a warning like the following one: Command ‘drbdmeta /dev/drbd0 v08 /dev/sda4 internal create-md’ terminated with exit code 40 drbdadm aborting If this is the case, try zeroing the destination partition disk with dd: [root@master1 ~]# dd if=/dev/zero of=/dev/sda4 After the process is started, the following two commands should be run on the PRIMARY server ONLY: [root@master1 ~]# drbdadm — –overwrite-data-of-peer primary r0 [root@master1 ~]# mkfs.ext3 /dev/drbd0 [root@master1 ~]# mount /hadoop It will take some time to synchronize based on the size of the disk and the “syncer” rate defined in your drbd.config. You can check the status of this process from /proc/drbd: [root@master1 ~]# cat /proc/drbd version: 8.2.6 (api:88/proto:86-88) GIT-hash: 3e69822d3bb4920a8c1bfdf7d647169eba7d2eb4 build by buildsvn@c5-x8664-build, 2008-10-03 11:30:17 0: cs:SyncSource st:Primary/Secondary ds:UpToDate/Inconsistent C r— ns:18440304 nr:0 dw:27072452 dr:18511901 al:11746 bm:12767 lo:14 pe:12 ua:246 ap:1 oos:84438604 [==&gt;.................] sync’ed: 18.0% (82459/100465)M finish: 0:14:31 speed: 96,904 (77,472) K/sec [root@master1 ~]# cat /proc/drbd version: 8.2.6 (api:88/proto:86-88) GIT-hash: 3e69822d3bb4920a8c1bfdf7d647169eba7d2eb4 build by buildsvn@c5-x8664-build, 2008-10-03 11:30:17 0: cs:Connected st:Primary/Secondary ds:UpToDate/UpToDate C r— ns:196492540 nr:0 dw:195994668 dr:3687117 al:1355324 bm:68 lo:0 pe:0 ua:0 ap:0 oos:0 5. Install Hadoop RPMs from Cloudera We used the web-based configurator provided by Cloudera (https://my.cloudera.com/), which builds an RPM containing repos for your custom configuration and the rest of their distribution. The resulting RPM is then installed on BOTH master nodes. Important Note: When we defined the hostname in the web-based configurator, we gave the name of the shared virtual hostname that is bound to the VIP as described earlier. [root@master1 ~]# rpm -ivh cloudera-repo-0.1.0-1.noarch.rpm Since we’re only installing the master nodes, we skip the datanode and tasktracker RPMs on these hosts: [root@master1 ~]# yum -y install hadoop hadoop-conf-pseudo hadoop-jobtracker \ &gt; hadoop-libhdfs.x86_64 hadoop-namenode hadoop-native.x86_64 hadoop-secondarynamenode After this, we install our custom-generated Hadoop config from an RPM named hadoop-conf- plus the cluster name and hostname that we defined in the config generator: [root@master1 ~]# yum -y install hadoop-conf-prod-hadoop.domain.com [root@master1 ~]# alternatives –display hadoop hadoop – status is auto. link currently points to /etc/hadoop/conf.prod-hadoop.domain.com /etc/hadoop/conf.empty – priority 10 /etc/hadoop/conf.pseudo – priority 30 /etc/hadoop/conf.prod-hadoop.domain.com – priority 60 Current `best’ version is /etc/hadoop/conf.prod-hadoop.domain.com. We also disable automatic startup of the Hadoop processes on the node since they will be managed by the Heartbeat package: [root@master1 ~]# chkconfig hadoop-namenode off [root@master1 ~]# chkconfig hadoop-secondarynamenode off [root@master1 ~]# chkconfig hadoop-jobtracker off On the primary server only, we must start the namenode for the first time and format HDFS: [root@master1 ~]# chown -R hadoop:hadoop /hadoop [root@master1 ~]# /sbin/runuser -s /bin/bash – hadoop -c ‘hadoop namenode -format’ Finally, dismount the DRBD volume mounted at /hadoop since it will be brought online through Heartbeat: [root@master1 ~]# umount /hadoop 6. Configure Heartbeat There are many options available for the Heartbeat configuration. Here, we attempt to show only the basics. While this example should work in most cases, you may wish to extend the configurations to take advantage of other features that the package provides. There are three key files that we edit to configure the Heartbeat package: /etc/ha.d/ha.cf /etc/ha.d/haresources /etc/ha.d/authkeys The first, ha.cf, defines the general settings of the cluster. Our example: ## start of ha.cf debugfile /var/log/ha.debug logfile /var/log/ha.log logfacility local0 keepalive 1 initdead 60 deadtime 5 mcast eth0 239.0.0.22 694 1 0 node master1.domain.com master2.domain.com auto_failback off ## end of ha.cf Additional parameters and their meanings can be found at http://linux-ha.org/ha.cf/. The second file we’ll edit, haresources, defines all cluster resources that will fail over from one node to the next. The resources include the shared IP address of the cluster, the DRBD resource “r0″ (from /etc/drbd.conf), the file system mount, and the three Hadoop master node initiation scripts that are invoked with the “start” parameter upon failover. This file must be the same on both nodes in the cluster. Note that the leading host name defines the preferred node. The IPaddr stated in this file will be the virtual IP that we chose in our planning phase. This IP will be brought up on the active node on the aliased interface bond0:0: ## start of haresources master1.domain.com IPaddr::192.168.4.115 master1.domain.com drbddisk::r0 master1.domain.com Filesystem::/dev/drbd0::/hadoop::ext3::defaults master1.domain.com hadoop-namenode master1.domain.com hadoop-secondarynamenode master1.domain.com hadoop-jobtracker ## end of haresources The last file file to be edited, authkeys, should also be the same on both servers. Our example: ## start of authkeys auth 1 1 sha1 hadoop-master ## end of authkeys Change permissions on the authkeys file to keep the shared secret protected: [root@master1 ~]# chmod 0600 /etc/ha.d/authkeys Enable Heartbeat service to start on boot, and start the service on each node: [root@master1 ~]# chkconfig –add heartbeat [root@master1 ~]# chkconfig heartbeat on [root@master1 ~]# service heartbeat start Setup Complete At this point, your master node processes should be brought up automatically by Heartbeat. If there are any problems, you should start resolving them by checking /var/log/ha.log for hints on what went wrong. If everything worked, feel free to test your failover by rebooting the master, pulling its power cord, or using whatever your favorite method of simulating a system failure may be. Based on the configuration values from this example, we find that the system takes about 10 seconds to bring everything back up online.</snippet></document><document id="711"><title>The Project Split</title><url>http://blog.cloudera.com/blog/2009/07/the-project-split/</url><snippet>Last Wednesday, we hosted a Hadoop meetup, and I gave a short talk about the new project split. How does the split change the project’s organization, and what does it mean for end users? The mailing lists and the source code repositories have been rearranged. For those doing development against Hadoop’s “trunk” branch, compiling Hadoop and using the various components in concert has become more complicated. My presentation slides cover which mailing lists to subscribe to, where the source repositories are located, and how to compile and run the development version of Hadoop.</snippet></document><document id="712"><title>File Appends in HDFS</title><url>http://blog.cloudera.com/blog/2009/07/file-appends-in-hdfs/</url><snippet>There is some confusion about the state of the file append operation in HDFS. It was in, now it’s out. Why was it removed, and when will it be reinstated? This post looks at some of the history behind HDFS capability for supporting file appends. Background Early versions of HDFS had no support for an append operation. Once a file was closed, it was immutable and could only be changed by writing a new copy with a different filename. This style of file access actually fits very nicely with MapReduce, where you write the output of a data processing job to a set of new files; this is much more efficient than manipulating the input files that are already in place. A file didn’t exist until it had been successfully closed (by calling FSDataOutputStream‘s close() method). If the client failed before it closed the file, or if the close() method failed by throwing an exception, then (to other clients at least), it was as if the file had never been written. The only way to recover the file was to rewrite it from the beginning. MapReduce worked well with this behavior, since it would simply rerun the task that had failed from the beginning. First Steps Toward Append It was not until the 0.15.0 release of Hadoop that open files were visible in the filesystem namespace (HADOOP-1708). Until that point, they magically appeared after they had been written and closed. At the same time, the contents of files could be read by other clients as they were being written, although only the last fully-written block was visible (see HADOOP-89). This made it possible to gauge the progress of a file that was being written, albeit in a crude manner. Additionally, tools such as hadoop fs -tail (and its web UI equivalent) were introduced and allowed users to view the contents of a file as it was being written, block by block. Stronger Requirements For some applications, the API offered by HDFS was not strong enough. For example, a database, such as HBase, which wishes to write its transaction log to HDFS, cannot do so in a reliable fashion. For this application, some form of sync operation is needed, which guarantees that the bytes up to a given point in the stream are persisted (like POSIX’s fsync). In the event of the process crashing, it can recover its previous state by playing through the transaction log, and then it can open the log in append mode to write new entries to it. Similarly, writing unbounded log files to HDFS is unsatisfactory, since it is generally unacceptable to lose up to a block’s worth of log records if the client writing the log stream fails. The application should be able to choose how much data it is prepared to lose, since there is a trade-off between performance and reliability. A database transaction log would opt for reliability, while an application for logging web page accesses might tolerate a few lost records in exchange for better throughput. HADOOP-1700 was opened in August 2007 to add an append operation to HDFS, available through the append() methods on FileSystem. (The issue also includes discussion about a truncate operation, which sets the end of the file to a particular position, causing data at the end of the file to be deleted. However, truncates have never been implemented.) A little under a year later, in July 2008, the append operation was committed in time for the 0.19.0 release of Hadoop Core. Implementing the append operation necessitated substantial changes to the core HDFS code. For example, in a pre-append world, HDFS blocks are immutable. However, if you can append to a file, then the last (incomplete) block is mutated, so there needs to be some way of updating its identity so that (for example) a datanode that is down when the block is updated is recognized as having an out-of-date version of the block when it become available again. This is solved by adding a generation stamp to each block, an incrementing integer that records the version of a particular block. There are a host of other technical challenges to solve, many of which are articulated in the design document attached to HADOOP-1700. Append Redux After the work on HADOOP-1700 was committed, it was noticed in October 2008 that one of the guarantees of the append function, that readers can read data that has been flushed (via FSDataOutputStream‘s sync() method) by the writer, was not working (HDFS-200 “In HDFS, sync() not yet guarantees data available to the new readers”). Further issues were found, which were related: HDFS-142 “Datanode should delete files under tmp when upgraded from 0.17″ HADOOP-4692 “Namenode in infinite loop for replicating/deleting corrupted block” HDFS-145 “FSNameSystem#addStoredBlock does not handle inconsistent block length correctly” HDFS-168 “Block report processing should compare g[e]neration stamp” Because of these problems, append support was disabled in the 0.19.1 release of Hadoop Core, and in the first release of the 0.20 branch 0.20.0. Configuration parameter dfs.support.append, which is false by default, was introduced (HADOOP-5332) to make it easy to enable or disable append functionality (note that append functionality is still unstable, so this flag should be set to true only on development or test clusters). This prompted developers to step back and take a fresh look at the problem. One of the first actions was to create an umbrella issue (HDFS-265 “Revisit append”) with a new design document attached, which aimed to build on the work done to date on appends and provide a foundation for solving the remaining implementation challenges in a coherent fashion. It provides input to the open Jira issues mentioned above—it does not seek to replace them. A group of interested Hadoop committers had a meeting at Yahoo!’s offices on May 22, 2009 to discuss the requirements for appends. They reached agreement on the precise semantics for the sync operation and renamed it to hflush in the design document in HDFS-265 to avoid confusion with other sync operations. They agreed on API3, which guarantees that data is flushed to all datanodes holding replicas for the current block but is not flushed to the operating system buffers or the datanodes’ persistent disks. At the time of this writing, not all of these issues have been fixed, but hopefully they will be fixed in time for a 0.21 release. The Future Getting appends supported has been a stormy ride. It’s not over yet, but when it is finished, it will enable a new class of applications to be built upon HDFS. When appends are done, what will be next? Record appends from multiple concurrent writers (like Google’s GFS)? Truncation? File writes at arbitrary offsets? Bear in mind, however, that every new feature adds complexity and therefore may compromise reliability or performance, so each must be very carefully considered before being added.</snippet></document><document id="713"><title>Hadoop Graphing with Cacti</title><url>http://blog.cloudera.com/blog/2009/07/hadoop-graphing-with-cacti/</url><snippet>An important part of making sure Apache Hadoop works well for all users is developing and maintaining strong relationships with the folks who run Hadoop day in and day out. Edward Capriolo keeps About.com�s Hadoop cluster happy, and we frequently chew the fat with Ed on issues ranging from administrative best practices to monitoring. Ed�s been an invaluable resource as we beta test our distribution and chase down bugs before our official releases. Today�s article looks at some of Ed�s tricks for monitoring Hadoop with Cacti through JMX. -Christophe You may have already read Philip�s Hadoop Metrics post, which provides a general overview of the Hadoop Metrics system. Here, we�ll examine Hadoop monitoring with Cacti through JMX. What is Cacti? Cacti is an RRD front end. You can learn more about it on the Cacti website. Cacti differs from Ganglia in that Cacti polls using SNMP or shell scripts while applications push data at Ganglia. Both Ganglia and Cacti have feature overlaps, but for those with a large Cacti deployment, installing a secondary statistic system just for Hadoop may not be an option. I have had great success over the years graphing everything from user CPU, NetApp disk reads to environmental sensors with Cacti. When I saw the information in Hadoop JMX, I started working on a set of Hadoop templates, hadoop-cacti-jtg. My goal was to provide visual representation for all pertinent Hadoop JMX information. Administrators and developers can use these templates to better manage Hadoop and understand how it is working behind the scenes. Currently, the package has several predefined graphs covering the Hadoop NameNode and DataNode. Let�s walk through some of them. Hadoop Capacity Hadoop Capacity provides the same type of information you get from monitoring a standard disk. The top black line represents the maximum capacity. This is all the possible storage on all currently active DataNodes. You also have the used and free capacity information stacked on top of each other. You can use these variables to trend your file system growth. In most cases your file system should be growing steadily, assuming you have batch processes running on a schedule. You may want to use a Cacti Threshhold alarm at 80%. If the alarm goes off, it�s good practice to clean up unused files, or you can take the lazy way and order more DataNodes :) If you are wondering why the sum of used plus free is not equal to capacity, then remember that Hadoop has reserve for each DataNode. Also, your disk file system might have a reserve. If a disk is solely devoted to serving HDFS, you can tune the reserve down with the following string: tunefs -m &lt;percent&gt; Live vs. Dead Nodes The Hadoop live and dead node information is available on the NameNode�s web interface. This stack-style graph shows both values together. Blue represents the number of live DataNodes, while the red area of the graph shows the number of dead DataNodes. If you are using the Cacti Threshhold system, you can use it to set off a warning if the number of Dead DataNodes exceeds 20%. NameNode Stats Hadoop JMX gives us a breakdown of file operations by type. This graph provides details about requests to which the NameNode is responding. I ran several teragens and terasorts from the examples.jar. Below, we can see the process both creating and reading files from the system as the map reduce jobs run. DataNode Blocks The DataNode statistics are similar to the NameNode statistics. This graph template can be applied to each DataNode, allowing you to track BlocksRead, BlocksWritten, BlocksRemoved, and BlocksReplicated. You can use this to find �hot spots� in your data. A hot spot is a piece of data that is commonly or frequently accessed. Increasing the replication to those files would help by spreading the access to other DataNodes. Cacti Extras Cacti offers many excellent out-of-the box features. The following add-on features are helpful for monitoring Hadoop deployments. You can find these on the Cacti site: Linux Full CPU Graph – Adds IOWait and other kernel states. The default CPU graph only shows nice, user, and system. Linux Full Memory Graph – The standard memory graph does not show swap usage. Disk Utilization Graph – You can graph bytes written to physical devices from SNMP. This is helpful for underlying disk utilization and maximum possible disk performance. RealTime Plugin – Used to graph data at 5-second intervals. By default, Cacti is running at 1-minute or 5-minute intervals, which is not helpful for Hadoop since the JMX is probably updating at 5-minute intervals. However, it is generally useful for real time reporting of other SNMP information. THold Plugin – The Threshold plugin creates some overlap between Nagios and Cacti, and sends alarms when data exceeds high or low values. Aggregate Plugin – The aggregate plugin is ideal for graphing clusters into a single graph. You may want to graph the �Open File Count� across several nodes � this plugin makes the graphing process fast and easy. Where to go from Here If you want to see the Hadoop Cacti templates in action, check out the Live Sample (user: hadoop, password: hadoop). To get started, simply follow the Installation Instructions. The project has the Apache V2 license. You can view the Source Repository. A Hudson system provides the latest build if you want to dig into the project source code.</snippet></document><document id="714"><title>Debugging MapReduce Programs With MRUnit</title><url>http://blog.cloudera.com/blog/2009/07/debugging-mapreduce-programs-with-mrunit/</url><snippet>The distributed nature of MapReduce programs makes debugging a challenge. Attaching a debugger to a remote process is cumbersome, and the lack of a single console makes it difficult to inspect what is occurring when several distributed copies of a mapper or reducer are running concurrently. Furthermore, operations that work on small amounts of input (e.g., saving the inputs to a reducer in an array) fail when running at scale, causing out-of-memory exceptions or other unintended effects. A full discussion of how to debug MapReduce programs is beyond the scope of a single blog post, but I’d like to introduce you to a tool we designed at Cloudera to assist you with MapReduce debugging: MRUnit. MRUnit helps bridge the gap between MapReduce programs and JUnit by providing a set of interfaces and test harnesses, which allow MapReduce programs to be more easily tested using standard tools and practices. While this doesn’t solve the problem of distributed debugging, many common bugs in MapReduce programs can be caught and debugged locally. For this purpose, developers often try to use JUnit to test their MapReduce programs. The current state of the art often involves writing a set of tests that each create a JobConf object, which is configured to use a mapper and reducer, and then set to use the LocalJobRunner (via JobConf.set(”mapred.job.tracker”, “local”)). A MapReduce job will then run in a single thread, reading its input from test files stored on the local filesystem and writing its output to another local directory. This process provides a solid mechanism for end-to-end testing, but has several drawbacks. Developing new tests requires adding test inputs to files that are stored alongside one’s program. Validating correct output also requires filesystem access and parsing of the emitted data files. This involves writing a great deal of test harness code, which itself may contain subtle bugs. Finally, this process is slow. Each test requires several seconds to run. Users often find themselves aggregating several unrelated inputs into a single test (violating a unit testing principle of isolating unrelated tests) or performing less exhaustive testing due to the high barriers to test authorship. The easiest way to test MapReduce programs is to include as little Hadoop-specific code as possible in one’s application. Parsers can operate on instances of String instead of Text, and mappers should instantiate instances of MySpecificParser to tokenize input data rather than embed parsing code in the body of MyMapper.map(). Your MySpecificParser implementation can then be tested with ordinary JUnit tests. Another class or method could then be used to perform processing on parsed lines. But even with those components separately tested, your map() and reduce() calls should still be tested individually, as the composition of separate classes may cause unintended bugs to surface. MRUnit provides test drivers that accept programmatically specified inputs and outputs, which validate the correct behavior of mappers and reducers in isolation, as well as when composed in a MapReduce job. For instance, the following code checks whether the IdentityMapper emits the same (key, value) pair as output that it receives as input: import junit.framework.TestCase;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.Mapper;
import org.apache.hadoop.mapred.lib.IdentityMapper;
import org.junit.Before;
import org.junit.Test;

public class TestExample extends TestCase {

  private Mapper mapper;
  private MapDriver driver;

  @Before
  public void setUp() {
    mapper = new IdentityMapper();
    driver = new MapDriver(mapper);
  }

  @Test
  public void testIdentityMapper() {
    driver.withInput(new Text("foo"), new Text("bar"))
            .withOutput(new Text("foo"), new Text("bar"))
            .runTest();
  }
} The MapDriver orchestrates the test process, feeding the input (“foo” and “bar”) record to the IdentityMapper when its runTest() method is called. It also passes a mock OutputCollector implementation to the mapper. The driver then validates the output received by the OutputCollector against the expected output (”foo” and “bar”) record. If the actual and expected outputs mismatch, a JUnit assertion failure is raised, informing the developer of the error. More test drivers exist for testing individual reducers, as well as mapper/reducer compositions. End-to-end tests involving JobConf configuration code, InputFormat and OutputFormat implementations, filesystem access, and larger scale testing are still necessary. But many errors can be quickly identified with small tests involving a single, well-chosen input record, and a suite of regression tests allows correct behavior to be assured in the face of ongoing changes to your data processing pipeline. We hope MRUnit helps your organization test code, find bugs, and improve its use of Hadoop by facilitating faster and more thorough test cycles. MRUnit is open source and is included in Cloudera’s Distribution for Hadoop. For more information about MRUnit, including where to get it and how to use its API, see the MRUnit documentation page.</snippet></document><document id="715"><title>Rackspace Upgrades to Cloudera’s Distribution for Apache Hadoop</title><url>http://blog.cloudera.com/blog/2009/06/rackspace-upgrades-to-clouderas-distribution-for-hadoop/</url><snippet>Apache Hadoop moves fast. Users often find that they need to upgrade after just a few months. Upgrading can be a daunting task, especially if you are several versions behind. We�ve been working with Rackspace for a while now, and they recently embarked on an upgrade from Hadoop 0.15.3 to Cloudera�s Distribution for Hadoop based on 0.18.3. Stu Hood, Search Team Technical Lead at Rackspace, was kind enough to document their experience, and we�re happy to share it with you here. -Christophe Upgrading to the Cloudera Distribution Apache Hadoop plays an integral part in the email analytics performed at Rackspace Email and Apps, and our installation of Apache Hadoop 0.15.3 ran smoothly for 18 months after we deployed it in January 2008. By the time we decided to upgrade to Cloudera�s Distribution for Apache Hadoop in June 2009, our production cluster had performed almost 600,000 MapReduce jobs. In the past, we have deployed Hadoop along with our primary MapReduce application by checking the entire Hadoop distribution and our configuration into version control. Deploying a new slave for the cluster involved running custom scripts to create users, directories, and install dependencies. There were a few important reasons to upgrade a cluster as trusty as ours to Cloudera�s Distribution for Hadoop (version 0.18.3): Hadoop improves rapidly (since version 0.15.3 was released, over 1500 JIRA issues were resolved). The Cloudera Distribution contains backported patches that are considered stable, but have not been applied to previous versions by the Apache project, such as the FairScheduler. Some of these patches fix critical bugs, add new features, or improve performance. Cloudera�s configuration RPMs maintain the optimal settings for the installed version of Hadoop. Tweaking these settings manually would involve far more research than we can afford. Standardizing on a Red Hat deployment infrastructure like RPM and YUM makes it much easier to track the latest stable version of Hadoop. Steps Configure Hadoop In order to take advantage of Cloudera�s recommended configuration values, we decided to use Cloudera�s Configurator for Hadoop to generate the configuration that we would be using on the upgraded cluster. We started by following the steps at https://my.cloudera.com/, using parameters that matched our current configuration. Since we were upgrading an existing cluster, it was important that the data directories matched up in our new configuration. The following table describes mapping between entries made in the GUI as well as those in the generated configuration files: Step 2: NameNode Metadata Path(s) dfs.name.dir Step 3: Secondary NameNode Metadata Path(s) fs.checkpoint.dir Step 5: TaskTracker Intermediate Data Path(s) mapred.local.dir Step 5: HDFS Data Path(s) dfs.data.dir Note that the configurator does not support the type of variable expansion that Hadoop�s configuration files sometimes do. One such example is ${hadoop.tmp.dir} expanding to the Hadoop temporary directory. If one of your previous configuration values used variable expansion for ${username}, you would need to replace ${username} with the name of the user that you had previously used to run the Hadoop daemons. In our case, we needed to replace instances of ${username} in the dfs.data.dir and dfs.name.dir values with user “hadoopuser.” When we reached the end of the configurator, we downloaded the generated hadoop-site.xml* files and the Cloudera Repository RPM, and then recorded our repository ID. To double check that our data directories were configured properly, we compared the values (from the table above) in the new hadoop-site.xml* files against our previous configuration. If you see any mismatches at this step, you will probably want to restart the configurator until the resulting files are consistent. Upgrade At this point, it was time to jump into the upgrade. We installed the Configurator RPM, which we had downloaded earlier on all machines in our cluster by walking through the steps in the config guide. After listing out the available configuration packages with yum search hadoop-conf, we installed the matching packages for each class of machine in the cluster using yum install $packagename. At this point, the new version of Hadoop was installed, but not running. In order to swap out the running version of Hadoop, and create a backup of the current filesystem, we needed to follow the steps leading up to the �Install New Version� step from the Hadoop Wiki upgrade page. After walking through those preparation instructions and successfully shutting down the cluster, it was time to make the switch. Cloudera�s Distribution for Hadoop creates user �hadoop� and this user runs all of the necessary services/daemons for the cluster. If your cluster had previously been running with a different username (ours was running as �hadoopuser�) you will need to give the new user ownership of various different directories. We ran� # chown -R hadoop $directory �for each of the following configured directories: * dfs.data.dir, * dfs.name.dir, * fs.checkpoint.dir, * mapred.local.dir, * hadoop.tmp.dir, * /var/log/hadoop (FIXME: the hardcoded(?) log directory) Once the �hadoop� user had access to the necessary directories, we were ready to upgrade the Namenode. We ran the following command from our Namenode machine, so that the process would start in the background and begin upgrading its checkpoint: $ sudo -u hadoop /usr/lib/hadoop/bin/hadoop-daemon.sh –config “/etc/hadoop/conf” start namenode -upgrade We watched the �Upgrades� section of the DFS status page at http://$namenode:50070/ while waiting for the Namenode upgrade to complete, and then we started up the remaining Hadoop services on their respective machines using the instructions from the �Managing Hadoop Services� section of the config guide. Finalize Code Changes Once our cluster was upgraded, we needed to port our Hadoop jobs to the Hadoop 0.18.3 API. There were actually only minor changes in the MapReduce and FileSystem APIs between 0.15.3 and 0.18.3: Our OutputFormats needed to extend FileOuputFormat, rather than OutputFormatBase. FileSystem.listPaths() was removed, in favor of .globPaths(). Finalizing the Upgrade After verifying that our newly updated jobs were running correctly against the cluster, we were ready to make the changes permanent. The dfsadmin -finalizeUpgrade command runs in the background and cleans up the outdated copies of blocks left behind by the upgrade, freeing disk space. $ sudo -u hadoop hadoop dfsadmin -finalizeUpgrade Future Now that we’ve upgraded to the Cloudera distribution using the configurator, it will be much easier to stay at the bleeding edge of Hadoop development (or the cutting edge, if we choose stability over features). We can also add the Cloudera repository RPM to our base server image and add a single command to pull down the entire distribution from Yum. Finally, we can conveniently install the packages for Pig and Hive to give our developers more options for their processing jobs.</snippet></document><document id="716"><title>Parallel LZO: Splittable Compression for Apache Hadoop</title><url>http://blog.cloudera.com/blog/2009/06/parallel-lzo-splittable-compression-for-hadoop/</url><snippet>Yesterday, Chris Goffinet from Digg made a great blog post about LZO and Hadoop. Many users have been frustrated because LZO has been removed from Apache Hadoop’s core, and Chris highlights a great way to mitigate this while the project identifies an alternative with a compatible license. We liked the post so much, we asked Chris to share it with our audience. Thanks Chris! -Christophe So at Digg, we have been working our own Apache Hadoop cluster using Cloudera�s distribution. One of the things we have been working through is how can we split our large compressed data and run them in parallel on Hadoop? One of the biggest drawbacks from compression algorithms like Gzip is that you can�t split them into multiple mappers. This is where LZO comes in. Lempel-Ziv-Oberhumer (LZO) is a lossless data compression algorithm that is focused on decompression speed. The LZO library implements a number of algorithms with the following features: Compression is comparable in speed to deflate compression. On modern architectures, decompression is very fast; in non-trivial cases able to exceed the speed of a straight memory-to-memory copy due to the reduced memory-reads. Requires an additional buffer during compression (of size 8 kB or 64 kB, depending on compression level). Requires no additional memory for decompression other than the source and destination buffers. Allows the user to adjust the balance between compression quality and compression speed, without affecting the speed of decompression. This is great until you start trying to actually get LZO working on Hadoop. First off, it gets really confusing when its now removed from Hadoop 0.20+ because of GPL restrictions. I first came across a blog post by Johan Oskarsson that discussed this. Unfortunately when you dive into HADOOP-4640 you find out it�s against 0.20. Cloudera�s distribution uses a modified version of 0.18.3. The patch from HADOOP-4640 applies pretty cleanly besides a few things. On top of this, you need HADOOP-2664 which enables LZOP codec. You actually need this because the compressor on most Linux systems is `lzop` and that differs from the traditional LzoCodec bundled in 0.18. So how do we get all of this working? First off grab both modified patches from my Github account. Once you have those, apply the patches to your Cloudera distribution. Then be sure to rebuild. After that�s done and you have redeployed to your clients and production cluster you need to modify your hadoop-site.xml on the client side. &lt;property&gt;
&lt;name&gt;io.compression.codecs&lt;/name&gt;

&lt;value&gt;org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.LzopCodec&lt;/value&gt;
&lt;description&gt;A list of the compression codec classes that can be used for compression/decompression.&lt;/description&gt;
&lt;/property&gt; Once that is completed, go ahead and upload your large LZO file to your Hadoop cluster. So lets say you uploaded the file: $ hadoop fs -put large_file.lzo /tmp/large_file.lzo The next step is you need to index your LZO file, so that hadoop knows how to split the file into multiple mappers. The Indexer.jar in the my Github account will be used for this process. Now you need to run the Indexer.jar and tell it what file to generate an index file for. $ hadoop jar Indexer.jar /tmp/large_file.lzo After that�s completed, you�re almost there! The index file will be created in /tmp. Now all you need to do is run a map/reduce job and your set! Don�t forget to set the -inputFormat parameter. Here is a code snippet using wordcount example: #!/bin/sh
HADOOP_HOME=/usr/lib/hadoop
$HADOOP_HOME/bin/hadoop  jar $HADOOP_HOME/contrib/streaming/hadoop-0.18.3-7-streaming.jar \
-input /tmp/large_file.lzo \
-output wc_test \
-inputformat org.apache.hadoop.mapred.LzoTextInputFormat \
-mapper 'cat' \
-reducer 'wc -l'</snippet></document><document id="717"><title>A Great Week for Apache Hadoop: Summit Roundup</title><url>http://blog.cloudera.com/blog/2009/06/a-great-week-for-hadoop-summit-west-roundup/</url><snippet>On June 10th, more than 750 people from around the world descended on the Santa Clara Marriott to share their love for a little stuffed elephant named Hadoop. It was a good week to be part of this exploding community, and I want to extend Cloudera’s heartfelt thanks to everyone who made it possible, especially our friends at Yahoo! who organized this Summit. Most importantly, I want to thank all of you who were able to participate. I know many of you couldn�t make it to California this time, so I hope to see you at the Hadoop Summit East in October. For those of you who couldn�t join us, I thought I would post my notes on a few of the highlights. Apache Hadoop Goes Mainstream: About 300 developers attended last year�s summit, primarily from web companies and research labs. They were joined by a few forward-thinking venture capitalists. This year�s audience was both larger and different. In addition to the vibrant developer community, there was a flood of users of Hadoop. Though the audience was still dominated by web companies, attendees included traditional enterprise users with applications ranging from finance to biotech. There were technology previews from IBM and Sun. Major companies like Amazon joined our commercial efforts around Hadoop. VCs had also stepped up to sponsor status. Take-away? You ain�t seen nothing yet. Hadoop In Print: Yahoo! Developer Network gave away 500 copies of Tom White�s book, “Hadoop: The Definitive Guide,” published by O�Reilly. If you missed your copy, I’ve heard that when they aren’t busy developing AWS, Amazon has been known to sell a few books here and there. Cloudera Presentation Slides: Several Cloudera employees spoke at the Summit, and we have posted slides from those talks on the Hadoop Wiki. If you spoke, please put your slides up as well. Here are direct links to the Cloudera talks: The Growing Hadoop Community, Christophe Bisciglia Hadoop Configuration and Deployment, Matt Massie Running Hadoop in the Cloud, Tom White Job Scheduling for Hadoop, Matei Zaharia Cloudera Announces New Distribution Features: We see an increasing number of users moving data between Hadoop and more traditional database products, and more and more usage moving to the cloud – especially Amazon. To that end, we�ve released two new features, and a collection of new packages, that make Hadoop easier to use. Sqoop: Database Import for Hadoop. Brainchild of Aaron Kimball, Sqoop is an extensible command-line tool that copies data from a relational database into Hadoop. Sqoop uses JDBC to inspect the database schema, and automatically generates all of the code necessary to move the data. It can import data from any database over JDBC, and includes an extension to allow better performance in MySQL by using the mysqldump command. EBS Integration for Hadoop on AWS: Tom White had a busy month. Besides finishing his book, he spent some time thinking about how Hadoop runs on Amazon Web Services, and came up with new code to make that better. Hadoop clusters on EC2 have always needed to copy data from S3 when they started up, and write results back to S3 before they powered down. While Amazon�s Elastic MapReduce makes this round-trip much easier operationally, EMR doesn�t support tools like Pig and Hive. Using Tom�s work, Cloudera is able to store data blocks on EBS volumes, and to connect them to EC2 nodes running Hadoop as needed. This delivers better throughput and more disks per node at lower cost , since EBS is cheaper than S3. Since no copies are required at startup and shutdown, your EC2 instances run for less time, saving CPU costs. Best of all, these changes to Hadoop work with Hive, Pig, Sqoop, and the rest of the Hadoop family. You can now load data, run jobs in your favorite language, turn your cluster off, and pick up exactly where you left off later. All your data survives. Preview Release of 0.20 Packages: Matt Massie and Todd Lipcon doubled down to get our testing release packaged so that those of you who crave the bleeding edge can start experimenting with version 0.20 of Hadooop today. Over the next few weeks, we�ll be bringing in changes from other leading Hadoop developers, upgrading our customers, and releasing stable packages to the community. Hadoop Developer Offsite With so many Hadoop developers in the Bay Area, we decided to invite the Hadoop committers and some active developers to Cloudera�s offices. We wanted to collaborate without the assistance of email lists, JIRA, hudson, or any other technology designed to make our lives easier. We used sticky notes to identify issues in parallel, identified consensus with clusters, and broke off into smaller teams to explore solutions. Out of this, we identified five things we love and hate about Hadoop, the biggest upcoming challenges for the project and a wish list for the future. We broke into sub-projects to make concrete plans to address these issues, and we posted the meeting notes online. We�ll continue to host such meetings, and to work with other leaders in the development community. Bottom line, as Hadoop grows up, we need to grow with it, and meetings like this are a great way to coordinate development efforts with the needs of the community. Yahoo! Distribution of Hadoop: Long known for their leadership in the Hadoop development community, Yahoo! stepped it up again by releasing the source code that they run on their alpha clusters to the community at large. There are some things you can only learn about Hadoop from running at Y!�s scale, and while this is not a stable production distribution, their source-only (available via github) release provides 17 patches slated for inclusion in later versions of Hadoop. Cloudera is working closely with the team at Yahoo! to fold these patches into the next release of our distribution, along with dozens of patches we have developed to support customer workloads, and a half dozen or so from our friends at Amazon to improve performance on AWS. As big players like Yahoo! and Amazon continue to open their development processes, Cloudera can deliver more stable, better tested, and ultimately, more trusted code to our enterprise customers and the community at large in the packages you know and love (RPMs, Debian Packages, AMIs, etc). It�s not always easy for big companies to be open, so we�d like to thank and congratulate everyone involved. HBase, Wow: HBase has endured its share of criticism over the last year, but based on last week�s presentation, many of those problems have been addressed. HBase has made incredible strides in terms of reliability, availability and performance. Version 0.20 the first-ever �performance� release, and is focused on improving random access, scan and insert times. Check out these slides for details. We�re looking at an order of magnitude performance improvement, with random reads on par with traditional RDBMS. The other major improvement involves ZooKeeper integration, and eliminates the single point of failure in the master node. This strengthens the case for including HBase with the Cloudera Distribution for Hadoop. Please let us know if you want HBase support. In Summary: We had a great time at the summit � we learned a lot and got to talk to a lot of smart people. We�re looking forward to October�s Hadoop Summit East in New York City!</snippet></document><document id="718"><title>Analyzing Apache logs with Apache Pig</title><url>http://blog.cloudera.com/blog/2009/06/analyzing-apache-logs-with-pig/</url><snippet>(guest blog post by Dmitriy Ryaboy) A number of organizations donate server space and bandwidth to the Apache Foundation; when you download Apache Hadoop, Tomcat, Maven, CouchDB, or any of the other great Apache projects, the bits are sent to you from a large list of mirrors. One of the ways in which Cloudera supports the open source community is to host such a mirror. In this blog post, we will use Pig to examine the download logs recorded on our server, demonstrating several features that are often glossed over in introductory Pig tutorials—parameter substitution in PigLatin scripts, Pig Streaming, and the use of custom loaders and user-defined functions (UDFs). It’s worth mentioning here that, as of last week, the Cloudera Distribution for Hadoop includes a package for Pig version 0.2 for both Red Hat and Ubuntu, as promised in an earlier post. It’s as simple as apt-get install pig or yum install hadoop-pig. There are many software packages that can do this kind of analysis automatically for you on average-sized log files, of course. However, many organizations log so much data and require such custom analytics that these ordinary approaches cease to work. Hadoop provides a reliable method for scaling storage and computation; PigLatin provides an expressive and flexible language for data analysis. Our log files are in Apache’s standard CombinedLogFormat. It’s a tad more complicated to parse than tab- or comma- delimited files, so we can’t just use the built-in PigLoader().� Luckily, there is already a custom loader in the Piggybank built specifically for parsing these kinds of logs. First, we need to get the PiggyBank from Apache. The PiggyBank is a collection of useful add-ons (UDFs) for Pig, contributed by the Pig user community. There are instructions on the Pig website for downloading and compiling the PiggyBank. Note that you will need to make sure to add pig.jar to your CLASSPATH environment variable before running ant. Now, we can start our PigLatin script by registering the piggybank jarfile and defining references to methods we will be using.  register /home/dvryaboy/src/pig/trunk/piggybank.jar; DEFINE LogLoader org.apache.pig.piggybank.storage.apachelog.CombinedLogLoader(); DEFINE DayExtractor org.apache.pig.piggybank.evaluation.util.apachelogparser.DateExtractor('yyyy-MM-dd'); By the way — the PiggyBank contains another useful loader, called MyRegExLoader, which can be instantiated with any regular expression when you declare it with a DEFINE statement. Useful in a pinch. While we are working on our script, it may be useful to run in local mode, only reading a small sample data set (a few hundred lines). In production we will want to run on a different file. Moreover, if we like the reports enough to automate them, we may wish to run the report every day, as new logs come in. This means we need to parameterize the source data location. We will also be using a database that maps geographic locations to IPs, and we probably want to parametrize that as well.  %default LOGS 'access_log.small' %default GEO 'GeoLiteCity.dat' To specify a different value for a parameter, we can use the -param flag when launching the pig script: # pig -x mapreduce -f scripts/blogparse.pig -param LOGS='/mirror.cloudera.com/logs/access_log.*' For mapping IPs to geographic locations, we use a third-party database from MaxMind.� This database maps IP ranges to countries, regions, and cities.� Since the data from MaxMind lists IP ranges, and our logs list specific IPs, a regular join won’t work for our purposes. Instead, we will write a simple script that takes a parsed log as input, looks up the geo information using MaxMind’s Perl module, and outputs the log with geo data prepended. The script itself is simple — it reads in a tuple representing a parsed log record, checks the first field (the IP) against the database, and prints the data back to STDOUT :  #!/usr/bin/env perl use warnings; use strict; use Geo::IP::PurePerl;

my ($path)=shift; my $gi = Geo::IP::PurePerl-&gt;new($path);

while (&lt;&gt;) { chomp; if (/([^\t]*)\t(.*)/) { my ($ip, $rest) = ($1, $2); my ($country_code, undef, $country_name, $region, $city) = $gi-&gt;get_city_record($ip); print join("\t", $country_code||'', $country_name||'', $region||'', $city||'', $ip, $rest), "\n"; } } Getting this script into Pig is a bit more interesting. The Pig Streaming interface provides us with a simple way to ship scripts that will process data, and cache any necessary objects (such as the GeoLiteCity.dat file we downloaded from MaxMind).� However, when the scripts are shipped, they are simply dropped into the current working directory. It is our responsibility to ensure that all dependencies—such as the Geo::IP::PurePerl module—are satisfied. We could install the module on all the nodes of our cluster; however, this may not be an attractive option. We can ship the module with our script—but in Perl, packages are represented by directories, so just dropping the .pm file into cwd will not be sufficient, and Pig doesn’t let us ship directory hierarchies.� We solve this problem by packing the directory into a tarball, and writing a small Bash script called “ipwrapper.sh” that will set up our Perl environment when invoked:  #!/usr/bin/env bash tar -xzf geo-pack.tgz PERL5LIB=$PERL5LIB:$(pwd) ./geostream.pl $1 The geo-pack.tgz tarball simply contains geostream.pl and Geo/IP/PurePerl.pm . We also want to make the GeoLiteCity.dat file available to all of our nodes. It would be inefficient to simply drop the file in HDFS and reference it directly from every mapper, as this would cause unnecessary network traffic.� Instead, we can instruct Pig to cache a file from HDFS locally, and use the local copy. We can relate all of the above to Pig in a single instruction:  DEFINE iplookup `ipwrapper.sh $GEO` ship ('ipwrapper.sh') cache('/home/dvryaboy/tmp/$GEO#$GEO'); We can now write our main Pig script. The objective here is to load the logs, filter out obviously non-human traffic, and using the rest, calculate the distribution of downloads by country and by Apache project. Load the logs:  logs = LOAD '$LOGS' USING LogLoader as (remoteAddr, remoteLogname, user, time, method, uri, proto, status, bytes, referer, userAgent); Filter out records that represent non-humans (Googlebot and such), aren’t Apache-related, or just check the headers and do not download contents.  logs = FILTER logs BY bytes != '-' AND uri matches '/apache.*';

-- project just the columns we will need logs = FOREACH logs GENERATE remoteAddr, DayExtractor(time) as day, uri, bytes, userAgent;

-- The filtering function is not actually in the PiggyBank. -- We plan on contributing it soon. notbots = FILTER logs BY (NOT org.apache.pig.piggybank.filtering.IsBotUA(userAgent)); Get country information, group by country code, aggregate.  with_country = STREAM notbots THROUGH `ipwrapper.sh $GEO` AS (country_code, country, state, city, ip, time, uri, bytes, userAgent);

geo_uri_groups = GROUP with_country BY country_code;

geo_uri_group_counts = FOREACH geo_uri_groups GENERATE group, COUNT(with_country) AS cnt, SUM(with_country.bytes) AS total_bytes;

geo_uri_group_counts = ORDER geo_uri_group_counts BY cnt DESC;

STORE geo_uri_group_counts INTO 'by_country.tsv'; The first few rows look like: Country Hits Bytes USA 8906 2.0458781232E10 India 3930 1.5742887409E10 China 3628 1.6991798253E10 Mexico 595 1.220121453E9 Colombia 259 5.36596853E8 At this point, the data is small enough to plug into your favorite visualization tools. We wrote a quick-and-dirty python script to take logarithms and use the Google Chart API to draw this map: This is pretty interesting. Let’s do a breakdown by US states. Note that with the upcoming Pig 0.3 release, you will be able to have multiple stores in the same script, allowing you to re-use the loading and filtering results from earlier steps. With Pig 0.2, this needs to go in a separate script, with all the required DEFINEs, LOADs, etc.  us_only = FILTER with_country BY country_code == 'US';

by_state = GROUP us_only BY state;

by_state_cnt = FOREACH by_state GENERATE group, COUNT(us_only.state) AS cnt, SUM(us_only.bytes) AS total_bytes;

by_state_cnt = ORDER by_state_cnt BY cnt DESC;

store by_state_cnt into 'by_state.tsv'; Theoretically, Apache selects an appropriate server based on the visitor’s location, so our logs should show a heavy skew towards California. Indeed, they do (recall that the intensity of the blue color is based on a log-scale). Now, let’s get a breakdown by project. To get a rough mapping of URI to Project, we simply get the directory name after /apache in the URI. This is somewhat inaccurate, but good for quick prototyping. This time around, we won’t even bother writing a separate script — this is a simple awk job, after all! Using streaming, we can process data the same way we would with basic Unix utilities connected by pipes.  uris = FOREACH notbots GENERATE uri;

-- note that we have to escape the dollar sign for $3, -- otherwise Pig will attempt to interpret this as a Pig variable. project_map = STREAM uris THROUGH `awk -F '/' '{print \$3;}'` AS (project);

project_groups = GROUP project_map BY project;

project_count = FOREACH project_groups GENERATE group, COUNT(project_map.project) AS cnt;

project_count = ORDER project_count BY cnt DESC;

STORE project_count INTO 'by_project.tsv'; We can now take the by_project.tsv file and plot the results (in this case, we plotted the top 18 projects, by number of downloads). We can see that Tomcat and Httpd dwarf the rest of the projects in terms of file downloads, and the distribution appears to follow a power-law. We’d love to hear how folks are using Pig to analyze their data. Drop us a line, or comment below!</snippet></document><document id="719"><title>The Smart Grid: Hadoop at the Tennessee Valley Authority (TVA)</title><url>http://blog.cloudera.com/blog/2009/06/smart-grid-hadoop-tennessee-valley-authority-tva/</url><snippet>For the last few months, we’ve been working with the TVA to help them manage hundreds of TB of data from America’s power grids. As the Obama administration investigates ways to improve our energy infrastructure, the TVA is doing everything they can to keep up with the volumes of data generated by the “smart grid.” But as you know, storing that data is only half the battle. In this guest blog post, the TVA’s Josh Patterson goes into detail about how Hadoop enables them to conduct deeper analysis over larger data sets at considerably lower costs than existing solutions. -Christophe The Smart Grid At the Tennessee Valley Authority (TVA) we collect phasor measurement unit (PMU) data on behalf of the North American Electric Reliability Corporation (NERC) to help ensure the reliability of the bulk power system in North America. The Tennessee Valley Authority (TVA) is a federally owned corporation in the United States created by congressional charter in May 1933 to provide flood control, electricity generation, and economic development in the Tennessee Valley. NERC is a self-regulatory organization, subject to oversight by the U.S. Federal Energy Regulatory Commission and governmental authorities in Canada. TVA has been selected by NERC as the repository for PMU data nationwide. PMU data is considered part of the measurement data for the generation and transmission portion of the so called “smart grid”. PMU Data Collection There are currently 103 active PMU devices placed around the Eastern United States that actively send TVA data while new PMU devices come online regularly. PMU devices sample high voltage electric system busses and transmission lines at a substation several thousand times a second which is then reported for collection and aggregation. PMU data is a GPS time-stamped stream of those power grid measurements which is transmitted at 30 times a second each consisting of a timestamp and a floating point value. The types of information a PMU point can contain are: Voltage (A,B, C phase in positive, negative, or zero sequence) magnitude and angle Current (A,B, C phase in positive, negative, or zero sequence) magnitude and angle Frequency dF/dt (change in frequency over time) Digitals Status flags Commonly just positive sequence voltages and currents are transmitted but there is the possibility for all three phases. There can be several measured voltage and current phasors per PMU (each phasor having a magnitude and an angle value), a variable number of digitals (typically 1 or 2), and one of each of the remaining 3 types of data; on average there will be around 16 total measurements sent per PMU. Should a company wish to send all three phases or a combination of positive, negative, or zero sequence data, then the number of measurements obviously increases. The amount of this time-series data created by even a regional area of PMU devices provides a unique architectural demand on the TVA infrastructure. The flow of data from measurement device to TVA is as follows: A measurement device located at the substation (the PMU) samples various data values, timestamps them via a GPS clock, and sends them over fiber or other suitable lines to a central location. For some participant companies this may be a local concentrator or it may be a direct connection to TVA itself. Communication between TVA and these participants is commonly a VPN tunnel over a LAN-to-LAN connection but several partners utilize a MPLS connection for more remote regions. After a few network hops the data is sent to a TVA developed data concentrator termed the Super Phasor Concentrator (or SPDC) which accepts these PMUs’ input, ordering them into the correct time-aligned sequence – compensating for any missing data or delay introduced by network congestion or latency. Once organized by the SPDC, its modular architecture allows this data to be operated on by third party algorithms via a simple plug-in layer. The entirety of the stream, currently involving 19 companies, 10 different manufacturers of PMU devices, and 103 PMUs – each reporting an average of 16 measured values at a rate of 30 samples a second – with a possibility of 9 different encodings (and this only from the Eastern United States), is passed to one of three servers running an archiving application which writes the data to a size optimized fixed length binary file to disk. A real-time data stream is simultaneously forwarded to a server program hosted by TVA which passes the conditioned data in a standard phasor data protocol (IEEE C37.118-2005) to client visualization tools for use at participant companies. An agent moves PMU archive files into the Hadoop cluster via an FTP interface Alternatively, regulators such as NERC or approved researchers can directly request this data over secure VPN tunnels for operation at their remote location. TVA currently has around 1.5 trillion points of time-series data in 15TB of PMU archive files. The rate of incoming PMU data is growing very quickly with more and more PMU devices coming online regularly. We expect to have around 40TB of PMU data by the end of 2010 with 5 years worth of PMU data estimated to be at half a petabyte (500TB). The Case For Hadoop At TVA Our initial problem was how to reliably store PMU data and make it available and reliable at all times. There are many brand name solutions in the storage world that come with a high price tag and the assumption of reliable hardware. With large amounts of data that spans many disks; even at a high mean time to fail (MTTF) a system will experience hardware failures quite frequently. We liked the idea of being able to lose whole physical machines and still have an operational file system due to Hadoop’s aggressive replication scheme. The more we talked with other groups using HDFS the more we came away with the impression that HDFS worked as advertised and shined even with amounts of data the “reliable hardware” struggled with. Our discussions and findings also indicated that HDFS was quite good at moving data and included multiple ways to interface with it out of the box. In the end, Hadoop is a good fit for this project in that it allows us to employ commodity hardware and open source software at a fraction of the price of proprietary systems to achieve a much more manageable expenditure curve as our repository grows. The other side of the equation is that eventually the NERC and its designated research institutions are to be able to access the data and run operations on the data. The concept of “moving computation to the data” with map-reduce made Hadoop an even more attractive choice, especially given its price point. Many of the proposed uses of our PMU data ranged from simple pattern scans to complex data mining operations. The type of analysis and algorithms that we want to run aren’t well suited to be run in SQL. It became obvious that we were more in the market for a batch processing system such as map-reduce as opposed to a large relational database system. We were also impressed with the very robust open source ecosystem that Hadoop enjoys; Many projects built on Hadoop are actively being developed such as: Hive HBase Pig This thriving community was very interesting to us as it gives TVA a wealth of quality tools with which to analyze PMU data using analysis techniques that are necessary to understand this data. After reviewing the factors above, we concluded that employing Hadoop at TVA kills 2 birds with 1 stone — it solves our storage issues with HDFS and provides a robust computing platform with map reduce for researchers around North America. PMU Data Analysis at TVA Currently our analysis needs and wants are evolving with our nascent ideas on how best to use PMU data. Current techniques and algorithms on the board or in beta include Washington State’s Oscillation Monitoring System Basic averages and standard deviation over frequency data Fast Fourier transform filters including: Wiener Filter Kalman Filter Low Pass Filter High Pass Filter Band Pass Filter Indexing of power grid anomalies Various visualization rendering techniques such as creating power grid map tiles to watch the power grid over time and in history We are currently writing map reduce applications to be able to crunch far greater amounts of power grid information than has be previously possible. Using traditional techniques to calculate something as simple as an average frequency over time can be an extremely tedious process because of the need to traverse terabytes of information; map-reduce allows us to not only parallelize the operation but also get much higher disk read speeds by moving the computation to the data. As we evolve our analysis techniques we plan to expand our range of indexing techniques from simple scans to more complex data mining techniques to better understand how the power grid reacts to fluctuations and how previously thought discrete anomalies may, in fact, be interconnected. Additionally, we are also adding other devices such as Frequency Disturbance Recorders (FDRs, a.k.a. F-NET devices which are developed by Virginia Tech) to our network. Although these devices send samples at a third of the rate of PMU devices with a reduced measurement set, there exists the potential for many hundreds of these less expensive meters to come online which would effectively double our storage requirements. This FDR data would be interesting in that the extra data would allow us to create a more complete picture of the power grid and its behavior. Hadoop would allow us to continue scaling up to meet the extra demand not only for storage but for processing with map reduce as well. Hadoop gives us the flexibility and scalability to meet future demands that can be placed upon the project with respect to data scale, processing complexity, and processing speed. Looking Forward With Hadoop As we move forward using Hadoop, there are a few areas we’d like to see improved. Security is a big deal in our field, especially given the nature of the data and agencies involved. We would like to see security continue to be improved by the Hadoop community as a whole as time goes on. Security internally and externally is a big part of what we do, so we are always examining our production environment to make sure we fulfill our requirements. We also are looking at ways to allow multiple research projects to coexist on the same system, such that they share the same infrastructure but can queue up their own jobs and download the results from their own private account area while only having access to the data that their project allows. Research can be a competitive business and we are looking for unique ways to allow researchers to work with the same types of data while feeling comfortable about their specific work remaining private; additionally we are required to maintain the privacy of all the data providers – researchers will only be allowed to access a filtered set of measurements as allowed by the data providers or as deemed available for research by the NERC. In our first discussions about whether or not we would explore cloud computing as an option for processing our PMU data, we wanted to know if there was a “Redhat-like” entity in the space that could answer questions and provide support for Hadoop. Cloudera has definitely stepped up to the plate to fulfill this role for Hadoop. Cloudera provides exceptional support in a very dynamic space, a space in which many companies have no experience and many consulting firms can provide no solid advice. Cloudera was quick to make sure that Hadoop was right for us and then provided extremely detailed answers to all of our questions and what-if scenarios. Their whole team was exceptionally adept in getting back to us on a myriad of details most sales or “front line support” teams would be stymied by. Cloudera’s distribution for Hadoop and guidance on hardware acquisition helped in saving us money and getting our evaluation of Hadoop off the ground in a very short amount of time.</snippet></document><document id="720"><title>Introducing Sqoop</title><url>http://blog.cloudera.com/blog/2009/06/introducing-sqoop/</url><snippet>In addition to providing you with a dependable release of Hadoop that is easy to configure, at Cloudera we also focus on developing tools to extend Hadoop’s usability, and make Hadoop a more central component of your data infrastructure. In this vein, we’re proud to announce the availability of Sqoop, a tool designed to easily import information from SQL databases into your Hadoop cluster. Sqoop (“SQL-to-Hadoop”) is a straightforward command-line tool with the following capabilities: Imports individual tables or entire databases to files in HDFS Generates Java classes to allow you to interact with your imported data Provides the ability to import from SQL databases straight into your Hive data warehouse After setting up an import job in Sqoop, you can get started working with SQL database-backed data from your Hadoop MapReduce cluster in minutes. Motivation Hadoop MapReduce is a powerful tool; its flexibility in parsing unstructured or semi-structured data means that there is a lot of potential for creative applications. But your analyses are only as useful as the data which they process. In many organizations, large volumes of useful information are locked away in disparate databases across the enterprise. HDFS, Hadoop’s distributed file system represents a great place to bring this data together, but actually doing so is a cumbersome task. Consider the task of processing access logs and analysing user behavior on your web site. Users may present your site with a cookie that identifies who they are. You can log the cookies in conjunction with the pages they visit. This lets you coordinate users with their actions. But actually matching their behavior against their profiles or their previously recorded history requires that you look up information in a database. If several MapReduce programs needed to do similar joins, the database server would experience very high load, in addition to a large number of concurrent connections, while MapReduce programs were running, possibly causing performance of your interactive web site to suffer. The solution: periodically dump the contents of the users database and the action history database to HDFS, and let your MapReduce programs join against the data stored there. Going one step further, you could take the in-HDFS copy of the users database and import it into Hive, allowing you to perform ad-hoc SQL queries against the entire database without working on the production database. Sqoop makes all of the above possible with a single command-line. Example Usage Continuing the example above, let’s say that our front end servers connected to a MySQL database named website, stored on db.example.com. The website database has several tables, but the one we are most interested in is one named USERS. This table has several columns; it might have been created from a SQL statement like: CREATE TABLE USERS (
  user_id INTEGER NOT NULL PRIMARY KEY,
  first_name VARCHAR(32) NOT NULL,
  last_name VARCHAR(32) NOT NULL,
  join_date DATE NOT NULL,
  zip INTEGER,
  state CHAR(2),
  email VARCHAR(128),
  password_hash CHAR(64));
 Importing this table into HDFS could be done with the command: you@db$ sqoop --connect jdbc:mysql://db.example.com/website --table USERS \
    --local --hive-import
 This would connect to the MySQL database on this server and import the USERS table into HDFS. The –-local option instructs Sqoop to take advantage of a local MySQL connection which performs very well. The –-hive-import option means that after reading the data into HDFS, Sqoop will connect to the Hive metastore, create a table named USERS with the same columns and types (translated into their closest analogues in Hive), and load the data into the Hive warehouse directory on HDFS (instead of a subdirectory of your HDFS home directory). Suppose you wanted to work with this data in MapReduce and weren’t concerned with Hive. When storing this table in HDFS, you might want to take advantage of compression, so you’d like to be able to store the data in SequenceFiles. In this case, you might want to import the data with the command: you@db@ sqoop --connect jdbc:mysql://db.example.com/website --table USERS \
    --as-sequencefile Sqoop will also emit a Java class named USERS with getter methods for each of the columns of the table. They support the majority of SQL’s types including optionally-null values. The data will be loaded into HDFS as a set of SequenceFiles; you can use the USERS.java class to work with the data in your MapReduce analyses. Sqoop can also connect to other databases besides MySQL; anything with a JDBC driver should work. If you are running locally on a MySQL server the import will be especially high-performance, but a MapReduce-based import mechanism allows remote database connections as well. Authenticated connections with usernames and passwords are also supported. Several other options allow you to control which columns of a table are imported, and other aspects of the import process. The full reference manual is available at www.cloudera.com/hadoop-sqoop. A Closer Look In this section I’ll briefly outline how Sqoop works under the hood. In an earlier blog post, I described the DBInputFormat, a connector that allows Hadoop MapReduce programs to read rows from SQL databases. DBInputFormat allows Hadoop to read input from JDBC: a Java interface to databases that most popular database vendors (Oracle, MySQL, Postgresql, etc.) implement. In order to use DBInputFormat you need to write a class that deserializes the columns from the database record into individual data fields to work with. This is pretty tedious—and entirely algorithmic. Sqoop auto-generates class definitions to deserialze the data from the database. These classes can also be used to store the results in Hadoop’s SequenceFile format, which allows you to take advantage of built-in compression within HDFS too. The classes are written out as .java files that you can incorporate in your own data processing pipeline later. The class definition is created by taking advantage of JDBC’s ability to read metadata about databases and tables. When Sqoop is invoked, it retrieves the table’s metadata, writes out the class definition for the columns you want to import, and launches a MapReduce job to import the table body proper. Hadoop users know that moving large volumes of data can be a time-intensive operation. While it provides a reliable implementation-independent mechanism to read database tables, using a MapReduce JDBC job to import data from a remote database is often inefficient. Database vendors usually provide an export tool that exports data in a more high-performance manner. Sqoop is capable of using alternate import strategies as well. By examining the connect string URL that tells Sqoop which database to connect to, Sqoop will choose alternate import strategies as appropriate to the database. We’ve already implemented the ability to take advantage of MySQL’s export tool called mysqldump. We’ll add support for other systems as soon as we can. Getting Sqoop The first beta release of Sqoop is available today as part of Cloudera’s Distribution for Hadoop. It installs as part of the same RPM (or Debian package) that contains Hadoop itself. Hadoop users who aren’t using our distribution can apply the patch that is contributed to Apache Hadoop as issue HADOOP-5815, and compile it themselves, but Sqoop won’t be part of the standard Hadoop release for some time (at least until version 0.21.0). mysqldump support is added in HADOOP-5844, and Hive integration is provided in HADOOP-5887. You can read the documentation for Sqoop at http://www.cloudera.com/hadoop-sqoop. You can also get some basic usage information from Sqoop itself by running sqoop –-help after it’s installed. We also did a preview of this tool at the May Bay Area Hadoop User Group meet-up; you can catch the presentation here: We hope you find this tool useful—please check it out! Then let us know your feedback on GetSatisfaction. Bug reports and feature requests especially welcome.</snippet></document><document id="721"><title>Common Questions and Requests From Our Users</title><url>http://blog.cloudera.com/blog/2009/05/common-questions-and-requests-from-our-users/</url><snippet>A few months ago we announced the Cloudera Distribution for Hadoop.� We’re happy to report that lots of people have started using our distribution, and our GetSatisfaction product (which is essentially a message board about our products) has seen lots of good Hadoop questions and answers.� We thought it would be worthwhile to share some of the interesting questions and requests we’ve seen from our users. Question: How do I backup my name node metadata? (Editor’s note: The information in this section pertains to CDH3 only. For CDH4, refer to the�CDH4 High Availability Guide.)�The name node (NN) stores all of the HDFS metadata, which includes file names, directory structures, and block locations.� This metadata is stored in memory for fast lookup, but the NN also maintains two on-disk data structures to ensure that metadata is persisted.� The first structure stored is a snapshot of the in-memory metadata, and the second structure stored is an edit log of changes that have been made since the snapshot was last taken.� The secondary name node (2NN) is in charge of fetching the snapshot and edit log from the NN and merging the two into a new snapshot, which is then sent back to the NN.� Once the NN gets the new snapshot, it clears its edit log, and the process repeats.� Take a look at our other blog post about multi-host secondary name nodes for more information about configuring the 2NN. There are two types of metadata backups that one should implement, and each type solves a different problem.� I will talk about each of these backup strategies separately.� The first backup strategy is used to ensure that no metadata is lost in the event of a NN failure, whether that failure be disks dying, power supplies catching fire, or some other unforeseen loss of the NN or its local data.� The way to avoid losing NN metadata in the event of a crash is to configure dfs.name.dir such that it writes to several local disks and at least one NFS mount.� dfs.name.dir takes a comma-separated list of local filesystem paths, so an example configuration might look like “/hdd1/hadoop/dfs/name,/hdd2/hadoop/dfs/name,/mnt/nfs/hadoop/dfs/name”.� The purpose of storing data on several local hard drives is to avoid data loss in the case of a single drive failing.� The purpose of storing data on a NFS mount is to avoid data loss in the case of the NN machine going down entirely.� With at least two local drives and one NFS mount storing the same NN metadata, you should be well protected from losing any data from a crash.� To be fair, NFS isn’t the only solution for mounting a remote file system, but it’s the de facto standard for Hadoop. The second backup strategy is used to allow recovery from accidental data loss due to user error (such as a careless hadoop fs -rmr /*).� As mentioned earlier, the 2NN is able to fetch the NN’s metadata snapshot and edits log over HTTP.� That said, if you’d like to perform hourly or nightly backups of the NN metadata, you can do so by querying the following URLs: Snapshot: http://nn.domain.com:50070/getimage?getimage=1 Edits log: http://nn.domain.com:50070/getimage?getedit=1 Note that using LVM snapshots to backup the snapshot and edit log is also a good idea; LVM snapshots allow for more reliable backups. To recover from a NN failure, or to restore from a backup, just take the edits log and snapshot — either from the NFS server or from your backup archive — and place them in the following places: Snapshot: dfs.name.dir/current/fsimage Edits log: dfs.name.dir/current/edits Note that the NN daemon should not be running when you change its snapshot and edits log. Bonus link: learn more about protecting data node metadata. Request: I want Cloudera’s Distribution for Hadoop on Mac OS X. We distribute our Distribution for Hadoop by providing RPMs, DEBs, and AMIs.� RPMs are installable on Redhat-based Linux distributions such as CentOS, RHEL, and Fedora. DEBs are installable on Debian-based Linux distributions such as Debian and Ubuntu. AMIs are machine images used for running our distribution in EC2. We’ve had several people request packaging for Mac OS X.� Our near-term solution for getting our distribution on Macs is to provide tarballs similar to the tarballs you download for vanilla Hadoop.� Perhaps at some later time we’ll provide self-installing DMGs, but we don’t have them on the road map. Question: My MapReduce jobs throw Exceptions saying that I’ve ran out of disk space, but I have plenty of disk space.� What’s up? Most users who run into this have misconfigured hadoop.tmp.dir. If you’re using vanilla Hadoop, then hadoop.tmp.dir will be configured to /tmp.� This is problematic, because most Linux installations have a quota on /tmp, making Hadoop think it’s out of disk space when it tries to write temporary data.� Be sure to configure hadoop.tmp.dir to a directory that has plenty of space; it’s fine for hadoop.tmp.dir to write to the same partitions (not directories, though) as dfs.data.dir, dfs.name.dir, etc.� As long as each of these parameters write to different folders, Hadoop will manage disk space in a reasonable way. Also worth noting is that mapred.local.dir should be configured to write to multiple disks, rather than relying on hadoop.tmp.dir. Request: We want Pig 0.2.0! Yes, we know :).� We’ve had several requests for Pig 0.2.0 to be included in our distribution.� It’s coming soon!� Stay tuned for an announcement. Question: I have a big NFS server at my disposal; how can I use it for my Hadoop cluster? One of Hadoop’s design goals was to avoid having data be stored in a single place such as a NFS server.� Hadoop is able to analyze and compute lots of data because data is distributed across many nodes, allowing for spatial locality when computing, and also allowing for files to be read from several data nodes in parallel.� If a NFS mount were used to store HDFS data, then the NFS server would certainly become a bottleneck and would slow down your entire cluster, because several task trackers would request data from the NFS server at once, probably lighting the NFS server on fire and bringing your job to a crawl.� Though a NFS server can’t really help you compute more data faster, it can make your operational tasks easier.� An ops engineer can use NFS to ensure that each node has the same Hadoop code, dependent files (e.g., if you’re using Hadoop Streaming), and consistent home directories.� As long as Hadoop is not reading or writing HDFS data to or from a NFS mount, the NFS server should not be a bottleneck.� A NFS server could also be used to collect Hadoop logs from all machines in a small cluster, though using Scribe for log collection is a much more scalable solution. I have more questions! Do you have more questions about Hadoop, Pig, Hive, or our Distribution for Hadoop?� There are several ways in which you can get your questions answered.� If you have a general Hadoop, Hive, or Pig question, then you are most likely to get the best response on the user lists: pig-user, hive-user, hadoop-user.� Cloudera engineers participate in these lists a lot as well.� If you have questions about Cloudera’s Distribution for Hadoop, then post a message to our GetSatisfaction message board.� We’re always happy to help out.</snippet></document><document id="722"><title>Building a distributed concurrent queue with Apache ZooKeeper</title><url>http://blog.cloudera.com/blog/2009/05/building-a-distributed-concurrent-queue-with-apache-zookeeper/</url><snippet>In my first few weeks here at Cloudera, I’ve been tasked with helping out with the Apache ZooKeeper system, part of the umbrella Hadoop project. ZooKeeper is a system for coordinating distributed processes. In a distributed environment, getting processes to act in any kind of synchrony is an extremely hard problem. For example, simply having a set of processes wait until they’ve all reached the same point in their execution – a kind of distributed barrier – is surprisingly difficult to do correctly. ZooKeeper offers an API to facilitate this sort of distributed coordination. For example, it is often used to serve locks to client processes – locks are just another kind of coordination primitive – in the form of small files that ZooKeeper tracks. In order to be useful, ZooKeeper must be both highly reliable and available as systems will rely upon it as a critical component. For example, if locks cannot be taken, processes cannot make progress and the whole system will grind to a halt. ZooKeeper is built on a suite of reliable distributed systems techniques and protocols, and is typically run on a cluster of machines so that if some should fail, the remaining ones can continue to provide service. Under the hood, ZooKeeper is responsible for ordering calls made by clients so that each request is processed atomically and in a fixed and firm order. One of my first contributions to the project was a set of bindings to allow programs written in the Python language to act as clients to a ZooKeeper cluster. ZooKeeper was natively written in Java, and there are already C and Perl bindings. Adding Python bindings increases the number of people that can use the system, and brings the strengths of Python, such as rapid prototyping, to bear when designing distributed systems. The Python ZooKeeper bindings are available from the ZooKeeper SVN repository and should be part of the 3.2 release, planned for the next couple of weeks. To use the bindings now, you can either check out the latest version of the code from the SVN repository, or download a tarball containing a recent snapshot here. The zookeeper module exposes the ZooKeeper API to Python, so to get started all you need do is add import zookeeper to your Python script once the module is installed. Instructions on getting up and running are at the end of this post. To illustrate some of the ZooKeeper API, I’ve written a distributed FIFO queue in Python – the source code is here – which I wanted to share. The combination of Python and Zookeeper meant that I was able to write the queue in just over 60 lines of code, and most of that deals with local coordination issues between two threads rather than any tricky issues trying to make remote processes behave correctly. I can only give a taste here of how programming with Python and ZooKeeper works. I hope there’s enough here to convince you that ZooKeeper might make a useful component for distributed systems that need a little herding. ZooKeeper ZooKeeper provides a tree abstraction where every node in that tree (or znode, in ZooKeeper parlance) is a file on which a variety of simple operations can be performed. ZooKeeper orders operations on znodes so that they occur atomically. Therefore there is no need to use complex locking protocols to ensure that only one process can access a znode at a time. The tree represents a hierarchical namespace, so that many distinct distributed systems can use a single ZooKeeper instance without worrying about their files having the same name. Each znode has some associated data – up to a megabyte in current builds – that can be updated atomically. Every update to a znode increases its version number, which allows clients to perform compare-and-swap operations by reading the version and then updating a znode only if the version is still the one that was read. As a notification mechanism, ZooKeeper provides watches, which are callback methods that are called asynchronously when an event of interest occurs. Watches are attached, typically, to an individual znode. When that znode changes any watcher on the znode will be fired asynchronously on the client. Many methods of the ZooKeeper API have an optional watch argument. Some languages have to work hard to provide callable objects as parameters, but Python makes this easy as callables are first class language constructs. Simply pass any callable, like a method or a lambda expression, to the zookeeper module and when an event of interest occurs, the callable will be executed. This call comes from a separate thread of execution, so great care must be taken to ensure that unexpected things do not happen due to your watcher being fired at an arbitrary point in the execution of your script. Normally you will use watchers to notify another thread of a state change. It will often be the case that the main thread will be waiting for the watcher to fire before it can continue. An example of this is in the __init__ method of our ZooKeeperQueue when we try to connect to the server. Compared to the time a script takes to execute, connections can take a long time to run. So it’s useful that the ZooKeeper API allows us to connect asynchronously, in case there were any work that we wanted to get done while we were waiting for the connection to be established. However, in our case, we just want to wait until the connection is successful, and so we need a mechanism to wait for the watcher to notify us. A useful tool for this inter-thread communication is the Condition object in Python, which represents a condition variable, a well-known concurrent programming abstraction. Condition objects may be acquired and released just like locks, but they also expose an API to wait for a notification from another thread and to fire that notification. While a thread is waiting on a Condition it goes to sleep, leaving the operating system with some free CPU to dedicate to other processes. Once a Condition is notified, a thread that is waiting on it is woken up and allowed to continue execution once the notifying thread has released the Condition. This leads to a simple pattern for communicating between watchers and the main thread. Here’s an excerpt from the connection code: def watcher(handle,type,state,path):
    print "Connected"
    self.cv.acquire()
    self.connected = True
    self.cv.notify()
    self.cv.release()

self.cv.acquire()
self.handle = zookeeper.init("localhost:2181", watcher, 10000, 0)
self.cv.wait(10.0) First we define our watcher which takes four parameters (if you want to provide more parameters or local state to a watcher, one way to do it is to wrap a function call in a local lambda which captures the state). The next line acquires an exclusive lock on a condition variable cv. Why do this now? Once we set our watcher in place, it could be fired at any time – even before the main thread makes progress to the next line of code. If we don’t prevent it from sending a notification on the condition variable before we’re ready to look for it, the notification could get lost and we could wait forever. Notifications aren’t buffered – if no one is waiting on a condition variable, no one gets woken up. Then the code initialises ZooKeeper. The zookeeper module gives us an integer handle which we can use to refer to our connection in the future (we can open many connections per client). The next line tells us to wait until we receive a notification on the condition variable that the connection has succeeded. The parameter is a timeout in seconds, after which if we are still not connected we presume that something is wrong and abort. The ZooKeeper queue A FIFO queue is a simple data structure where producers put items in, and consumers retrieve them in the order they were put in. There are only two operations on a basic queue: enqueue adds an item and dequeue removes it. Despite their simplicity, queues crop up very often in distributed systems – for example, in job submission systems where clients submit requests to a set of workers which serve the requests on a first-come, first-served basis. The ZooKeeper queue is structured very simply. All items are stored as znodes under a single top-level znode which represents a queue instance. Consumers can retrieve items by getting and then deleting a child of the top-level znode. The code creates a queue by calling a single create command. If the queue already exists, the Python module will throw an exception which we catch. This is a design decision that is still in review – future versions of the bindings might return integer error codes, and rely on the user to throw an exception if required. zookeeper.create(self.handle,self.queuename,"queue top level", [ZOO_OPEN_ACL_UNSAFE],0) The first two arguments to this call identify the connection to the ZooKeeper service and the name of the znode. The third is the data the znode contains. We won’t be accessing the data so we write some placeholder text. The fourth argument is an access control list of permissions that controls who can access the znode in the future. ZooKeeper provides fairly fine-grained control over access, but the subject is beyond the scope of this post. What we have done here is to create the queue znode so that any client can read or write to it. Adding and deleting items from the queue Although I explained how consumers retrieve items from the queue, I said nothing about how they make sure they are retrieving items in FIFO order. What we would like is a way of naming each item such that later items are ordered lexicographically after earlier ones. If we can retrieve items in the same order, we’ll have our queue. Thankfully, ZooKeeper provides a very handy flag for the create call that helps us out. Specifying the zookeeper.CREATE_SEQUENCE flag appends each znode name with an sequence number suffix that increases monotonically with each new znode that is created. ZooKeeper ensures that the sequence numbers are applied in order and are not reused. Enqueuing an item is therefore a simple one liner. We don’t have to take out any locks to ensure that access to the queue znode is serialised. Items may be queued concurrently, and ZooKeeper takes care of assigning sequence numbers to them in the order they were received. Dequeuing an item is also straightforward, but a bit more involved. First we retrieve a list of all the items waiting to be queued from ZooKeeper with the get_children procedure call. Then, after sorting the list of items on the client, we get the contents of the znode (i.e. the item’s data) and then try to delete it. It is possible that this deletion will fail because some other consumer has managed to successfully retrieve the item beforehand. We could ensure that this would never happen by organising for a queue-wide lock – this is easily implemented in ZooKeeper (although left as an exercise for the reader). However, this would severely impact performance by only allowing a single consumer to access the queue at one time. Instead, the client simply deals with the failed delete – again, indicated via an exception – and moves on to the next child znode in the list. If the client reaches the end of the list without successfully deleting an item, it should issue another get_children call to make sure that no items were added while the original list was being scanned. Once the get_children call returns an empty list, the dequeue procedure gives up and returns None. Blocking reads Sometimes we might want to block until an item is available to retrieve. It would be inefficient to copy exactly the non-blocking approach and simply loop, issuing get_children requests until an item was found. Instead, we can leverage ZooKeeper’s watcher mechanism to provide an asynchronous notification when a new znode is created as a child of the queue znode. The code to accomplish this is a combination of the patterns we’ve seen already in the dequeue and connection code. def block_dequeue(self):
    def queue_watcher(handle,event,state,path):
        self.cv.acquire()
        self.cv.notify()
        self.cv.release()
    while True:
        self.cv.acquire()
        children = sorted(zookeeper.get_children(self.handle, self.queuename, queue_watcher))
        for child in children:
            data = self.get_and_delete(self.queuename+"/"+children[0])
            if data != None:
                self.cv.release()
                return data
        self.cv.wait()
        self.cv.release() First the client acquires a lock to prevent the watcher sending a notification when the client is unready. Then, as in the dequeue method, the client retrieves a list of items, but here a watcher parameter is specified. The watcher will fire whenever any event is seen that is relevant to the queue znode. The watcher acquires the lock – blocking until the client has given it up – and then notifies the client that there may be more items available. The client only waits for this notification if all the children returned from get_children have already been consumed by others – otherwise it will successfully retrieve an item and return it. Once all possible items have been exhausted, the client waits on the condition variable. After being woken up, it repeats the same list-read-delete-wait loop. Failure modes ZooKeeper operations can fail in a number of ways. In order to keep this example simple, most errors are raised as exceptions and the queue aborts. A more robust implementation should catch errors at every ZooKeeper invocation, as many can be recovered from with a little effort. The zookeeper.CONNECTIONLOSS error condition is particularly worth noting. ZooKeeper may drop a client connection at any time, due to physical link loss, network congestion or other connection problem. This can cause ZooKeeper API invocations to abort before the ZooKeeper cluster is able to inform the client of the operation’s success. This is problematic for our queue, as enqueue operations may or may not have succeeded when we receive a CONNECTIONLOSS error. There are several approaches we can take to this problem. The first is to blindly retry enqueue when a connection is lost. This could result in an item being queued several times, but for some systems this is not a significant problem. For example, if a web page is crawled twice, apart from the time cost there will be no hardship caused to a indexing engine. For some applications, duplication of enqueue operations is problematic. The obvious ‘solution’ is to check whether an item is in the queue after it has been queued. However, it is possible that a consumer will have retrieved and deleted the item between the connection loss event and the subsequent reconnection and existence check. Instead, a two-phase protocol is necessary where a producer marks an item as ‘consumable’ only when it is sure it is in the queue, by atomically updating its associated data with a flag. Consumers may only retrieve items for which the flag is set. If a connection loss occurs during the setting of this flag, recovery is easier as the set call may be reissued – if the item is no longer present in the queue, the only possible explanation is that the original flag update succeeded and the item has been consumed. This is not built into the example code, but a production system should implement a similar form of connection loss recovery. Taking care of failure modes like this one often comprises most of the work of building a distributed system. The key is to understand every exception that API calls can throw, and to know what your code does in every circumstance. Using the queue To use the queue, you must first make sure you have built and installed both the C client libraries and the zookeeper Python module. There are two prerequisite packages: the cppunit development package and the Python development package. On yum-based systems, these are named cppunit-devel and python-devel. Both packages are available through standard platform package managers like yum, apt and Darwin ports. As a prerequisite to building the C client libraries, the Java based-server must be built. This auto-generates some header files that the C libraries rely on. From the root directory of the downloaded distribution: ant The C client libraries for ZooKeeper must be installed as the Python module makes use of them to actually communicate with a ZooKeeper cluster. It’s easiest to build these from source. From the src/c directory, type the following: autoreconf -if ./configure make &amp;&amp; sudo make install The downloadable package contains the source code for the Python module. To build and install, one command should do the trick from the src/contrib/zkpython directory: ant install To test the installation, start a Python shell and type import zookeeper. If you don’t see any errors or warnings, the module has been built and installed successfully. The bindings have been tested with Python 2.3, 2.4, 2.5 and 2.6, and are known not to work with 2.2 and earlier. We haven’t yet tested them against Python 3.x – we’d love to hear your feedback about your experiences with the latest versions of Python. To run the queue example, you must have a ZooKeeper server running on the local machine at port 2181 (to change the location of the server, edit the string passed to zookeeper.__init__). The Java-based server will have been built when you ran ant from the root directory of the distribution earlier. Before the server can run, it needs a configuration file to read: cat &gt;&gt; conf/zoo.cfg tickTime=2000 dataDir=/tmp/zookeeper clientPort=2181 Now you can run bin/zkServer.sh start to start a standalone server on the local machine. To stop the server in the future, run bin/zkServer.sh stop. You’re finally ready to run the queue example: python queue.py The example is very simple. It queues three items, and then dequeues them. Wrapping up I hope that I’ve shown you that ZooKeeper is a very useful system, with powerful primitives that makes writing tricky distributed concurrent programs easier. There are many applications that ZooKeeper could help you build – lock servers, name services, metadata stores and even a unique kind of filesystem can be built in a straightforward way using the ZooKeeper API. The project is active and always looking for volunteers. ZooKeeper integration is already being built into HBase, and there are moves to bring greater reliability to Hadoop and HDFS by delegating some server functionality to ZooKeeper. As far as the Python bindings go, the next version will include better documentation, some more Python niceties such as default parameters and docstrings, and a more Pythonic wrapper object to wrap up some of the bookkeeping that ZooKeeper requires.</snippet></document><document id="723"><title>Announcing Cloudera Certification for Apache Hadoop</title><url>http://blog.cloudera.com/blog/2009/05/cloudera-certification-for-hadoop/</url><snippet>As Apache Hadoop continues to turn heads at startups and big enterprises alike, Cloudera has received several requests to offer certification in addition to our popular training programs. Certification is a critical component of any software ecosystem, and especially so for open source projects with quickly�expanding�user bases. Certification allows developers to ensure their skills are up to date, and allows employers and customers to confidently identify individuals that are up for the challenge of solving problems with Hadoop. To that end, we are happy to announce Cloudera Certification for Hadoop. Starting next month,�participants�looking to document their experience from our Hadoop Training programs can register for our Cloudera Certified Hadoop Professional (CCHP) exam. You’ll receive a paper certificate, but more importantly, we’ll record your test date so we can verify your certification to third parties like employers and potential clients. Our first scheduled certification exam is on June 23rd in Washington DC. Register here.</snippet></document><document id="724"><title>Announcing Hadoop World: NYC 2009: RFP Open</title><url>http://blog.cloudera.com/blog/2009/05/hadoop-summit-east-2009-rfp-open/</url><snippet>Lately, we’ve been spending a lot of time on the East Coast, and one thing is clear: Hadoop is everywhere. Hadoop usage on the East Coast tends to be slightly different. There are still web companies with armys of tech gurus, but there are also many “regular” industries and enterprises using and exploring Hadoop. It’s time to get together and learn a thing or two from one other. Hadoop World: NYC 2009 will take place on October 2nd, and focus on two areas of interest to enterprise users. We’ve opened requests for proposals at: http://www.eventbrite.com/event/352689905 Development and Administration: Core Hadoop: Areas for Development, Major Upcoming Contributions, Functional Deep Dives Administration: Large Cluster Overviews, Performance Tips, Resource Management, High Availability Developer: IDEs, QA Best Practices, Sharing Code / Data / Clusters, Higher Level Abstractions Hadoop Applications: Lessons from the Web: What can traditional industries learn from companies with web scale data? Industry Case Studies: Finance, Telecom / Utilities, Retail, Biotech, etc. Integration with Existing Systems: Databases, BI Tools, Message Buses and other Infrastructure New Ideas / Applications: Big Ideas for Hadoop Hosted Solutions: Hadoop and the Cloud We’ll close the RFP on July 31st and announce the schedule soon thereafter. More details including discounted / early registration and sponsorship info available at: http://www.cloudera.com/hadoop-world-nyc</snippet></document><document id="725"><title>Protecting per-DataNode Metadata</title><url>http://blog.cloudera.com/blog/2009/05/protecting-per-datanode-metadata/</url><snippet>Administrators of HDFS clusters understand that the HDFS metadata is some of the most precious bits they have. While you might have hundreds of terabytes of information stored in HDFS, the NameNode’s metadata is the key that allows this information, spread across several million “blocks” to be reassembled into coherent, ordered files. The techniques to preserve HDFS NameNode metadata are well established. You should store several copies across many separate local hard drives, as well as at least one remote hard drive mounted via NFS. (To do this, list multiple directories, on separate mount points, in your dfs.name.dir configuration variable.) You should also run the SecondaryNameNode on a separate machine, which will result in further off-machine backups of “checkpointed” HDFS state made on an hourly basis. But an aspect of HDFS that is talked about less frequently is the metadata stored on individual DataNodes. Each DataNode keeps a small amount of metadata allowing it to identify the cluster it participates in. If this metadata is lost, then the DataNode cannot participate in an HDFS instance and the data blocks it stores cannot be reached. The bug HADOOP-5342, “DataNodes do not start up because InconsistentFSStateException on just part of the disks in use” describes a condition where the DataNode metadata is corrupted across all the DataNodes, causing a cluster to be inaccessible. When an HDFS instance is formatted, the NameNode generates a unique namespace id for the instance. When DataNodes first connect to the NameNode, they bind to this namespace id and establish a unique “storage id” that identifies that particular DataNode in the HDFS instance. This data as well as information about what version of Hadoop was used to create the block files, is stored in a filed named VERSION in the ${dfs.data.dir}/current directory. In some conditions, the VERSION file can be removed or corrupted. The issue logged in Hadoop’s JIRA suggested that it occurs when some disks are mounted read-only, an upgrade is performed, and then the disks are remounted read-write, causing VERSION files to go out of sync. Unfortunately, we’ve seen this happen during routine operation as well, with no read-only disks, and no upgrades. While no programmatic fix to this problem is currently available, a workable solution to this problem is a simple piece of preventative maintenance: for each DataNode, keep a copy of ${dfs.data.dir}/current/VERSION stored in a separate directory, possibly off-machine. If this bug ever manifests, restore the backed-up VERSION files and restart HDFS. The bug does not affect the block files themselves. The VERSION files won’t change once they’re created, so you only need to back them up when they’re first created, or after you upgrade your HDFS instance. If you’re storing data on multiple disks per node, you should note that while the VERSION file on each node is unique, it is the same across all disks on the node. We’re interested in knowing what other HDFS best practices you’ve developed at your organization. Please share them in the comments section.</snippet></document><document id="726"><title>10 MapReduce Tips</title><url>http://blog.cloudera.com/blog/2009/05/10-mapreduce-tips/</url><snippet>This piece is based on the talk “Practical MapReduce” that I gave at Hadoop User Group UK on April 14. 1. Use an appropriate MapReduce language There are many languages and frameworks that sit on top of MapReduce, so it’s worth thinking up-front which one to use for a particular problem. There is no one-size-fits-all language; each has different strengths and weaknesses. Java: Good for: speed; control; binary data; working with existing Java or MapReduce libraries. Pipes: Good for: working with existing C++ libraries. Streaming: Good for: writing MapReduce programs in scripting languages. Dumbo (Python), Happy (Jython), Wukong (Ruby), mrtoolkit (Ruby): Good for: Python/Ruby programmers who want quick results, and are comfortable with the MapReduce abstraction. Pig, Hive, Cascading: Good for: higher-level abstractions; joins; nested data. While there are no hard and fast rules, in general, we recommend using pure Java for large, recurring jobs, Hive for SQL style analysis and data warehousing, and Pig or Streaming for ad-hoc analysis. 2. Consider your input data “chunk” size Are you generating large, unbounded files, like log files? Or lots of small files, like image files? How frequently do you need to run jobs? Answers to these questions determine how your store and process data using HDFS. For large unbounded files, one approach (until HDFS appends are working) is to write files in batches and merge them periodically. For lots of small files, see The Small Files Problem. HBase is a good abstraction for some of these problems too, so may be worth considering. 3. Use SequenceFile and MapFile containers SequenceFiles are a very useful tool. They are: Splittable. So they work well with MapReduce: each map gets an independent split to work on. Compressible. By using block compression you get the benefits of compression (use less disk space, faster to read and write), while keeping the file splittable still. Compact. SequenceFiles are usually used with Hadoop Writable objects, which have a pretty compact format. A MapFile is an indexed SequenceFile, useful for if you want to do look-ups by key. However, both are Java-centric, so you can’t read them with non-Java tools. The Thrift and Avro projects are the places to look for language-neutral container file formats. (For example, see Avro’s DataFileWriter although there is no MapReduce integration yet.) 4. Implement the Tool interface If you are writing a Java driver, then consider implementing the Tool interface to get the following options for free: -D to pass in arbitrary properties (e.g. -D mapred.reduce.tasks=7 sets the number of reducers to 7) -files to put files into the distributed cache -archives to put archives (tar, tar.gz, zip, jar) into the distributed cache -libjars to put JAR files on the task classpath public class MyJob extends Configured implements Tool {

  public int run(String[] args) throws Exception {
    JobConf job = new JobConf(getConf(), MyJob.class);
    // run job ...
  }

  public static void main(String[] args) throws Exception {
    int res = ToolRunner.run(new Configuration(),
      new MyJob(), args);
    System.exit(res);
  }
} By taking this step you also make your driver more testable, since you can inject arbitrary configurations using Configured’s setConf() method. 5. Chain your jobs It’s often natural to split a problem into multiple MapReduce jobs. The benefits are a better decomposition of the problem into smaller, more-easily understood (and more easily tested) steps. It can also boost re-usability. Also, by using the Fair Scheduler, you can run a small job promptly, and not worry that it will be stuck in a long queue of (other people’s) jobs. ChainMapper and ChainReducer (in 0.20.0) are worth checking out too, as they allow you to use smaller units within one job, effectively allowing multiple mappers before and after the (single) reducer: M+RM*. Pig and Hive do this kind of thing all the time, and it can be instructive to understand what they are doing behind the scenes by using EXPLAIN, or even by reading their source code, to make you a better MapReduce programmer. Of course, you could always use Pig or Hive in the first place… 6. Favor multiple partitions We’re used to thinking that the output data is contained in one file. This is OK for small datasets, but if the output is large (more than a few tens of gigabytes, say) then it’s normally better to have a partitioned file, so you take advantage of the cluster parallelism for the reducer tasks. Conceptually, you should think of your output/part-* files as a single “file”: the fact it is broken up is an implementation detail. Often, the output forms the input to another MapReduce job, so it is naturally processed as a partitioned output by specifying output as the input path to the second job. In some cases the partitioning can be exploited. CompositeInputFormat, for example, uses the partitioning to do joins efficiently on the map-side. Another example: if your output is a MapFile, you can use MapFileOutputFormat’s getReaders() method to do lookups on the partitioned output. For small outputs you can merge the partitions into a single file, either by setting the number of reducers to 1 (the default), or by using the handy -getmerge option on the filesystem shell: % hadoop fs -getmerge hdfs-output-dir local-file This concatenates the HDFS files hdfs-output-dir/part-* into a single local file. 7. Report progress If your task reports no progress for 10 minutes (see the mapred.task.timeout property) then it will be killed by Hadoop. Most tasks don’t encounter this situation since they report progress implicitly by reading input and writing output. However, some jobs which don’t process records in this way may fall foul of this behavior and have their tasks killed. Simulations are a good example, since they do a lot of CPU-intensive processing in each map and typically only write the result at the end of the computation. They should be written in such a way as to report progress on a regular basis (more frequently than every 10 minutes). This may be achieved in a number of ways: Call setStatus() on Reporter to set a human-readable description of the task’s progress Call incrCounter() on Reporter to increment a user counter Call progress() on Reporter to tell Hadoop that your task is still there (and making progress) 8. Debug with status and counters Using the Reporter’s setStatus() and incrCounter() methods is a simple but effective way to debug your jobs. Counters are often better than printing to standard error since they are aggregated centrally, and allow you to see how many times a condition has occurred. Status descriptions are shown on the web UI so you can monitor a job and keep and eye on the statuses (as long as all the tasks fit on a single page). You can send extra debugging information to standard error which you can then retrieve through the web UI (click through to the task attempt, and find the stderr file). You can do more advanced debugging with debug scripts. 9. Tune at the job level before the task level Before you start profiling tasks there are a number of job-level checks to run through: Have you set the optimal number of mappers and reducers? The number of mappers is by default set to one per HDFS block. This is usually a good default, but see tip 2. The number of reducers is best set to be the number of reduce slots in the cluster (minus a few to allow for failures). This allows the reducers to complete in a single wave. Have you set a combiner (if your algorithm allows it)? Have you enabled intermediate compression? (See JobConf.setCompressMapOutput(), or equivalently mapred.compress.map.output). If using custom Writables, have you provided a RawComparator? Finally, there are a number of low-level MapReduce shuffle parameters that you can tune to get improved performance. 10. Let someone else do the cluster administration Getting a cluster up and running can be decidely non-trivial, so use some of the free tools to get started. For example, Cloudera provides an online configuration tool, RPMs, and Debian packages to set up Hadoop on your own hardware, as well as scripts to run on Amazon EC2. Do you have a MapReduce tip to share? Please let us know in the comments.</snippet></document><document id="727"><title>5 Common Questions About Apache Hadoop</title><url>http://blog.cloudera.com/blog/2009/05/5-common-questions-about-hadoop/</url><snippet>There�s been a lot of buzz about Apache Hadoop lately. Just the other day, some of our friends at Yahoo! reclaimed the terasort record from Google using Hadoop, and the folks at Facebook let on that they ingest 15 terabytes a day into their 2.5 petabyte Hadoop-powered data warehouse. But many people still find themselves wondering just how all this works, and what it means to them. We get a lot of common questions while working with customers, speaking at conferences, and teaching new users about Hadoop. If you, or folks you know, are trying to wrap their head around this Hadoop thing, we hope you find this post helpful. Introduction: Throwing out Fundamental Assumptions When Google began ingesting and processing the entire web on a regular basis, no existing system was up for the task. Managing and processing data at this scale was simply never considered before. To address the massive scale of data first introduced by the web, but now commonplace in many industries, Google built systems from the ground up to reliably store and process petabytes of data. Many of us have been trained and conditioned to accept the common assumptions that traditional systems are built upon. Hadoop throws many of these assumptions out. If you can set aside these fundamental assumptions you’ll be one step closer to understanding the power of Hadoop. Assumption 1: Hardware can be reliable. It is true that you can pay a significant premium for hardware with a mean time to failure (MTTF) that exceeds its expected lifespan. However, working with web scale data requires thousands of disks and servers. Even an MTTF of 4 years results in nearly 5 failures per week in a cluster of 1,000 nodes. For a fraction of the cost, using commodity hardware with an MTTF of 2 years, you can expect just shy of 10 failures per week. In the big scheme of things, these scenarios are nearly identical, and both require fundamentally rethinking fault tolerance. In order to provide reliable storage and computation at scale, fault tolerance must be provided through software. When this is achieved, the economics of �reliable� hardware quickly fall apart. Assumption 2: Machines have identities. Once you accept that all machines will eventually fail, you need to stop thinking of them as individuals with identities, or you will quickly find yourself trying to identify a machine which no longer exists. It is obvious that if you want to leverage many machines to accomplish a task, they must be able to communicate with each other. This is true, but to deal effectively with the reality of unreliable hardware, communication must be implicit. It must not depend on machine X sending some data Y to machine Z, but rather some machine saying that some other machine must process some data Y. If you maintain explicit communication at scale, you face a verification problem at least as large as your data processing problem. This shift from explicit to implicit communication allows the underlying software system to reliably ensure data is stored and processed without requiring the programmer to verify successful communication, or more importantly, allow them to make mistakes doing so. Assumption 3: A data set can be stored on a single machine. When working with large amounts of data, we are quickly confronted with data sets that exceed the capacity of a single disk and are intractable for a single processor. This requires changing our assumptions about how data is stored and processed. A large data set can actually be stored in many pieces across many machines to facilitate parallel computation. If each machine in your cluster stores a small piece of each data set, any machine can process part of any data set by reading from its local disk. When many machines are used in parallel, you can process your entire data set by pushing the computation to the data and thus conserving precious network bandwidth. By abandoning these three assumptions, you can began to understand how “shared nothing” architecture principles enable Hadoop to provide reliable infrastructure using unreliable commodity hardware. With this laid out, let’s tackle a few questions we hear a lot… Does Hadoop replace databases or other existing systems? No. Hadoop is not a database nor does it need to replace any existing data systems you may have. Hadoop is a massively scalable storage and batch data processing system. It provides an integrated storage and processing fabric that scales horizontally with commodity hardware and provides fault tolerance through software. Rather than replace existing systems, Hadoop augments them by offloading the particularly difficult problem of simultaneously ingesting, processing and delivering/exporting large volumes of data so existing systems can focus on what they were designed to do: whether that be serve real time transactional data or provide interactive business intelligence. Furthermore, Hadoop can absorb any type of data, structured or not, from any number of sources. Data from many sources can be joined and aggregated in arbitrary ways enabling deeper analyses than any one system can provide. Lastly, these results can be delivered to any existing enterprise system for further use independent of Hadoop. For example, consider an RDBMS used for serving real-time data and ensuring transactional consistency. Asking that same database to generate complex analytical reports over large volumes of data is performance intensive and detracts from its ability to do what it does well. Hadoop is designed to store vast volumes of data, process that data in arbitrary ways, and deliver that data to wherever it is needed. By regularly exporting data from an RDBMS, you can tune your database for its interactive workload and use Hadoop to conduct arbitrarily complex analysis offline without impacting your real-time systems. How does MapReduce relate to Hadoop and other systems? Hadoop is an open source implementation of both the MapReduce programming model, and the underlying file system Google developed to support web scale data. The MapReduce programming model was designed by Google to enable a clean abstraction between large scale data analysis tasks and the underlying systems challenges involved in ensuring reliable large-scale computation. By adhering to the MapReduce model, your data processing job can be easily parallelized and the programmer doesn�t have to think about the system level details of synchronization, concurrency, hardware failure, etc. What limits the scalability of any MapReduce implementation is the underlying system storing your data and feeding it to MapReduce. Google chose not to use a RDBMS for this because the overhead incurred from maintaining indexes, relationships and transactional guarantees isn�t necessary for batch processing. Rather, they designed a new distributed file system, from the ground up, that worked well with �shared nothing� architecture principles. They purposefully abandoned indexes, relationships and transactional guarantees because they limit horizontal scalability and slow down loading and batch data processing for semi and unstructured data. Some RDBMSs provide MapReduce functionality. In doing so, they provide an easy way for programmers to create more expressive queries than SQL allows in a way that does not induce any additional scalability constraints on the database. However, MapReduce alone does not address the fundamental challenge of scaling out a RDBMS in a horizontal manner. If you need indexes, relationships and transactional guarantees, you need a database. If you need a database, one that supports MapReduce will allow for more expressive analysis than one that does not. However, if your primary need is a highly scalable storage and batch data processing system, you will often find that Hadoop utilizes commodity resources effectively to deliver a lower cost per TB for data storage and processing. How do existing systems interact with Hadoop? Hadoop often serves as a sink for many sources of data because Hadoop allows you to store data cost effectively and process that data in arbitrary ways at a later time. Because Hadoop doesn’t maintain indexes or relationships, you don’t need to decide how you want analyze your data in advance. Let�s look at how various systems get data into Hadoop. Databases: Hadoop has native support for extracting data over JDBC. Many databases also have bulk dump / load functionality. In either case, depending on the type of data, it is easy to dump the entire database to Hadoop on a regular basis, or just export the updates since the last dump. Often, you will find that by dumping data to Hadoop regularly, you can store less data in your interactive systems and lower your licensing costs going forward. Log Generators: Many systems, from web servers to sensors, generate log data and some do so at astonishing rates. These log records often have a semi-structured nature and change over time. Analyzing these logs is often difficult because they don�t �fit� nicely in relational databases and take too long to process on a single machine. Hadoop makes it easy for any number of systems to reliably stream any volume of logs to a central repository for later analysis. You often see users dump each day�s logs into a new directory so they can easily run analysis over any arbitrary time-frame’s worth of logs. Scientific Equipment: As sensor technology improves, we see many scientific devices ranging from imagery systems (medical, satellite, etc) to DNA sequencers to high energy physics detectors, generating data at rates that vastly exceed both the write speed and capacity of a single disk. These systems can write data directly to Hadoop and as the data generation rate and processing demands increase, they can be addressed simply adding more commodity hardware to your Hadoop cluster. Hadoop is agnostic to what type of data you store. It breaks data into manageable chunks, replicates them, and distributes multiple copies across all the nodes in your cluster so you can process your data quickly and reliably later. You can now conduct analysis that consumes all of your data. These aggregates, summaries, and reports can be exported to any other system using raw files, JDBC, or custom connectors. How can users across an organization interact with Hadoop? One of the great things about Hadoop is that it exposes massive amounts of data to a diverse set of users across your organization. It creates and drives a culture of data, which in turn empowers individuals at every level to make better business decisions. When a DBA designs and optimizes a database, she considers many factors. First and foremost is the structure of the data, the access patterns for the data, and the views / reports required of the data. These early decisions limit the types of queries the database can respond to efficiently. As business users request more views on the data, it becomes a constant struggle to maintain performance and deliver new reports in timely manner. Hadoop enables a DBA to optimize the database for its primary workload, and regularly export the data for analytical purposes. Once data previously locked up in database systems is available for easy processing, programmers can transform this data in any number of ways. They can craft more expressive queries and generate more data / cpu intensive reports without impacting production database performance. They can build pipelines that leverage data from many sources for research, development, and business processes. Programmers may find our online Hadoop training useful, especially Programming with Hadoop. But working closely with data doesn’t stop with DBAs and programmers. By providing simple high-level interfaces, Hadoop enables less technical users to ask quick, ad-hoc questions about any data in the enterprise. This enables everyone from product managers, to analysts, to executives to participate in, and drive a culture focused on data. For more details, check out our online training for Hive (a data warehouse for Hadoop with an SQL interface) and Pig (a high level language for ad-hoc analysis). How do I understand and predict the cost of running Hadoop? One of the nice things about Hadoop is that understanding your costs in advance is relatively simple. Hadoop is free software and it runs on commodity hardware, which includes cloud providers like Amazon. It has been demonstrated to scale beyond tens of petabytes (PB). More importantly, it does so with linear performance characteristics and cost. Hadoop uses commodity hardware, so every month, your costs decrease or provide more capacity for the same price point. You probably have a vendor you like, and they probably sell a dual quad-core (8 cores total) machine with 4 1TB SATA disks (you specifically don�t want RAID). Your budget and workload will decide whether you want 8 or 16GB of RAM per machine. Hadoop uses thee-way replication for data durability, so your �usable� capacity will roughly be your raw disk capacity divided by 3. For machines with 4x1TB disks, 1 TB is a good estimate for usable space, as it leaves some overhead for intermediate data and the like. It also makes the math really easy. If you use EC2, two extra large instances provide about 1 usable TB. Armed with your initial data size, the growth rate of that data, and the cost per usable TB, you should be able to estimate the cost of your Hadoop cluster. You will also incur operational costs, but, because the software is common across all the machines, and requires little per-machine tuning, the operational cost scales sub-linearly. To make things even easier, Cloudera provides our distribution for both local deployments and EC2 free of charge under the Apache 2.0 license. You can learn more at http://www.cloudera.com/hadoop Summary: Looking Forward Today, many enterprise users have the ability to generate and access ever increasing volumes of data. A TB really isn’t that much anymore, and when you consider that you can buy such a disk for about $100, you need to reconsider how much you pay for the surrounding infrastructure to manage that data. Looking beyond reliable storage and processing, Hadoop represents an understanding of how computer architecture has fundamentally changed in recent years and how those trends effect the foreseeable future. As processors get more parallel, disk density increases, and software becomes more effective at managing unreliable hardware, Hadoop enables you to reliably scale with your data and align costs with commodity trends.</snippet></document><document id="728"><title>Using Cloudera’s Hadoop AMIs to process EBS datasets on EC2</title><url>http://blog.cloudera.com/blog/2009/05/using-clouderas-hadoop-amis-to-process-ebs-datasets-on-ec2/</url><snippet>A while back, we noticed a blog post From Arun Jacob over at Evri (if you haven’t seen Evri before, it’s a pretty impressive take on search UI). We were particularly interested in helping Arun and others use EC2 and Hadoop to process data stored on EBS as Amazon makes many public data sets available. After getting started, Arun volunteered to write up his experience, and we’re happy to share it on the Cloudera blog.  -Christophe Background A couple of weeks ago I managed to get a Hadoop cluster up and running on EC2 using the /src/contrib/ec2 scripts found in the 0.18.3 version of Hadoop. This experience was not entirely pain free, and in order to spin up clusters without a lot of hand mods, I was going to have to modify those scripts to work with an explicit AMI in order to work around some of the issues I had run into. Fortunately, before I could get going on that rewrite, Christophe from Cloudera contacted me and let me know that they had addressed all of those issues, and invited me to try instantiating and running my MapReduce job using their scripts and AMIs. Prerequisites Prior to running any of the scripts/code below, I did the following: Signed up for Amazon S3 and EC2, see the EC2 Getting Started Guide . Installed the Amazon EC2 command line API tools and put them on my path. Set the following environment variables: AWS_ACCOUNT_ID — available through your AWS Account info AWS_ACCESS_KEY_ID –same AWS_SECRET_ACCESS_KEY — same EC2_CERT — fully qualified path to cert file. EC2_HOME — where you installed EC2 API. EC2_PRIVATE_KEY — fully qualified path to private key file. The file needs to have permissions set to 600 (rw for user only) in order to be used during initial cluster setup. Installed the 0.18.3 version of Hadoop for local debugging purposes. Installed the IBM MapReduce Tools for Eclipse plugin to my version of Eclipse (3.4.1). NOTE: I was unable to get the plugin to debug against an actual cluster, but I could run my MapReduce jobs in isolation against the local FS, and this allowed me to catch a lot of issues even before running on my local single node Hadoop cluster. Installed the ElasticFox and S3 Organizer FireFox plugins. Initializing the Cluster There was no drama here. When it was time to create another (larger) cluster up on EC2 for another job that we were going to run, I downloaded the scripts from Cloudera, put in my Amazon Key and security group info as instructed , and ran ./hadoop-ec2 launch-cluster {name of cluster} {number of instances} That’s it. While I’m happy that I worked with the original ec2 scripts from hadoop/src/contrib, I’m even happier that Cloudera’s rewrite makes this a one line operation. All HDFS initialization, master-slave connections, and other cluster setup was done for me. I also set up access to the Hadoop JobTracker and NameNode (HDFS) Web UIs, which allow me to track and debug job and HDFS status. I modified the hadoop master security group, granting http access to ports 50030 (JobTracker) and 50070 (NameNode). Note that you should constrain access to those nodes using CIDR block notation, because they have no local auth. There are many ways to do this, including using the ec2 command line API , but I used ElasticFox, a great UI for doing one-off things like setting up security groups. The instructions for adding permissions to the master are found in the Cloudera EC2 setup page. Slave node TaskTracker UIs are useful because they allow you to drill into specific task logs. However, the JobTracker UI tries to link to those UIs via their Amazon Internal DNS names. In order to get the slave node TaskTracker UIs to work, you would need to use hadoop-ec2 to start a local proxy, and use foxyproxy to set up your browser to work with the hadoop-ec2 proxy. Details on creating a pattern based proxy to access Amazon Internal DNS names using foxyproxy can be found in the troubleshooting section of the Amazon Elastic MapReduce Documentation. To start the proxy, run hadoop-ec2 proxy myclustername Make sure that foxyproxy is configured to talk to the proxy on port 6666. Accessing the Cluster The most logical way to get to the cluster is via the master instance. Starting jobs, uploading files to HDFS, and other tasks are all done from the master. The master (via NameNode and JobTracker) takes care of distributing input data to the nodes and running jobs (machine) local to those nodes. There are a couple of ways to access the master. From the elastic fox UI, select the master instance, then click on the key icon from the Instances tab. This opens up a shell to the master. You can also get command line access using the Cloudera hadoop-ec2 script: {location of scripts}/bin/hadoop-ec2 login myclustername Either way is required to actually run a job on the cluster. From the master it is very easy to ssh into the nodes — something I’ve found necessary when debugging map/reduce logic gone awry: ssh {internal or external dns name of slave node} is all that is required, since ssh keygen was taken care of during cluster setup. Running A Job Now that the cluster is up, accessible, and ready to go, I want to run a MapReduce job on it. At Evri one of the many sources we process as we go about creating a data graph of people, places, and things is Wikipedia. Wikipedia not only contains relevant facts about entities — it also contains a lot of useful metadata about those entities. Some of that metadata is immediately available in the article. The more interesting data is obtained by traversing Wikipedia dumps and aggregating basic metadata. One interesting piece of metadata is the number of inlinks (links to) for each Wikipedia article from other Wikipedia articles. An article with a lot of inlinks tends to be better known than an article with fewer inlinks. Collecting inlink count per article helps us get a baseline of how popular that article is. In order to do that, I actually need to extract all of the outlinks (links away) from each Wikipedia article, tracking where they are linking to. Then I need to aggregate the link destinations and summarize the link counts for each unique destination. This is a perfect candidate for a MapReduce job — transforming data in one form to another. It is very analogous to the canonical word frequency count MapReduce job. One of the great things about the intersection of Hadoop and EC2 is the ability to use Amazon’s Public Datasets as input data for a MapReduce. That’s right, huge amounts of maintained, clean data, there for the taking. The dataset that is most useful to us is the Freebase Wikipedia Extraction (WEX) data set. Freebase processes the raw (wikitext) Wikipedia dumps and converts the wiki text to XML, which makes programmatic analysis of Wikipedia much easier for the rest of us. This dump is available from their website, but they’ve gone the extra mile and have uploaded the results to Amazon in TSV (tab separated value) format, which is exactly what we need to process the data in Hadoop — thanks Freebase! The Source Code We’ve defined exactly what we need to do above: collate all links to all Wikipedia pages and aggregate their counts. We will do this by counting the frequency of all links in the Map phase as we process all Wikipedia articles during the Map phase. We will emit link URIs as keys, and counts as values. Hadoop will then collate the data and group it under the key we specified during the Map phase, and then call our Reduce logic during the Reduce phase. Our reduce logic will sum up all counts for each URI, giving us inlink counts for that URI. I have attached the source code, but want to point out some of the relevant parts in the mapper and reducer. I use JDOM and some dependent libs (Xerces, Jaxen) to parse the XML. For more fundamental questions about Hadoop MapReduce classes, I recommend the basic Hadoop Tutorial. The Mapper class is declared as follows: it takes in a line number and text field, and for each URI it finds, maps counts of outlinks to a count associated with the URI. This is done by implementing the Mapper interface with the appropriate input parameters — line number = LongWritable, line text = Text) and output (URI = Text, count = LongWritable). Hadoop needs to provide alternative implementations to standard Java classes like Long and String because it implements a custom serialization format. public class LinkMapper extends MapReduceBase implements Mapper {
...
} The Mapping function is pretty simple. It is passed a line count and String representing each line in the freebase-articles.tsv. It parses the XML, and pushes all outlink counts to the passed in Collector. All mapping functionality updates the Collector with key/value pairs, and updates the Reporter with status, counter value updates, or errors. /**
* calculates frequency of targets in processed XML
*/
@Override
public void map(LongWritable lineCt, Text line, OutputCollector collector, Reporter reporter) throws IOException {
     String[] parts = line.toString().split("\t");
    // the xml should be in the 4th split.
    if(parts.length == 4) {
       String xml = parts[3];
         // fix up xml so that JDOM can parse it.
       xml = xml.replaceFirst("xmlns:xhtml=\" \"", "xmlns:xhtml=\"foo\"");
       String xmlProtocol = "";

       String fullXml = xmlProtocol+xml;
         for (Map.Entry entry : parseXml(fullXml).entrySet()) {
             collector.collect(entry.getKey(), entry.getValue());
         }

    }
}
 The parseXML() function called by map() uses XPath to locate all outlinks and puts them into a map, so instances of the same outlink are detected and the count is incremented. The wikipedize() method creates a wikpedia specific URI from the target text. /**
* gets a map of counts by link for a given XML fragment.
* @param parser
* @param xmlFragment
* @return map of URIs mapped to counts
* @throws IOException
*/
protected  Map parseXml(String xmlFragment) throws IOException {
Map parsedLinks = new HashMap();
        logger.debug("parsing "+xmlFragment);
     SAXBuilder builder = new SAXBuilder();

        // command line should offer URIs or file names
        try {
          StringReader reader = new StringReader(xmlFragment);
         Document doc = builder.build(reader);

         XPath xp = XPath.newInstance("//target");

         List targets = xp.selectNodes(doc);

         for(Object obj : targets) {
             Element target = (Element)obj;
             Text wikiLink = new Text(wikipedize(target.getText()));
             LongWritable newCount  = ONE;
             LongWritable count = parsedLinks.get(wikiLink);
            if(count != null) {
               newCount = new LongWritable(count.get()+1);
            }

            logger.debug("putting key="+wikiLink.toString()+", value="+newCount.get());
            parsedLinks.put(wikiLink,newCount);
        }

    }
    catch (JDOMException e) {
      logger.error(xmlFragment + " is not well-formed.");
      logger.error(e.getMessage());
    }
    catch (IOException e) {
      logger.error("Could not check " + xmlFragment);
      logger.error(" because " + e.getMessage());
    }  

  return parsedLinks;

} The Reducer is also very simple. It sums the counts for each key and updates the collector with the sum. public class URIReducer extends MapReduceBase
    implements Reducer {

     @Override
     public void reduce(Text key, Iterator values,
            OutputCollector collector,
            Reporter reporter)
        throws IOException {

             long totalSum = 0;
             while(values.hasNext()) {
             LongWritable value = values.next();
                 totalSum += value.get();
             }
             collector.collect(key,new LongWritable(totalSum));
    }
} Note that since this reducer is simply an aggregator, I can also use it as a Combiner, which is in effect a local reduce for a slave node prior to the overall aggregation and sorting that happens in order to run the global reduce. The Combiner is set (just like the Mapper and Reducer) in the JobConf class used to run the job. This code is contained in the WexLinkDriver class definition in the attached code: 
...
JobClient client = new JobClient();
JobConf conf = new JobConf(getConf(),
com.evri.infocloud.wexlinks.WexLinkDriver.class);

conf.setJobName("test job");
conf.setOutputKeyClass(Text.class);
conf.setOutputValueClass(LongWritable.class);

conf.setMapperClass(LinkMapper.class);
conf.setCombinerClass(URIReducer.class);
conf.setReducerClass(URIReducer.class);

conf.setInputFormat(TextInputFormat.class);
conf.setOutputFormat(TextOutputFormat.class);

FileInputFormat.setInputPaths(conf,new Path(input[1]));
FileOutputFormat.setOutputPath(conf,new Path(output[1]));

client.setConf(conf);
JobClient.runJob(conf);
...
 Building the Jar File The source code needs to be bundled into a jar and uploaded to the master node. Jars that the source code rely on also need to be bundled into that jar. The jar contains the following directories: com.* (source code path) lib (all jars the source depends on) META-INF (contains MANIFEST.MF, specifying Main-Class to run) I created a build.xml to copy the lib directory to the correct location where it can be jarred up relative to the classes and the META-INF directory:     &lt;property name="classes.dir" value="target/classes"/&gt;

    &lt;target name="init"&gt;
        &lt;mkdir dir="${classes.dir}"/&gt;
        &lt;copy includeemptydirs="false" todir="${classes.dir}"&gt;
            &lt;fileset dir="src" excludes="**/*.launch, **/*.java"/&gt;
        &lt;/copy&gt;
    &lt;copy includeemptydirs="false" todir="${classes.dir}/lib"&gt;
            &lt;fileset dir="lib" includes="**/*.jar"/&gt;
        &lt;/copy&gt;
    &lt;/target&gt;
    &lt;target depends="init" name="build"&gt;
        &lt;echo message="${ant.project.name}: ${ant.file}"/&gt;
        &lt;javac debug="true" debuglevel="${debuglevel}" destdir="target/classes"
            source="${source}" target="${target}"&gt;
            &lt;src path="src"/&gt;
            &lt;classpath refid="fb-wex-inlink-aggregator.classpath"/&gt;
        &lt;/javac&gt;
    &lt;/target&gt; This creates the right structure under target/classes, which gets jarred up in a later step: 
    &lt;target depends="build, test" name="hadoop-jar"&gt;
    	&lt;jar destfile="target/inlink-aggregator.jar"
        	manifest="src/META-INF/MANIFEST.MF"
        	basedir="${classes.dir}" &gt;
    	&lt;/jar&gt;
    &lt;/target&gt;
 See the attached build.xml for the entire listing. Getting The Dataset The Dataset, as mentioned above, is available as an Elastic Block Store — available to be mounted by an EC2 instance. Mounting an EBS volume is as easy as (a) connecting the EBS volume to an EC2 instance (in this case, the master) as a virtual device on that EC2 instance, and (b) mounting that instance, i.e. running mount {device} {directory}. I connect the EBS volume to my master EC2 instance via elastic fox, although, as mentioned above, it is possible to do this with the EC2 command line tools. Via ElasticFox, I select the Volumes and Snapshots tab. click on the ‘+’ button to add a new volume. input the size of the volume I need (Freebase-WEX needs 66GB). Input the snapshot ID of the volume I want to attach to (the snapshot ID of Freebase WEX is snap-1781757e) Now that I have the volume I need to attach it to the master. right click on the volume. select ‘Attach this volume’ menu option. input the instance id of the master node. Once I’ve attached the volume to the instance, I ssh into the instance and mount the device. If I’ve attached the EBS volume as /dev/sdb, I mount like this: mount /dev/sdb freebase-wex Uploading the Data to the Cluster Now that I’ve mounted the Freebase WEX data, I am going to use {mount dir}/rawd/articles/freebase-wex-2009-01-12-articles.tsv as the input data for my hadoop job. In order to do this I need to get articles.tsv onto the cluster HDFS. Since the Cloudera scripts have taken care of HDFS Namenode initialization, all I need to do is create an input directory and upload the tsv file to it: hadoop fs -mkdir input hadoop fs -copyFromLocal freebase-wex/rawd/articles/freebase-wex-2009-01-12-articles.tsv input Running the Job In order to run the job, I need to upload the jar to the master node. I copy the jar to the master node: hadoop-ec2 push my-cluster-name inlink-aggregator.jar and then run the jar as follows hadoop jar inlink-aggregator.jar input output Note that the output directory cannot exist when the job is run. Results The results of the MapReduce job are in your output dir, which is in HDFS. Copy those results down to the master node: hadoop fs -copyToLocal {HDFS output dir} {desired output dir on local FS} and (if you want) save them to S3 using the /usr/bgin/s3cmd.rb file that comes on the Cloudera AMI. In order to use s3cmd, you need to set the AWS_ACCESS_KEY and AWS_SECRET_ACCESS_KEY environment variables. s3cmd put --force output/* s3://hadoop-freebase-wex-demo/output From S3 you can download them to your machine using the S3 sync tool of your choice. I use S3Organizer to do this. NOTE: you can upload directly to S3 and skip the above. In the cluster master: (1) edit /etc/hadoop/conf/hadoop-site.xml, and add the following: &lt;property&gt;
 &lt;name&gt;fs.s3n.awsAccessKeyId&lt;/name&gt;
 &lt;value&gt;YOUR-AWS-ACCESS-KEY-ID&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
 &lt;name&gt;fs.s3n.awsSecretAccessKey&lt;/name&gt;
 &lt;value&gt;YOUR-AWS-SECRET-ACCESS-KEY&lt;/value&gt;
&lt;/property&gt; (2) restart NameNode so that HDFS picks up the changes service hadoop-namenode restart (3) run hadoop jar inlink-aggregator.jar input s3n://infocloud-subject-output-1/run1 As a final note, I noticed that unless I specified the S3 output dir as a subdir of a top level S3 bucket, I recieved the following exception: java.lang.IllegalArgumentException: Path must be absolute: s3n://infocloud-subject-output-1 In subsequent MapReduce jobs, this logic also applies to the input dir (if you are using an S3 bucket as input). Details on this issue can be found here. Debugging/Analyzing the Results While the job ran fine, examination of the results shows that the counts are suspiciously low. For example Barack Obama only has 57 inlinks. At this point I’m going to refactor the mapping code and start tracking map state using Counters. I have a suspicion that (a) the tsv format may not be consistent, or (b) the XML I’m parsing is not valid. In order to test those theories I’m going to instrument the code with counters. In order to use Counters I need to create an enum that contains the states I want to represent. public enum MapState {
ERROR_INVALID_XML,
ERROR_IO_EXCEPTION,
XML_PARSED,
ERROR_RUNTIME_EXCEPTION,
}; I first instrumented the existing code and ran it to see if I was getting exceptions or just not parsing XML. I used the Reporter interface to instrument the code, using the enums I had created: reporter.incrCounter(MapState.XML_PARSED, 1); I ran, and saw no exceptions, but the XML_PARSED count was 574251, which seems very low, considering that the input file has 4183153 records. I then modified the mapping code to check every line part to see if it was the XML, and re-ran: @Override
public void map(LongWritable lineCt, Text line,
OutputCollector collector, Reporter reporter) throws IOException { String[] parts = line.toString().split("\t"); String xml = null; try { // the xml is not located in the same split from row to row, find first one. for(int i = 0; i &lt; parts.length;i++) { if(parts[i].startsWith(ARTICLES_XMLNS)) { xml = parts[i]; break; } } if(xml != null) { // fix up the XML so that JDOM can parse it. xml = xml.replaceFirst("xmlns:xhtml=\" \"", "xmlns:xhtml=\"foo\""); String xmlProtocol = ""; String fullXml = xmlProtocol+xml; for (Map.Entry entry : parseXml(fullXml).entrySet()) { collector.collect(entry.getKey(), entry.getValue()); } reporter.incrCounter(MapState.XML_PARSED, 1); } } catch (JDOMException e) { logger.error(xml+ " is not well-formed."); logger.error(e.getMessage()); reporter.incrCounter(MapState.ERROR_INVALID_XML, 1); } catch (IOException e) { logger.error("Could not check " + xml); logger.error(" because " + e.getMessage()); reporter.incrCounter(MapState.ERROR_IO_EXCEPTION, 1); } catch (RuntimeException ex) { logger.error("RUNTIME EXCEPTION: "+ex.getMessage()); logger.error("xml: "+xml); reporter.incrCounter(MapState.ERROR_RUNTIME_EXCEPTION, 1); throw ex; } } When re-running this job, I noticed it ran a lot slower. When I checked my counter values via the web UI after the job completed the XML_PARSED counter was set to 4,180,888, much more in line with the line count . So apparently I _was_ missing a lot of data. A quick check on Barack Obama reveals that his page inlink count went from 77 to 11851, which is more in line with what I would expect. The Web UI also provides access to the logs, provided that (a) you open up the TaskTracker port on the slave security group, and (b) you use the Amazon public DNS name instead of the Amazon private DNS name. Summary The ease with which I was able to get a job running on EC2 using the Cloudera scripts and AMIS helped me resolve focus on the logic related issues of my MapReduce code instead of thrashing around getting the cluster functional. The other nice thing about the Cloudera scripts is that I didn’t have to spend time learning the (182!) EC2 scripts. The hadoop-ec2 script, in combination with ElasticFox for finding and mounting an EBS volume, was all I really needed. If it isn’t clear by now, I am really, really happy that Cloudera has taken the time to do this, and do it very well. Bloopers There were a couple of potholes along the way that I want to mention should anyone else fall into them. (1) File permissions on my EC2 private key needed to be set to 0600, otherwise the hadoop-ec2 launch-cluster script would fail trying to set up passwordless ssh between master and slaves. (2) Logging. I was logging the entire blurb of XML prior to refactoring the code to look for the XML in ‘non standard’ locations. When I started up the job again, it would run just fine, and then I would start seeing: /04/29 17:50:50 INFO mapred.JobClient: Task Id : attempt_200904291618_0002_m_000041_0, Status : FAILED
java.io.IOException: Task process exit with nonzero status of 1.
        at org.apache.hadoop.mapred.TaskRunner.runChild(TaskRunner.java:462)
        at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:403)

09/04/29 17:50:50 WARN mapred.JobClient: Error reading task outputhttp://domU-12-31-39-02-CA-45.compute-1.internal:50060/tasklog?plaintext=true&amp;taskid=attempt_200904291618_0002_m_000041_0&amp;filter=stdout
09/04/29 17:50:50 WARN mapred.JobClient: Error reading task outputhttp://domU-12-31-39-02-CA-45.compute-1.internal:50060/tasklog?plaintext=true&amp;taskid=attempt_200904291618_0002_m_000041_0&amp;filter=stderr Eventually, this would happen across a majority of slaves and the master would consider the job ‘failed’. Of course, I could not actually see what went wrong with the task because the JobClient couldn’t read the error output. I did manage to look at the logs for several of the task trackers that failed, and decided to stop logging the XML. The issue resolved itself at that point. (3) Debugging: I would recommend always running a MapReduce job over a small set of data on a local single node install prior to pushing it out to the cluster. While this didn’t reproduce the logging issue above, it did catch some late night null pointer exception goofs. Since debugging failures over any distributed environment is a detective game at best, you want to show up with something that has been reasonably tested before throwing it across an N node cluster.</snippet></document><document id="729"><title>What’s New in Hadoop Core 0.20</title><url>http://blog.cloudera.com/blog/2009/05/what%e2%80%99s-new-in-hadoop-core-020/</url><snippet>Hadoop Core version 0.20.0 was released on April 22. In this post I will run through some of the larger or more significant user-facing changes since the first 0.19 release—there were 262 Jiras fixed for this release (fewer than the 303 in 0.19.0). The full list, which includes many bug fixes, can be found in the change log (or in Jira), and in the release notes. The JDiff documentation provides a view of what changed to the public APIs. You can download Hadoop Core from an Apache Mirror. Thanks to everyone who contributed to this release! Core The two main components in Hadoop Core are the distributed filesystem (HDFS) and MapReduce. There are plans, however, to move these components into their own Hadoop subprojects, so they can have their own release cycles, and to make it easier to manage their development (with separate mailing lists). Hadoop Core would remain, with the vestiges of other filesystems (local, S3, KFS), and other core infrastructure for I/O, such as RPC libraries, serialization, compression, MapFiles and SequenceFiles. Splitting Core, HDFS, and MapReduce did not happen for the 0.20.0 release, so you will find HDFS and MapReduce in a single download as usual, but there are some changes in preparation for this split. Notably, hadoop-site.xml has been broken into three: core-site.xml, hdfs-site.xml, and mapred-site.xml (HADOOP-4631). You can still use a single hadoop-site.xml, albeit with a warning. The default files have also been split, although they no longer are bundled in the conf directory; instead you can view them in the form of HTML files in the docs directory of the distribution (or online: core-default.html, hdfs-default.html, mapred-default.html). Also, the control scripts start-all.sh and stop-all.sh have been deprecated, in favor of controlling the two sets of daemons separately (using start-dfs.sh, start-mapred.sh, stop-dfs.sh, stop-mapred.sh). It’s a minor change, but the ability to comment out entries in the slaves file with the # character (HADOOP-4454) is a very useful one in practice. Hadoop configuration files now support XInclude elements for including portions of another configuration file (HADOOP-4944). This mechanism allows you to make configuration files more modular and reusable. There is a lot of work going on around security in Hadoop. Most of the changes will appear in future releases, but one that made it into 0.20.0 is service-level authorization (HADOOP-4348). The idea is that you can set up ACLs (access control lists) to enforce which clients can communicate with which Hadoop daemons. (Note that this mechanism is advisory: it is not secure since the user and group identities are not authenticated. This will be remedied in a future release.) For example, you might restrict the set of users or groups who can submit MapReduce jobs. See the documentation for further details. One thing that’s no longer in Hadoop Core are the LZO compression libraries, which had to be removed due to license incompatibilities (HADOOP-4874). If you can use GPL’d code then you can still find the LZO compression code at the hadoop-gpl-compression project on Google Code. You may also be interested in LzoTextInputFormat (HADOOP-4640), which uses a pre-generated index of LZO-compressed input files to split the compressed files for MapReduce processing. Note that LzoTextInputFormat is not in 0.20.0 either, so you need to build it yourself. HDFS HDFS append has been disabled in the 0.20.0 release due to stability issues (HADOOP-5332). It has also been disabled in 0.19.1, which means that there is currently no stable Hadoop release with a functioning HDFS append function. The configuration property dfs.support.append can be set to true if you wish to try out the append feature, but this should not be used in production. You can track the progress of the work to get append working in HADOOP-5744. There is a new command for cluster administrators to save the HDFS namespace (HADOOP-4826). By running hadoop dfsadmin -saveNamespace (when in safe mode), the namenode will dump its namespace to disk. This is useful when doing a planned namenode restart since the edits log doesn’t need to be replayed when the namenode comes back up. MapReduce The biggest change in this release is the introduction of a new Java API for MapReduce, dubbed “Context Objects” (HADOOP-1230). The Streaming and Pipes APIs are unchanged, and, in fact, the Pipes API already looks like the new Java one (some of the ideas were tried out there first). The change was motivated by a desire to make the API easier to evolve in the future, by making Mapper and Reduce abstract classes (not interfaces) and by introducing a Context object (hence the name) as a central place to get information from the MapReduce framework. Context objects also replace OutputCollectors for emitting values from the map() or reduce() functions. There are also the following changes: JobConf no longer exists. Use Configuration to hold job configuration. It’s now easier to get the job configuration in the map() or reduce() method: just call context.getConfiguration() on the Context object you have been passed. The new API supports a “pull” style of iteration. Previously, if you wanted to process a batch of records in one go (in the mapper), you would have to store them in instance variables in the Mapper class. With the new API you have the option of calling nextKeyValue() within the map() method to advance to the next record. You can also control how the mapper is run by overriding the run() method. There are no IdentityMapper and IdentityReducer classes in the new API, since Mapper and Reducer perform the identity function by default. The new API is not backward compatible with the old one (which is still present but deprecated—there is no published date as to when it will be removed) so you will need to re-write your applications to use it. Some examples that come with Hadoop have been migrated, so you can look at these (for example WordCount) for hints on how to change your code (basically a few method signature changes). Be aware, that some MapReduce libraries (such as MultipleInputs and MultipleOutputs) have not been moved to the new API yet so you should check before starting a migration. The new API is found in the org.apache.hadoop.mapreduce package and subpackages; the old API is in org.apache.hadoop.mapred. Multiple task assignment (HADOOP-3136). This optimization allows the jobtracker to assign multiple tasks to a tasktracker on each heartbeat, which can help improve utilization. A configuration parameter mapred.reduce.slowstart.completed.maps was introduced at the same time (default 0.05), which specifies the proportion of map tasks in a job that need to be complete before any reduce tasks are scheduled. Input formats have seen some interesting improvements. With HADOOP-3293, FileInputFormat (the most widely-used input format) does a better job of choosing the hosts that have the maximum amount of data for a given file split. You don’t need to make any changes to you code for this to work. In a separate development, CombineFileInputFormat, introduced in 0.20 (HADOOP-4565), is a new type of file input format that can combine multiple files into a single split, while taking data locality into account. This input format can be a useful remedy for the small files problem. It may also be useful for consuming multiple HDFS blocks in one mapper, if you are finding your mappers are running too quickly, without having to change the block size of your files. Gridmix2 (HADOOP-3770) is the second generation of the suite of benchmarks to model typical cluster MapReduce workloads seen in practice. Find out more about it in the src/benchmarks directory. It was always faintly embarrassing that the firepower of a Hadoop cluster has difficulty calculating pi to more than a few places of decimals. The new version of the PiEstimator example (HADOOP-4437) uses a Halton sequence to produce a better distribution of random input, and as a result produces a more accurate estimate of pi. Contrib Two new contrib modules appeared in the 0.20 branch. HDFS Proxy is a new contrib module for running a proxy that exposes the (read-only) HSFTP interface to HDFS. This may be useful if you want to provide secure read-only access for users who don’t have direct access to a cluster. Vaidya is a tool for diagnosing problems with MapReduce jobs by looking at the job history and configuration after a job has run. It has a set of rules that detect common problems and suggests improvements you can make to your code or configuration to avoid them.</snippet></document><document id="730"><title>High Energy Hadoop</title><url>http://blog.cloudera.com/blog/2009/05/high-energy-hadoop/</url><snippet>We asked Brian Bockelman, a Post Doc Research Associate in the Computer Science &amp; Engineering Department at the University of Nebraska–Lincoln, to tell us how Hadoop is being used to process the results from High-Energy Physics experiments.  His response gives insights into the kind and volume of data that High-Energy Physics experiments generate and how Hadoop is being used at the University of Nebraska. -Matt In the least technical language, most High Energy Physics (HEP) experiments involve colliding highly energetic, common particles together in order to create small, exotic, and incredibly short-lived particles.  The Large Hadron Collider (LHC), in particular, collides protons together at an energy of 7 TeV per particle (design energy; turn-on energy will be less than this).  The headline search for the LHC is the search for the Higgs boson, but it really probes an energy scale never before seen–it’s expected to be ripe with new physics beyond the Standard Model. There are two major components of a HEP experiment–the accelerator and the detector (in the case of the LHC, the detectors).  The accelerator ring at the LHC is a 27 km ring that runs underneath Switzerland at a depth of 100 to 175 meters, depending on the part of the ring.  The design is such that small bunches of protons (about 2808 bunches in the ring at a time) travel around the rings and are collided inside huge particle detectors.  The design is that a collision occurs once every 25 nanoseconds (ns), but it will probably be closer to one collision every 75 ns when it turns on later this year. There are four major experiments on the LHC rings–ATLAS, CMS, LHCb, and ALICE.  ATLAS and CMS are general purpose detectors, while LHCb and ALICE are specialized ones–I work for CMS computing (not a physicist).  You can think of a detector as an incredibly specialized digital camera wrapped around the beam pipe.  Compact Muon Solenoid (CMS) is a layered design, with highly accurate silicon detector nearest to the beam pipe, a very large magnet (which causes charged particles to change course, allowing us to distinguish charged vs non-charged particles, as well as get their energy), and huge, heavy outer layers that capture the energy of very energetic particles.  In CMS, we will be getting collisions at a rate of 40MHz; each collision has about 1MB worth of data coming from the various components.  This immediately is a problem: 40MHz x 1MB = 320 Tbps, a completely unmanageable amount of data.  Luckily, nature provides an answer: many of these beam crossings will result in non-energetic collisions (particles glancing off each other instead of direct collisions) and quantum physics dictates that even in highly energetic collisions, the outcome is statistically determined.  This means that a huge percentage of beam crossings are completely uninteresting; there’s a very complex custom compute system (called the trigger) that sifts through the data and records only the most interesting ones.  This cuts down the entire collision rate to about 300Hz. The raw data coming in at 300Hz x 1MB transfers from the online computing system to the offline computing system, which is not time-sensitive and does the majority of the processing.  This data goes through the following transformations on its journey from a network in Switzerland down to a physicist sitting anywhere in the world: Raw sensor data is turned into reconstructed data.  This takes the information about which pixels / elements are turned on and off into possible tracks of particles as well as a lot of data about how the information was constructed.  The reconstructed data is again about 1MB. Reconstructed data is turned into analysis-oriented data (AOD).  This strips out the per-event data until it only contains physics activity–no more detector information, just tracks and types of particles that came out of the event.  The AOD data is about 110KB. The AOD data is turned physicist-specific ntuples.  This will be a small, column-oriented format containing exactly the data the physicist is interested in.  This could be as small as a KB per event. At each transformation, the data is organized into more specific datasets–at the last step, where the physicist selects his specific data, he is usually very specific.  The billions of events in all of CMS are condensed down into a few thousand interesting events. Because of the computational and data requirements, not all of the transformations can occur centrally at CERN.  CERN has just enough computing power to write the raw data to tape and reconstruct a small percentage of the data (for data quality monitoring); immediately, it transfers the raw data to large data centers around the world (called Tier 1 centers; there are 7 for CMS), where the raw data is again written to tape and the reconstruction and transformation into AODs is performed.  From the Tier 1s, the data (right now, the reconstructed data–the reconstruction algorithms are new enough that folks don’t yet trust the AODs) is transferred to Tier 2s, where it is analyzed by physicists. In the US, there are 7 Tier 2 sites (about 50 worldwide for CMS); each had a target of 200 TB in 2008 and should get up to 400TB in 2009.  Each T2 has to be accessible by all the worldwide collaborators–so we see a wide variety of jobs–and is maintained by 2 administrators: about one for the computing side, one for the storage side. So, what are the characteristics of the Tier 2 system?  We have: A large amount of data (400TB is still big these days, right?  It seems so small sometimes.) A large data rate: Tier 2s download the data the physicists are interested in; this means that we need to be able to burst data as fast as possible–preferably in the range of 10Gbps. Need for reliable, but not archival storage.  Losing data is a huge headache, but at least most of it can be re-downloaded.  For user data, it can usually be regenerated, but folks can be seriously upset if they get pushed back by a few weeks. A small amount of effort (For running cutting-edge technology, 2 people get spread awfully thin). A need for interoperability for other sites.  Not all T2 sites will ever use the same technology, and the Tier 1 sites need deep integration with tape libraries; we need to use standard interfaces to transfer data between sites. So, how did we end up looking at Hadoop?  Grid computing in the LHC has existed for around 10 years, so there are other choices available in the area that we can choose from and have many years of experience with.  However, we believe that the Hadoop Distributed File System (HDFS) is superior to these for Tier 2s because of: Manageability: A lot of the storage systems in our area are designed either for reliable hardware or to be backed up by tape, and then adopted to the Tier 2 sites.  It turns out that, beyond a certain number of boxes, even reliable hardware is “unreliable”.  Because HDFS is designed from ground-up with commodity hardware in mind, some of the common tasks–decommissioning a node for example–are straightforward and built into the filesystem itself.  In the case of many node failures, the fsck utility is *wonderful*: simple, fast, and straightforward. Reliability: The replication features of HDFS work strikingly well.  We keep two replicas, which makes data loss theoretically fairly unlikely but still possible: we find that the replication features work just as well as advertised.  This is very important: it decouples the loss of a hardware with admin interventions. Usability: This is perhaps less important to the physicists (who have large computing organizations that can provide a layer to make I/O transparent to the end user) as it is to our site: having a mountable file system through FUSE means that we can now re-use the system we’ve built for the physicists for a wide range of users (not to mention that we can offer MapReduce services to our campus users). Scalability: By having the worker nodes and the storage systems co-located, we get far superior scalability compared to having a separate storage cluster.  Hands-down, there are few systems that can pump out as much data as fast as HDFS does. We use HDFS as a generic, distributed file system for now by mounting it onto our worker nodes with FUSE.  On top of that, we layer SRM (which is a generic web-services interface) and a Globus GridFTP server (a standard grid protocol for WAN transfers).  The FUSE components allow our physicists’ applications–consisting of several million lines of C++, probably of comparable size to the Linux kernel–to access HDFS without modification, while the two grid components allow us to interoperate with other, non-Hadoop sites. So, where do I see us going in the future?  I understand that the batch scheduler we are using (Condor) is starting to explore ways to integrate HDFS into its own system in order to deliver both compute and data scheduling, which has many potential possibilities.  I believe that the physicists’ transformation and reduction of data is very similar to the MapReduce paradigm, and there might someday be small explorations into using the MapReduce components of Hadoop–but that’s pretty far off.  With millions of lines, and a new detector coming online, the emphasis in this field is on stability and bullet-proofing existing systems. What do I see as needed developments?  Off the top of my head, here’s what I think: As we work with national labs, there’s an emphasis on security.  We’re getting awfully anxious about seeing strong authentication come to the file system. As we are getting closer to having a production system, we’d really like to see a high-availability solution for HDFS start to happen–especially if it included a way to automatically sync checkpoints from within the system, instead of externally. The physicist’s workflow isn’t entirely event based: it’s somewhat akin to a column store.  Each event has many, many pieces of data.  If a physicist only needs a small subset of the data (such as, a few KB out of a MB event), then it’s much more convenient to use a column-based layout (putting similar types of data contiguously in the file) instead of an event-based layout (putting each event contiguously in the file).  The underlying application they use, ROOT, uses somewhat of a hybrid between the two.  Because of this, the I/O patterns can be horribly random.  I’d really like to have a strong understanding of HDFS’s random I/O performance, a somewhat predictable model or guideline for how it works out, and a strong knowledge of how this affects the ROOT-based I/O. I guess you can say that HEP is a large field with a long background in big computing.  A lot of folks are watching the investigations happening in Nebraska, and seem enthusiastic to see it succeed.  However, as in any field with a broad range of users and sites, we are proceeding carefully and cautiously, and trying to do thorough investigations so we make sure we understand the pro’s and con’s before diving in.  HDFS has held up very well in our trials at Nebraska; I fully expect it to be accepted as a CMS storage element and probably deployed at other sites who are also looking for new storage solutions.</snippet></document><document id="731"><title>Debian packages for Apache Hadoop</title><url>http://blog.cloudera.com/blog/2009/04/debian-packages-for-hadoop/</url><snippet>When we announced Cloudera’s Distribution for Apache Hadoop last month, we asked the community to give us feedback on what features they liked best and what new development was most important to them. Almost immediately, Debian and Ubuntu packages for Hadoop emerged as the most popular request. A lot of customers prefer Debian derivatives over Red Hat, and installing RPMs on top of Debian, while possible with tools like alien, is a pain to say the least. After some weeks of development and testing, we are happy to announce the Cloudera APT Repository. APT is the standard package distribution mechanism for Ubuntu and Debian, and by simply pointing your machines at our repository, you can have Hadoop installed within minutes. Our Debian packages are comprised of the same components as our RPM based distribution, including: Standard Linux service management – we package scripts in /etc/init.d for all of the major components of the Hadoop system Native libraries on supported platforms – there are separate architecture-dependent packages for Hadoop Pipes, libhdfs, and native-code compression acceleration. Extra Hadoop-based tools – along with core Hadoop, we have packages available for Pig and Hive To get you started with Cloudera’s Distribution for Hadoop on Debian and Ubuntu, we’ve written up a short tutorial. Check it out, and remember to let us know what you think!</snippet></document><document id="732"><title>Apache Pig Training Now Available Online</title><url>http://blog.cloudera.com/blog/2009/04/pig-training-now-available-online/</url><snippet>Today I did a web search for “pig training” using my favorite search engine. I was wildly entertained by the results, and have embedded my favorite for your viewing pleasure. However, when I stopped laughing, I realized that this probably isn’t what most people reading this blog would have hoped to find. To that end, I am happy to announce that Cloudera’s Online Apache Hadoop Training now includes two sessions on Apache Pig. A few weeks back, our friend Alan Gates from Y! came to the Cloudera offices and delivered a great lecture on Pig and worked with us to produce a screencast for the exercises. We hope this makes it easier for the community to understand what types of processing Pig enables and make informed decisions about the various tools and higher level languages in the Hadoop ecosystem. Oink!</snippet></document><document id="733"><title>Using Apache Hadoop to Annotate Billions of Web Documents with Semantics</title><url>http://blog.cloudera.com/blog/2009/04/using-hadoop-to-annotate-billions-of-web-documents-with-semantics/</url><snippet>Welcome to the first guest post on the Cloudera blog. The other day, we saw Toby from�Swingly tweeting about using Apache Hadoop to process millions of other tweeters’ tweets. We were curious, and Toby put together a great writeup about how they use Hadoop to crunch data. We have a few other guest posts in the pipeline, but if you are doing something really fun with Hadoop and want to share, we’d love to hear from you. Get in touch! -Christophe How can you run hundreds of memory intensive annotation tasks across�billions of web documents to build a sweet semantic search engine�before the sun goes nova? �Use Hadoop. Now the slightly longer version: I’m a software engineer working for a�brand new start-up called Swingly. �We are based in Northern�Dallas and are working on creating a new semantic search engine that�given a question can find answers on the open web (that includes both�tons of standard web documents as well as large structured data�sources!) Oh and before I forget, standard stuff: These are my views and don’t�represent the views of any company, my cat, or my father’s brother’s�nephew’s cousin’s former roommate. Now down to the details. Our goal is to build the largest searchable�semantic index of web content ever created (emphasis on the semantic�part there). In order to do this we’re annotating terabytes of�documents with information extraction and text analysis tools. The�linguistic cues extracted from this process forms the backbone of our�search index. This heavy use of NLP technology on a large scale is�what really sets Swingly’s approach to semantic search apart from our�competition – instead of merely mining data from Wikipedia we’re�actually finding and managing billions of new facts each day. We currently extract more than 500 kinds of named entities and�hundreds of types of relations, attributes, and events from�unstructured text. This is no small job: While these are great�resources, it takes a heck a lot of computing power to actually deploy�these systems for a large scale operation! Here’s where Apache Hadoop comes�in. Our Hadoop cluster has about 100 cores and runs 24/7, so we wanted to�make the best possible use of this hardware. We made some minor�configuration tweaks to suit our needs. Specifically, we have adjusted�the default number of map and reduce tasks, as well as set up multiple�secondary name nodes for added data protection. Our processing task consists of loading the compressed raw text data�(e.g. html, pdf, etc) into the DFS, and starting a sequence of jobs�over that data. Intermediary steps include such tasks as html zoning,�content filtering, and named entity recognition. We use standard�Hadoop sequence files throughout this process, although we have�implemented our own “Writable” classes for serializing our extracted�metadata.��� There were a few extra considerations we had to address. For one, some�of our linguistic processes are in a state of flux due to�experimentation with new algorithms and various tweaking, thus some of�the data would periodically need to be reprocessed. To avoid�reprocessing all of our data from scratch, we are saving the�intermediate output between steps in our process so that we can�reprocess only a specific section and thus save valuable machine time.�Another factor we had to consider was reliability. Our first runs�would often crash or stall due to software problems or memory issues.�We have set a number of timeouts and fault tolerance parameters�throughout our Hadoop configuration code to allow tasks to gracefully�fail without interrupting an entire process (e.g. using�SkipBadRecords). Another, relatively recent improvement we have made was to permanently store the necessary jar files on the DFS. This�became necessary because the myriad of NLP tools we are using created�significant jar bloat, resulting in our job jars exceeding 500 MB in�size. Needless to say such bloat added a frustrating overhead during�testing and bug fixing, so we found a workaround by invoking some�classloader voodoo and automatically copying the necessary jars to the�DFS and using Hadoop’s distributed cache mechanism to put the jars in�the classpath. There are also a few extra tweaks like computing�checksums to avoid unneeded copying of jars. With regards to my tweet about loading twitter statuses (how we hooked�up with Cloudera): We’re actually using the same extraction tools�we’ve deployed for the open web data to extract semantic information�from the twitter stream. This effort is just getting off the ground,�but we’re already finding tons of interesting trending and zeitgeist�information from this process. Let me know if there are any questions or necessary elaborations on�any of this! It’s a pleasure to share our exciting work with�community, and we hope to go live soon. You can find my contact details at�http://www.toluju.com/</snippet></document><document id="734"><title>The Second Hadoop UK User Group Meeting</title><url>http://blog.cloudera.com/blog/2009/04/hadoop-uk-user-group-meeting/</url><snippet>Last Tuesday – on my second day of work at Cloudera – I went to London to check out the second UK Hadoop User Group meetup, kindly hosted by Sun in a nice meeting room not far from the river Thames. We saw a day of talks from people heavily involved with Hadoop, both on the development and usage side and more often than not a bit of both. It was a great opportunity to put a selection of people all interested in Hadoop technology in the same room and find out what the current status and future directions of the project are. There were around 55 attendees from a variety of organisations, both academic and professional. Tom White and I were there representing Cloudera, and there were attendees from Microsoft, HP, the Apache Software Foundation and the incredibly fashionable guys from Last.fm. The slides and talks have been made available by the organisers here – they’re well worth checking out if you want to get a cross-section of some current activity around Hadoop. I’ve written up some notes on the talks which you can find below. Practical MapReduce If you’re actively building applications with Hadoop, then Tom’s Practical MapReduce slide deck should be required reading. Tom gave ten invaluable tips for building high-quality MapReduce applications. For example, it’s no longer as necessary to defray job start-up cost by squeezing all your processing into a single job. Improvements to schedulers (see Matei Zaharia’s posts on the new schedulers and upcoming improvements) have made it more practical to structure your computations in manageable, job-sized units, and planned work on the overhead of a job should improve the situation yet further. Another tip is to implement the Tool interface. A job that implements Tool can offload a number of tedious configuration tasks to the framework, such as the treatment of generic command-line parameters. This frees up developer time for more important tasks, as well as ensuring uniform treatment of common functionality. Learning From Data – Apache Mahout Isabel Drost from ASF spoke about Apache Mahout, a toolbox of machine learning techniques and algorithms that make use of Hadoop. Machine learning is becoming hugely important to technology, and drives some of the coolest applications from statistical machine translation to automatic recommendation algorithms. The ML community have long understood that a most effective way to tailor generic learning algorithms is to train them on huge amounts of data. This can generally be done in parallel, and so Hadoop is clearly a great fit. Mahout – which is an Indian word roughly meaning ‘elephant driver’ – implements well-known algorithms for clustering, classification and evolutionary programming. Mahout had a 0.1 release only recently, and has been making great strides in the year since the project was started. One of the strengths of Mahout is that there is an attendant community of experts in machine learning who are willing to help you get started building applications making use of these powerful techniques. Isabel mentioned that they’re looking for contributors and volunteers – if you want to help out, head over to the Apache Mahout project page and see how you can get involved. Indexing Massive Datasets Craig McDonald from the University of Glasgow spoke about the Terrier project. Terrier is a platform that supports research into information retrieval, which includes areas like search and natural language question answering. In particular, Craig told us about his experiences building an indexing platform that could handle pretty large data sets – the latest corpus that they’re intending to process is 25 TB and over a billion documents – using Hadoop. Terrier sees linear speedup in the time taken to index as the number of nodes in their cluster increases. This is a good example of how Hadoop scales to large amounts of data as required. In order to realise this speedup, Craig told us they had to correct a mistake they made by only using one reducer – this added a significant serial component to their computation and, as students of Amdahl’s law will know, the ability of a computation to scale with more parallel resources is limited by the ratio of the serial part of the computation to the parallel part. Thankfully, the number of reducers isn’t a fundamental limitation of the Hadoop framework, and a bit of reconfiguration to use multiple reducers gave some impressive speedup figures. MapReduce and Matrix Maths In the afternoon, Paolo Castagna of HP Labs walked us through an implementation of the PageRank algorithm in MapReduce. PageRank is the algorithm used by Google to determine how ‘important’ web pages are relative to one another. Essentially, it involves simulating the actions of an itinerant web surfer who follows links at random for ever. This gives rise to a probability distribution over all web pages, and the more ‘likely’ pages are considered more important. Computing this distribution via simulation requires storing the entire graph of web page links in a massive, sparse matrix and then running the simulation over all pages in parallel. Again, this is the sort of thing that Hadoop is incredibly good at, and Paolo showed us how simple an efficient implementation of such a seemingly complex algorithm could be once you’ve got MapReduce to do the heavy lifting for you. HBase Michael Stack from PowerSet rounded off the day with an overview of HBase, an open-source implementation of Google’s BigTable. Michael’s slides give a good introduction to the project, and gave some indication of the way things are planned to progress for the next couple of releases. Performance is becoming very important to HBase, as there are users who want to start serving large web sites from an HBase store. This is one of the focus points for 0.20, along with moving control plane functionality into ZooKeeper, the reliable coordination service. Multi-row transactions are coming as well, which will help those who currently have to store all data that must only be accessed under a lock in a single row. Conclusions The question-and-answer sessions, as well as the informal chats between sessions, gave rise to some lively and interesting debate. It’s clear that some confusion remains over the relative merits of RDBMs systems versus MapReduce-based technologies. At Cloudera we’ve put a lot of effort into our training courses that help elucidate the different problem domains that each approach is suited to, and the onus is on the community to continue to articulate that distinction. It’s evident from the presentations, and the buzz around the meetup, that Hadoop is successfully being used to do a lot of serious data processing in real projects, and that people are excited about where the technology is going to go from here. Thanks to Johan for organising the day, and to Sun for hosting us and (eventually) finding the pizza for lunch :)</snippet></document><document id="735"><title>Configuring Eclipse for Apache Hadoop Development (a screencast)</title><url>http://blog.cloudera.com/blog/2009/04/configuring-eclipse-for-hadoop-development-a-screencast/</url><snippet>Update (added 5/15/2013): The information below is dated; see this post for current instructions about configuring Eclipse for Hadoop contributions. One of the perks of using Java is the availability of functional, cross-platform IDEs.� I use vim for my daily editing needs, but when it comes to navigating, debugging, and coding large Java projects, I fire up Eclipse. Typically, when you’re developing Map-Reduce applications, you simply point Eclipse at the Apache Hadoop jar file, and you’re good to go.� (Cloudera’s Hadoop training VM has a fully-configured example.) However, when you want to dig deeper to explore�and modify�Hadoop’s internals themselves, you’ll want to configure Eclipse to build Hadoop.� Because there’s generated code and a complicated ant build.xml file, this takes some tinkering.� Now that I have the full Hadoop Eclipse experience going (it took me a few tries), I’ve prepared a screencast that will help guide you through it, from downloading Eclipse to debugging one of its unit tests.� You’ll also want to reference the EclipseEnvironment Hadoop wiki page, which has more details. Eclipse for Hadoop Development from Cloudera. We’re interested in your feedback!� Use the comments below to share your Eclipse experiences. P.S.: I “filmed” the screencast on Linux, but the same steps work on Mac OS X. Since Eclipse will be running under Java 1.5 in OS X, be sure to check that “Preferences…Installed JREs” references JVM 1.6. Sometimes Eclipse is finicky; you might have to do “Refresh” and “Build Project” several times. The key bindings are a bit different too: substitute Command (?) for Control.</snippet></document><document id="736"><title>Hive and JobTracker Needed Logos…</title><url>http://blog.cloudera.com/blog/2009/04/hive-and-jobtracker-needed-logos/</url><snippet>In the process of working on a few things here I wanted to add some links to launch Apache Hive and the Hadoop Jobtracker. At first I considered just adding the links but I found myself wanting a button of some sort; an icon for them. I didn’t want to just use the (awesomely cute) Apache Hadoop logo elephant because these things are related to and part of Hadoop, but they aren’t Hadoop itself… What to do? Well, I grabbed Illustrator and spent a bit of time putting together these icons. What do you think? We’ve opened up a ticket with the Hadoop project to contribute these to the project.</snippet></document><document id="737"><title>Cloudera’s Distribution for Apache Hadoop: Making Hadoop Easier for a Sysadmin</title><url>http://blog.cloudera.com/blog/2009/04/clouderas-distribution-for-hadoop-making-hadoop-easier-for-a-sysadmin/</url><snippet>A few weeks ago we announced Cloudera’s Distribution for Apache Hadoop, and I want to spend some time showing how our distribution makes a sysadmin’s job a little easier. Perhaps the most useful features in our distribution, at least for sysadmins, are RPM packages and init scripts.� RPMs are the standard way of installing software on a Red Hat Linux distribution (RHEL, Fedora Core, CentOS).� They give sysadmins a one-command install, and they install libraries, binaries, init scripts, log files, man pages, and configuration files in places where Linux users expect them, typically /usr/lib, /usr/bin, /etc/init.d, /var/log, /usr/share/man, and /etc, respectively.� RPMs are also very easy to uninstall and upgrade. Init scripts are the standard way to start, stop, and restart daemon processes on a Linux system. They allow sysadmins to start and stop daemons with the /sbin/service script, and they use a standard parameter interface, namely start, stop, or restart (e.g., sudo /sbin/service hadoop-datanode start). Init scripts also make sure that the daemon runs as the correct user, which in Hadoop’s case is the hadoop user.� Lastly, init scripts are used to start daemons at boot time, allowing daemons to survive reboots. Perhaps I’ve convinced you that our distribution for Hadoop is easier to install. Simplifying the installation process� for a single machine doesn’t make a huge impact.� However, when you’re deploying Hadoop on many machines, and when you’re using configuration management tools such as Puppet, Bcfg2, Chef, Cfengine, etc., RPMs and init scripts make sysadmin work slightly better. I’ve written two Puppet implementations to demonstrate this point: the first installs Hadoop from a tarball; the second installs from a RPM. For those of you familiar with Puppet, you’ll notice that the RPM installation is more Puppet-friendly and generally less complicated.� If you’d like to learn a little more about Puppet, the type reference and getting started docs are good starting points. RPMs only work on Red Hat Linux distributions such as Fedora Core, CentOS, and RHEL. We’re currently working on providing DEBs, which are equivalent to RPMs for Debian-based distributions such as Debian and Ubuntu. If you have any questions about our distribution for Hadoop, or if you’d like to see packages for other systems (FreeBSD or Mac OS X anyone?), then drop us a question on our Get Satisfaction forums.</snippet></document><document id="738"><title>Upcoming Functionality in “Fair Scheduler 2.0″</title><url>http://blog.cloudera.com/blog/2009/04/upcoming-functionality-in-fair-scheduler-20/</url><snippet>(guest blog post by Matei Zaharia) As Hadoop clusters grow in size and data volume, it becomes more and more useful to share them between multiple users and to isolate these users. If User 1 is running a ten-hour machine learning job for example, this should not impair a User 2 from running a 2-minute Hive query. In November, I blogged about how Hadoop 0.19 supports pluggable job schedulers, and how we worked with Facebook to implement a Fair Scheduler for Hadoop using this new functionality. The Fair Scheduler gives each user a configurable share of the cluster when he/she has running jobs, but assigns these resources to other users when the user is inactive. Since last fall, the Fair Scheduler has been picked up by Hadoop users outside Facebook, including the Google/IBM academic Hadoop cluster. It’s also received extensive testing and patches from Yahoo!. Furthermore, we’ve included the Fair Scheduler in Cloudera’s Distribution for Hadoop, where it is integrated right into the JobTracker management UI. Through production experiences, testing, and feedback from users, we’ve made a lot of improvements to the Fair Scheduler, some of which are available now and others which will come out in the next major version, which I’m calling “Fair Scheduler 2.0″. Here is a summary of the upcoming functionality: Fair sharing has changed from giving equal shares to each job to giving equal shares to each user. This means that users that submitted many jobs don’t get an advantage over users running a few jobs. It’s also possible to give different weights to different users. The fair scheduler now supports killing tasks from other users’ jobs if they are not giving them up. For each pool (by default there is one pool per user, but one can also have specially named pools), there’s a configurable timeout after which it can kill other jobs’ tasks to start running. This means that it’s possible to provide “service guarantees” for production jobs that are sharing a cluster with experimental queries. The scheduler can now assign multiple tasks per heartbeat, which is important for maintaining high utilization in large clusters. A technique called delay scheduling increases data locality for small jobs, improving performance in a data warehouse workload with many small jobs such as Facebook’s. The internal logic has been simplified so that the scheduler can support different scheduling policies within each pool, and in particular we plan to support FIFO pools. Many users have requested FIFO pools because they want to be able to queue up batch workflows on the same cluster that’s running more interactive jobs. Many bug fixes and performance improvements were contributed or suggested by a team stress-testing the scheduler at Yahoo!. The same team has also contributed Forrest web-based documentation for the fair scheduler (to be available in Hadoop 0.20). As a grad student and the original developer of the Fair Scheduler, I’ve had a great experience interacting with the Hadoop community to improve the scheduler. The fact that production experience at Facebook, large-scale testing at Yahoo!, and wishes from other users are being combined into this single piece of software is a testament to the strength of Hadoop’s open-source model. The next release of the Fair Scheduler (likely in Hadoop 0.21, although we will also release back-ports to older Hadoop versions) will make it easier to manage multi-user clusters, give FIFO scheduling to users who desire it, improve performance and reduce the need for manual intervention with misbehaving jobs. You can also be sure that we’ll continue supporting the scheduler in Cloudera’s Distribution for Hadoop.</snippet></document><document id="739"><title>Configuration Parameters: What can you just ignore?</title><url>http://blog.cloudera.com/blog/2009/03/configuration-parameters-what-can-you-just-ignore/</url><snippet>Configuring a Hadoop cluster is something akin to voodoo. There are a large number of variables in hadoop-default.xml that you can override in hadoop-site.xml. Some specify file paths on your system, but others adjust levers and knobs deep inside Hadoop’s guts. Unfortuately, there’s little or no documentation on how to set them well. Is there a single optimal configuration? Are there some settings that can just be “set to 11?” At Cloudera, we’re working hard to make Hadoop easier to use and to make configuration less painful. Our Hadoop Configuration Tool gives you a web-based guide to help set up your cluster. Once it’s running, though, you might want to look under the hood and tune things a bit. The rest of this post discusses why it’s a bad idea to just set all the limits as high as they’ll go, and gives you some pointers to get started on finding a happy medium. Why can’t you just set all the limits to 1,000,000? Increasing most settings has a direct impact on memory consumption. Increasing DataNode and TaskTracker settings, therefore, has an adverse impact on RAM available to individual MapReduce tasks. On large hardware, they can be set generously high. In general though, unless you have several dozen more more nodes working together, dialing up settings very high wastes system resources like RAM that could be better applied to running your mapper and reducer code. That having been said, here’s a list of some things that can be cranked up higher than the defaults by a fair margin: File descriptor limits A busy Hadoop daemon might need to open a lot of files. The open fd ulimit in Linux defaults to 1024, which might be too low. You can set to something more generous — maybe 16384. Setting this an order of magnitude higher (e.g., 128K) is probably not a good idea. No individual Hadoop daemon is supposed to need hundreds of thousands of fds; if it’s consuming that many, then there’s probably an fd leak or other bug that needs fixing. This would just mask the true problem until errors started showing up somewhere else. You can view your ulimits in bash by running: 
$ ulimit -a
 To set the fd ulimit for a process, you’ll need to be root. As root, open a shell, and run: 
# ulimit -n 16384
 You can then run the Hadoop daemon from that shell; the ulimits will be inherited. e.g.: 
# sudo -u hadoop $HADOOP_HOME/bin/hadoop-daemon.sh start namenode
 You can also set the ulimit for the hadoop user in /etc/security/limits.conf; this mechanism will set the value persistently. Make sure pam_limits is enabled for whatever auth mechanism the hadoop daemon is using. The entry will look something like: 
hadoop hard nofile 16384
 If you’re running our distribution, we ship a modified version of Hadoop 0.18.3 that includes HADOOP-4346, a fix for the “soft fd leak” that has affected Hadoop since 0.17, so this should be less critical for our users. Users of the official Apache Hadoop release are affected by the fd leak for all 0.17, 0.18, and 0.19 versions. (The fix is committed for 0.20.) For the curious, we’ve published a list of all differences between our release of Hadoop and the stock 0.18.3 release. If you’re running Linux 2.6.27, you should also set the epoll limit to something generous; maybe 4096 or 8192. 
# echo 4096 &gt; /proc/sys/fs/epoll/max_user_instances
 Then put the following text in /etc/sysctl.conf: 
fs.epoll.max_user_instances = 4096
 See http://pero.blogs.aprilmayjune.org/2009/01/22/hadoop-and-linux-kernel-2627-epoll-limits/ for more details. Internal settings If there is more RAM available than is consumed by task instances, set io.sort.factor to 25 or 32 (up from 10). io.sort.mb should be 10 * io.sort.factor. Don’t forget, multiply io.sort.mb by the number of concurrent tasks to determine how much RAM you’re actually allocating here, to prevent swapping. (So 10 task instances with io.sort.mb = 320 means you’re actually allocating 3.2 GB of RAM for sorting, up from 1.0 GB.) An open ticket on the Hadoop bug tracking database suggests making the default value here 100. This would likely result in a lower per-stream cache size than 10 MB. io.file.buffer.size – this is one of the more “magic” parameters. You can set this to 65536 and leave it there. (I’ve profiled this in a bunch of scenarios; this seems to be the sweet spot.) If the NameNode and JobTracker are on big hardware, set dfs.namenode.handler.count to 64 and same with mapred.job.tracker.handler.count. If you’ve got more than 64 GB of RAM in this machine, you can double it again. dfs.datanode.handler.count defaults to 3 and could be set a bit higher. (Maybe 8 or 10.) More than this takes up memory that could be devoted to running MapReduce tasks, and I don’t know that it gives you any more performance. (An increased number of HDFS clients implies an increased number of DataNodes to handle the load.) mapred.child.ulimit should be 2–3x higher than the heap size specified in mapred.child.java.opts and left there to prevent runaway child task memory consumption. Setting tasktracker.http.threads higher than 40 will deprive individual tasks of RAM, and won’t see a positive impact on shuffle performance until your cluster is approaching 100 nodes or more. Conclusions Configuring Hadoop for “optimal performance” is a moving target, and depends heavily on your own applications. There are settings that need to be moved off their defaults, but finding the best value for each is difficult. Our configurator for Hadoop will do a reasonable job of getting you started. We’d love to hear from you about your own configurations. Did you discover a combination of settings that really made your cluster sing? Please share in the comments. The photo of Nigel’s amplifier is from the movie This is Spinal Tap, distributed by Embassy Pictures.</snippet></document><document id="740"><title>Announcing Cloudera’s Distribution for Apache Hadoop</title><url>http://blog.cloudera.com/blog/2009/03/cloudera-distribution-for-hadoop/</url><snippet>One of the repeating themes we have heard while working with our customers and the community is that Apache Hadoop configuration and deployment is a pain. Often times, Hadoop is the first truly distributed system that�administrators encounter, and the problem is made worse by the lack of standardized packages and deployment tools. And some releases are buggy. And upgrades are hard. And the list goes on. In order for Hadoop to truly disrupt the enterprise, it needs to be just as easy to configure, deploy and manage as any other piece of software. We’d like to take a step in that direction and share our distribution with the community. We developed our distribution to improve reliability and operations for our support customers, and while they will always be the first to receive updates and hot�fixes, the community will never be far behind. You can expect additional features over time, but I’m happy to say that the first release includes: RPM Deployment – Never again wonder which files go in which directories and if your component versions are compatible. RPM was designed for this. In addition to Hadoop, we have RPMs for compatible versions of �Hive and Pig in this release. Standard Linux Service Management – Your IT staff knows how to work with RPMs and init level services. Now they know how to work with Hadoop. Public YUM Repository – We’ll make sure it’s easy to stay up to date with the latest stable version of Hadoop. Simple Web Based Configuration Assistance – Do you know what the optimal setting for�mapred.child.ulimit is? another.arcane.parameter? Well, we have�some ideas, and are always learning more. To share that, we’ve created a configurator that asks a few important questions about your hardware and computes sensible values for all of your configuration parameters. Our distribution is currently based on Hadoop 0.18.3, and all of our changes are Apache 2.0 licensed. We’ve patched in some stable features from later versions and included some patches we are still in the process of committing to Apache. Check it out: http://www.cloudera.com/hadoop</snippet></document><document id="741"><title>Hadoop Metrics</title><url>http://blog.cloudera.com/blog/2009/03/hadoop-metrics/</url><snippet>Hadoop’s NameNode, SecondaryNameNode, DataNode, JobTracker, and TaskTracker daemons all expose runtime metrics. These are handy for monitoring and ad-hoc exploration of the system and provide a goldmine of historical data when debugging. In this post, I’ll first talk about saving metrics to a file.  Then we’ll walk through some of the metrics data.  Finally, I’ll show you how to configure sending metrics to other systems and explore them with jconsole. Dumping metrics to a file The simplest way to configure Hadoop metrics is to funnel them into a user-configurable file on the machine running the daemon.  Metrics are organized into “contexts” (Hadoop currently uses “jvm”, “dfs”, “mapred”, and “rpc”), and each context is independently configured.  Setup your conf/hadoop-metrics.properties to use FileContext like so: # Configuration of the “dfs” context for file dfs.class=org.apache.hadoop.metrics.file.FileContext dfs.period=10 # You’ll want to change the path dfs.fileName=/tmp/dfsmetrics.log # Configuration of the “mapred” context for file mapred.class=org.apache.hadoop.metrics.file.FileContext mapred.period=10 mapred.fileName=/tmp/mrmetrics.log # Configuration of the “jvm” context for file jvm.class=org.apache.hadoop.metrics.file.FileContext jvm.period=10 jvm.fileName=/tmp/jvmmetrics.log # Configuration of the “rpc” context for file rpc.class=org.apache.hadoop.metrics.file.FileContext rpc.period=10 rpc.fileName=/tmp/rpcmetrics.log With this configuration Hadoop daemons will dump their metrics to text files at ten second intervals.  Once your daemons are up and running, you’ll start seeing those log files populated. Exploring the metrics data Let’s explore some of the data.  If you want to dig right in, I’ve uploaded the metrics log files from a Hadoop cluster running locally on my machine. The “jvm” context contains some basic stats from the JVM: memory usage, thread counts, garbage collection, etc.  All Hadoop daemons use this context (the implementation is in the JvmMetrics class).  Because I’m running all the daemons locally for this blog post, my “jvmmetrics.log” has lines from all the deamons. jvm.metrics: hostName=doorstop.local, processName=DataNode, sessionId=, gcCount=15, gcTimeMillis=58, logError=0, logFatal=0, logInfo=159, logWarn=0, memHeapCommittedM=7.4375, memHeapUsedM=5.5513763, memNonHeapCommittedM=23.1875, memNonHeapUsedM=16.977356, threadsBlocked=0, threadsNew=0, threadsRunnable=7, threadsTerminated=0, threadsTimedWaiting=8, threadsWaiting=6 jvm.metrics: hostName=doorstop.local, processName=SecondaryNameNode, sessionId=, gcCount=11, gcTimeMillis=53, logError=0, logFatal=0, logInfo=12, logWarn=3, memHeapCommittedM=7.4375, memHeapUsedM=4.345642, memNonHeapCommittedM=23.1875, memNonHeapUsedM=16.32586, threadsBlocked=0, threadsNew=0, threadsRunnable=5, threadsTerminated=0, threadsTimedWaiting=4, threadsWaiting=2 [...] In the “dfs” context, “dfs.FSNamesystem” contains information similar to what you’d expect from the NameNode status page: capacity, number of files, under-replicated blocks, etc. dfs.FSNamesystem: hostName=doorstop.local, sessionId=, BlocksTotal=44, CapacityRemainingGB=78, CapacityTotalGB=201, CapacityUsedGB=0, FilesTotal=60, PendingReplicationBlocks=0, ScheduledReplicationBlocks=0, TotalLoad=1, UnderReplicatedBlocks=44 The DataNode and NameNode daemons also spit out metrics summarizing how many operations they’ve performed in the last interval period. JobTrackers and TaskTrackers use the “mapred” context to summarize their counters. This data is similar to what you’d find in the JobTracker’s status page. mapred.jobtracker: hostName=doorstop.local, sessionId=, jobs_completed=0, jobs_submitted=1, maps_completed=10, maps_launched=10, reduces_completed=0, reduces_launched=1 mapred.tasktracker: hostName=doorstop.local, sessionId=, mapTaskSlots=2, maps_running=0, reduceTaskSlots=2, reduces_running=1, tasks_completed=10, tasks_failed_ping=0, tasks_failed_timeout=0 In this context you’ll also see transient per-job counter data (shuffle stats and job counters). Though most metrics persist as long as the process is running, per-job metrics are cleared out after the job completes. Configuring other outputs So far, we’ve been sending metrics data into a file, but, as we’ve seen, you can configure which class handles metrics updates. There aren’t many implementations of MetricsContext available, but a very useful one is GangliaContext.  Specifying that class, along with a “servers” configuration option, enables you to send metrics to Ganglia 3.0.  I recommend making the “servers” field use the real domain names (not “localhost”), because, in a cluster setting, ganglia will propagate 127.0.0.1 as the machine reporting these metrics, which isn’t unique. Be wary (HADOOP-4675) that GangliaContext is incompatible with Ganglia 3.1 (the latest version).  See the Hadoop wiki page for more information. Accessing metrics via JMX Some (but not all) of Hadoop’s metrics are available also via Java Management Extensions (JMX).  The easiest way to see these is to fire up jconsole (which comes with your Java SDK) and point it to your daemon. Then, browse the metrics! NameNodes and DataNodes both implement “MBeans” to report the data to JMX. Hadoop’s RPC servers expose metrics to JMX as well.  All the daemons except the SecondaryNameNode have RPC servers. You’ll find that jconsole is easiest if you’re running it as the same user as the Hadoop daemons (or root), on the same machine as the Java processes you’re interested in. Hadoop’s default hadoop-env.sh already has the “-Dcom.sun.management.jmxremote” option to enable JMX.  JMX can also be configured for remote access.  Refer to Sun’s documentation for the full details, but, in short, place “monitorRole secretPassword” in a file called “jmxremote.password” (with permissions 600), and modify lines in hadoop-env.sh like so: # Extra Java runtime options. Empty by default. export HADOOP_OPTS=”-Dcom.sun.management.jmxremote.authenticate=true -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.password.file=$HADOOP_CONF_DIR/jmxremote.password” # Command specific options appended to HADOOP_OPTS when specified export HADOOP_NAMENODE_OPTS=”-Dcom.sun.management.jmxremote $HADOOP_NAMENODE_OPTS -Dcom.sun.management.jmxremote.port=8004″ export HADOOP_SECONDARYNAMENODE_OPTS=”-Dcom.sun.management.jmxremote $HADOOP_SECONDARYNAMENODE_OPTS -Dcom.sun.management.jmxremote.port=8005″ export HADOOP_DATANODE_OPTS=”-Dcom.sun.management.jmxremote $HADOOP_DATANODE_OPTS -Dcom.sun.management.jmxremote.port=8006″ export HADOOP_BALANCER_OPTS=”-Dcom.sun.management.jmxremote $HADOOP_BALANCER_OPTS -Dcom.sun.management.jmxremote.port=8007″ export HADOOP_JOBTRACKER_OPTS=”-Dcom.sun.management.jmxremote $HADOOP_JOBTRACKER_OPTS -Dcom.sun.management.jmxremote.port=8008″ export HADOOP_TASKTRACKER_OPTS=”-Dcom.sun.management.jmxremote.port=8009″ You’ll now be able to point jconsole to the appropriate port and authenticate. If you want to monitor your Hadoop deployment with Hyperic (plugin here) or Nagios, using JMX is an easy way to bridge the gap between Hadoop and your monitoring software. There are a fair number of command-line oriented JMX tools, including cmdline-jmxclient, Twiddle (part of JBoss), check_jmx (a Nagios plugin), and many others. Hadoop will soon have its own (HADOOP-4756), too! Note: some metrics are collected by a periodic timer thread, which is disabled by default. To enable these metrics, use org.apache.hadoop.metrics.spi.NullContextWithUpdateThread as the metrics implementation in conf/hadoop-metrics.properties. — June 8, 2009 Behind the Scenes Hadoop’s metrics are part of the org.apache.hadoop.metrics package.  If you want to implement your own metrics handler, you’ll need to implement your own MetricsContext, typically extending AbstractMetricsContext.  If you want to add new metrics, see NameNodeMetrics for an example.</snippet></document><document id="742"><title>Database Access with Apache Hadoop</title><url>http://blog.cloudera.com/blog/2009/03/database-access-with-hadoop/</url><snippet>Editor’s note (added Nov. 9. 2013): Valuable data in an organization is often stored in relational�database systems. To access that data, you could use external APIs as�detailed in this blog post below, or you could use Apache Sqoop, an�open source tool (packaged inside CDH) that allows users to import data from a relational�database into Apache Hadoop for further processing. Sqoop can also export�those results back to the database for consumption by other clients. Apache Hadoop’s strength is that it enables ad-hoc analysis of unstructured or semi-structured data. Relational databases, by contrast, allow for fast queries of very structured data sources. A point of frustration has been the inability to easily query both of these sources at the same time. The DBInputFormat component provided in Hadoop 0.19 finally allows easy import and export of data between Hadoop and many relational databases, allowing relational data to be more easily incorporated into your data processing pipeline. This blog post explains how the DBInputFormat works and provides an example of using DBInputFormat to import data into HDFS. DBInputFormat and JDBC First we’ll cover how DBInputFormat interacts with databases. DBInputFormat uses JDBC to connect to data sources. Because JDBC is widely implemented, DBInputFormat can work with MySQL, PostgreSQL, and several other database systems. Individual database vendors provide JDBC drivers to allow third-party applications (like Hadoop) to connect to their databases. Links to popular drivers are listed in the resources section at the end of this post. To start using DBInputFormat to connect to your database, you’ll need to download the appropriate database driver from the list in the resources section (see the end of this post), and drop it into the $HADOOP_HOME/lib/ directory on your Hadoop TaskTracker machines, and on the machine where you launch your jobs from. Reading Tables with DBInputFormat The DBInputFormat is an InputFormat class that allows you to read data from a database. An InputFormat is Hadoop’s formalization of a data source; it can mean files formatted in a particular way, data read from a database, etc. DBInputFormat provides a simple method of scanning entire tables from a database, as well as the means to read from arbitrary SQL queries performed against the database. Most queries are supported, subject to a few limitations discussed at the end of this article. Configuring the job To use the DBInputFormat, you’ll need to configure your job. The following example shows how to connect to a MySQL database and load from a table: CREATE TABLE employees ( employee_id INTEGER NOT NULL PRIMARY KEY, name VARCHAR(32) NOT NULL);   Listing 1: Example table schema JobConf conf = new JobConf(getConf(), MyDriver.class); conf.setInputFormat(DBInputFormat.class); DBConfiguration.configureDB(conf, �com.mysql.jdbc.Driver�, �jdbc:mysql://localhost/mydatabase�); String [] fields = { �employee_id�, "name" }; DBInputFormat.setInput(conf, MyRecord.class, �employees�, null /* conditions */, �employee_id�, fields); // set Mapper, etc., and call JobClient.runJob(conf);   Listing 2: Java code to set up a MapReduce job with DBInputFormat This example code will connect to mydatabase on localhost and read the two fields from the employees table. The configureDB() and setInput() calls configure the DBInputFormat. The first call specifies the JDBC driver implementation to use and what database to connect to. The second call specifies what data to load from the database. The MyRecord class is the class where data will be read into in Java, and "employees" is the name of the table to read. The "employee_id" parameter specifies the table’s primary key, used for ordering results. The section “Limitations of the InputFormat” below explains why this is necessary. Finally, the fields array lists what columns of the table to read. An overloaded definition of setInput() allows you to specify an arbitrary SQL query to read from, instead. After calling configureDB() and setInput(), you should configure the rest of your job as usual, setting the Mapper and Reducer classes, specifying any other data sources to read from (e.g., datasets in HDFS) and other job-specific parameters. Retrieving the data The DBInputFormat will read from the database, but how does data get to your mapper? The setInput() method used in the example above took, as a parameter, the name of a class which will hold the contents of one row. You’ll need to write an implementation of the DBWritable interface to allow DBInputFormat to populate your class with fields from the table. DBWritable is an adaptor interface that allows data to be read and written using both Hadoop’s internal serialization mechanism, and using JDBC calls. Once the data is read into your custom class, you can then read the class’ fields in the mapper. The following example provides a DBWritable implementation that holds one record from the employees table, as described above: class MyRecord implements Writable, DBWritable { long id; String name; public void readFields(DataInput in) throws IOException { this.id = in.readLong(); this.name = Text.readString(in); } public void readFields(ResultSet resultSet) throws SQLException { this.id = resultSet.getLong(1); this.name = resultSet.getString(2); } public void write(DataOutput out) throws IOException { out.writeLong(this.id); Text.writeString(out, this.name); } public void write(PreparedStatement stmt) throws SQLException { stmt.setLong(1, this.id); stmt.setString(2, this.name); } } Listing 3: DBWritable implementation for records from the employees table A java.sql.ResultSet object represents the data returned from a SQL statement. It contains a cursor representing a single row of the results. This row will contain the fields specified in the setInput() call. In the readFields() method of MyRecord, we read the two fields from the ResultSet. The readFields() and write() methods that operate on java.io.DataInput and DataOutput objects are part of the Writable interface used by Hadoop to marshal data between mappers and reducers, or pack results into SequenceFiles. Using the data in a mapper The mapper then receives an instance of your DBWritable implementation as its input value. The input key is a row id provided by the database; you’ll most likely discard this value. public class MyMapper extends MapReduceBase implements Mapper { public void map(LongWritable key, MyRecord val, OutputCollector output, Reporter reporter) throws IOException { // Use val.id, val.name here output.collect(new LongWritable(val.id), new Text(val.name)); } }   Listing 4: Example mapper using a custom DBWritable Writing results back to the database A companion class, DBOutputFormat, will allow you to write results back to a database. When setting up the job, call conf.setOutputFormat(DBOutputFormat.class); and then call DBConfiguration.configureDB() as before. The DBOutputFormat.setOutput() method then defines how the results will be written back to the database. Its three arguments are the JobConf object for the job, a string defining the name of the table to write to, and an array of strings defining the fields of the table to populate. e.g., DBOutputFormat.setOutput(job, "employees", "employee_id", "name");. The same DBWritable implementation that you created earlier will suffice to inject records back into the database. The write(PreparedStatement stmt) method will be invoked on each instance of the DBWritable that you pass to the OutputCollector from the reducer. At the end of reducing, those PreparedStatement objects will be turned into INSERT statements to run against the SQL database. Limitations of the InputFormat JDBC allows applications to generate SQL queries which are executed against the database; the results are then returned to the calling application. Keep in mind that you will be interacting with your database via repeated SQL queries. Therefore: Hadoop may need to execute the same query multiple times. It will need to return the same results each time. So any concurrent updates to your database, etc, should not affect the query being run by your MapReduce job. This can be accomplished by disallowing writes to the table while your MapReduce job runs, restricting your MapReduce’s query via a clause such as “insert_date &lt; yesterday,” or dumping the data to a temporary table in the database before launching your MapReduce process. In order to parallelize the processing of records from the database, Hadoop will execute SQL queries that use ORDER BY, LIMIT, and OFFSET clauses to select ranges out of tables. Your results, therefore, need to be orderable by one or more keys (either PRIMARY, like the one in the example, or UNIQUE). In order to set the number of map tasks, the DBInputFormat needs to know how many records it will read. So if you’re writing an arbitrary SQL query against the database, you will need to provide a second query that returns the number of rows that the first query will return (e.g., by using COUNT and GROUP BY). With these restrictions in mind, there’s still a great deal of flexibility available to you. You can bulk load entire tables into HDFS, or select large ranges of data. For example, if you want to read records from a table that is also being populated by another source concurrently, you might set up that table to attach a timestamp field to each record. Before doing the bulk read, pick the current timestamp, then select all records with timestamps earlier than that one. New records being fed in by the other writer will have later timestamps and will not affect the MapReduce job. Finally, be careful to understand the bottlenecks in your data processing pipeline. Launching a MapReduce job with 100 mappers performing queries against a database server may overload the server or its network connection. In this case, you’ll achieve less parallelism than theoretically possible, due to starvation, disk seeks, and other performance penalties. Limitations of the OutputFormat The DBOutputFormat writes to the database by generating a set of INSERT statements in each reducer. The reducer’s close() method then executes them in a bulk transaction. Performing a large number of these from several reduce tasks concurrently can swamp a database. If you want to export a very large volume of data, you may be better off generating the INSERT statements into a text file, and then using a bulk data import tool provided by your database to do the database import. Conclusions DBInputFormat provides a straightforward interface to read data from a database into your MapReduce applications. You can read database tables into HDFS, import them into Hive, or use them to perform joins in MapReduce jobs. By supporting JDBC, it provides a common interface to a variety of different database sources. This is probably best not used as a primary data access mechanism; queries against database-driven data are most efficiently executed within the database itself, and large-scale data migration is better done using the bulk data export/import tools associated with your database. But when analysis of ad hoc data in HDFS can be improved by the addition of some additional relational data, DBInputFormat allows you to quickly perform the join without a large amount of setup overhead. DBOutputFormat then allows you to export results back to the same database for combining with other database-driven tables. DBInputFormat is available in Hadoop 0.19 and is provided by HADOOP-2536, a patch started by Fredrik Hedberg and further developed by Enis Soztutar. A backport of this patch that can be applied to Hadoop 0.18.3 is available at the above link. This article is based on a talk I gave at the SF Bay Hadoop User Group meetup on Feburary 18th; the slides from that talk are available as a PDF. Resources DBInputFormat documentation Wikipedia on JDBC Popular JDBC drivers: MySQL: Connector/J PostgreSQL JDBC Oracle JDBC (Note: DBInputFormat currently does not work with Oracle, but this should change soon.)</snippet></document><document id="743"><title>Multi-host SecondaryNameNode Configuration</title><url>http://blog.cloudera.com/blog/2009/02/multi-host-secondarynamenode-configuration/</url><snippet>You might think that the SecondaryNameNode is a hot backup daemon for the NameNode. You’d be wrong. The SecondaryNameNode is a poorly understood component of the HDFS architecture, but one which provides the important function of lowering NameNode restart time. This blog post describes how to configure this daemon in a large-scale environment. The default Hadoop configuration places an instance of the SecondaryNameNode on the same node as the NameNode. A more scalable configuration involves configuring the SecondaryNameNode on a different machine. About the SecondaryNameNode The NameNode is responsible for the reliable storage and interactive lookup and modification of the metadata for HDFS. To maintain interactive speed, the filesystem metadata is stored in the NameNode’s RAM. Storing the data reliably necessitates writing it to disk as well. To ensure that these writes do not become a speed bottleneck, instead of storing the current snapshot of the filesystem every time, a list of modifications is continually appended to a log file called the EditLog. Restarting the NameNode involves replaying the EditLog to reconstruct the final system state. The SecondaryNameNode periodically compacts the EditLog into a “checkpoint;” the EditLog is then cleared. A restart of the NameNode then involves loading the most recent checkpoint and a shorter EditLog containing only events since the checkpoint. Without this compaction process, restarting the NameNode can take a very long time. Compaction ensures that restarts do not incur unnecessary downtime. The duties of the SecondaryNameNode end there; it cannot take over the job of serving interactive requests from the NameNode. Although, in the event of the loss of the primary NameNode, an instance of the NameNode daemon could be manually started on a copy of the NameNode metadata retrieved from the SecondaryNameNode. Why should this run on a separate machine? Scalability. Creating the system snapshot requires about as much memory as the NameNode itself occupies. Since the memory available to the NameNode process is a primary limit on the size of the distributed filesystem, a large-scale cluster will require most or all of the available memory for the NameNode. Durability. When the SecondaryNameNode creates a checkpoint, it does so in a separate copy of the filesystem metadata. Moving this process to another machine also creates a copy of the metadata file on an independent machine, increasing its durability. Configuring the SecondaryNameNode on a remote host An HDFS instance is started on a cluster by logging in to the NameNode machine and running $HADOOP_HOME/bin/start-dfs.sh (or start-all.sh). This script starts a local instance of the NameNode process, logs into every machine listed in the conf/slaves file and starts an instance of the DataNode process, and logs into every machine listed in the conf/masters file and starts an instance of the SecondaryNameNode process. The masters file does not govern which nodes become NameNodes or JobTrackers; those are started on the machine(s) where bin/start-dfs.sh and bin/start-mapred.sh are executed. A more accurate filename might be “secondaries,” but that’s not currently the case. Put each machine where you intend to run a SecondaryNameNode in the conf/masters file, one per line. (Note: currently, only one SecondaryNameNode may be configured in this manner.) Modify the conf/hadoop-site.xml file on each of these machines to include the following property: &lt;property&gt;
  &lt;name&gt;dfs.http.address&lt;/name&gt;
  &lt;value&gt;namenode.host.address:50070&lt;/value&gt;
  &lt;description&gt;
    The address and the base port where the dfs namenode web ui will listen on.
    If the port is 0 then the server will start on a free port.
  &lt;/description&gt;
&lt;/property&gt;
 This second step is less obvious than the first and works around a subtlety in Hadoop’s data transfer architecture. Traffic between the DataNodes and the NameNode occurs over a custom RPC protocol; the port for this protocol is specified in the URI supplied to the fs.default.name property. The NameNode also runs a Jetty web servlet engine on port 50070. This servlet engine generates status pages detailing the NameNode’s operation. It also communicates with the SecondaryNameNode. The SecondaryNameNode actually performs an HTTP GET request to retrieve the current FSImage (checkpoint) and EditLog from the NameNode; it uses HTTP POST to upload the new checkpoint back to the NameNode. The conf/hadoop-default.xml file sets dfs.http.address to 0.0.0.0:50070; the NameNode listens on this host mask and port (by default, all inbound interfaces on port 50070), and the SecondaryNameNode attempts to use the same value as an address to connect to. It special-cases 0.0.0.0 as “localhost.” Running the SecondaryNameNode on a different machine requires telling that machine where to reach the NameNode. Usually this setting could be placed in the hadoop-site.xml file used by all daemons on all nodes. In an environment such as Amazon EC2, though, where a node is known by multiple addresses (one public IP and one private IP), it is preferable to have the SecondaryNameNode connect to the NameNode over the private (unmetered bandwidth) IP address, while you connect to the public IP address for status pages. Specifying dfs.http.address as anything other than 0.0.0.0 on the NameNode will cause it to bind to only one address instead of all available ones. In conclusion, larger deployments of HDFS will require a remote SecondaryNameNode, but doing so requires a subtle configuration tweak, to ensure that the SecondaryNameNode can communicate back to the remote NameNode. References: HDFS Architecture SecondaryNameNode HDFS Documentation</snippet></document><document id="744"><title>The Small Files Problem</title><url>http://blog.cloudera.com/blog/2009/02/the-small-files-problem/</url><snippet>Small files are a big problem in Hadoop — or, at least, they are if the number of questions on the user list on this topic is anything to go by. In this post I’ll look at the problem, and examine some common solutions. Problems with small files and HDFS A small file is one which is significantly smaller than the HDFS block size (default 64MB). If you’re storing small files, then you probably have lots of them (otherwise you wouldn’t turn to Hadoop), and the problem is that HDFS can’t handle lots of files. Every file, directory and block in HDFS is represented as an object in the namenode’s memory, each of which occupies 150 bytes, as a rule of thumb. So 10 million files, each using a block, would use about 3 gigabytes of memory. Scaling up much beyond this level is a problem with current hardware. Certainly a billion files is not feasible. Furthermore, HDFS is not geared up to efficiently accessing small files: it is primarily designed for streaming access of large files. Reading through small files normally causes lots of seeks and lots of hopping from datanode to datanode to retrieve each small file, all of which is an inefficient data access pattern. Problems with small files and MapReduce Map tasks usually process a block of input at a time (using the default FileInputFormat). If the file is very small and there are a lot of them, then each map task processes very little input, and there are a lot more map tasks, each of which imposes extra bookkeeping overhead. Compare a 1GB file broken into 16 64MB blocks, and 10,000 or so 100KB files. The 10,000 files use one map each, and the job time can be tens or hundreds of times slower than the equivalent one with a single input file. There are a couple of features to help alleviate the bookkeeping overhead: task JVM reuse for running multiple map tasks in one JVM, thereby avoiding some JVM startup overhead (see the mapred.job.reuse.jvm.num.tasks property), and MultiFileInputSplit which can run more than one split per map. Why are small files produced? There are at least two cases The files are pieces of a larger logical file. Since HDFS has only recently supported appends, a very common pattern for saving unbounded files (e.g. log files) is to write them in chunks into HDFS. The files are inherently small. Imagine a large corpus of images. Each image is a distinct file, and there is no natural way to combine them into one larger file. These two cases require different solutions. For the first case, where the file is made up of records, the problem may be avoided by calling HDFS’s sync() method (which came with appends, although see this discussion) every so often to continuously write large files. Alternatively, it’s possible to write a program to concatenate the small files together (see Nathan Marz’s post about a tool called the Consolidator which does exactly this). For the second case, some kind of container is needed to group the files in some way. Hadoop offers a few options here. HAR files Hadoop Archives (HAR files) were introduced to HDFS in 0.18.0 to alleviate the problem of lots of files putting pressure on the namenode’s memory. HAR files work by building a layered filesystem on top of HDFS. A HAR file is created using the hadoop archive command, which runs a MapReduce job to pack the files being archived into a small number of HDFS files. To a client using the HAR filesystem nothing has changed: all of the original files are visible and accessible (albeit using a har:// URL). However, the number of files in HDFS has been reduced. Reading through files in a HAR is no more efficient than reading through files in HDFS, and in fact may be slower since each HAR file access requires two index file reads as well as the data file read (see diagram). And although HAR files can be used as input to MapReduce, there is no special magic that allows maps to operate over all the files in the HAR co-resident on a HDFS block. It should be possible to built an input format that can take advantage of the improved locality of files in HARs, but it doesn’t exist yet. Note that MultiFileInputSplit, even with the improvements in HADOOP-4565 to choose files in a split that are node local, will need a seek per small file. It would be interesting to see the performance of this compared to a SequenceFile, say. At the current time HARs are probably best used purely for archival purposes. Sequence Files The usual response to questions about “the small files problem” is: use a SequenceFile. The idea here is that you use the filename as the key and the file contents as the value. This works very well in practice. Going back to the 10,000 100KB files, you can write a program to put them into a single SequenceFile, and then you can process them in a streaming fashion (directly or using MapReduce) operating on the SequenceFile. There are a couple of bonuses too. SequenceFiles are splittable, so MapReduce can break them into chunks and operate on each chunk independently. They support compression as well, unlike HARs.  Block compression is the best option in most cases, since it compresses blocks of several records (rather than per record). It can be slow to convert existing data into SequenceFiles. However, it is perfectly possible to create a collection of SequenceFiles in parallel. (Stuart Sierra has written a very useful post about converting a tar file into a SequenceFile — tools like this are very useful, and it would be good to see more of them). Going forward it’s best to design your data pipeline to write the data at source direct into a SequenceFile, if possible, rather than writing to small files as an intermediate step. Unlike HAR files there is no way to list all the keys in a SequenceFile, short of reading through the whole file. (MapFiles, which are like SequenceFiles with sorted keys, maintain a partial index, so they can’t list all their keys either — see diagram.) SequenceFile is rather Java-centric. TFile is designed to be cross-platform, and be a replacement for SequenceFile, but it’s not available yet. HBase If you are producing lots of small files, then, depending on the access pattern, a different type of storage might be more appropriate. HBase stores data in MapFiles (indexed SequenceFiles), and is a good choice if you need to do MapReduce style streaming analyses with the occasional random look up. If latency is an issue, then there are lots of other choices — see Richard Jones’ excellent survey of key-value stores.</snippet></document><document id="745"><title>HDFS Reliability</title><url>http://blog.cloudera.com/blog/2009/01/hdfs-reliability/</url><snippet>We’ve been talking to enterprise users of Hadoop about existing and new projects, and lots of them are asking questions about reliability and data integrity.  So we wrote up a short paper entitled HDFS Reliability to summarize the state of the art and provide advice.  We’d like to get your feedback, too, so please leave a comment.</snippet></document><document id="746"><title>State of the Elephant 2008</title><url>http://blog.cloudera.com/blog/2009/01/state-of-the-elephant-2008/</url><snippet>It’s a new year, the time when we take a moment to look back at the previous one, and forward to what might be coming next. In the world of Hadoop a lot happened in 2008. Organization At the beginning of the year, Hadoop was a sub-project of Lucene. In January, Hadoop became a Top Level Project at Apache, in recognition of its success and diversity of community. This allowed sub-projects to be added, the first of which was HBase, previously a contrib project. ZooKeeper, a service for coordinating distributed systems, and which had been hosted at SourceForge, became a Hadoop sub-project in May. Then in October, Pig (a platform for analyzing large datasets) graduated from the Apache Incubator to become another Hadoop sub-project. Finally, Hive, which provides data warehousing for Hadoop, moved from being a Hadoop Core contrib project to its own sub-project in November. The creation of new projects is a sign of healthy growth in the Hadoop eco-system. The core mailing list traffic has not shown the huge growth that it did in the proceeding year, but the developer list remains the most trafficked Apache list (both at the time of writing, and for all time). In the new year Hadoop Core will have MapReduce and HDFS extracted to become standalone sub-projects, which will help ease the traffic burden on developers and users. Over the course of 2008 the base of Hadoop committers grew in diversity. Hadoop Core had 13 committers at the beginning of the year, from 4 distinct organizations; by the end of the year there were 21 committers from 9 distinct organizations. Releases Hadoop Core had four major releases in 2008, following a quarterly release cycle: 0.16.x, 0.17.x, 0.18.x, and 0.19.x. There were also many minor releases for bug fixes. HBase, which has a release after every major Hadoop release, had three releases in 2008 (the 0.19.0 release will be available soon). ZooKeeper made one major release (3.0.x). Pig had one major release (0.1.x), the next one will be a fairly major re-write of the core to introduce a new types system. Hive is yet to do a release. Community The first ever Hadoop Summit was hosted by Yahoo! at their Sunnyvale offices in March, and brought together Hadoop users and developers for a series of excellent presentations. The slides and videos are available at Yahoo! Research, and are well worth a look. ApacheCon US 2008, held in New Orleans in November had a dedicated Hadoop track, called Hadoop Camp. There was also a Hadoop training course; and Cloudera ran a Hadoop Hack contest to let conference goers dabble in Hadoop. 2008 saw a number of Hadoop User Groups (affectionately known as HUGs) spring up, including ones in the UK, the Bay Area, Los Angeles, New York, and South Korea. The academic and research community wholeheartedly embraced Hadoop in 2008. There are now a number of institutions that use Hadoop in their courses (including Brandeis University, University of Maryland, University of Washington, Carnegie Mellon University, UC Berkeley; see Google’s Code University for more), and well as research. A Record Owen O’Malley wrote a MapReduce program and ran it on a 910 node Hadoop cluster to win the 2008 Terabyte Sort benchmark. The program sorted 1TB of data in 3.48 minutes (209 seconds), beating the previous record of 297 seconds. Owen provided more details on the Yahoo! Developer Blog. In November, Google announced that their MapReduce implementation sorted 1TB in 68 seconds running on 1000 machines. James Hamilton provided more analysis on the two results. Related Projects The number of open source projects in the distributed computing space continues to grow relentlessly. Here are some that came to prominence in 2008, and have some connection to Hadoop (if only because they are used in conjunction with Hadoop, or perform similar functions). In no particular order: Mahout, an Apache Lucene sub-project to create scalable machine learning libraries that run on Hadoop Jaql, a query language for JSON data CloudBase, a data warehouse system build on Hadoop Cassandra, a distributed storage service Cascading, an API for building dataflows for Hadoop MapReduce Scribe, a service for aggregating log data Tashi, an Apache incubator project for cloud computing for large datasets Disco, a MapReduce implementation in Erlang/Python Hypertable, a distributed data storage system, modeled on Google’s Bigtable CloudStore, a distributed filesystem with Hadoop integration (formerly Kosmos filesystem) Some Predictions for 2009 Hadoop Core 1.0 will be released. (Notice I didn’t say what it will include, or when it will be!) There will be new Hadoop sub-projects, possibily Core contrib modules that see wider use and are promoted. More projects outside Apache will build on Hadoop. There will be increased adoption of Hadoop outside the web domain (e.g. retail, finance, bioinformatics, etc.). (With apologies to ApacheCon for the post title.)</snippet></document><document id="747"><title>What’s New in Hadoop Core 0.19</title><url>http://blog.cloudera.com/blog/2008/12/whats-new-in-hadoop-core-019/</url><snippet>The first release (0.19.0) from the 0.19 branch of Apache�Hadoop Core was made on November 24. Many changes go into a release like this, and it can be difficult to get a feel for the more significant ones, even with the detailed Jira log, change log, and release notes. (There’s also JDiff documentation, which is a great way to see how the public API changed, via a JavaDoc-like interface.) This post gives a high-level feel for what’s new. Core Hadoop Core now requires Java 6 to run (HADOOP-2325). Most installations are using Java 6 already due to the performance boost it gives over Java 5, but those that aren’t will need to upgrade. HDFS The most talked about change to HDFS, at least while it was being worked on, was HDFS appends (HADOOP-1700). People quoted the Jira issue number without risk of being misunderstood (“Is 1700 done yet?”). Well, it’s in 0.19.0, so you can now re-open HDFS files and add content to the end of the file. The biggest use so far of this feature is for making HBase’s redo log reliable. Administrators can now set space quotas on directory trees in HDFS (HADOOP-3938). Previously, you could only set a limit of the number of files and directories created in a certain directory (this was useful to stop folks eating up namenode memory). Full details in the guide. Something that folks have been asking for is access times on files (HADOOP-1869), as it helps administrators see which files haven’t been used for a while and therefore may be eligible for archival or deletion. Access times are hard to implement efficiently in a distributed filesystem, so you can specify their precision as a trade-off. By default they have hourly granularity, but you can change this via the dfs.access.time.precision property; setting it to 0 to turn off access times entirely. Filesystem checksums (HADOOP-3941). While HDFS uses checksums internally to ensure data integrity, it’s not possible to retrieve the checksums to use them for file content comparison, for example. This has changed with the addition of a new getFileChecksum(Path f) method on FileSystem, which returns a FileChecksum object. Most FileSystem implementations will return a null FileChecksum (meaning no checksum algorithm is implemented for the filesystem, or the filesystem doesn’t expose checksums), but HDFS returns a non-null FileChecksum that can be used to check if two files are the same. DistCp uses this API to see if two files are the same when performing an update. The addition of a Thrift interface (HADOOP-3754) to Hadoop’s FileSystem interface makes it easy to use HDFS (or indeed any Hadoop filesystem) from languages other than Java. The API comes as a part of the thriftfs contrib module, and includes pre-compiled stubs for a number of languages including Perl, PHP, Python and Ruby, so you can get up and running quickly without having to compile Thrift. MapReduce There are new library classes that support multiple inputs and multiple outputs for MapReduce jobs. Multiple inputs (HADOOP-372) allow application writers to specify input formats and mappers on a per path basis. For example, if you change your file format, it’s more elegant to have two mappers, one for the old file format, and one for the new, rather than putting all the logic for interpreting both formats in one mapper class. Or if you want to consume both text and binary files in the same job you can do so by using multiple input formats for your job. See the MultipleInputs class for details. Multiple outputs (HADOOP-3149) allows you to create several outputs for your jobs. The format of each output, and which records go in which output is under the application’s control, and it’s even possible to have different types for each output. This feature is provided by the MultipleOutputs class. There is another, related, library for writing multiple outputs that was introduced in an earlier release (MultipleOutputFormat). It is perhaps unfortunate that there are two libraries, but they have different strengths and weaknesses: for example, MultipleOutputFormat has more flexibility over the names of the output files, whereas MultipleOutputs has more flexibility over the output types. Chaining maps in a single job (HADOOP-3702). Most data processing problems in the real world that use MapReduce consist of a chain of MapReduce jobs, so this change may be helpful, as it collapses a chain of maps into a single task. The general case of (MR)+ is not handled, rather M+RM*, where the chain of maps before the reduce are executed as a single map task, and any maps after the reduce are executed in the reduce task. See ChainMapper and ChainReducer for how to use this feature. Skipping bad records (HADOOP-153). When processing terabytes of data it is common for applications to encounter bad records that they don’t know how to handle. If the failure occurs in a thrid party library (e.g. a native library), then there is often little that the MapReduce application writer can do. This feature allows the application to skip records that cause the program to fail, so that it can complete the job successfully, with only a small (controllable) number of discarded records. See the MapReduce user guide for documentation. Total order partitions (HADOOP-3019). As a spin-off from the TeraSort record, Hadoop now has library classes for efficiently producing a globally sorted output. InputSampler is used to sample a subset of the input data, and then TotalOrderPartitioner is used to partition the map outputs into approximately equal-sized partitions. Very neat stuff — well worth a look, even if you don’t need to use it. Streaming options (HADOOP-3722). Prior to 0.19 the way you passed options to jobs was different for Java MapReduce, Streaming and Pipes. For example, to pass a configuration parameter to a Streaming job, you would use say -jobconf name=value, whereas in Java you would use -D name=value. The options have been unified to use the Java convention, as implemented by ToolRunner and GenericOptionsParser. The old options have been deprecated and will be removed in a future release, so you should update your programs when you upgrade to a 0.19 release. Job scheduling has long been a pain point for many users of Hadoop (the FIFO scheduler is particularly unfair in a multi-user environment), so it was good to see not one, but two, new schedulers being introduced: the fair share scheduler (HADOOP-3746) and the capacity scheduler (HADOOP-3445). Both are possible due to the new pluggable scheduling API (HADOOP-3412), which allows third-party schedulers to be used in Hadoop. The schedulers are not the last word in scheduling, so expect to see further improvements in future releases. There have been several improvements in giving administrators and programmers more control over MapReduce resource usage. Task JVM re-use (HADOOP-249) allows the tasks from the same job to run (sequentially) in the same JVM, which reduces JVM startup overhead. (The default is not to share JVM instances, see the user guide.) Also, more work has been done to provide controls for managing memory-intensive jobs (HADOOP-3759, HADOOP-3581). For example, you can kill tasks whose child processes consume more than a given amount of memory. Contrib Hive (HADOOP-3601), the distributed data warehouse system, was in Hadoop Core contrib in 0.19, although it has since been moved to be its own Hadoop subproject. Hive provides a SQL-like interface to data stored in HDFS. Development is very active, so if you are interested in using Hive, you should consider using a more recent version. Chukwa (HADOOP-3719) and FailMon (HADOOP-3585) are both (independent) attempts to use Hadoop to monitor and analyze large distributed systems. They provide components to gather information about the system, and feed it into HDFS for later analysis by a series of pre-defined MapReduce jobs. Now you’ve seen what’s in Hadoop 0.19.0 you can download it from an Apache Mirror. Finally, thanks to everyone who contributed to this release.</snippet></document><document id="748"><title>Testing Apache Hadoop</title><url>http://blog.cloudera.com/blog/2008/12/testing-hadoop/</url><snippet>As a developer coming to Apache Hadoop it is important to understand how testing is organized in the project. For the most part it is simple — it’s really just a lot of JUnit tests — but there are some aspects that are not so well known. Running Hadoop Unit Tests Let’s have a look at some of the tests in Hadoop Core, and see how to run them. First check out the Hadoop Core source, and from the top-level directory type the following command. (Warning: it will take a few hours to run the whole test suite, so you may not want to do this straight away.) ant clean test The test target runs all of the core tests and the tests in contrib, by depending on test-core and test-contrib. Often, you only want to run one of these targets, and, furthermore, you may only want to run a single unit test, when you are working on a single class, for instance. To do this, use the testcase property. For example, to only run the TestConfiguration test, use the following (which will take a matter of seconds to run): ant -Dtestcase=TestConfiguration test-core You can use Ant wildcards in the spec: ant -Dtestcase=mapred/Test*Format test-core This will test TestFileInputFormat, TestFileOutputFormat and so on (but not TestFileInputFormatPathFilter). If you want to do more complex includes and excludes, then use the test.include and test.exclude properties: ant -Dtest.include=mapred/Test*Format -Dtest.exclude=mapred/TestFileOutputFormat test-core This will run the same tests as before, except it will exclude TestFileOutputFormat. The default value of test.include is Test*, so when you don’t specify any of these properties, you run all the tests beginning with “Test”. Some tests are included which don’t begin with “Test”: they are intended to be run manually. For example, Jets3tS3FileSystemContractTest, which runs against Amazon S3 (and therefore costs money) can be run like this: ant -Dtestcase=Jets3tS3FileSystemContractTest test-core (For this one you will need to set up some S3 credentials in src/test/hadoop-site.xml, as explained on the Hadoop Wiki). Turning to the contrib modules, there are a couple of ways to run the tests for a single module. From the top-level you can just filter appropriately. For example, to run just the streaming tests: ant -Dtestcase=streaming/**/Test* test-contrib Alternatively, you can change to the contrib’s subdirectory and invoke ant test from there. Writing Tests In Hadoop If you are adding a feature or fixing a bug, you need to write a test for the change. When you submit a patch, as well as running the test suite Hudson will look for new or changed tests, and if none are present will mark the issue with a -1 vote. Only if it is unreasonably difficult to test the change will not writing a test for the feature be justified. Hadoop provides a few useful tools and extension points for writing tests for testing Hadoop itself. Chief among the test classes to know about are the mini clusters, MiniDFSCluster and MiniMRCluster. These start HDFS and MapReduce clusters where each daemon runs in its own thread rather than its own process. Note however that MapReduce tasks run in a separate JVM to the tasktracker, so they are not in the same JVM as the test which launched them. This is important to understand if you want to communicate state between the task and the test, as you need to do so using the local filesystem or similar. Many tests need to run a MapReduce job, and to do this the ClusterMapReduceTestCase base class is especially convenient as it takes care of setting up and tearing down the mini cluster for you. For more information on these classes have a look at their source code (there is no published documentation on them). Why do the tests take so long to run? Having the mini clusters is a great boon as it allows us to write tests that exercise the whole Hadoop stack, but this comes at the price of test execution time. The current time to run the tests is over three hours, which means that developers don’t run the tests as often as they might. Kent Beck, in his book Extreme Programming Explained advocated the 10 minute build, since it allows the developer to get rapid feedback on the validity of changes they make to the system. The three hours it takes for the Hadoop build forces developers to work on something else while the tests run, and when failures occur, the context switch back to the previous task inevitably takes longer. So, how can we shorten the run times? There is a real tension here as the last thing we want to do is discourage developers from writing tests. Yet, if we keep growing the suite of tests as we have done, then the time to run them all will become prohibitive. Here are a few ideas to improve things: It’s often the case that each test doesn’t need its own brand new mini cluster. To pick a random example, TestJobName has two test methods that could easily test using the same mini cluster without fear of interacting (they run serially anyway). Many tests don’t need the full machinery of the mini clusters to run against. There is a lot of scope for making a test more focused, and writing it as a true unit test, i.e. one that tests the unit and not the whole system. Introducing a mock framework would facilitate this. HADOOP-1257 is a nice idea to use Hadoop itself to run the unit tests in parallel. Developers with access to a cluster will be able to run the tests in a fraction of the time. There are a few tests which take a disproportionate amount of time to run. It would be wise to focus on making these faster to run. Here are the top 10 from a recent run (times in seconds): % cat testout.txt | grep '[junit]' | awk '$2 =="Running" {printf "%s ", $3}; $2 == "Tests" {print $11}' \ | awk '{print $2, $1}' | sort -nr | head -10 617.415 org.apache.hadoop.hdfs.server.namenode.TestUnderReplicatedBlocks 597.529 org.apache.hadoop.mapred.TestJobTrackerRestart 338.584 org.apache.hadoop.mapred.TestMiniMRDFSSort 286.994 org.apache.hadoop.hdfs.TestDatanodeBlockScanner 277.45 org.apache.hadoop.mapred.TestJobTrackerRestartWithLostTracker 248.275 org.apache.hadoop.hdfs.TestDFSStorageStateRecovery 187.19 org.apache.hadoop.mapred.TestQueueManager 184.225 org.apache.hadoop.mapred.TestMultiFileInputFormat 178.838 org.apache.hadoop.mapred.TestTaskTrackerMemoryManager 174.269 org.apache.hadoop.mapred.TestMiniMRWithDFS Hudson provides a breakdown of test times by package too, which you can sort by clicking on the “Duration” column. Clover is a test coverage tool, which used to run as a part of the Hudson nightly build (and will be re-instated soon, hopefully). The latest version has a feature called Test Optimization that allows you test “only what you need”. I don’t know how this works, but it looks intriguing. HDFS and MapReduce are being split out of core, and this may give us an opportunity to improve things, not least because the project will be decomposed into smaller chunks. Imagine if we had a set of unit tests for each project that could run in 10 minutes or less, and a set of integration tests that took longer to run, but tested the whole system. The smaller set of tests would be useful during development of a new feature or bugfix, and once this passed it would be acceptable to submit the patch to Hudson, which would run the integration tests. This would be a good place to be. I plan to create Jiras to improve some of these aspects of testing in Hadoop. Comments, suggestions and help are all welcome.</snippet></document><document id="749"><title>Securing an Apache Hadoop Cluster Through a Gateway</title><url>http://blog.cloudera.com/blog/2008/12/securing-a-hadoop-cluster-through-a-gateway/</url><snippet>(Added 6/4/2013) Please note the instructions below are deprecated. Please refer to the CDH4 Security Guide for up-to-date procedures. A few weeks ago we ran an Apache Hadoop hackathon. ApacheCon participants were invited to use our 10-node Hadoop cluster to explore Hadoop and play with some datasets that we had loaded on in advance. One challenge we had to face was, how do we do this in a secure way? Hadoop does not offer much in the way of security. Hadoop provides a rudimentary file permission system on its distributed filesystem, HDFS, but does not verify the appropriateness of the username you are using. (Whatever username you use to start your local Hadoop client process is used as your HDFS username; this account does not necessarily need to exist on the machines which host the HDFS NameNode or DataNodes.) Even more problematically, anyone who can connect to the JobTracker can submit arbitrary code to run with the authority of the account used to start the Hadoop TaskTrackers on each node. While there is not a perfect solution to multitenancy in a Hadoop environment, by using a proxying gateway, you can at least control which users have access to your cluster. The rest of this post describes how to set up such a gateway configuration. The basic idea behind a proxying gateway is that direct connections to the JobTracker, NameNode, and slave machines (hosting the DataNodes and TaskTrackers) are disallowed except from within your internal network. These machines can all have private IP addresses. We configured our Hadoop machines to use addresses in the 10.x.x.x space. A separate machine, called the gateway, has a public IP address. Users can connect to the gateway machine through ssh and set up a SOCKS tunnel which “passes through” connections to the internal nodes. The primary benefit of this system is that only users who have linux accounts on the gateway machine can access the Hadoop cluster. Client Configuration For a user to connect to our Hadoop cluster, they’d have to modify their hadoop-site.xml file to include our master node’s DNS address in the fs.default.name and mapred.job.tracker fields. We gave them a prototype hadoop-site.xml file to download that looked like the following: &lt;?xml version="1.0"?&gt; &lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt; &lt;!-- Put site-specific property overrides in this file. --&gt; &lt;configuration&gt; &lt;!-- put your hadoophack username in here twice as comma,separated,values --&gt; &lt;property&gt; &lt;name&gt;hadoop.job.ugi&lt;/name&gt; &lt;value&gt;YOUR_USER_NAME,YOUR_USER_NAME&lt;/value&gt; &lt;/property&gt; &lt;!-- If you changed your tunnel port, change it here. If you don't know what this means, then leave it alone. --&gt; &lt;property&gt; &lt;name&gt;hadoophack.tunnel.port&lt;/name&gt; &lt;value&gt;2600&lt;/value&gt; &lt;/property&gt; &lt;!-- change these only if you know what you're doing --&gt; &lt;property&gt; &lt;name&gt;mapred.reduce.tasks&lt;/name&gt; &lt;value&gt;8&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.child.java.opts&lt;/name&gt; &lt;value&gt;-Xmx512m&lt;/value&gt; &lt;/property&gt; &lt;!-- Don't change anything below here --&gt; &lt;property&gt; &lt;name&gt;mapred.submit.replication&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.block.size&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://server1.cloudera.com:9000/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;server1.cloudera.com:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapred.system.dir&lt;/name&gt; &lt;value&gt;/hadoop/mapred/system&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.socks.server&lt;/name&gt; &lt;value&gt;localhost:${hadoophack.tunnel.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.rpc.socket.factory.class.default&lt;/name&gt; &lt;value&gt;org.apache.hadoop.net.SocksSocketFactory&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;   The username that is applied to files you create and use in HDFS is taken, by default, from whatever username you use to start your Hadoop jobs. Since a user might not have the same user account on our cluster as on their home machine, we wanted them to override this behavior. The hadoop.job.ugi parameter contains a comma-delimited list containing first the username, and then whatever UNIX-style groups should be associated with the user. We asked our users to change this to the username we assigned to their account. Since you must be in at least one group (which can have the same name as your username), your username needs to be entered in this field twice. The fs.default.name and mapred.job.tracker fields were filled in with our head node (server1)’s DNS name. Since Hadoop MapReduce determines the mapred.system.dir property at job creation time, regardless of values set at the JobTracker or TaskTrackers, we set that for them as well as the default block size, and the default number of reduce tasks. Finally, this file also instructs Hadoop to make all connections through a SOCKS proxy (by way of the hadoop.socks.server and hadoop.rpc.socket.factory.class.default properties). This means that given the presence of a tunnel to our gateway node, Hadoop’s connections should be made through this tunnel. Server Configuration Since properties are initially set at the client when a Hadoop job is started, and then inherited throughout the rest of the system, a configuration change was important inside the cluster as well. The hadoop-site.xml file that we propagated to the master node as well as all of the slaves contained the setting: &lt;property&gt; &lt;name&gt;hadoop.rpc.socket.factory.class.default&lt;/name&gt; &lt;value&gt;org.apache.hadoop.net.StandardSocketFactory&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt;   The nodes inside the cluster should connect directly to one another — these should not route connections through the SOCKS proxy. So we set the socket factory to the standard one, and made sure that the parameter was marked as final so that jobs couldn’t override it. One “gotcha” of Hadoop is that the HDFS instance has a canonical name associated with it, based on the DNS name of the machine — not its IP address. If you provide an IP address for the fs.default.name, it will reverse-DNS this back to a DNS name, then subsequent connections will perform a forward-DNS lookup on the canonical DNS name (and will not forward the DNS lookups through the SOCKS proxy). For users outside our firewall, this means trouble. So even though server1.cloudera.com has a private IP address, we created a public DNS “A” record for it on our public DNS server — pointing to the private IP address. The upshot of this is that if you’ve got a connection to the gateway, you’ll be able to transparantly use the master server’s JobTracker and NameNode instances as though they were on your network. If not, you’re out of luck. Much more secure! We also found that if a user mistakenly tried to run bin/start-dfs.sh (or start-all, etc) on their own machine, they would start a DataNode which would connect to our NameNode. This caused a lot of seemingly-inexplicable HDFS access and stability issues. The solution? Control which machines can register as DataNodes. We added a hosts file to HDFS which provides a whitelist of machines which can act as DataNodes. In hadoop-site.xml, we added the setting: &lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/conf/hosts&lt;/value&gt; &lt;/property&gt;   The dfs.hosts setting indicates a filename which contained the list of DNS names associated with our desired DataNodes, one name per line. Restart HDFS with that, and the errant user’s DataNode goes away, and the issues were resolved. Client Usage Instructions We then created user accounts on the gateway machine for all our hackathon participants. For them to use the Hadoop cluster, they had to open an SSH session to the gateway machine, opening a dynamic port-forwarding tunnel. This isn’t as hard as it sounds; it’s accomplished by: you@localhost:~$ ssh -D 2600 clouderaUsername@gateway.cloudera.com   The -D 2600 instructs SSH to open a SOCKS proxy on local port 2600. The SOCKS-based SocketFactory in Hadoop will then create connections forwarded over this SOCKS proxy. After this connection is established, you can minimize the ssh session and forget about it. Then just run Hadoop jobs in another terminal the normal way: you@localhost:~$ $HADOOP_HOME/bin/hadoop fs -ls / you@localhost:~$ $HADOOP_HOME/bin/hadoop jar myJarFile.jar myMainClass etc...   FoxyProxy A disadvantage of putting your nodes behind a firewall is that users can no longer access the web-based status monitoring tools that come with Hadoop. The JobTracker status page on port 50030 and NameNode status page on port 50070 are inaccessible. Configuring a web browser to route all traffic through the SOCKS proxy wouldn’t be a good idea, because then they’d be unable to access any other web sites they might need. By using FoxyProxy, a set of regular expression-based rules can be set up which determine what URLs get forwarded through a proxy, and what URLs are accessed without one. Users were instructed to download FoxyProxy, a free extension for FireFox. They were told to add rules forwarding http://server*.cloudera.com:*/* and http://10.1.130.*:*/* through a SOCKS proxy at localhost:2600. Now they were able to see the status pages, browse the DFS through their web browser, and access the error logs associated with the Hadoop daemons in case something went wrong with their jobs. This is used in conjunction with the SSH-based tunnel which they opened earlier. Advanced Server Configuration with IPTables If we just wanted to leave our servers with private IPs only, we could have left it at that. But we wanted these servers to also be able to connect out to the rest of the Internet. Doing this requires giving the machines public IP addresses again (or configuring them behind a NAT firewall). How to ensure that we could open outbound connections, but drop all inbound connections? We employed some simple IPTables rules to drop all incoming connections on the ethernet device with the public IP address. The output of /sbin/iptables -L on any of the private-network machines is as follows: Chain INPUT (policy DROP) target prot opt source destination ACCEPT all -- static-internal.reverse.cloudera.com/28 anywhere DROP all -- anywhere anywhere ACCEPT all -- anywhere anywhere ACCEPT all -- anywhere anywhere Chain FORWARD (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination ACCEPT all -- anywhere anywhere ACCEPT all -- anywhere anywhere   This tells the machines to accept any inbound connections in the /28 where we host our machines, but drop all other inbound new connections. Outbound connections to any destination are allowed. Future Steps… We started doing this process a little later than we’d have liked, and learned along the way what did and did not work so well. Configuring FoxyProxy turned out to be a cumbersome exercise for users. (There are a lot of button clicks and windows involved in setting up those relatively straightforward rules.) We’ve also found out that FoxyProxy can export its rules in a “PAC” file, which other users can then import. Newer users of our cluster just download and install this file in their FoxyProxy config. Ultimately, what we’d like best is to install an HTTP Forwarding Proxy on the gateway node, which would allow our users to see the server status and logs without having to install software and configure a dynamic proxy. Currently this is still on the to-do list though. Hadoop is pretty straightforward to use, but securing it can be tricky. There were a bunch of steps we had to follow above, but we achieved the end goal of imposing few demands on our legitimate users. Overall, this provided us with much better assurances about our cluster. The two major areas where this does not provide security are that users can impersonate one another with respect to the distributed filesystem, and they also all run processes in the same user’s process space — so they could theoretically interfere with other Hadoop user processes, or the Hadoop daemons themselves. But clamping down on who has access to the Hadoop cluster in the first place goes a long way toward keeping a system healthy. Stay tuned soon, for when we announce the contest winners!</snippet></document><document id="750"><title>Job Scheduling in Apache Hadoop</title><url>http://blog.cloudera.com/blog/2008/11/job-scheduling-in-hadoop/</url><snippet>(guest blog post by Matei Zaharia) When Apache Hadoop started out, it was designed mainly for running large batch jobs such as web indexing and log mining. Users submitted jobs to a queue, and the cluster ran them in order. However, as organizations placed more data in their Hadoop clusters and developed more computations they wanted to run, another use case became attractive: sharing a MapReduce cluster between multiple users. The benefits of sharing are tremendous: with all the data in one place, users can run queries that they may never have been able to execute otherwise, and costs go down because system utilization is higher than building a separate Hadoop cluster for each group. However, sharing requires support from the Hadoop job scheduler to provide guaranteed capacity to production jobs and good response time to interactive jobs while allocating resources fairly between users. This July, the scheduler in Hadoop became a pluggable component and opened the door for innovation in this space. The result was two schedulers for multi-user workloads: the Fair Scheduler, developed at Facebook, and the Capacity Scheduler, developed at Yahoo. The Fair Scheduler arose out of Facebook’s need to share its data warehouse between multiple users. Facebook started using Hadoop to manage the large amounts of content and log data it accumulated every day. Initially, there were only a few jobs that needed to run on the data each day to build reports. However, as other groups within Facebook started to use Hadoop, the number of production jobs increased. In addition, analysts started using the data warehouse for ad-hoc queries through Hive (Facebook’s SQL-like query language for Hadoop), and more large batch jobs were submitted as developers experimented with the data set. Facebook’s data team considered building a separate cluster for the production jobs, but saw that this would be extremely expensive, as data would have to be replicated and the utilization on both clusters would be low. Instead, Facebook built the Fair Scheduler, which allocates resources evenly between multiple jobs and also supports capacity guarantees for production jobs. The Fair Scheduler is based on three concepts: Jobs are placed into named “pools” based on a configurable attribute such as user name, Unix group, or specifically tagging a job as being in a particular pool through its jobconf. Each pool can have a “guaranteed capacity” that is specified through a config file, which gives a minimum number of map slots and reduce slots to allocate to the pool. When there are pending jobs in the pool, it gets at least this many slots, but if it has no jobs, the slots can be used by other pools. Excess capacity that is not going toward a pool’s minimum is allocated between jobs using fair sharing. Fair sharing ensures that over time, each job receives roughly the same amount of resources. This means that shorter jobs will finish quickly, while longer jobs are guaranteed not to get starved. The scheduler also includes a number of features for ease of administration, including the ability to reload the config file at runtime to change pool settings without restarting the cluster, limits on running jobs per user and per pool, and use of priorities to weigh the shares of different jobs. There is currently no support for preemption of long tasks, but this is being added in HADOOP-4665, which will allow you to set how long each pool will wait before preempting other jobs’ tasks to reach its guaranteed capacity. The Fair Scheduler has been in production use at Facebook since August. You can find it in the Hadoop trunk code under src/contrib/fairscheduler, and there are also versions of the scheduler for Hadoop 0.17 and Hadoop 0.18 on its JIRA page. All of these versions come with a README file explaining how to set up the scheduler that is placed under src/contrib/fairscheduler. The Capacity Scheduler from Yahoo offers similar functionality to the Fair Scheduler but takes a somewhat different philosophy. In the Capacity Scheduler, you define a number of named queues. Each queue has a configurable number of map and reduce slots. The scheduler gives each queue its capacity when it contains jobs, and shares any unused capacity between the queues. However, within each queue, FIFO scheduling with priorities is used, except for one aspect – you can place a limit on percent of running tasks per user, so that users share a cluster equally. In other words, the capacity scheduler tries to simulate a separate FIFO/priority cluster for each user and each organization, rather than performing fair sharing between all jobs. The Capacity Scheduler also supports configuring a wait time on each queue after which it is allowed to preempt other queues’ tasks if it is below its fair share. Documentation for the scheduler can be built as described in its README file under src/contrib/capacity-scheduler in the Hadoop trunk SVN. Now that the Fair Scheduler and Capacity Scheduler are available, there has been increased focus on other aspects of multi-user Hadoop clusters, such as isolating users and improving performance for the short interactive jobs seen in these environments. This has led to some exciting scheduling-related patches you can expect to see in future Hadoop releases: HADOOP-4487, which adds a number of security features to isolate users. HADOOP-3136, which lets the scheduler launch multiple tasks per heartbeat, improving “ramp-up time”. HADOOP-4664, 4513 and 4372, which parallelize job initialization to launch small jobs faster. HADOOP-2014, which chooses input blocks from overloaded racks when launching non-local maps. HADOOP-3759 and 657, which take into account tasks’ memory and disk space requirements to prevent oversubscribing nodes. HADOOP-4667, which improves locality for small jobs in the fair scheduler by letting it look at multiple jobs to select a local task. With the recent progress on scheduling, Hadoop is quickly growing to support the kind of multi-user data warehouse seen at Facebook: short interactive jobs, large batch jobs, and guaranteed-capacity production jobs sharing a cluster and delivering results quickly while maintaining high throughput. With a job scheduler that protects production jobs, users can try interesting R&amp;D experiments on your data set and gain valuable insights without worrying about affecting mission-critical jobs.</snippet></document><document id="751"><title>Introducing Hadoop Development Status</title><url>http://blog.cloudera.com/blog/2008/11/introducing-hadoop-development-status/</url><snippet>We’re happy to announce a new tool we have been developing here at Cloudera: Hadoop Development Status. Hadoop Development Status aims to help the Hadoop community understand its direction, health, and participants. The project currently monitors the most active contributors according to mailing list traffic, the most watched JIRA tickets, and aggregate traffic volumes on the Hadoop mailing lists. The graph of messages per month on the Hadoop Core lists shows a sustained growth in traffic. During this time, new sub-projects have been added to the Hadoop Top Level Project (HBase, ZooKeeper, Pig, Hive), but we haven’t created graphs for them yet. In fact, HBase was in core as a contrib module until February this year, when it became a sub-project. The growth in traffic on the core lists makes them difficult to follow, and this is one of the reasons for the planned partitioning of Core into Core, HDFS and MapReduce sub-projects, and the promotion of Hive into a sub-project. Contributions to Hadoop take many forms, including writing code, answering the questions of other users, and creating documentation. The number of messages sent to the mailing list is just one measure of how active a contributor is: you can see such a graph here. A perennial problem in open source projects is planning what’s in the next release. When folks are scratching their own itches, it’s difficult to predict what will be implemented, and when. JIRA has a few features that allow users to indicate their preferences, and these features can help planners prioritize features: folks can vote on an issue to say that they want this feature, or they can watch an issue to be informed of changes as development on it progresses. Voting is not widely used on the Hadoop JIRA, but watching is, and the number of watchers gives some level of interest in a new feature (at least among developers). On this basis, the ranked list of watched issues says that TFile, the Capacity Scheduler, X-Trace, MapReduce context objects, and processing multiple splits per mapper are the issues to watch. We have plenty of other ideas that will help us all understand the Hadoop community better, and we plan to be very active in developing this tool. However, we definitely haven’t thought of every way to improve it, so please email us with suggestions and improvements. –Alex, Jeff, and Tom</snippet></document><document id="752"><title>Sending Files to Remote Task Nodes with Hadoop MapReduce</title><url>http://blog.cloudera.com/blog/2008/11/sending-files-to-remote-task-nodes-with-hadoop-mapreduce/</url><snippet>It is common for a MapReduce program to require one or more files to be read by each map or reduce task before execution. For example, you may have a lookup table that needs to be parsed before processing a set of records. To address this scenario, Hadoop’s MapReduce implementation includes a distributed file cache that will manage copying your file(s) out to the task execution nodes. The DistributedCache was introduced in Hadoop 0.7.0; see HADOOP-288 for more detail on its origins. There is a great deal of existing documentation for the DistributedCache: see the Hadoop FAQ, the MapReduce Tutorial, the Hadoop Javadoc, and the Hadoop Streaming Tutorial. Once you’ve read the existing documentation and understand how to use the DistributedCache, come on back. How is the DistributedCache accessed by a MapReduce job? As of Hadoop 0.18.2, the action happens at the very beginning of the run method of the TaskRunner object. The process of copying the files to the local node is referred to as “localizing the archives”, and it’s performed before starting the local JVM for each task with a call to the static method getLocalCache of the DistributedCache class. Where does the DistributedCache store data? When debugging a MapReduce job, it can be useful to know where the DistributedCache is putting the cached files on the local nodes. By default, Hadoop stores these files under /tmp/hadoop-&lt;user.name&gt;/mapred/local/taskTracker/archive Every MapReduce task uses the directory specified by the configuration parameter mapred.local.dir to store scratch information; currently, this parameter defaults to &lt;hadoop.tmp.dir&gt;/mapred/local, and hadoop.tmp.dir defaults to /tmp/hadoop-&lt;user.name&gt;. The static method getCacheSubdir method of the TaskTracker class uses the constants TaskTracker.SUBDIR and TaskTracker.CACHEDIR to construct the path under mapred.local.dir where you can find the locally cached files. How big is the DistributedCache? The local.cache.size parameter controls the size of the DistributedCache. By default, it’s set to 10 GB. How can you add files to the DistributedCache with Hive? Hive uses the syntax “ADD FILE path/to/file” to pass a file to the DistributedCache. An alternative: pack the files into the job’s jarfile When submitting a job via Hadoop Streaming, you can use the -file option to package a file with the jarfile containing your MapReduce program. Using the -file option is the preferred way to package your scripts for execution with Hadoop Streaming, as documented in the Hadoop Streaming Tutorial. Note the difference between -file, which packs a file on your local filesystem with the jarfile containing your MapReduce program, and the -cacheFile option, which takes any URI accessible by the task nodes (including an HDFS URI). The former will be copied to the task nodes with the rest of the job, while the latter will be explicitly fetched by the DistributedCache. An example Okay, you’re now ready to try your hand at a nonstandard application of the above techniques. Instead of reading a file with configuration information, you are going to use the -file option to Hadoop Streaming to distribute an arbitrary Python module required by your job. First, go grab the zip file under the “Installation to Non-Standard Location” from the NLTK site. Once you’ve pulled it down to your local box and unzipped it, hop into the download directory and run: zip -r nltkandyaml.zip nltk yaml mv ntlkandyaml.zip /path/to/where/your/mapper/will/be/nltkandyaml.mod This will create a zipfile of the two modules you’ll need, with the .mod extension. Because the compressed file will be packaged inside of the job’s jarfile when the job is submitted to the JobTracker, it will be decompressed during the unpacking of the jarfile if you leave it with the .zip extension. If we had kept the .zip extension, the file would have been moved inside the lib/ folder of the task’s working directory when copied to the remote node; with the .mod extension, modules will remain inside the task’s working directory. You can now import the nltk module for use in your Python script: import zipimport importer = zipimport.zipimporter('nltkandyaml.mod') yaml = importer.load_module('yaml') nltk = importer.load_module('nltk') In an upcoming post, we’ll look at how we can use NLTK and Hadoop Streaming to do powerful natural language processing over corpora of nearly arbitrary size. Note: In the 0.19.x release family, the options for Hadoop streaming have changed (see HADOOP-3722). There is also discussion about modifying the “-file” syntax for Hadoop streaming at HADOOP-2622.</snippet></document><document id="753"><title>Configuring and Using Scribe for Hadoop Log Collection</title><url>http://blog.cloudera.com/blog/2008/11/configuring-and-using-scribe-for-hadoop-log-collection/</url><snippet>As promised in my post about installing Scribe for log collection, I’m going to cover how to configure and use Scribe for the purpose of collecting Hadoop logs.  In this post I’ll describe how to create the Scribe Thrift client for use in Java, add a new log4j Appender to Hadoop, configure Scribe, and collect logs from each node in a Hadoop cluster. At the end of the post, I will link to all source and configuration files mentioned in this guide. What’s the advantage of collecting Hadoop’s logs? Collecting Hadoop’s logs in to one single log file is very convenient for debugging and monitoring purposes. Essentially, what Scribe lets you do is tail -f your entire cluster, which is really cool to see :). Having one single log file on one, or even just a few, machines makes log analysis insanely simpler. Making log analysis simpler means the creation of monitoring and reporting tools just got a lot easier as well. Let’s get started … Create the Scribe Thrift Client for Java In order to use Scribe with Java, you need to use Thrift to create a Scribe client stub, which is a collection of generated Java files to be used by our custom log4j Appender, which will be covered later. For now, cd in to your Scribe distribution directory (I used trunk), and then cd in to the ‘if’ directory.  Now you’re in $SCRIBE_DIST/if.  Edit scribe.thrift by changing the ‘include’ line that includes fb303.thrift to the following: include "/path/to/your/thrift/contrib/fb303/if/fb303.thrift" Now cd in to your Thrift distribution and then in to contrib/fb303/if, which will bring you to $THRIFT_DISTRIBUTION/contrib/fb303/if. Edit fb303.thrift by changing the ‘include’ line that includes reflection_limited.thrift to the following: include "/path/to/your/thrift/if/reflection_limited.thrift" Once you’ve edited each of these thrift files, return back to $SCRIBE_ROOT/if and run: thrift --gen java scribe.thrift This will most likely present you with a bunch of deprecation warnings that you should ignore. The important thing is that you now have a folder called ‘gen-java’ that contains three Java classes, which all encapsulate the Scribe Thrift Java client. The last steps are to create Thrift and fb303 jars. Let’s start with Thrift. cd to $THRIFT_HOME/lib/java and simply run ‘ant’ to build libthrift.jar. Once ant finishes with no errors, libthrift.jar should appear in the folder you ran ant from. As for fb303, cd to $THRIFT_HOME/contrib/fb303/java and run ‘ant’ to build libfb303.jar. Once ant finishes with no errors, libfb303.jar should appear in $THRIFT_HOME/contrib/fb303/java/build/lib, not in the same directory that ant was ran from. Copy libthrift.jar and libfb303.jar to $HADOOP_HOME/lib, where they can be used by our custom log4j Appender. Create a custom log4j Appender to write logs to Scribe To create a custom Appender, you must extend org.apache.log4j.AppenderSkeleton and overwrite the following methods: activateOptions(), append(LoggingEvent event), close(), and requiresLayout(). Learn more about these methods’ signatures here. Reminder: I’ve posted a working Appender at the end of this post if you’d like to see the entire source code. activateOptions() should look something like this: synchronized(this) {
  this.list = new ArrayList(1);
  this.sock = new TSocket(new Socket("127.0.0.1", 1463));
  this.transport = new TFramedTransport(sock);
  this.protocol = new TBinaryProtocol(transport, false, false);
  this.client = new Client(protocol, protocol);
  this.transport.open();
}
 Note that you’ll need this in a try-catch block. The activateOptions() method initializes the Appender. append(LoggingEvent event) should look something like this: synchronized(this) {
  String message = this.layout.format(event);
  LogEntry entry = new LogEntry("hadoop", message);
  list.clear();
  list.add(entry);
  client.Log(list);
}
 Again, you’ll need this in a try-catch block. append(LoggingEvent event) actually does the logging. You can customize this message as much as you would like. One essential modification is to include the class that issued the log and the hostname of the machine that it occurred on. The issuing class is easy: look in to the getLocationInformation() method mentioned here. As for the hostname, I was unable to fetch the hostname programmatically (I kept getting a null hostname), so I had to choose an alternative solution. I’ll talk more about this in the next section. Finally, close() should have a single line: this.transport.close(), and requiresLayout() should return true;. Configure log4j to use your Appender Now that we have an Appender that will write to Scribe, we need to tell log4j to use it. Edit $HADOOP_CONF/log4j.properties (probably $HADOOP_ROOT/conf/log4j.properties), and add the following lines: log4j.appender.scribe=package.of.your.Appender
log4j.appender.scribe.hostname=yoursever.yourdomain.com
log4j.appender.scribe.layout=org.apache.log4j.PatternLayout
log4j.appender.scribe.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n
 Then, ensure that “scribe” is in your “hadoop.root.logger” variable: hadoop.root.logger=INFO,console,scribe
 and also ensure that “scribe” is in your “log4j.rootLogger” variable: log4j.rootLogger=${hadoop.root.logger}, EventCounter, scribe
 Note: this last change may be redundant, but I did it anyway. Essentially what we just did was define a new logging output, and we called it “scribe.” We told log4j what Appender to use for Scribe, and we told our Scribe Appender what the hostname of this node is (see more info below). We also told log4j the log format that we’re interested in. The following line calls our Appender’s setHostname(String hostname) method, which is the (less-than-ideal) solution to getting the hostname. log4j.appender.scribe.hostname=yourserver.yourdomain.com
 This means that we need to modify our Appender to have a getter and setter for the “hostname” private field. They should be public methods, the setter taking a single argument of type String with no return type, and the getter taking no arguments, returning a String. Configuring and running Scribe So let’s reiterate what we just did. We built a Scribe Thrift client that we use in our custom log4j Appender. We then configured log4j to use this new Appender. Lastly, we just need to configure Scribe and get it running. Working with the Scribe examples makes it very clear how one should configure Scribe. Take a look at $SCRIBE_DISTRIBUTION/examples/*.conf to get a sense of what does what. Your Scribe pipeline should be configured as follows: Hadoop writes a log -&gt; log4j sends this log to our Appender -&gt; the Appender sends a Scribe message to 127.0.0.1:1463 -&gt; the local Scribe server forwards the message to your central Scribe server on port 1464 -&gt; the central Scribe server stores the log Note that the local and central Scribe servers should be configured to have secondary storage. The local server should write secondary storage to its own disk, in a writable directory, and the central server should write secondary storage to /tmp, which is guaranteed to be a writable directory. Also note that you should choose a clever max_size value, one that will work conveniently with HDFS if you ever plan to write these logs to HDFS. In my example below, I set this value to 128000000 (128MB). Once you’ve configured Scribe, run your central server on any node other than the NameNode, and run the local server on every node. Finally, bounce Hadoop and tail your central server’s log file. You’ll be able to see exactly what’s happening in your Hadoop cluster in real time. Pretty rad, huh? This guide turned out to be much longer than I anticipated. There are a lot of files to configure, a decent amount of code to write, and lots of pieces to glue together. For your convenience, I’ve posted example source and configuration files below: central_server.conf (Scribe central server configuration) local_server.conf (Scribe local server configuration) log4j.properties (Make sure to update log4j.appender.scribe.hostname ScribeAppender.java and AsyncScribeAppender.java Scaling Scribe The configuration above will not scale. First, Hadoop creates a decent amount of log data, so storing these on a local disk won’t work in the long run. Second, having one machine collect logs from many hundreds or thousands of computers will clearly not work either. In a very large cluster, many different “central” Scribe servers should be installed behind a virtual IP or a reverse proxy. In a setup like this, Scribe connections will be multiplexed to several servers, and Scribe servers can be added or removed without system failure. If you choose to use a reverse proxy, you can poll the the fb303 counters of the central Scribe servers to perform simple load balancing. Note, however, that Scribe connections are persistent. That is, they are not created for each logged message. Instead client-server connections stay open for long periods of time, making complex control flow logic difficult. If you have any questions, then please shoot us an email. Otherwise, expect another post soon that covers Hadoop+Scribe benchmarking.</snippet></document><document id="754"><title>Installing Scribe For Log Collection</title><url>http://blog.cloudera.com/blog/2008/10/installing-scribe-for-log-collection/</url><snippet>Scribe is a newly released log collection tool that dumps log files from various nodes in a cluster to Scribe servers, where the logs are stored for further use.  Facebook describes their usage of Scribe by saying, “[Scribe] runs on thousands of machines and reliably delivers tens of billions of messages a day.”  It turns out that Scribe is rather difficult to install, so the hope of this post is to help those of you attempting to install Scribe.  The first step is to get dependencies installed. Dependencies Scribe has many dependencies that must be installed in order for Scribe to be built properly. They are listed here: ruby (ruby and ruby-dev) python (python and python-dev) libevent (libevent and libevent-dev) boost v1.36 Thrift fb303 (included in Thrift in contrib/fb303) The order in which these are installed is important.  First, you must install libevent, then libevent-dev, then boost, then Thrift, and finally fb303.  I installed libevent and libevent-dev from RPMs, whereas boost, Thrift, and fb303 were installed from source.  I was unable to get Scribe, Thrift, and fb303 to locate the boost libraries and includes correctly with the default boost install directories, so I installed boost in /usr/local/boost, /usr/local/boost/bin, /usr/local/boost/lib, and /usr/local/boost/include.  Run ‘./configure –help’ when configuring boost to see how to specify these options. When configuring Thrift and fb303, you must specify your location of boost with the “–with-boost=/path/to/boost/root” option and also set your BOOST_ROOT environment variable. Finally, you must make sure your LD_LIBRARY_PATH environment variable contains the lib folders that house the Thrift, fb303, boost, and libevent C++ libraries.  LD_LIBRARY_PATH follows the same pattern as the PATH variable.  That is, directories that contain libraries are separated by colons.  If you forget to set your LD_LIBRARY_PATH variable, then you’ll get the following error when running scribed: scribed: error while loading shared libraries: libboost_system-gcc41-mt-1_36.so.1.36.0: cannot open shared object file: No such file or directory Install Scribe Once you’ve successfully installed all dependencies, installing Scribe is easy.  Scribe ships with a fairly comprehensive README file, but the instructions involving boost’s configuration are slightly incorrect.  I needed to pass the “–with-boost=/usr/local/boost” options while configuring.  However, the README file says to use “–with-boost /usr/local/boost”.  Here is my full configure statement: ./configure --with-boost=/usr/local/boost --with-boost-system=boost_system-gcc41-mt-1_36 --with-boost-filesystem=boost_filesystem-gcc41-mt-1_36 We installed Scribe from trunk instead of releases/scribe-2.0. Configure Scribe Scribe ships with a few good examples in $SCRIBE_ROOT/examples; just take a look at the README in the examples directory, and you should be ready to rock.  However, the README doesn’t document that the scribe_ctrl and scribe_cat programs are in the examples directory. I hope this tutorial was helpful!  Send us an email if you have any issues.  We’ll make a follow-up post later talking about more in-depth Scribe configurations and benchmarking.</snippet></document><document id="755"><title>Thrift, Scribe, Hive, and Cassandra: Open Source Data Management Software</title><url>http://blog.cloudera.com/blog/2008/10/thrift-scribe-hive-and-cassandra-open-source-data-management-software/</url><snippet>Apache Hadoop exists within a rich ecosystem of tools for processing and analyzing large data sets. At Facebook, my previous employer, we contributed a few projects of note to this ecosystem, all under the Apache 2.0 license: Thrift: A cross-language RPC framework that powers many of Facebook’s services, include search, ads, and chat. Among other things, Thrift defines a compact binary serialization format that is often used to persist data structures for later analysis. Scribe: A Thrift service for distributed logfile collection. Scribe was designed to run as a daemon process on every node in your data center and to forward log files from any process running on that machine back to a central pool of aggregators. Because of its ubiquity, a major design point was to make Scribe consume as little CPU as possible. Hive: Once the data has been serialized using Thrift and collected using Scribe, it can be loaded into a Hadoop cluster for analysis. Running Hive above your Hadoop cluster will allow you to query the data using a SQL-like syntax; Hive will also manage the partitioning of logs inside the Hadoop Distributed File System. Cassandra: If you’ve got millions of users requesting and updating data, Cassandra can help you scale with your community. Cassandra was designed to power inbox search at Facebook and is now storing an index of around 35 TB. Design points included incremental scalability and low system administration overhead; Cassandra could be useful in many places where a horizontally partitioned (“sharded”) MySQL instance is currently deployed. I was recently invited by Robert Grossman of Open Data to speak about these projects at the inaugural Cloud Computing and Its Applications conference in Chicago. You can check out the slides from my talk below: All of these projects have small but growing user communities. I hope you’ll find them useful for your data management projects, and I look forward to seeing a few new users on the mailing lists soon. – Jeff Hammerbacher, VP Product and Chief Scientist, Cloudera</snippet></document><document id="756"><title>Welcome to Cloudera’s Hadoop blog!</title><url>http://blog.cloudera.com/blog/2008/10/welcome-to-clouderas-hadoop-blog/</url><snippet>We’ve created this blog as a place to post tips, tricks and insights on using Hadoop and related projects for the next generation of data storage and analysis. Of course, we’re also active on the Hadoop mailing lists and other public forums, but we wanted a place where we could capture some of the lessons we learn as we work with the community and our customers. Except for this inaugural post, everything here is intended to be long on signal and short on noise. We promise: No marketing fluff, no sales pitches — just useful technical information aimed at people trying to solve hard data problems. With that, I’ll get out of the way. – Mike Olson, CEO, Cloudera</snippet></document><document id="757"><title>What’s New in Apache Hadoop 0.21</title><url>http://blog.cloudera.com/blog/2010/08/what%E2%80%99s-new-in-apache-hadoop-0-21/</url><snippet>Apache Hadoop 0.21.0 was released on August 23, 2010. The last major release was 0.20.0 in April last year, so it’s not surprising that there are so many changes in this release, given the amount of activity in the Hadoop development community. In fact, there were over 1300 issues fixed in JIRA (Common, HDFS, MapReduce), the issue tracker used for Apache Hadoop development. Bear in mind that the 0.21.0 release, like all dot zero releases, isn’t suitable for production use. With such a large delta from the last release, it is difficult to grasp the important new features and changes. This post is intended to give a high-level view of some of the more significant features introduced in the 0.21.0 release. Of course, it can’t hope to cover everything, so please consult the release notes (Common, HDFS, MapReduce) and the change logs (Common, HDFS, MapReduce) for the full details. Also, please let us know in the comments of any features, improvements, or bug fixes that you are excited about. You can download Hadoop 0.21.0 from an Apache Mirror. Thanks to everyone who contributed to this release! Project Split Organizationally, a significant chunk of work has arisen from the project split, which transformed a single Hadoop project (called Core) into three constituents: Common, HDFS, and MapReduce. HDFS and MapReduce both have dependencies on Common, but (other than for running tests) MapReduce has no dependency on HDFS. This separation emphasizes the fact that MapReduce can run on alternative distributed file systems (although HDFS is still the best choice for sheer throughput and scalability), and it has made following development easier since there are now separate lists for each subproject. There is one release tarball still, however, although it is laid out a little differently from previous releases, since it has a subdirectory containing each of the subproject source files. From a user’s point of view little has changed as a result of the split. The configuration files are divided into core-site.xml, hdfs-site.xml, and mapred-site.xml (this was supported in 0.20 too), and the control scripts are now broken into three (HADOOP-4868): in addition to the bin/hadoop script, there is a bin/hdfs script and a bin/mapreduce script for running HDFS and MapReduce daemons and commands, respectively. The bin/hadoop script still works as before, but issues a deprecation warning. Finally, you will need to set the HADOOP_HOME environment variable to have the scripts work smoothly. Common The 0.21.0 release is technically a minor release (traditionally Hadoop 0.x releases have been major, and have been allowed to break compatibility with the previous 0.x-1 release) so it is API compatible with 0.20.2. To make the intended stability and audience of a particular API in Hadoop clear to users, all Java members with public visibility have been marked with classification annotations to say whether they are Public, or Private (there is also LimitedPrivate which signifies another, named, project may use it), and whether they are Stable, Evolving, or Unstable (HADOOP-5073). Only elements marked as Public appear in the user Javadoc (Common, MapReduce; note that HDFS is all marked as private since it is accessed through the FileSystem interface in Common). The classification interface is descibed in detail in Towards Enterprise-Class Compatibility for Apache Hadoop by Sanjay Radia. This release has seen some significant improvements to testing. The Large-Scale Automated Test Framework, known as Herriot (HADOOP-6332), allows developers to write tests that run against a real (possibly large) cluster. While there are only a dozen or so tests at the moment, the intention is that more tests will be written over time so that regression tests can be shared and run against new Hadoop release candidates, thereby making Hadoop upgrades more predictable for users. Hadoop 0.21 also introduces a fault injection framework, which uses AOP to inject faults into a part of the system that is running under test (e.g. a datanode), and asserts that the system reacts to the fault in the expected manner. Complementing fault injection is mock object testing, which tests code “in the small”, at the class-level rather than the system-level. Hadoop has a growing number of Mockito-based tests for this purpose (MAPREDUCE-1050). Among the many other improvements and new features, a couple of small ones stand out: the ability to retrieve metrics and configuration from Hadoop daemons by accessing the URLs /metrics and /conf in a browser (HADOOP-5469, HADOOP-6408). HDFS Support for appends in HDFS has had a rocky history. The feature was introduced in the 0.19.0 release, and then disabled in 0.19.1 due to stability issues. The good news is that the append call is back in 0.21.0 with a brand new implementation (HDFS-265), and may be accessed via FileSystem‘s append() method. Closely related—and more interesting for many applications, such as HBase—is the Syncable interface that FSDataOutputStream now implements, which brings sync semantics to HDFS (HADOOP-6313). Hadoop 0.21 has a new filesystem API, called FileContext, which makes it easier for applications to work with multiple filesystems (HADOOP-4952). The API is not in widespread use yet (e.g. it is not integrated with MapReduce), but it has some features that the old FileSystem interface doesn’t, notably support for symbolic links (HADOOP-6421, HDFS-245). The secondary namenode has been deprecated in 0.21. Instead you should consider running a checkpoint node (which essentially acts like a secondary namenode) or a backup node (HADOOP-4539). By using a backup node you no longer need an NFS-mount for namenode metadata, since it accepts a stream of filesystem edits from the namenode, which it writes to disk. New in 0.21 is the offline image viewer (oiv) for HDFS image files (HADOOP-5467). This tool allows admins to analyze HDFS metadata without impacting the namenode (it also works with older versions of HDFS). There is also a block forensics tool for finding corrupt and missing blocks from the HDFS logs (HDFS-567). Modularization continues in the platform with the introduction of pluggable block placement (HDFS-385), an expert-level interface for developers who want to try out new placement algorithms for HDFS. Other notable new features include: Support for efficient file concatenation in HDFS (HDFS-222) Distributed RAID filesystem (HDFS-503) – an erasure coding filesystem running on HDFS, designed for archival storage since the replication factor is reduced from 3 to 2, while keeping the likelihood of data loss about the same. (Note that the RAID code is a MapReduce contrib module since it has a dependency on MapReduce for generating parity blocks.) MapReduce The biggest user-facing change in MapReduce is the status of the new API, sometimes called “context objects”. The new API is now more broadly supported since the MapReduce libraries (in org.apache.hadoop.mapreduce.lib) have been ported to use it (MAPREDUCE-334). The examples all use the new API too (MAPREDUCE-271). Nevertheless, to give users more time to migrate to the new API, the old API has been un-deprecated in this release (MAPREDUCE-1735), which means that existing programs will compile without deprecation warnings. The LocalJobRunner (for trying out MapReduce programs on small local datasets) has been enhanced to make it more like running MapReduce on a cluster. It now supports the distributed cache (MAPREDUCE-476), and can run mappers in parallel (MAPREDUCE-1367). Distcp has seen a number of small improvements too, such as preserving file modification times (HADOOP-5620), input file globbing (HADOOP-5472), and preserving the source path (MAPREDUCE-642). Continuing the testing theme, this release is the first to feature MRUnit, a contrib module that helps users write unit tests for their MapReduce jobs (HADOOP-5518). Other new contrib modules include Rumen (MAPREDUCE-751) and Mumak (MAPREDUCE-728), tools for modelling MapReduce. The two are designed to work together: Rumen extracts job data from historical logs, which Mumak then uses to simulate MapReduce applications and clusters on a cluster. Gridmix3 is also designed to work with Rumen traces. The job history log analyzer is another tool that gives information about MapReduce cluster utilization (HDFS-459). On the job scheduling front there have been updates to the Fair Scheduler, including global scheduling (MAPREDUCE-548), preemption (MAPREDUCE-551), and support for FIFO pools (MAPREDUCE-706). Similarly, the Capacity Scheduler now supports hierarchical queues (MAPREDUCE-824), and admin-defined hard limits (MAPREDUCE-532). There is also a brand new scheduler, the Dynamic Priority Scheduler, which dynamically changes queue shares using a pricing model (HADOOP-4768). Smarter speculative execution has been added to all schedulers using a more robust algorithm, called Longest Approximate Time to End (LATE) (HADOOP-2141). Finally, a couple of smaller changes: Streaming combiners are now supported, so that the -combiner option may specify any streaming script or executable, not just a Java class. (HADOOP-4842) On the successful completion of a job, the MapReduce runtime creates a _SUCCESS file in the output directory. This may be useful for applications that need to see if a result set is complete just by inspecting HDFS. (MAPREDUCE-947) What’s Not In Finally, it bears mentioning what didn’t make it into 0.21.0. The biggest omission is the new Kerberos authentication work from Yahoo! While a majority of the patches are included, security is turned off by default, and is unlikely to work if enabled (certainly there is no guarantee that it will provide any level of security, since it is incomplete). A full working security implementation will be available in 0.22, and also the next version of CDH. Also, Sqoop, which was initially developed as a Hadoop contrib module, is not in 0.21.0, since it was moved out to become a standalone open source project hosted on github.</snippet></document></searchresult>