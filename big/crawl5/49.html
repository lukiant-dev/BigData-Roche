<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
<title>How-to: Write and Run Giraph Jobs on Hadoop  | Cloudera Developer Blog</title>

<meta name="keywords" content="hadoop, hadoop training, cloudera, hadoop tutorial, hadoop certification, apache hadoop, hadoop download, big data, open source" />
<meta name="description" content="" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<meta name="msvalidate.01" content="8857B9071A02F989DE3F8BEE557BB584" />

<link rel="search" type="application/opensearchdescription+xml" href="/assets/opensearch.xml" title="Cloudera" />

<meta property="og:title" content="How-to: Write and Run Giraph Jobs on Hadoop"/>
<meta property="og:type" content="article"/>
<meta property="og:url" content="http://blog.cloudera.com/blog/2014/02/how-to-write-and-run-giraph-jobs-on-hadoop/"/>
<meta property="og:site_name" content="Cloudera Developer Blog"/>


<link rel="icon" href="/wp-content/themes/solutionset/assets/favicon.ico" type="image/x-icon" /> 
<link rel="stylesheet" media="all" type="text/css" href="/wp-content/themes/solutionset/assets/css/960.css?070910" />
<link rel="stylesheet" media="all" type="text/css" href="/wp-content/themes/solutionset/assets/css/reset.css?070910" />
<link rel="stylesheet" media="all" type="text/css" href="/wp-content/themes/solutionset/assets/css/all.css?20120620" />
<link rel="stylesheet" media="all" type="text/css" href="/wp-content/themes/solutionset/assets/css/wp.css?20120620" /> 

<!--[if lt IE 7]><link rel="stylesheet" type="text/css" href="http://blog.cloudera.com/wp-content/themes/solutionset/assets/css/ie6.css?20120605" media="screen"/><![endif]-->
<!--[if lt IE 8]><link rel="stylesheet" type="text/css" href="http://blog.cloudera.com/wp-content/themes/solutionset/assets/css/ie6-7.css?20120605" media="screen"/><![endif]-->
<!--[if lt IE 9]><link rel="stylesheet" type="text/css" href="http://blog.cloudera.com/wp-content/themes/solutionset/assets/css/ie.css?20120605" media="screen"/><![endif]-->

<script type="text/javascript" src="/wp-content/themes/solutionset/assets/js/modernizr-2.6.1.min.js"></script>
<script type="text/javascript" src="/wp-content/themes/solutionset/assets/js/mootools-1.2.4-yui.js"></script>
<script type="text/javascript" src="/wp-content/themes/solutionset/assets/js/mootools-1.2.4.4-more-yui.js"></script>
<script type="text/javascript" src="/wp-content/themes/solutionset/assets/js/jquery-1.6.2.min.js"></script>
<script type="text/javascript"> jQuery.noConflict(); </script>
<script type="text/javascript" src="/wp-content/themes/solutionset/assets/js/jquery.colorbox-min.js"></script>
<script type="text/javascript" src="/wp-content/themes/solutionset/assets/js/global.js?20120605"></script>
<script type="text/javascript">var switchTo5x=true;</script>
<script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script>
<script type="text/javascript">stLight.options({publisher: "ur-aa86c136-1042-b30d-950-dd905bb179a0", doNotHash: true, doNotCopy: true, hashAddressBar: false});</script>


<link rel="pingback" href="http://blog.cloudera.com/xmlrpc.php" />
<link rel="alternate" type="application/rss+xml" title="Cloudera Developer Blog &raquo; Feed" href="http://blog.cloudera.com/feed/" />
<link rel="alternate" type="application/rss+xml" title="Cloudera Developer Blog &raquo; Comments Feed" href="http://blog.cloudera.com/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Cloudera Developer Blog &raquo; How-to: Write and Run Giraph Jobs on Hadoop Comments Feed" href="http://blog.cloudera.com/blog/2014/02/how-to-write-and-run-giraph-jobs-on-hadoop/feed/" />
<link rel='stylesheet' id='prettify-gc-syntax-highlighter-css'  href='http://blog.cloudera.com/wp-content/plugins/prettify-gc-syntax-highlighter/prettify.css?ver=3.3.2' type='text/css' media='all' />
<link rel='stylesheet' id='cptchStylesheet-css'  href='http://blog.cloudera.com/wp-content/plugins/captcha/css/style_wp_before_3.8.css?ver=3.3.2' type='text/css' media='all' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://blog.cloudera.com/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://blog.cloudera.com/wp-includes/wlwmanifest.xml" /> 
<link rel='prev' title='Spark is Now Generally Available for Cloudera Enterprise' href='http://blog.cloudera.com/blog/2014/02/spark-is-now-generally-available-for-cloudera-enterprise/' />
<link rel='next' title='This Month in the Ecosystem (January 2014)' href='http://blog.cloudera.com/blog/2014/02/this-month-in-the-ecosystem-january-2014/' />
<meta name="generator" content="WordPress 3.3.2" />
<link rel='canonical' href='http://blog.cloudera.com/blog/2014/02/how-to-write-and-run-giraph-jobs-on-hadoop/' />
<link rel='shortlink' href='http://blog.cloudera.com/?p=25182' />


<script type="text/javascript">
 var _gaq = _gaq || [];
 _gaq.push(['_setAccount', 'UA-2275969-16']);
 _gaq.push(['_trackPageview']);

 (function() {
   var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
   ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
   var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
 })();
</script>


</head>
<body class="single single-post postid-25182 single-format-standard devcenter">
			
		
			
	<header id="site-head">
<nav class="properties">
            <div class="container">
                <ul>
                    <!--<li><a href="http://www.cloudera.com">Cloudera.com</a></li>-->
                     <!--<li><a href="http://university.cloudera.com">Cloudera University</a></li>
                   <li><a href="${config.LINK_CCP}/display/DOC/Documentation">Documentation</a></li>-->
                    <li><a id="support_home_page" href="http://cloudera.com/content/support/en/home.html" class="active">Support</a></li>
                    <li><a href="http://cloudera.com/content/dev-center/en/home.html">Developers</a></li>
                  <!--<li><a href="http://cloudera.com/content/cloudera/en/partners.html">PARTNERS</a></li>-->
                   
                </ul>
                <ul class="user">
                    <li>
                       <!--<a id="signinLink" class="hidden" href="https://clouderapkb.echolane.cs3.force.com/idp/login?app=0spQ00000004CD5">Sign In</a>-->
<a id="signinLink" class="hidden" href="https://cloudera.secure.force.com">Sign In</a>
                    </li>
                    <li><a id="registerLink" class="hidden" href="http://cloudera.com/content/support/en/user-registration.html">Register</a></li>
                    <li><a href="http://cloudera.com/content/cloudera/en/about/contact-us.html">Contact Us</a></li>
                    <li><a href="http://cloudera.com/content/support/en/downloads.html">Downloads</a></li>
                    <li>
                        <div id="dropdownAction" class="dropdown" style="display:none">
                            <a id="lnkDropdowntoogle" data-toggle="dropdown" class="dropdown-toggle" href="#">

                            </a>
                            <ul aria-labelledby="dropdownMenu" tole="menu" class="dropdown-menu">
                                <li><a href="http://cloudera.com/content/support/en/edit-user-profile.html" id="editProfileLink" tabindex="-1">Edit Profile</a></li>
                                <li class="divider"></li>
                                <li>
                                <a id="logoutLink" tabindex="-1" href="#">Logout</a>
                                </script>
                                </li>
                            </ul>
                        </div>
                    </li>
                </ul>
            </div>
            <div class="bg-fix"></div>
        </nav>
<!--</div>-->

<div class="wrapper">
    <div class="bg-fix"></div>
    <h1 class="logo">
        <a href="http://cloudera.com/content/cloudera/en/home.html">Cloudera</a>
    </h1>

<nav class="site">
        <ul>
    <li class="">
 <a href="http://community.cloudera.com" data-link="external">Community</a>
</li>
<li class="">
 <a href="http://cloudera.com/content/support/en/documentation.html" data-link="external">Documentation</a>
</li>
 <li class="">
                    <a href="http://cloudera.com/content/support/en/downloads.html" data-link="external">Downloads</a>
   </li>
     <li class="">
                    <a href="http://university.cloudera.com" data-link="external">Training</a>
     </li>
<li class="">
                    <a href="http://blog.cloudera.com" data-link="external" class="active">Blogs</a>
                    <nav class="subnav menu"> <nav><ul>
<li><a href="http://vision.cloudera.com">Cloudera Vision</a></li>
<!--<li><a href="http://blog.cloudera.com/blog">Developer Blog</a></li>-->
</ul>
</nav> </nav>
</li>
            
        </ul>
    </nav>


    <div class="form-holder">
		
	    <form action="http://cloudera.com/content/cloudera/en/search.html" id="site-search" method="get" novalidate> 
	        <label for="q" class="visuallyhidden">Search</label> 
	        <input type="search" name="q" id="q" placeholder="Search"><i class="icon-search"></i> 
	    </form>
    </div>
    </div><!--</div>-->
        </header>
				
	<div role="main" class="main">
		<div class="wrapper">
			<section class="two-col">

	
<aside class="left-col">

				<nav>
			<ul class=" ">
			
								
							<li class="">
				<a
					href="http://www.cloudera.com/content/cloudera/en/why-cloudera/hadoop-and-big-data.html"
					title="Hadoop &amp; Big Data"
					class=""
					target="_blank"				>
					Hadoop &amp; Big Data				</a>

							</li>
			
					<li class="">
				<a
					href="http://www.cloudera.com/content/cloudera/en/why-cloudera/our-customers.html"
					title="Our Customers"
					class=""
					target="_blank"				>
					Our Customers				</a>

							</li>
			
					<li class="">
				<a
					href="http://www.cloudera.com/content/cloudera/en/why-cloudera/faqs.html"
					title="FAQs"
					class=""
					target="_blank"				>
					FAQs				</a>

							</li>
			
					<li class="current">
				<a
					href="/blog/"
					title="Blog"
					class="blog"
									>
					Blog				</a>

									<ul>
									<li class="">
				<a
					href="/blog/category/accumulo/"
					title="Accumulo (1)"
					class=""
									>
					Accumulo (1)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/avro/"
					title="Avro (16)"
					class=""
									>
					Avro (16)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/bigtop/"
					title="Bigtop (6)"
					class=""
									>
					Bigtop (6)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/books/"
					title="Books (6)"
					class=""
									>
					Books (6)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/careers/"
					title="Careers (14)"
					class=""
									>
					Careers (14)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/cdh/"
					title="CDH (127)"
					class=""
									>
					CDH (127)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/cloud-2/"
					title="Cloud (9)"
					class=""
									>
					Cloud (9)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/cloudera-life/"
					title="Cloudera Life (3)"
					class=""
									>
					Cloudera Life (3)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/cloudera-manager/"
					title="Cloudera Manager (61)"
					class=""
									>
					Cloudera Manager (61)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/community/"
					title="Community (182)"
					class=""
									>
					Community (182)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/data-collection/"
					title="Data Collection (17)"
					class=""
									>
					Data Collection (17)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/data-science/"
					title="Data Science (26)"
					class=""
									>
					Data Science (26)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/distribution/"
					title="Distribution (36)"
					class=""
									>
					Distribution (36)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/events/"
					title="Events (37)"
					class=""
									>
					Events (37)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/flume/"
					title="Flume (18)"
					class=""
									>
					Flume (18)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/general/"
					title="General (327)"
					class=""
									>
					General (327)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/guest/"
					title="Guest (77)"
					class=""
									>
					Guest (77)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/hadoop/"
					title="Hadoop (293)"
					class=""
									>
					Hadoop (293)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/hardware/"
					title="Hardware (3)"
					class=""
									>
					Hardware (3)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/hbase/"
					title="HBase (124)"
					class=""
									>
					HBase (124)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/hdfs/"
					title="HDFS (45)"
					class=""
									>
					HDFS (45)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/hive/"
					title="Hive (62)"
					class=""
									>
					Hive (62)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/how-to/"
					title="How-to (53)"
					class=""
									>
					How-to (53)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/hue/"
					title="Hue (30)"
					class=""
									>
					Hue (30)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/impala/"
					title="Impala (63)"
					class=""
									>
					Impala (63)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/kite-sdk/"
					title="Kite SDK (11)"
					class=""
									>
					Kite SDK (11)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/mahout-2/"
					title="Mahout (5)"
					class=""
									>
					Mahout (5)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/mapreduce/"
					title="MapReduce (71)"
					class=""
									>
					MapReduce (71)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/meet-the-engineer/"
					title="Meet The Engineer (18)"
					class=""
									>
					Meet The Engineer (18)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/oozie/"
					title="Oozie (25)"
					class=""
									>
					Oozie (25)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/ops/"
					title="Ops And DevOps (19)"
					class=""
									>
					Ops And DevOps (19)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/pig/"
					title="Pig (35)"
					class=""
									>
					Pig (35)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/quickstart-vm/"
					title="QuickStart VM (5)"
					class=""
									>
					QuickStart VM (5)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/search/"
					title="Search (21)"
					class=""
									>
					Search (21)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/security-2/"
					title="Security (15)"
					class=""
									>
					Security (15)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/spark/"
					title="Spark (9)"
					class=""
									>
					Spark (9)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/sqoop/"
					title="Sqoop (20)"
					class=""
									>
					Sqoop (20)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/support/"
					title="Support (4)"
					class=""
									>
					Support (4)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/testing/"
					title="Testing (8)"
					class=""
									>
					Testing (8)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/this-month-in-the-ecosystem/"
					title="This Month In The Ecosystem (8)"
					class=""
									>
					This Month In The Ecosystem (8)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/tools/"
					title="Tools (6)"
					class=""
									>
					Tools (6)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/training-2/"
					title="Training (42)"
					class=""
									>
					Training (42)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/use-case/"
					title="Use Case (59)"
					class=""
									>
					Use Case (59)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/whirr/"
					title="Whirr (6)"
					class=""
									>
					Whirr (6)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/yarn/"
					title="YARN (12)"
					class=""
									>
					YARN (12)				</a>

							</li>
			
					<li class="">
				<a
					href="/blog/category/zookeeper/"
					title="ZooKeeper (24)"
					class=""
									>
					ZooKeeper (24)				</a>

							</li>
			
					<li class="">
				<a
					href="/archive/"
					title="Archives by Month"
					class=""
									>
					Archives by Month				</a>

							</li>
			
							</ul>
							</li>
			
						
			    
			
				<div style="clear:both"></div>
			</ul>
			</nav>
			<div class="menu-special">
				<ul>
							
				
				
		
				
		
				
				
				
				

		
					</ul>
			</div>
			
</aside>


<section>
			<h1 class="heading ">How-to: Write and Run Giraph Jobs on Hadoop</h1>
			
			<script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
			
			<ul class="post-info">
				<li>by Mirko Kämpf</li>
				<li>February 05, 2014</li>
				<li class="comment"><a href="#comments">8 comments</a></li>
				
			</ul>
			
			<div class="text-block">
				<p><strong>Create a test environment for writing and testing Giraph jobs, or just for playing around with Giraph and small sample datasets.</strong></p>
<p><a href="http://giraph.apache.org">Apache Giraph</a> is a scalable, fault-tolerant implementation of graph-processing algorithms in Apache Hadoop clusters of up to thousands of computing nodes. Giraph is in use at companies like Facebook and PayPal, for example, to help represent and analyze the billions (or even trillions) of connections across massive datasets. Giraph was inspired by <a href="http://kowshik.github.io/JPregel/pregel_paper.pdf">Google&#8217;s Pregel</a> framework and integrates well with Apache Accumulo, Apache HBase, Apache Hive, and Cloudera Impala.</p>
<p>Currently, the upstream <a href="http://giraph.apache.org/quick_start.html">&#8220;quick start&#8221; document</a> explains how to deploy Giraph on a Hadoop cluster with two nodes running Ubuntu Linux. Although this setup is appropriate for lightweight development and testing, using Giraph with an enterprise-grade <a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html">CDH-based cluster</a> requires a slightly more robust approach.</p>
<p>In this how-to, you will learn how to use Giraph 1.0.0 on top of CDH 4.x using a simple example dataset, and run example jobs that are already implemented in Giraph. You will also learn how to set up your own Giraph-based development environment. The end result will be a setup (not intended for production) for writing and testing Giraph jobs, or just for playing around with Giraph and small sample datasets. (In future posts, I will explain how to implement your own graph algorithms and graph generators as well as how to export your results to <a href="https://gephi.org/">Gephi</a>, the &#8220;Adobe Photoshop for graphs&#8221;, through Impala and JDBC for further inspection.)</p>
<h2>How Giraph Works</h2>
<p>As in the core MapReduce framework, in Giraph, all data and workload distribution related details are hidden behind an easy-to-use API. Currently, Giraph API is used on top of MR1 via a slightly unstable mechanism, although a YARN-based implementation is also possible (also to be discussed in a future post). I recommend, to turn off preemption in your cluster if you plan to run Giraph jobs.</p>
<p>In Giraph, a worker node or a slave node is a host (either a physical server or even a virtualized server) that performs the computation and stores data in HDFS. Such workers load the graph and keep the full graph or just a part of it (in case of distributed graph analysis) in memory. Very large graphs are partitioned and distributed across many worker nodes.</p>
<p>(Note: the term <em>partition</em> has a different meaning in Giraph. Here, the partitioning of the graph is not necessarily the result of the application of a graph-partitioning algorithm. Rather, it is more a way to group data based on a vertex hash value and the number of workers. This approach is comparable to the partitioning that is done during the shuffle-and-sort phase in MapReduce.)</p>
<p>A Giraph algorithm is an iterative execution of &#8220;super-steps&#8221;? that consist of a message exchange phase followed by an aggregation and node or edge property update phase. While vertices and edges are held in memory, the nodes exchange messages in parallel. Therefore, all worker nodes communicate and send each other small messages, usually of very low data volume.</p>
<p>After this message-exchange phase, a kind of aggregation is done. This leads to an update of the vertex and/or edge properties and a super-step is finished. Now, the next super-step can be executed and nodes communicate again. This approach is known as <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.128.7679&amp;rep=rep1&amp;type=pdf">Bulk Synchronous Processing (BSP)</a>. The BSP model is vertex based and generally works with a configurable graph model <em style="font-weight: bold;">G</em><<em>I</em>,<em>V</em>,<em>E</em>,<em>M</em>>, where <strong><em>I</em></strong> is a vertex ID, <strong><em>V</em></strong> a vertex value, <strong><em>E</em></strong> an edge value, and <strong><em>M</em></strong> a message data type, which all implement the Writable interface (which is well known from the Hadoop API).</p>
<p>However, the iterative character of many graph algorithms is a poor fit for the MapReduce paradigm, especially if the graph is a very large one like the full set of interlinked Wikipedia pages. In MapReduce, a data set is streamed into the mappers and aggregation of intermediate results is done in reducers. One MapReduce job can implement one super-step. In a next super-step, the whole graph structure &#8212; together with the stored intermediate state of the previous step &#8212; have to be loaded from HDFS and stored at the end again. Between processing steps, the full graph is loaded from HDFS and stored there, which is a really large overhead. And let<span style="font-style: italic;">&#8216;</span>s not forget that the intermediate data requires local storage on each worker node while it passes the shuffle-sort phase.</p>
<p>For very large graphs, it is inefficient to repeatedly store and load the more or less fixed structural data. In contrast, the BSP approach loads the graph only once, at the outset. The algorithms assume that runtime-only messages are passed between the worker nodes. To support fault-tolerant operation the intermediate state of the graph can be stored from time to time, but usually not after each super-step.</p>
<p>MapReduce-based implementations of graph algorithms are demonstrated in the book <a href="http://lintool.github.io/MapReduceAlgorithms/"><em>Data-Intensive Text Processing with MapReduce</em></a> (J. Lin, C. Dyer).</p>
<h2>Hands-On</h2>
<p>The starting point for the hands-on portion of this how-to is Clouder&#8217;s QuickStart VM, which contains a virtualized pseudo-distributed Hadoop cluster managed by Cloudera Manager. The VM has many useful modules already pre-installed, including Git client, Maven, and the Eclipse IDE. In this environment, you can deploy and run Giraph benchmarks as functional tests (rather than for performance reasons).</p>
<p><strong>Prepare and Test the Virtual Hadoop cluster</strong></p>
<p>The Cloudera QuickStart VM requires a 64-bit environment and a minimum of 4GB of RAM allocated to the virtual machine.</p>
<p>Our deployment here requires the following software/hardware setup:</p>
<ul>
<li>Hardware: (either a real PC or the Cloudera QuickStart VM, download it <a href="http://www.cloudera.com/content/support/en/downloads/download-components/download-products.html?productID=F6mO278Rvo&amp;version=1">here</a>)</li>
<ul>
<li>Dual-core 2 GHz CPU (64-bit architecture)</li>
<li>4GB RAM (6GB would be much better)</li>
<li>25GB hard drive space</li>
<li>100 Mbps NIC</li>
</ul>
<li>CentOS 6.4 (64-bit)</li>
<li>Admin account (in Quickstart-VM)</li>
<ul>
<li>username: cloudera</li>
<li>password: cloudera</li>
</ul>
<li>Hostname: localhost</li>
<ul>
<li>IP address: 127.0.0.1</li>
<li>Network mask: 255.255.255.0</li>
</ul>
<li>Oracle JDK (version 1.6.0_32)</li>
<li>Apache Hadoop (CDH 4.x)</li>
<li>Apache Giraph 1.0.0</li>
</ul>
<p>An accompanying Github project for this tutorial, called <a href="https://github.com/kamir/giraphl">giraphl</a>, contains related material such as a convenient bootstrap script. This script will save you time as you do not have to type all commands provided in this tutorial. Those who want a more manual experience, however, can follow the steps below.</p>
<p><strong>Test the Hadoop Deployment</strong></p>
<p>First, start the VM and confirm that all required services are up and running. Either you use command <code>sudo jps</code>, which should list at least the following services:</p>
<ul>
<li>NameNode</li>
<li>SecondaryNameNode</li>
<li>DataNode</li>
<li>JobTracker</li>
<li>TaskTracker</li>
</ul>
<p>Or, open Cloudera Manager in a browser at <a href="http://localhost:7180/">http://localhost:7180/</a>.</p>
<p align="center"><a href="http://blog.cloudera.com/wp-content/uploads/2014/01/giraph1.png"><img title="giraph1" src="http://blog.cloudera.com/wp-content/uploads/2014/01/giraph1.png" alt="" width="600" height="305" /></a></p>
<p align="center"><strong>Figure 1: </strong>Cloudera Manager shows the service status of a healthy Hadoop cluster in a QuickStart VM. You can deploy the Hadoop client configuration to the local host, which is used during the tutorial.</p>
<p>The HDFS and MapReduce services are required at a minimum for this how-to. If all services are not in a healthy state, start troubleshooting now.</p>
<p>A prerequisite for running MapReduce jobs in the QuickStart VM (because it uses Cloudera Manager) is to deploy the client configuration. If you skip this step, the <code>hadoop</code> command will use the local job runner by default. So, you need to do this sequence of steps:</p>
<ol>
<li>Go to Cloudera Manager at <a href="http://localhost:7180/">http://localhost:7180/</a>.</li>
<li>Login using &#8220;cloudera&#8221; for both the username and password.</li>
<li>Click the &#8220;Actions&#8221; menu directly below &#8220;Add Cloudera Management Services&#8221; and select &#8220;Deploy Client Configuration&#8221;. (Note: There are several similar dropdowns, so refer to the screenshot in Figure 1 for the correct one.)</li>
<li>Click the blue &#8220;Deploy Client Configuration&#8221;? button to confirm your selection. In less than a minute, you&#8217;ll see confirmation that the configuration set up in Cloudera Manager is available to the clients of the different services (such as MapReduce) on the cluster.</li>
</ol>
<p><strong>Running a MapReduce Job</strong></p>
<p>Some MapReduce jobs can now be executed. For a first test, use the TeraGen and TeraSort jobs to create some random test data and to sort the data set via MapReduce.</p>
<p>As user &#8220;cloudera&#8221;?, call the following commands:</p>
<p>Run TeraGen:</p>
<pre class="prettyprint ">$ hadoop jar /usr/lib/hadoop-0.20-mapreduce/hadoop-examples.jar
  teragen 50000 TESTFILE
  </pre>
<p>&nbsp;</p>
<p>Run TeraSort:</p>
<pre class="prettyprint ">$ hadoop jar /usr/lib/hadoop-0.20-mapreduce/hadoop-examples.jar
  terasort TESTFILE TESTFILE.sorted
  </pre>
<p>&nbsp;</p>
<p>While the jobs are running you can monitor them via the Web user interface:</p>
<ul>
<li>NameNode daemon: http://127.0.0.1:50070</li>
<li>JobTracker daemon: http://127.0.0.1:50030</li>
</ul>
<p>Your Hadoop cluster is now ready for action. But before writing a Giraph job, let&#8217;s review some Giraph concepts.</p>
<h2>Anatomy of a Giraph Program</h2>
<p>A Giraph program consists of a master, many workers, and an Apache <a href="http://zookeeper.apache.org/">ZooKeeper</a> ensemble that maintains the global state of the application or the Giraph job. The active master works as an application coordinator &#8212; it synchronizes the super-steps. If the master fails, another master from the ZooKeeper master queue can be activated. (Thus, fault tolerance is built-in.) Workers get a partition of the graph assigned from the master before the super-step starts. Each worker handles the I/O operations, the computational part of the algorithm, and the messaging between the nodes.</p>
<p>Figure 2 illustrates the three phases of the data flow in a Giraph application. It starts with loading the graph (Phase 1): The master assigns the InputSplits to workers, which are responsible for loading the data. This process is similar to classical MapReduce processing, where each mapper task is responsible for processing one InputSplit. Phase 2 is the iterative part of the algorithm, which consists of concatenated super-steps. Finally, each worker contributes the data from all partitions for which it was responsible to the OutputFormat to write results to HDFS, HBase, or even Accumulo.</p>
<p align="center"><img title="giraph-f2" src="http://blog.cloudera.com/wp-content/uploads/2014/01/giraph-f2.png" alt="" width="600" height="400" /></p>
<p align="center"><strong>Figure 2:</strong> The Giraph data flow in a classical MapReduce implementation</p>
<p>In classical Hadoop 1.x, a map-only job initializes the BSP based implementation of large-scale graph algorithms. After workers load the initial graph data (Phase 1 in Figure 2), the super-steps are executed (Phase 2 in Figure 2). Within each super-step, the method compute( &#8230; )<strong> </strong>of all given vertexes is called. A vertex has the data, which describes the state of the node, and its compute method is part of the implementation of a single algorithm. So you need special vertex implementations for different algorithms.</p>
<p>Some algorithms require an iterative update of a certain node property or vertex value (for example, the page rank of a node). Therefore, you have to store such data outside the compute method as a vertex attribute. All attributes within a vertex are visible within this vertex only and should be in the private part of the vertex class. Don&#8217;t forget to initialize the variable before the program starts!</p>
<p>One simple example is the single-source shortest path (SSSP) algorithm. Here, you store the current lowest distance to a selected source vertex as a node property in each vertex. After a new message is received, the values can be updated with the new lowest value at the end of each iteration.</p>
<p>Sometimes you need access to node properties from other nodes. To make such values global accessible, use a persistent aggregator. The aggregator has to be initialized before the program starts, and in each super-step, every vertex can send its current value there. Such an aggregator uses a hashmap to store a value for each vertex id. In the case of a persistent aggregator, the data is persistent across all super-steps and the data will be accessible by all vertexes. Developers should be careful to not collect too many large data objects in an aggregator.</p>
<h2>Deploying Giraph</h2>
<p>In order to download Giraph from a repository and <a href="http://giraph.apache.org/build.html">build</a> it, you need Git and Maven 3 installed. Both packages are preinstalled and configured in QuickStart VM. Giraph can support multiple versions of Hadoop, and here you&#8217;ll use the hadoop-2.0.0 profile.</p>
<p>First, choose a directory for Giraph. (Most people use /usr/local/giraph, but this is not a strong requirement.) You should define this location as a system variable called GIRAPH_HOME and export it via the .bashrc file in the home directory of the user cloudera. Therefore, you have to edit /home/cloudera/.bashrc. Add the following line:</p>
<p><code>export GIRAPH_HOME=/usr/local/giraph</code></p>
<p>(Note: At the time of this writing, it was not possible to compile giraph-1.1.0. Therefore, the latest stable release, version 1.0.0, is used here.)</p>
<p>Let&#8217;s define a version and a profile identifier variable by adding:</p>
<pre>export GV=1.0.0
export PRO=hadoop_2.0.0
</pre>
<p>&nbsp;</p>
<p>to the file /home/cloudera/.bashrc and refresh the environment variables with:</p>
<pre class="prettyprint lang-LINUX">
$ source ~/.bashrc
</pre>
<p>&nbsp;</p>
<p>Clone the Giraph project from a Github mirror:</p>
<pre class="prettyprint lang-LINUX">$ sudo mkdir /usr/local/giraph
$ cd $GIRAPH_HOME
$ cd ..
$ sudo git clone https://github.com/apache/giraph.git
$ sudo chown -R cloudera:hadoop giraph
</pre>
<p>&nbsp;</p>
<p>Check out the stable branch, and build Giraph by running the following commands. (Note: there is no space character between -D and skipTests):</p>
<pre class="prettyprint ">$ cd $GIRAPH_HOME
$ git checkout release-$GV
$ mvn package -DskipTests -Dhadoop=non_secure -P $PRO
</pre>
<p>&nbsp;</p>
<p>The argument <code>-<strong>D</strong>skipTests</code> will skip the testing phase to save some time. As you are not working with a secure cluster setup, you have to select the non-secure mode by using the option <code>-<strong>D</strong>hadoop=non_secure</code>.</p>
<p>The build procedure may take a while. You may have to execute the build command multiple times. But finally you should see output similar to the following:</p>
<pre class="prettyprint ">[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Apache Giraph Parent .................... 	SUCCESS [0.720s]
[INFO] Apache Giraph Core ....................... 	SUCCESS [21.551s]
[INFO] Apache Giraph Hive I/O ................. 	SUCCESS [10.896s]
[INFO] Apache Giraph Examples ............... 	SUCCESS [7.088s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 40.526s
[INFO] Finished at: Thu Dec 05 08:36:52 PST 2013
[INFO] Final Memory: 43M/569M
[INFO] ------------------------------------------------------------------------</pre>
<p>&nbsp;</p>
<p>At the end, you should have the distributable Giraph JAR file:</p>
<pre class="prettyprint ">giraph-core/target/giraph-$GV-for-$PRO-alpha-jar-with-dependencies.jar
</pre>
<p>&nbsp;</p>
<p>and the Giraph examples JAR file:</p>
<pre class="prettyprint ">giraph-examples/target/giraph-examples-$GV-for-$PRO-alpha-jar-with-dependencies.jar</pre>
<p>&nbsp;</p>
<p>For convenience, I prefer to create symbolic links to the jar files. Usually such links are managed by the Linux <a href="http://linux.about.com/library/cmd/blcmdl8_alternatives.htm"><code>alternatives</code></a> command.</p>
<pre class="prettyprint ">$ cd $GIRAPH_HOME
$ sudo ln -s
giraph-core/target/giraph-$GV-for-$PRO-alpha-jar-with-dependencies.jar
giraph-core.jar
$ sudo ln -s
giraph-examples/target/giraph-examples-$GV-for-$PRO-alpha-jar-with-dependencies.jar
giraph-ex.jar
</pre>
<p>&nbsp;</p>
<p><strong>Preparing Sample Data</strong></p>
<p>The next step is to prepare some example data sets in HDFS. The official Giraph quick-start tutorial uses the <code>tiny_graph.txt</code> file, which contains a small test network. (Later, this network can be loaded via <code>JsonLongDoubleFloatDoubleVertexInputFormat</code> input format.)</p>
<p align="center"><img title="giraph3" src="http://blog.cloudera.com/wp-content/uploads/2014/01/giraph3.png" alt="" width="300" height="274" /></p>
<p align="center"><strong>Figure 3:</strong> The tiny sample graph plotted in Gephi.</p>
<p>Copy the text below:</p>
<p>[0,0,[[1,1],[3,3]]]<br /> [1,0,[[0,1],[2,2],[3,1]]]<br /> [2,0,[[1,2],[4,4]]]<br /> [3,0,[[0,3],[1,1],[4,4]]]<br /> [4,0,[[3,4],[2,4]]]</p>
<p>into a new empty file named <code>tiny_graph.txt</code> in your QuickStart VM and then copy this file to HDFS into a directory named ginput, which is in your HDFS home directory.</p>
<p>Next, create some working directories in HDFS where you will store the input data sets and results:</p>
<pre class="prettyprint ">$ hadoop fs -mkdir ginput
$ hadoop fs -mkdir goutput
</pre>
<p>&nbsp;</p>
<p>To copy the manually created input file to HDFS, you should use the Hadoop filesystem shell. Please make the current directory in your shell the one in which you stored the <code>tiny_graph.txt</code> file and then type:</p>
<pre class="prettyprint ">$ hadoop fs -put tiny_graph.txt ginput</pre>
<p>&nbsp;</p>
<p>The meaning of the data in each line is:</p>
<pre class="prettyprint ">[source_id,source_value,[[dest_id, edge_value],...]]</pre>
<p>&nbsp;</p>
<p>The <code>source_value</code> property describes the node and the <code>edge_value</code> is an edge property (such as the correlation link strength). There are five nodes and 12 directed edges in this graph.</p>
<p>Another example data set is available <a href="http://ece.northwestern.edu/%7Eaching/shortestPathsInputGraph.tar.gz">here</a>. Download and decompress the file, then upload it to HDFS. But first you have to install wget via yum package manager.</p>
<pre class="prettyprint ">$ sudo yum install wget
$ wget http://ece.northwestern.edu/~aching/shortestPathsInputGraph.tar.gz
$ tar zxvf shortestPathsInputGraph.tar.gz
$ hadoop fs -put shortestPathsInputGraph ginput
</pre>
<p>&nbsp;</p>
<p>With those files in place, you can now run some Giraph jobs.</p>
<h2>Running Giraph Jobs</h2>
<p>The Giraph library offers MapReduce programs, called GiraphJobs, but no additional services for the Hadoop cluster. Therefore, you do not have to deploy any configuration or services on your running Hadoop cluster; rather, just submit the existing map-only jobs. As soon as you do that via the hadoop command-line tool, you have to specify all required libraries. In this case, that will be the giraph-core.jar and the giraph-ex.jar files. The first one contains all Giraph-related code and has to be deployed via the <code>-libjars</code> option. This means that the jar files specified here are added to the distributed cache. During runtime, those libraries are available in the Java classpath on all cluster nodes running a task, which belongs to the submitted job.</p>
<p>Giraph jobs are MapReduce jobs. Although mappers usually do not communicate with other mappers, Giraph uses MapReduce only during the initialization phase and mapper-to-mapper communication is actually required.</p>
<p>A ZooKeeper quorum provides a global shared memory. You tell Giraph about the configuration of the ZooKeeper servers with the following property:</p>
<pre class="prettyprint ">-Dgiraph.zkList=&lt;zknode1&gt;:&lt;zkport1&gt;,&lt;zknode2&gt;:&lt;zkport2&gt; ...
</pre>
<p>&nbsp;</p>
<p>The Cloudera QuickStart VM has a running HBase cluster with ZooKeeper running on port 2181. So, use that instead of Giraph&#8217;s default ZooKeeper port, which is assumed to be port 22181.</p>
<p>The rest of this section will show you how to run Giraph jobs and what parameters are required. More details about using other existing algorithms and designing your own implementations will be explained in a subsequent post.</p>
<p><strong>PageRankBenchmark</strong></p>
<p>There are some benchmarks implemented in the package org.apache.giraph.benchmark. Run the PageRankBenchmark just to verify the setup and the build.</p>
<pre class="prettyprint ">$ hadoop jar giraph-ex.jar org.apache.giraph.benchmark.PageRankBenchmark
-Dgiraph.zkList=127.0.0.1:2181 -libjars giraph-core.jar
-e 1 -s 3 -v -V 50 -w 1
</pre>
<p>&nbsp;</p>
<table width="437" border="1" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><strong>Parameter Name</strong></p>
</td>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><code>-e<br /> --edgesPerVertex</code></p>
</td>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p>Number of edges per vertex that are generated</p>
</td>
</tr>
<tr>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><code>-s</code></p>
</td>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p>Number of super-steps to execute before finishing the job</p>
</td>
</tr>
<tr>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><code>-v</code></p>
</td>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p>Run the job in verbose mode.</p>
</td>
</tr>
<tr>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><code>-V<br /> --aggregateVertices</code></p>
</td>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p>Aggregate the vertices.</p>
</td>
</tr>
<tr>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><code>-w<br /> --workers</code></p>
</td>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p>Number of workers (the number of mappers that have to be started in parallel)</p>
</td>
</tr>
</tbody>
</table>
<p></p>
<p><strong>SimpleShortestPathsVertex</strong></p>
<p>The SimpleShortestPathsVertex job will be started to explain the specific command-line properties, which are slightly different from a classical Hadoop job.</p>
<p>It reads a graph from an input file stored in HDFS to compute the length of the shortest paths from one chosen source node to all other nodes. The current implementation of the algorithm will only process the first vertex in the input file. If you need the shortest path for all available vertexes, multiple jobs would have to be started. In each run you would specify the vertex id with a custom argument:</p>
<pre class="prettyprint ">-ca SimpleShortestPathsVertex.source=2</pre>
<p>&nbsp;</p>
<p>Here we will use the JsonLongDoubleFloatDoubleVertexInputFormat and IdWithValue- TextOutputFormat output file formats. The result file is a simple text file where each line consists of <strong>target_id</strong> <strong>length</strong> for each vertex in the graph. Length is the shortest path to a target node, starting from the single source node, defined by the custom property.</p>
<pre class="prettyprint ">$ hadoop jar giraph-ex.jar org.apache.giraph.GiraphRunner
-Dgiraph.zkList=127.0.0.1:2181 -libjars giraph-core.jar
org.apache.giraph.examples.SimpleShortestPathsVertex
-vif org.apache.giraph.io.formats.JsonLongDoubleFloatDoubleVertexInputFormat
-vip /user/training/ginput/tiny_graph.txt
-of org.apache.giraph.io.formats.IdWithValueTextOutputFormat
-op /user/training/goutput/shortestpathsC2 -ca SimpleShortestPathsVertex.source=2   -w 1
  </pre>
<p>&nbsp;</p>
<table style="width: 433px;" border="1" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><strong>Parameter Name</strong></p>
</td>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><code>-vif</code></p>
</td>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p>The VertexInputFormat for the job</p>
</td>
</tr>
<tr>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><code>-vip</code></p>
</td>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p>The path in HDFS from which the VertexInputFormat loads the data</p>
</td>
</tr>
<tr>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><code>-of</code></p>
</td>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p>The OutputFormat for the job</p>
</td>
</tr>
<tr>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><code>-op</code></p>
</td>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p>The path in HDFS to which the OutputFormat writes the data</p>
</td>
</tr>
<tr>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p><code>-ca</code></p>
</td>
<td style="font-family: Arial, Helvetica, sans-serif; font-size: 12px;">
<p>A custom argument is defined as a key value pair</p>
</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p align="center"><a href="http://blog.cloudera.com/wp-content/uploads/2014/01/giraph4.png"><img title="giraph4" src="http://blog.cloudera.com/wp-content/uploads/2014/01/giraph4.png" alt="" width="600" height="394" /></a></p>
<p align="center"><strong>Figure 4:</strong> The file browser app in Hue shows the content of the result file, which was generated by our first Giraph job.</p>
<h2>Conclusion</h2>
<p>Congratulations, you now have a running Hadoop cluster on your desktop and a fresh installation of the latest stable Giraph release, which is version 1.0.0. You should also now know how a Giraph job works and how to run existing algorithms.</p>
<p>In future posts, you will get a deep dive into Giraph development and also discover how to export data to Gephi via Hive, Impala (via JDBC) to visualize large graphs.</p>
<p>Any feedback is welcome in comments!</p>
<p><em>Mirko Kämpf is the lead instructor for the Cloudera Administrator Training for Apache Hadoop for Cloudera University.</em></p>
<p>&nbsp;</p>

				<div class="social-buttons">
<span class='st_facebook_large' displayText='Facebook'></span>
<span class='st_twitter_large' displayText='Tweet'></span>
<span class='st_linkedin_large' displayText='LinkedIn'></span>
<span class='st_googleplus_large' displayText='Google +'></span>
<span class='st_email_large' displayText='Email'></span>
				</div>
			</div>

			<div class="grid_2" style="margin:0">
  <div class="comments comments-2">
    <div class="field-under">
      <h4>Filed under:</h4>
      <ul class="post-categories">
	<li><a href="http://blog.cloudera.com/blog/category/general/" title="View all posts in General" rel="category tag">General</a></li>
	<li><a href="http://blog.cloudera.com/blog/category/hadoop/" title="View all posts in Hadoop" rel="category tag">Hadoop</a></li></ul>  	</div>
  	
  <a name="comments"></a>
  <div class="comments-head">
    <strong>8 Responses</strong>
   
  </div>
  <ul class="comments-list">
  	<li>
		<em class="comment-date">
			<a rel="nofollow" href="http://www.thecloudavenue.com/">Praveen Sripati</a> /
			February 05, 2014 / 2:00 PM		</em>
		<p>Lately Cloudera has been pushing Spark, which has Graphx. When to use Giraph and when Graphx?</p>
<p>Praveen</p>
	</li>
<ul class='children'>
	<li>
		<em class="comment-date">
			<a rel="nofollow" href="">Justin Kestelyn (@kestelyn)</a> /
			February 06, 2014 / 2:45 PM		</em>
		<p>Praveen,</p>
<p>Cloudera is agnostic on this issue; neither Graphx nor Giraph are in CDH today. You should probably ask contributors to those projects directly.</p>
	</li>
</li>
</ul>
</li>
	<li>
		<em class="comment-date">
			<a rel="nofollow" href="">Danny</a> /
			February 14, 2014 / 6:15 AM		</em>
		<p>It seems that Giraph 1.1.0 can only be compiled with Java 7:</p>
<p><a href="https://github.com/apache/giraph/commit/ac93c3b6c5bce5f22b293b29df91663ca7d7ce63" rel="nofollow">https://github.com/apache/giraph/commit/ac93c3b6c5bce5f22b293b29df91663ca7d7ce63</a></p>
<p>Is there a possibility to install the CDH4 cluster through the CDM with Java7?</p>
	</li>
</li>
	<li>
		<em class="comment-date">
			<a rel="nofollow" href="">Ghufran</a> /
			February 19, 2014 / 12:07 PM		</em>
		<p>Hello, </p>
<p>Thank you for this fantastic article. I have set-up Giraph via your quick start vm, but have had trouble running the SimpleShortestPathsVertex example. I received warnings saying output format is not recognised then the giraph job runs and fails producing no output in the hadoop fs.  I get the same error in my own personal set-up of Giraph as well and was wondering if I was missing something or if this was a current problem with the Giraph itself? </p>
<p>below is the output recieved from the quick start vm terminal:</p>
<p>14/02/18 09:44:25 INFO utils.ConfigurationUtils: No edge input format specified. Ensure your InputFormat does not require one.<br />
14/02/18 09:44:25 INFO utils.ConfigurationUtils: Setting custom argument [SimpleShortestPathsVertex.source] to [2] in GiraphConfiguration<br />
14/02/18 09:44:25 WARN job.GiraphConfigurationValidator: Output format vertex index type is not known<br />
14/02/18 09:44:25 WARN job.GiraphConfigurationValidator: Output format vertex value type is not known<br />
14/02/18 09:44:25 WARN job.GiraphConfigurationValidator: Output format edge value type is not known<br />
14/02/18 09:44:25 INFO job.GiraphJob: run: Since checkpointing is disabled (default), do not allow any task retries (setting mapred.map.max.attempts = 0, old value = 4)<br />
14/02/18 09:44:26 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.<br />
14/02/18 09:44:28 INFO mapred.JobClient: Running job: job_201402180900_0002<br />
14/02/18 09:44:29 INFO mapred.JobClient:  map 0% reduce 0%<br />
14/02/18 09:44:47 INFO mapred.JobClient: Job complete: job_201402180900_0002<br />
14/02/18 09:44:47 INFO mapred.JobClient: Counters: 4<br />
14/02/18 09:44:47 INFO mapred.JobClient:   Job Counters<br />
14/02/18 09:44:47 INFO mapred.JobClient:     Total time spent by all maps in occupied slots (ms)=16399<br />
14/02/18 09:44:47 INFO mapred.JobClient:     Total time spent by all reduces in occupied slots (ms)=0<br />
14/02/18 09:44:47 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0<br />
14/02/18 09:44:47 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0 </p>
<p>Thanks,  </p>
<p>Ghufran</p>
	</li>
</li>
	<li>
		<em class="comment-date">
			<a rel="nofollow" href="">Ghufran</a> /
			February 19, 2014 / 4:13 PM		</em>
		<p>I seemed to of solved my problem, by using the following class as the -of: </p>
<p>IdWithValueTextOutputFormat$IdWithValueVertexWriter</p>
<p>Ghufran</p>
	</li>
</li>
	<li>
		<em class="comment-date">
			<a rel="nofollow" href="http://www.sas.com">Stefan Beskow</a> /
			February 24, 2014 / 8:56 AM		</em>
		<p>Hi.</p>
<p>Thanks for the great article. It was very helpful to get Giraph up and running on a Cloudera VM. Now Im also trying to get Giraph up and running on Hadoop 2.0.0-cdh4.2.0 using a cluster with 60 nodes. When I run the sample application org.apache.giraph.examples.SimpleShortestPathsVertex with just 1 worker it works fine, but when I specify more than 1 worker it throws exception java.lang.IllegalArgumentException: checkLocalJobRunnerConfiguration as shown below. Is there a way to pass a command line parameter to Giraph so that it doesnt use the local job runner or do I need to update any of the Hadoop configuration files for this to work?</p>
<p>Here is the command I use to run sample application with 2 workers:<br />
hadoop jar giraph-examples-1.0.0-for-hadoop-2.0.0-cdh4.2.0-jar-with-dependencies.jar org.apache.giraph.GiraphRunner -Dgiraph.zkList=rdcgrd001.unx.sas.com:2181 -libjars giraph-examples-1.0.0-for-hadoop-2.0.0-cdh4.2.0-jar-with-dependencies.jar org.apache.giraph.examples.SimpleShortestPathsVertex -vif org.apache.giraph.io.formats.JsonLongDoubleFloatDoubleVertexInputFormat -vip /user/stbesk/input/tiny_graph.txt -of org.apache.giraph.io.formats.IdWithValueTextOutputFormat -op /user/stbesk/output/shortestpathsC2 -ca SimpleShortestPathsVertex.source=2 -w 2 -ca giraph.SplitMasterWorker=true</p>
<p>Here is the exception:<br />
14/02/24 00:20:23 INFO utils.ConfigurationUtils: No edge input format specified. Ensure your InputFormat does not require one.<br />
14/02/24 00:20:23 INFO utils.ConfigurationUtils: Setting custom argument [SimpleShortestPathsVertex.source] to [2] in GiraphConfiguration<br />
14/02/24 00:20:23 INFO utils.ConfigurationUtils: Setting custom argument [giraph.SplitMasterWorker] to [true] in GiraphConfiguration<br />
14/02/24 00:20:23 WARN job.GiraphConfigurationValidator: Output format vertex index type is not known<br />
14/02/24 00:20:23 WARN job.GiraphConfigurationValidator: Output format vertex value type is not known<br />
14/02/24 00:20:23 WARN job.GiraphConfigurationValidator: Output format edge value type is not known<br />
14/02/24 00:20:23 INFO job.GiraphJob: run: Since checkpointing is disabled (default), do not allow any task retries (setting mapred.map.max.attempts = 0, old value = 4)<br />
Exception in thread &#8220;main&#8221; java.lang.IllegalArgumentException: checkLocalJobRunnerConfiguration: When using LocalJobRunner, must have only one worker since only 1 task at a time!<br />
        at org.apache.giraph.job.GiraphJob.checkLocalJobRunnerConfiguration(GiraphJob.java:151)<br />
        at org.apache.giraph.job.GiraphJob.run(GiraphJob.java:225)<br />
        at org.apache.giraph.GiraphRunner.run(GiraphRunner.java:94)<br />
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)<br />
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)<br />
        at org.apache.giraph.GiraphRunner.main(GiraphRunner.java:124)<br />
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)<br />
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)<br />
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)<br />
        at java.lang.reflect.Method.invoke(Method.java:597)<br />
        at org.apache.hadoop.util.RunJar.main(RunJar.java:208)</p>
<p>Appreciate any help.</p>
<p>Thanks.<br />
Stefan</p>
	</li>
</li>
	<li>
		<em class="comment-date">
			<a rel="nofollow" href="http://www.cloudera.com">Mirko</a> /
			March 04, 2014 / 12:16 PM		</em>
		<p>Did you deploy the client configuration the right way? If a client has not the right cluster config it might use the local mode, so please check this setup.</p>
	</li>
</li>
	<li>
		<em class="comment-date">
			<a rel="nofollow" href="">Alexander Riggers</a> /
			April 09, 2014 / 6:23 AM		</em>
		<p>Hello Mirko</p>
<p>I was wondering if you know where I can find an overview of the supported algorithms for Apache Giraph? The API documentation is really messy and I can not figure out which algorithms they support.</p>
<p>Kind regards,<br />
Alex</p>
	</li>
</li>
  </ul>
  <a name="leave-comment"></a>
  <form action="/wp-comments-post.php" method="POST">
  	<div class="comment-form">
  		<h4>Leave a comment</h4>
  		<div class="row">
  			<input type="text" value="" class="txt" name="author"/>
  			<label>Name <span>required</span></label>
  		</div>
  		<div class="row">
  			<input type="text" value="" class="txt" name="email"/>
  			<label class="published">Email <span>required</span> <em>(will not be published)</em></label>
  		</div>
  		<div class="row">
  			<input type="text" value="" class="txt" name="url"/>
  			<label>Website</label>
  		</div>
  		<div class="row">
  			<textarea rows="10" cols="30" class="area" name="comment"></textarea>
  			<label>Comment</label>
  		</div>
  		<fieldset>
  			<input type="button" value="Leave Comment" class="btn cta"/>
  		</fieldset>
  	</div>
  	<input type='hidden' name='comment_post_ID' value='25182' id='comment_post_ID' />
<input type='hidden' name='comment_parent' id='comment_parent' value='0' />
  	<p class="cptch_block"><label>Prove you're human!<span class="required"> *</span></label><br />		<input type="hidden" name="cptch_result" value="JqP4" />
		<input type="hidden" name="cptch_time" value="1397909768" />
		<input type="hidden" value="Version: 2.4" />
		s&#105;x &times; 9 =  <input id="cptch_input" type="text" autocomplete="off" name="cptch_number" value="" maxlength="2" size="2" aria-required="true" required="required" style="margin-bottom:0;display:inline;font-size: 12px;width: 40px;" />	</p>  </form>
</div></section>




<!-- Google Code for New Remarketing Pixel -->
<!-- Remarketing tags may not be associated with personally identifiable information or placed on pages related to sensitive categories. For instructions on adding this tag and more information on the above requirements, read the setup guide: google.com/ads/remarketingsetup -->
<script type="text/javascript">
/* <![CDATA[ */
var google_conversion_id = 1035979479;
var google_conversion_label = "xel9CJ-P0QMQ15X_7QM";
var google_custom_params = window.google_tag_params;
var google_remarketing_only = true;
/* ]]> */
</script>
<script type="text/javascript" src="//www.googleadservices.com/pagead/conversion.js">
</script>

<noscript>
<div style="display:inline;"> <img height="1" width="1" style="border-style:none;" alt="" src="//googleads.g.doubleclick.net/pagead/viewthroughconversion/1035979479/?value=0&label=xel9CJ-P0QMQ15X_7QM&guid=ON&script=0"/> </div>
</noscript>
</section>
<span class="bg-fix"></span>
</div>
</div>
<footer id="global-footer">
<div class="footerContent parbase">
<footer>
  <div class="wrapper">
    <div class="bg-fix"></div>
    <nav>
      <ul>
        <li class="section"><a href="http://www.cloudera.com/content/cloudera/en/products-and-services.html">Products</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/products/cloudera-enterprise.html">Cloudera Enterprise</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cloudera-express.html">Cloudera Express</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cloudera-enterprise/cloudera-manager.html">Cloudera Manager</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html">CDH</a></li>
        <li><a href="http://www.cloudera.com/content/support/en/downloads.html">All Downloads</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/products-and-services/professional-services.html">Professional Services</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/training.html">Training</a></li>
      </ul>
      <ul>
        <li class="section"><a href="http://www.cloudera.com/content/cloudera/en/solutions.html">Solutions</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/solutions/enterprise-solutions.html">Enterprise Solutions</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/solutions/partner.html">Partner Solutions</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/solutions/industries.html">Industry Solutions</a></li>
      </ul>
      <ul>
        <li class="section"><a href="http://www.cloudera.com/content/cloudera/en/partners.html">Partners</a></li>
        <li class="section"><a href="http://www.cloudera.com/content/cloudera/en/resources/library.html">Resource Library</a></li>
        <li class="section"><a href="https://ccp.cloudera.com/display/SUPPORT/Get+Support">Support</a></li>
      </ul>
      <ul>
        <li class="section"><a href="http://www.cloudera.com/content/cloudera/en/about.html">About</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/about/hadoop-and-big-data.html">Hadoop &amp; Big Data</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/about/management.html">Management Team</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/about/board.html">Board</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/about/events.html">Events</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/about/press-center.html">Press Center</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/about/careers.html">Careers</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/about/contact-form.html">Contact Us</a></li>
        <li><a href="http://www.cloudera.com/content/cloudera/en/subscription-center.html">Subscription Center</a></li>
      </ul>
      <div class="locale-and-social" style="float:right">
        <div>
          <div class="locale-and-social">
            <div class="locale">
              <select onchange="this.options[this.selectedIndex].value &amp;&amp; (window.location = this.options[this.selectedIndex].value);" class="site-language">
                <option value="http://www.cloudera.com" name="English">English</option>
                <option value="http://www.cloudera.co.jp/">Japanese</option>
              </select>
            </div>
            <div class="social"><span class="follow">Follow us:</span><span class="share">Share:<i class="icon-share"></i></span>
              <ul>
                <li><a class="linkedIn" target="_blank" href="http://www.linkedin.com/company/cloudera">LinkedIn</a></li>
                <li><a class="twitter" target="_blank" href="http://twitter.com/cloudera">Twitter</a></li>
                <li><a class="facebook" target="_blank" href="http://www.facebook.com/cloudera">Facebook</a></li>
                <li><a class="youtube" target="_blank" href="http://www.youtube.com/user/clouderahadoop">YouTube</a></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </nav>
    <nav class="global-footer"><span class="logo"><a>Cloudera</a></span>
      <address>
      <span>Cloudera, Inc.</span> <span><a target="_blank" href="http://www.google.com/maps?q=1001+Page+Mill+Rd,+Palo+Alto,+CA+94306">1001 Page Mill Road Bldg 2</a></span> <span>Palo Alto, CA 94304</span>
      </address>
      <address>
      <span><a href="http://www.cloudera.com">www.cloudera.com</a></span> <span>US: 1-888-789-1488</span> <span>Intl: 1-650-362-0488</span>
      </address>
      <div class="copyright"><span><span class="piped">©2014 Cloudera, Inc. All rights reserved</span><span class="piped"><a href="http://www.cloudera.com/content/cloudera/en/terms-of-service.html">Terms &amp; Conditions</a></span><a href="http://www.cloudera.com/content/cloudera/en/privacy-policy.html">Privacy Policy</a></span> <span>Hadoop and the Hadoop elephant logo are trademarks of the <a target="_blank" href="http://www.apache.org/">Apache Software Foundation</a>.</span></div>
    </nav>
  </div>
</footer>
<script type='text/javascript' src='http://blog.cloudera.com/wp-content/plugins/prettify-gc-syntax-highlighter/prettify.js?ver=3.3.2'></script>
<script type='text/javascript' src='http://blog.cloudera.com/wp-content/plugins/prettify-gc-syntax-highlighter/launch.js?ver=3.3.2'></script>
<div class="modal" style="display:none">
  <div id="password-required">
    <div class="inner"> </div>
  </div>
</div>
<div class="tooltip" class="tooltip" style="display:none">
</div>
<script type="text/javascript" src="http://dnn506yrbagrg.cloudfront.net/pages/scripts/0011/2160.js"></script>
<script type="text/javascript">var _kiq = _kiq || [];</script> 
<script type="text/javascript" src="http://s3.amazonaws.com/ki.js/14646/2Sr.js" async></script>
</body></html>
